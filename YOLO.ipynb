{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chromedriver version (117.0.5938.88) detected in PATH at /usr/local/bin/chromedriver might not be compatible with the detected chrome version (116.0.5845.187); currently, chromedriver 116.0.5845.96 is recommended for chrome 116.*, so it is advised to delete the driver in PATH and retry\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/alexsciuto/Library/Mobile Documents/com~apple~CloudDocs/DataWithAlex/MSDA Classes/Network_Science/YOLO_Paper_Network_Analysis/venv/bin/etudier\", line 10, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/Users/alexsciuto/Library/Mobile Documents/com~apple~CloudDocs/DataWithAlex/MSDA Classes/Network_Science/YOLO_Paper_Network_Analysis/venv/lib/python3.9/site-packages/etudier/__init__.py\", line 36, in main\n",
      "    driver = webdriver.Chrome()\n",
      "  File \"/Users/alexsciuto/Library/Mobile Documents/com~apple~CloudDocs/DataWithAlex/MSDA Classes/Network_Science/YOLO_Paper_Network_Analysis/venv/lib/python3.9/site-packages/selenium/webdriver/chrome/webdriver.py\", line 45, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/alexsciuto/Library/Mobile Documents/com~apple~CloudDocs/DataWithAlex/MSDA Classes/Network_Science/YOLO_Paper_Network_Analysis/venv/lib/python3.9/site-packages/selenium/webdriver/chromium/webdriver.py\", line 56, in __init__\n",
      "    super().__init__(\n",
      "  File \"/Users/alexsciuto/Library/Mobile Documents/com~apple~CloudDocs/DataWithAlex/MSDA Classes/Network_Science/YOLO_Paper_Network_Analysis/venv/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py\", line 205, in __init__\n",
      "    self.start_session(capabilities)\n",
      "  File \"/Users/alexsciuto/Library/Mobile Documents/com~apple~CloudDocs/DataWithAlex/MSDA Classes/Network_Science/YOLO_Paper_Network_Analysis/venv/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py\", line 289, in start_session\n",
      "    response = self.execute(Command.NEW_SESSION, caps)[\"value\"]\n",
      "  File \"/Users/alexsciuto/Library/Mobile Documents/com~apple~CloudDocs/DataWithAlex/MSDA Classes/Network_Science/YOLO_Paper_Network_Analysis/venv/lib/python3.9/site-packages/selenium/webdriver/remote/webdriver.py\", line 344, in execute\n",
      "    self.error_handler.check_response(response)\n",
      "  File \"/Users/alexsciuto/Library/Mobile Documents/com~apple~CloudDocs/DataWithAlex/MSDA Classes/Network_Science/YOLO_Paper_Network_Analysis/venv/lib/python3.9/site-packages/selenium/webdriver/remote/errorhandler.py\", line 229, in check_response\n",
      "    raise exception_class(message, screen, stacktrace)\n",
      "selenium.common.exceptions.SessionNotCreatedException: Message: session not created: This version of ChromeDriver only supports Chrome version 117\n",
      "Current browser version is 116.0.5845.187 with binary path /Applications/Google Chrome.app/Contents/MacOS/Google Chrome\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102cece58 chromedriver + 5090904\n",
      "1   chromedriver                        0x0000000102ce3bc3 chromedriver + 5053379\n",
      "2   chromedriver                        0x000000010287f527 chromedriver + 447783\n",
      "3   chromedriver                        0x00000001028bcf96 chromedriver + 700310\n",
      "4   chromedriver                        0x00000001028bbf99 chromedriver + 696217\n",
      "5   chromedriver                        0x00000001028b6776 chromedriver + 673654\n",
      "6   chromedriver                        0x00000001028b2f93 chromedriver + 659347\n",
      "7   chromedriver                        0x00000001028ff776 chromedriver + 972662\n",
      "8   chromedriver                        0x00000001028f5433 chromedriver + 930867\n",
      "9   chromedriver                        0x00000001028c0042 chromedriver + 712770\n",
      "10  chromedriver                        0x00000001028c126e chromedriver + 717422\n",
      "11  chromedriver                        0x0000000102cae3b9 chromedriver + 4834233\n",
      "12  chromedriver                        0x0000000102cb355d chromedriver + 4855133\n",
      "13  chromedriver                        0x0000000102cba4f2 chromedriver + 4883698\n",
      "14  chromedriver                        0x0000000102cb428d chromedriver + 4858509\n",
      "15  chromedriver                        0x0000000102c860ec chromedriver + 4669676\n",
      "16  chromedriver                        0x0000000102cd2c58 chromedriver + 4983896\n",
      "17  chromedriver                        0x0000000102cd2e10 chromedriver + 4984336\n",
      "18  chromedriver                        0x0000000102ce37fe chromedriver + 5052414\n",
      "19  libsystem_pthread.dylib             0x00007ff817ab61d3 _pthread_start + 125\n",
      "20  libsystem_pthread.dylib             0x00007ff817ab1bd3 thread_start + 15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace the URL below with the link to the Google Scholar citations of the YOLO paper or any other paper.\n",
    "google_scholar_url = 'https://scholar.google.com/scholar?cites=6382612685700818764&as_sdt=40005&sciodt=0,10&hl=en'\n",
    "\n",
    "# Run étudier with desired options\n",
    "# Run étudier with desired options\n",
    "!etudier --pages 2 --depth 2 \"{google_scholar_url}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/h3/t97dnr1d7wsgh8lhwvb3g0r80000gp/T/ipykernel_46515/2506069379.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "\n",
       "<!--\n",
       "\n",
       "This Google Scholar network visualization was generated with\n",
       "https://github.com/edsu/etudier using the following command:\n",
       "\n",
       "% etudier --pages 2 --depth 2 https://scholar.google.com/scholar?cites=6382612685700818764&as_sdt=40005&sciodt=0,10&hl=en\n",
       "\n",
       "--> \n",
       "\n",
       "<html>\n",
       "  <head>\n",
       "    <meta charset=\"utf-8\" />\n",
       "    <style>\n",
       "      body {\n",
       "        overflow: hidden;\n",
       "        margin: 0;\n",
       "      }\n",
       "\n",
       "      text {\n",
       "        font-family: sans-serif;\n",
       "        pointer-events: none;\n",
       "      }\n",
       "    </style>\n",
       "  </head>\n",
       "\n",
       "  <body>\n",
       "    <script src=\"https://d3js.org/d3.v3.min.js\"></script>\n",
       "    <script>\n",
       "      var graph = {\n",
       "  \"nodes\": [\n",
       "    {\n",
       "      \"label\": \"Transformers in vision: A survey\",\n",
       "      \"id\": \"7522504961268153944\",\n",
       "      \"url\": \"https://dl.acm.org/doi/abs/10.1145/3505244\",\n",
       "      \"title\": \"Transformers in vision: A survey\",\n",
       "      \"authors\": \"S Khan, M Naseer, M Hayat, SW Zamir\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 1277,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=7522504961268153944&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 7\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"You only look once: Unified, real-time object detection\",\n",
       "      \"id\": \"6382612685700818764\",\n",
       "      \"title\": \"You only look once: Unified, real-time object detection\",\n",
       "      \"cited_by\": 38962,\n",
       "      \"modularity\": 7\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Attention mechanisms in computer vision: A survey\",\n",
       "      \"id\": \"15456065911372617945\",\n",
       "      \"url\": \"https://link.springer.com/article/10.1007/s41095-022-0271-y\",\n",
       "      \"title\": \"Attention mechanisms in computer vision: A survey\",\n",
       "      \"authors\": \"MH Guo, TX Xu, JJ Liu, ZN Liu, PT Jiang, TJ Mu\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 606,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=15456065911372617945&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Transformers in medical imaging: A survey\",\n",
       "      \"id\": \"982391967541643955\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S1361841523000634\",\n",
       "      \"title\": \"Transformers in medical imaging: A survey\",\n",
       "      \"authors\": \"F Shamshad, S Khan, SW Zamir, MH Khan\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 179,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=982391967541643955&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"A comprehensive survey on pretrained foundation models: A history from bert to chatgpt\",\n",
       "      \"id\": \"2452866517197292093\",\n",
       "      \"url\": \"https://arxiv.org/abs/2302.09419\",\n",
       "      \"title\": \"A comprehensive survey on pretrained foundation models: A history from bert to chatgpt\",\n",
       "      \"authors\": \"C Zhou, Q Li, C Li, J Yu, Y Liu, G Wang\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 109,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=2452866517197292093&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Visual attention network\",\n",
       "      \"id\": \"4773463079530656035\",\n",
       "      \"url\": \"https://link.springer.com/article/10.1007/s41095-023-0364-2\",\n",
       "      \"title\": \"Visual attention network\",\n",
       "      \"authors\": \"MH Guo, CZ Lu, ZN Liu, MM Cheng, SM Hu\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 235,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=4773463079530656035&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Segnext: Rethinking convolutional attention design for semantic segmentation\",\n",
       "      \"id\": \"761718241536208511\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper_files/paper/2022/hash/08050f40fff41616ccfc3080e60a301a-Abstract-Conference.html\",\n",
       "      \"title\": \"Segnext: Rethinking convolutional attention design for semantic segmentation\",\n",
       "      \"authors\": \"MH Guo, CZ Lu, Q Hou, Z Liu\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 113,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=761718241536208511&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"YOLOv5-Tassel: Detecting tassels in RGB UAV imagery with improved YOLOv5 based on transfer learning\",\n",
       "      \"id\": \"7104781172538541114\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/9889182/\",\n",
       "      \"title\": \"YOLOv5-Tassel: Detecting tassels in RGB UAV imagery with improved YOLOv5 based on transfer learning\",\n",
       "      \"authors\": \"W Liu, K Quijano, MM Crawford\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 115,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=7104781172538541114&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Towards an end-to-end framework for flow-guided video inpainting\",\n",
       "      \"id\": \"6491078858607146383\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Li_Towards_an_End-to-End_Framework_for_Flow-Guided_Video_Inpainting_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Towards an end-to-end framework for flow-guided video inpainting\",\n",
       "      \"authors\": \"Z Li, CZ Lu, J Qin, CL Guo\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 43,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=6491078858607146383&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"ISNet: Towards improving separability for remote sensing image change detection\",\n",
       "      \"id\": \"11978445553624214380\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/9772654/\",\n",
       "      \"title\": \"ISNet: Towards improving separability for remote sensing image change detection\",\n",
       "      \"authors\": \"G Cheng, G Wang, J Han\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 37,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=11978445553624214380&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Diagnosis of brain diseases in fusion of neuroimaging modalities using deep learning: A review\",\n",
       "      \"id\": \"5228146784334715443\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S1566253522002573\",\n",
       "      \"title\": \"Diagnosis of brain diseases in fusion of neuroimaging modalities using deep learning: A review\",\n",
       "      \"authors\": \"A Shoeibi, M Khodatars, M Jafari, N Ghassemi\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 21,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=5228146784334715443&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Swin-transformer-enabled YOLOv5 with attention mechanism for small object detection on satellite images\",\n",
       "      \"id\": \"9099615620722636165\",\n",
       "      \"url\": \"https://www.mdpi.com/2072-4292/14/12/2861\",\n",
       "      \"title\": \"Swin-transformer-enabled YOLOv5 with attention mechanism for small object detection on satellite images\",\n",
       "      \"authors\": \"H Gong, T Mu, Q Li, H Dai, C Li, Z He, W Wang, F Han\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 51,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=9099615620722636165&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Beyond self-attention: External attention using two linear layers for visual tasks\",\n",
       "      \"id\": \"10884589459641707712\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/9912362/\",\n",
       "      \"title\": \"Beyond self-attention: External attention using two linear layers for visual tasks\",\n",
       "      \"authors\": \"MH Guo, ZN Liu, TJ Mu, SM Hu\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 264,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=10884589459641707712&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Deep learning for reconstructing protein structures from cryo-EM density maps: Recent advances and future directions\",\n",
       "      \"id\": \"16262241741810610288\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S0959440X23000106\",\n",
       "      \"title\": \"Deep learning for reconstructing protein structures from cryo-EM density maps: Recent advances and future directions\",\n",
       "      \"authors\": \"N Giri, RS Roy, J Cheng\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 12,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=16262241741810610288&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"A unified multiscale learning framework for hyperspectral image classification\",\n",
       "      \"id\": \"17062002127391260256\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/9701344/\",\n",
       "      \"title\": \"A unified multiscale learning framework for hyperspectral image classification\",\n",
       "      \"authors\": \"X Wang, K Tan, P Du, C Pan\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 26,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=17062002127391260256&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Braingb: A benchmark for brain network analysis with graph neural networks\",\n",
       "      \"id\": \"2492869366552030070\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/9933896/\",\n",
       "      \"title\": \"Braingb: A benchmark for brain network analysis with graph neural networks\",\n",
       "      \"authors\": \"H Cui, W Dai, Y Zhu, X Kan, AAC Gu\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 29,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=2492869366552030070&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Attention-based deep meta-transfer learning for few-shot fine-grained fault diagnosis\",\n",
       "      \"id\": \"2308320307881605474\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S0950705123000953\",\n",
       "      \"title\": \"Attention-based deep meta-transfer learning for few-shot fine-grained fault diagnosis\",\n",
       "      \"authors\": \"C Li, S Li, H Wang, F Gu, AD Ball\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 15,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=2308320307881605474&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Study of the automatic recognition of landslides by using InSAR images and the improved mask R-CNN model in the Eastern Tibet Plateau\",\n",
       "      \"id\": \"28694845113345021\",\n",
       "      \"url\": \"https://www.mdpi.com/2072-4292/14/14/3362\",\n",
       "      \"title\": \"Study of the automatic recognition of landslides by using InSAR images and the improved mask R-CNN model in the Eastern Tibet Plateau\",\n",
       "      \"authors\": \"Y Liu, X Yao, Z Gu, Z Zhou, X Liu, X Chen, S Wei\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 16,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=28694845113345021&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"An improved apple object detection method based on lightweight YOLOv4 in complex backgrounds\",\n",
       "      \"id\": \"9917302194767651380\",\n",
       "      \"url\": \"https://www.mdpi.com/2072-4292/14/17/4150\",\n",
       "      \"title\": \"An improved apple object detection method based on lightweight YOLOv4 in complex backgrounds\",\n",
       "      \"authors\": \"C Zhang, F Kang, Y Wang\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 19,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=9917302194767651380&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Large-scale multi-modal pre-trained models: A comprehensive survey\",\n",
       "      \"id\": \"8402450993508627791\",\n",
       "      \"url\": \"https://link.springer.com/article/10.1007/s11633-022-1410-8\",\n",
       "      \"title\": \"Large-scale multi-modal pre-trained models: A comprehensive survey\",\n",
       "      \"authors\": \"X Wang, G Chen, G Qian, P Gao, XY Wei\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 18,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=8402450993508627791&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"YOLOv5-Fog: A multiobjective visual detection algorithm for fog driving scenes based on improved YOLOv5\",\n",
       "      \"id\": \"17059844040400317816\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/9851677/\",\n",
       "      \"title\": \"YOLOv5-Fog: A multiobjective visual detection algorithm for fog driving scenes based on improved YOLOv5\",\n",
       "      \"authors\": \"H Wang, Y Xu, Y He, Y Cai, L Chen, Y Li\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 20,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=17059844040400317816&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"AGs-Unet: Building Extraction Model for High Resolution Remote Sensing Images Based on Attention Gates U Network\",\n",
       "      \"id\": \"1609101033275109223\",\n",
       "      \"url\": \"https://www.mdpi.com/1424-8220/22/8/2932\",\n",
       "      \"title\": \"AGs-Unet: Building Extraction Model for High Resolution Remote Sensing Images Based on Attention Gates U Network\",\n",
       "      \"authors\": \"M Yu, X Chen, W Zhang, Y Liu\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 17,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=1609101033275109223&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Generating transferable adversarial examples against vision transformers\",\n",
       "      \"id\": \"9226375279866402413\",\n",
       "      \"url\": \"https://dl.acm.org/doi/abs/10.1145/3503161.3547989\",\n",
       "      \"title\": \"Generating transferable adversarial examples against vision transformers\",\n",
       "      \"authors\": \"Y Wang, J Wang, Z Yin, R Gong, J Wang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 10,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=9226375279866402413&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 5\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"A survey of transformers\",\n",
       "      \"id\": \"7749897961068121501\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S2666651022000146\",\n",
       "      \"title\": \"A survey of transformers\",\n",
       "      \"authors\": \"T Lin, Y Wang, X Liu, X Qiu\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 477,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=7749897961068121501&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Pre-trained models for natural language processing: A survey\",\n",
       "      \"id\": \"1539076789580815483\",\n",
       "      \"url\": \"https://link.springer.com/article/10.1007/s11431-020-1647-3\",\n",
       "      \"title\": \"Pre-trained models for natural language processing: A survey\",\n",
       "      \"authors\": \"X Qiu, T Sun, Y Xu, Y Shao, N Dai, X Huang\",\n",
       "      \"year\": \"2020\",\n",
       "      \"cited_by\": 1177,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=1539076789580815483&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Pre-trained models: Past, present and future\",\n",
       "      \"id\": \"966567457136989804\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S2666651021000231\",\n",
       "      \"title\": \"Pre-trained models: Past, present and future\",\n",
       "      \"authors\": \"X Han, Z Zhang, N Ding, Y Gu, X Liu, Y Huo, J Qiu\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 360,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=966567457136989804&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Multimodal learning with transformers: A survey\",\n",
       "      \"id\": \"10761248177036470713\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/10123038/\",\n",
       "      \"title\": \"Multimodal learning with transformers: A survey\",\n",
       "      \"authors\": \"P Xu, X Zhu, DA Clifton\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 107,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=10761248177036470713&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"A survey of visual transformers\",\n",
       "      \"id\": \"14136709172791920331\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/10088164/\",\n",
       "      \"title\": \"A survey of visual transformers\",\n",
       "      \"authors\": \"Y Liu, Y Zhang, Y Wang, F Hou, J Yuan\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 104,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=14136709172791920331&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Recent advances in deep learning based dialogue systems: A systematic survey\",\n",
       "      \"id\": \"13597902966753793310\",\n",
       "      \"url\": \"https://link.springer.com/article/10.1007/s10462-022-10248-8\",\n",
       "      \"title\": \"Recent advances in deep learning based dialogue systems: A systematic survey\",\n",
       "      \"authors\": \"J Ni, T Young, V Pandelea, F Xue\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 121,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=13597902966753793310&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Ammus: A survey of transformer-based pretrained models in natural language processing\",\n",
       "      \"id\": \"4431578198915484435\",\n",
       "      \"url\": \"https://arxiv.org/abs/2108.05542\",\n",
       "      \"title\": \"Ammus: A survey of transformer-based pretrained models in natural language processing\",\n",
       "      \"authors\": \"KS Kalyan, A Rajasekharan, S Sangeetha\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 141,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=4431578198915484435&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"ChatGPT: Jack of all trades, master of none\",\n",
       "      \"id\": \"2600515932282922845\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S156625352300177X\",\n",
       "      \"title\": \"ChatGPT: Jack of all trades, master of none\",\n",
       "      \"authors\": \"J Koco\\u0144, I Cichecki, O Kaszyca, M Kochanek, D Szyd\\u0142o\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 63,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=2600515932282922845&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Physformer: Facial video-based physiological measurement with temporal difference transformer\",\n",
       "      \"id\": \"8053588590478703627\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Yu_PhysFormer_Facial_Video-Based_Physiological_Measurement_With_Temporal_Difference_Transformer_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Physformer: Facial video-based physiological measurement with temporal difference transformer\",\n",
       "      \"authors\": \"Z Yu, Y Shen, J Shi, H Zhao\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 54,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=8053588590478703627&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"ChatGPT is not all you need. A State of the Art Review of large Generative AI models\",\n",
       "      \"id\": \"15393921212791157727\",\n",
       "      \"url\": \"https://arxiv.org/abs/2301.04655\",\n",
       "      \"title\": \"ChatGPT is not all you need. A State of the Art Review of large Generative AI models\",\n",
       "      \"authors\": \"R Gozalo-Brizuela, EC Garrido-Merchan\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 80,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=15393921212791157727&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Glycoinformatics in the artificial intelligence era\",\n",
       "      \"id\": \"7420567692305480707\",\n",
       "      \"url\": \"https://pubs.acs.org/doi/abs/10.1021/acs.chemrev.2c00110\",\n",
       "      \"title\": \"Glycoinformatics in the artificial intelligence era\",\n",
       "      \"authors\": \"D Bojar, F Lisacek\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 9,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=7420567692305480707&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Deep learning for depression recognition with audiovisual cues: A review\",\n",
       "      \"id\": \"3878971468388928610\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S1566253521002207\",\n",
       "      \"title\": \"Deep learning for depression recognition with audiovisual cues: A review\",\n",
       "      \"authors\": \"L He, M Niu, P Tiwari, P Marttinen, R Su, J Jiang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 62,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=3878971468388928610&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Museformer: Transformer with fine-and coarse-grained attention for music generation\",\n",
       "      \"id\": \"9919738130893761480\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper_files/paper/2022/hash/092c2d45005ea2db40fc24c470663416-Abstract-Conference.html\",\n",
       "      \"title\": \"Museformer: Transformer with fine-and coarse-grained attention for music generation\",\n",
       "      \"authors\": \"B Yu, P Lu, R Wang, W Hu, X Tan\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 12,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=9919738130893761480&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Video transformers: A survey\",\n",
       "      \"id\": \"15926311935982020340\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/10041724/\",\n",
       "      \"title\": \"Video transformers: A survey\",\n",
       "      \"authors\": \"J Selva, AS Johansen, S Escalera\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 41,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=15926311935982020340&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"KNN-contrastive learning for out-of-domain intent classification\",\n",
       "      \"id\": \"836183377042488989\",\n",
       "      \"url\": \"https://aclanthology.org/2022.acl-long.352/\",\n",
       "      \"title\": \"KNN-contrastive learning for out-of-domain intent classification\",\n",
       "      \"authors\": \"Y Zhou, P Liu, X Qiu\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 31,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=836183377042488989&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Contrast and generation make bart a good dialogue emotion recognizer\",\n",
       "      \"id\": \"17269928898680478649\",\n",
       "      \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/view/21348\",\n",
       "      \"title\": \"Contrast and generation make bart a good dialogue emotion recognizer\",\n",
       "      \"authors\": \"S Li, H Yan, X Qiu\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 34,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=17269928898680478649&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Dual-aspect self-attention based on transformer for remaining useful life prediction\",\n",
       "      \"id\": \"6376642340831106711\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/9737516/\",\n",
       "      \"title\": \"Dual-aspect self-attention based on transformer for remaining useful life prediction\",\n",
       "      \"authors\": \"Z Zhang, W Song, Q Li\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 53,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=6376642340831106711&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Deltar: Depth estimation from a light-weight tof sensor and rgb image\",\n",
       "      \"id\": \"12806372482493631350\",\n",
       "      \"url\": \"https://link.springer.com/chapter/10.1007/978-3-031-19769-7_36\",\n",
       "      \"title\": \"Deltar: Depth estimation from a light-weight tof sensor and rgb image\",\n",
       "      \"authors\": \"Y Li, X Liu, W Dong, H Zhou, H Bao, G Zhang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 7,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=12806372482493631350&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"A Transformer-based deep neural network model for SSVEP classification\",\n",
       "      \"id\": \"2160563741249466913\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S0893608023002319\",\n",
       "      \"title\": \"A Transformer-based deep neural network model for SSVEP classification\",\n",
       "      \"authors\": \"J Chen, Y Zhang, Y Pan, P Xu, C Guan\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 7,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=2160563741249466913&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 6\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Restormer: Efficient transformer for high-resolution image restoration\",\n",
       "      \"id\": \"16431204865977056518\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Zamir_Restormer_Efficient_Transformer_for_High-Resolution_Image_Restoration_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Restormer: Efficient transformer for high-resolution image restoration\",\n",
       "      \"authors\": \"SW Zamir, A Arora, S Khan, M Hayat\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 683,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=16431204865977056518&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"NTIRE 2023 challenge on efficient super-resolution: Methods and results\",\n",
       "      \"id\": \"16932998913230259066\",\n",
       "      \"url\": \"https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Li_NTIRE_2023_Challenge_on_Efficient_Super-Resolution_Methods_and_Results_CVPRW_2023_paper.html\",\n",
       "      \"title\": \"NTIRE 2023 challenge on efficient super-resolution: Methods and results\",\n",
       "      \"authors\": \"Y Li, Y Zhang, R Timofte, L Van Gool\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 73,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=16932998913230259066&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Simple baselines for image restoration\",\n",
       "      \"id\": \"498268664873674535\",\n",
       "      \"url\": \"https://link.springer.com/chapter/10.1007/978-3-031-20071-7_2\",\n",
       "      \"title\": \"Simple baselines for image restoration\",\n",
       "      \"authors\": \"L Chen, X Chu, X Zhang, J Sun\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 234,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=498268664873674535&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Lens-to-lens bokeh effect transformation. NTIRE 2023 challenge report\",\n",
       "      \"id\": \"9110410135628850117\",\n",
       "      \"url\": \"https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Conde_Lens-to-Lens_Bokeh_Effect_Transformation._NTIRE_2023_Challenge_Report_CVPRW_2023_paper.html\",\n",
       "      \"title\": \"Lens-to-lens bokeh effect transformation. NTIRE 2023 challenge report\",\n",
       "      \"authors\": \"MV Conde, M Kolmet, T Seizinger\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 17,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=9110410135628850117&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"NTIRE 2023 video colorization challenge\",\n",
       "      \"id\": \"7710701073211724386\",\n",
       "      \"url\": \"https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Kang_NTIRE_2023_Video_Colorization_Challenge_CVPRW_2023_paper.html\",\n",
       "      \"title\": \"NTIRE 2023 video colorization challenge\",\n",
       "      \"authors\": \"X Kang, X Lin, K Zhang, Z Hui\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 14,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=7710701073211724386&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"NTIRE 2023 Challenge on 360deg Omnidirectional Image and Video Super-Resolution: Datasets, Methods and Results\",\n",
       "      \"id\": \"13272942409666364496\",\n",
       "      \"url\": \"https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Cao_NTIRE_2023_Challenge_on_360deg_Omnidirectional_Image_and_Video_Super-Resolution_CVPRW_2023_paper.html\",\n",
       "      \"title\": \"NTIRE 2023 Challenge on 360deg Omnidirectional Image and Video Super-Resolution: Datasets, Methods and Results\",\n",
       "      \"authors\": \"M Cao, C Mou, F Yu, X Wang\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 15,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=13272942409666364496&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"NTIRE 2023 challenge on image denoising: Methods and results\",\n",
       "      \"id\": \"12297468854729392143\",\n",
       "      \"url\": \"https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Li_NTIRE_2023_Challenge_on_Image_Denoising_Methods_and_Results_CVPRW_2023_paper.html\",\n",
       "      \"title\": \"NTIRE 2023 challenge on image denoising: Methods and results\",\n",
       "      \"authors\": \"Y Li, Y Zhang, R Timofte, L Van Gool\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 12,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=12297468854729392143&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"NTIRE 2023 challenge on night photography rendering\",\n",
       "      \"id\": \"5364344454304770774\",\n",
       "      \"url\": \"https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Shutova_NTIRE_2023_Challenge_on_Night_Photography_Rendering_CVPRW_2023_paper.html\",\n",
       "      \"title\": \"NTIRE 2023 challenge on night photography rendering\",\n",
       "      \"authors\": \"A Shutova, E Ershov\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 13,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=5364344454304770774&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Efficient long-range attention network for image super-resolution\",\n",
       "      \"id\": \"3464402829187061665\",\n",
       "      \"url\": \"https://link.springer.com/chapter/10.1007/978-3-031-19790-1_39\",\n",
       "      \"title\": \"Efficient long-range attention network for image super-resolution\",\n",
       "      \"authors\": \"X Zhang, H Zeng, S Guo, L Zhang\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 70,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=3464402829187061665&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion\",\n",
       "      \"id\": \"15226748689642581218\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2023/html/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.html\",\n",
       "      \"title\": \"Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion\",\n",
       "      \"authors\": \"Z Zhao, H Bai, J Zhang, Y Zhang, S Xu\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 23,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=15226748689642581218&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Mst++: Multi-stage spectral-wise transformer for efficient spectral reconstruction\",\n",
       "      \"id\": \"4776651674359042439\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Cai_MST_Multi-Stage_Spectral-Wise_Transformer_for_Efficient_Spectral_Reconstruction_CVPRW_2022_paper.html\",\n",
       "      \"title\": \"Mst++: Multi-stage spectral-wise transformer for efficient spectral reconstruction\",\n",
       "      \"authors\": \"Y Cai, J Lin, Z Lin, H Wang, Y Zhang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 40,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=4776651674359042439&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Learning enriched features for fast image restoration and enhancement\",\n",
       "      \"id\": \"278490733091580759\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/9756908/\",\n",
       "      \"title\": \"Learning enriched features for fast image restoration and enhancement\",\n",
       "      \"authors\": \"SW Zamir, A Arora, S Khan, M Hayat\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 55,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=278490733091580759&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Aim 2022 challenge on super-resolution of compressed image and video: Dataset, methods and results\",\n",
       "      \"id\": \"1598910218006932443\",\n",
       "      \"url\": \"https://link.springer.com/chapter/10.1007/978-3-031-25066-8_8\",\n",
       "      \"title\": \"Aim 2022 challenge on super-resolution of compressed image and video: Dataset, methods and results\",\n",
       "      \"authors\": \"R Yang, R Timofte, X Li, Q Zhang, L Zhang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 22,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=1598910218006932443&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Rethinking alignment in video super-resolution transformers\",\n",
       "      \"id\": \"13813872909195716054\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper_files/paper/2022/hash/ea4d65c59073e8faf79222654d25fbe2-Abstract-Conference.html\",\n",
       "      \"title\": \"Rethinking alignment in video super-resolution transformers\",\n",
       "      \"authors\": \"S Shi, J Gu, L Xie, X Wang, Y Yang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 14,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=13813872909195716054&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Maniqa: Multi-dimension attention network for no-reference image quality assessment\",\n",
       "      \"id\": \"3715909162805048662\",\n",
       "      \"url\": \"https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Yang_MANIQA_Multi-Dimension_Attention_Network_for_No-Reference_Image_Quality_Assessment_CVPRW_2022_paper.html\",\n",
       "      \"title\": \"Maniqa: Multi-dimension attention network for no-reference image quality assessment\",\n",
       "      \"authors\": \"S Yang, T Wu, S Shi, S Lao, Y Gong\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 50,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=3715909162805048662&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"NTIRE 2023 challenge on stereo image super-resolution: Methods and results\",\n",
       "      \"id\": \"8627981696317437595\",\n",
       "      \"url\": \"https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Wang_NTIRE_2023_Challenge_on_Stereo_Image_Super-Resolution_Methods_and_Results_CVPRW_2023_paper.html\",\n",
       "      \"title\": \"NTIRE 2023 challenge on stereo image super-resolution: Methods and results\",\n",
       "      \"authors\": \"L Wang, Y Guo, Y Wang, J Li, S Gu\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 23,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=8627981696317437595&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Self-supervised video transformer\",\n",
       "      \"id\": \"11308628305789992240\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Ranasinghe_Self-Supervised_Video_Transformer_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Self-supervised video transformer\",\n",
       "      \"authors\": \"K Ranasinghe, M Naseer, S Khan\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 48,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=11308628305789992240&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Coarse-to-fine sparse transformer for hyperspectral image reconstruction\",\n",
       "      \"id\": \"2900405794489939688\",\n",
       "      \"url\": \"https://link.springer.com/chapter/10.1007/978-3-031-19790-1_41\",\n",
       "      \"title\": \"Coarse-to-fine sparse transformer for hyperspectral image reconstruction\",\n",
       "      \"authors\": \"Y Cai, J Lin, X Hu, H Wang, X Yuan, Y Zhang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 41,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=2900405794489939688&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Pyramid Attention Network for Image Restoration\",\n",
       "      \"id\": \"1292725169895823488\",\n",
       "      \"url\": \"https://link.springer.com/article/10.1007/s11263-023-01843-5\",\n",
       "      \"title\": \"Pyramid Attention Network for Image Restoration\",\n",
       "      \"authors\": \"Y Mei, Y Fan, Y Zhang, J Yu, Y Zhou, D Liu\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 123,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=1292725169895823488&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"NTIRE 2022 challenge on perceptual image quality assessment\",\n",
       "      \"id\": \"14640072730760349377\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Gu_NTIRE_2022_Challenge_on_Perceptual_Image_Quality_Assessment_CVPRW_2022_paper.html\",\n",
       "      \"title\": \"NTIRE 2022 challenge on perceptual image quality assessment\",\n",
       "      \"authors\": \"J Gu, H Cai, C Dong, JS Ren\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 68,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=14640072730760349377&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Coatnet: Marrying convolution and attention for all data sizes\",\n",
       "      \"id\": \"10110104334022835757\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper/2021/hash/20568692db622456cc42a2e853ca21f8-Abstract.html\",\n",
       "      \"title\": \"Coatnet: Marrying convolution and attention for all data sizes\",\n",
       "      \"authors\": \"Z Dai, H Liu, QV Le, M Tan\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 704,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=10110104334022835757&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"A convnet for the 2020s\",\n",
       "      \"id\": \"14443907969977981621\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html\",\n",
       "      \"title\": \"A convnet for the 2020s\",\n",
       "      \"authors\": \"Z Liu, H Mao, CY Wu, C Feichtenhofer\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 2126,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=14443907969977981621&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Scaling vision transformers\",\n",
       "      \"id\": \"13501013621324561884\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Zhai_Scaling_Vision_Transformers_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Scaling vision transformers\",\n",
       "      \"authors\": \"X Zhai, A Kolesnikov, N Houlsby\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 561,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=13501013621324561884&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Coca: Contrastive captioners are image-text foundation models\",\n",
       "      \"id\": \"13629397900287809862\",\n",
       "      \"url\": \"https://arxiv.org/abs/2205.01917\",\n",
       "      \"title\": \"Coca: Contrastive captioners are image-text foundation models\",\n",
       "      \"authors\": \"J Yu, Z Wang, V Vasudevan, L Yeung\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 470,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=13629397900287809862&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Mvitv2: Improved multiscale vision transformers for classification and detection\",\n",
       "      \"id\": \"1273811038957334386\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Mvitv2: Improved multiscale vision transformers for classification and detection\",\n",
       "      \"authors\": \"Y Li, CY Wu, H Fan, K Mangalam\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 278,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=1273811038957334386&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Inception transformer\",\n",
       "      \"id\": \"610621467807251926\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper_files/paper/2022/hash/94e85561a342de88b559b72c9b29f638-Abstract-Conference.html\",\n",
       "      \"title\": \"Inception transformer\",\n",
       "      \"authors\": \"C Si, W Yu, P Zhou, Y Zhou\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 145,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=610621467807251926&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Lit: Zero-shot transfer with locked-image text tuning\",\n",
       "      \"id\": \"18380770342348927722\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Zhai_LiT_Zero-Shot_Transfer_With_Locked-Image_Text_Tuning_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Lit: Zero-shot transfer with locked-image text tuning\",\n",
       "      \"authors\": \"X Zhai, X Wang, B Mustafa, A Steiner\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 227,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=18380770342348927722&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Simvlm: Simple visual language model pretraining with weak supervision\",\n",
       "      \"id\": \"9618435703828650575\",\n",
       "      \"url\": \"https://arxiv.org/abs/2108.10904\",\n",
       "      \"title\": \"Simvlm: Simple visual language model pretraining with weak supervision\",\n",
       "      \"authors\": \"Z Wang, J Yu, AW Yu, Z Dai, Y Tsvetkov\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 448,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=9618435703828650575&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Conditional positional encodings for vision transformers\",\n",
       "      \"id\": \"17870066505440679476\",\n",
       "      \"url\": \"https://arxiv.org/abs/2102.10882\",\n",
       "      \"title\": \"Conditional positional encodings for vision transformers\",\n",
       "      \"authors\": \"X Chu, Z Tian, B Zhang, X Wang, X Wei, H Xia\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 368,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=17870066505440679476&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Eva: Exploring the limits of masked visual representation learning at scale\",\n",
       "      \"id\": \"10588342779298269046\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2023/html/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.html\",\n",
       "      \"title\": \"Eva: Exploring the limits of masked visual representation learning at scale\",\n",
       "      \"authors\": \"Y Fang, W Wang, B Xie, Q Sun, L Wu\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 101,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=10588342779298269046&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Maxvit: Multi-axis vision transformer\",\n",
       "      \"id\": \"6784655767122395745\",\n",
       "      \"url\": \"https://link.springer.com/chapter/10.1007/978-3-031-20053-3_27\",\n",
       "      \"title\": \"Maxvit: Multi-axis vision transformer\",\n",
       "      \"authors\": \"Z Tu, H Talebi, H Zhang, F Yang, P Milanfar\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 158,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=6784655767122395745&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Internimage: Exploring large-scale vision foundation models with deformable convolutions\",\n",
       "      \"id\": \"6118595289890500680\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2023/html/Wang_InternImage_Exploring_Large-Scale_Vision_Foundation_Models_With_Deformable_Convolutions_CVPR_2023_paper.html\",\n",
       "      \"title\": \"Internimage: Exploring large-scale vision foundation models with deformable convolutions\",\n",
       "      \"authors\": \"W Wang, J Dai, Z Chen, Z Huang, Z Li\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 113,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=6118595289890500680&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Hornet: Efficient high-order spatial interactions with recursive gated convolutions\",\n",
       "      \"id\": \"12938213222665733645\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper_files/paper/2022/hash/436d042b2dd81214d23ae43eb196b146-Abstract-Conference.html\",\n",
       "      \"title\": \"Hornet: Efficient high-order spatial interactions with recursive gated convolutions\",\n",
       "      \"authors\": \"Y Rao, W Zhao, Y Tang, J Zhou\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 99,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=12938213222665733645&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Convnext v2: Co-designing and scaling convnets with masked autoencoders\",\n",
       "      \"id\": \"1388490151733704334\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2023/html/Woo_ConvNeXt_V2_Co-Designing_and_Scaling_ConvNets_With_Masked_Autoencoders_CVPR_2023_paper.html\",\n",
       "      \"title\": \"Convnext v2: Co-designing and scaling convnets with masked autoencoders\",\n",
       "      \"authors\": \"S Woo, S Debnath, R Hu, X Chen\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 61,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=1388490151733704334&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Davit: Dual attention vision transformers\",\n",
       "      \"id\": \"18356109755771918503\",\n",
       "      \"url\": \"https://link.springer.com/chapter/10.1007/978-3-031-20053-3_5\",\n",
       "      \"title\": \"Davit: Dual attention vision transformers\",\n",
       "      \"authors\": \"M Ding, B Xiao, N Codella, P Luo, J Wang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 93,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=18356109755771918503&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Patches are all you need?\",\n",
       "      \"id\": \"15188717593606933557\",\n",
       "      \"url\": \"https://arxiv.org/abs/2201.09792\",\n",
       "      \"title\": \"Patches are all you need?\",\n",
       "      \"authors\": \"A Trockman, JZ Kolter\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 221,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=15188717593606933557&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Efficientformer: Vision transformers at mobilenet speed\",\n",
       "      \"id\": \"12692106295877813680\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper_files/paper/2022/hash/5452ad8ee6ea6e7dc41db1cbd31ba0b8-Abstract-Conference.html\",\n",
       "      \"title\": \"Efficientformer: Vision transformers at mobilenet speed\",\n",
       "      \"authors\": \"Y Li, G Yuan, Y Wen, J Hu\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 86,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=12692106295877813680&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Pure transformers are powerful graph learners\",\n",
       "      \"id\": \"1854387804616571098\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper_files/paper/2022/hash/5d84236751fe6d25dc06db055a3180b0-Abstract-Conference.html\",\n",
       "      \"title\": \"Pure transformers are powerful graph learners\",\n",
       "      \"authors\": \"J Kim, D Nguyen, S Min, S Cho\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 54,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=1854387804616571098&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 2\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Multi-stage progressive image restoration\",\n",
       "      \"id\": \"14988305211504802629\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2021/html/Zamir_Multi-Stage_Progressive_Image_Restoration_CVPR_2021_paper.html\",\n",
       "      \"title\": \"Multi-stage progressive image restoration\",\n",
       "      \"authors\": \"SW Zamir, A Arora, S Khan, M Hayat\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 860,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=14988305211504802629&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Uformer: A general u-shaped transformer for image restoration\",\n",
       "      \"id\": \"14031000766044293652\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Wang_Uformer_A_General_U-Shaped_Transformer_for_Image_Restoration_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Uformer: A general u-shaped transformer for image restoration\",\n",
       "      \"authors\": \"Z Wang, X Cun, J Bao, W Zhou\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 599,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=14031000766044293652&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Ntire 2022 spectral recovery challenge and data set\",\n",
       "      \"id\": \"16381083981417715623\",\n",
       "      \"url\": \"https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Arad_NTIRE_2022_Spectral_Recovery_Challenge_and_Data_Set_CVPRW_2022_paper.html\",\n",
       "      \"title\": \"Ntire 2022 spectral recovery challenge and data set\",\n",
       "      \"authors\": \"B Arad, R Timofte, R Yahel, N Morag\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 39,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=16381083981417715623&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"NTIRE 2021 challenge on image deblurring\",\n",
       "      \"id\": \"6999508588420552953\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Nah_NTIRE_2021_Challenge_on_Image_Deblurring_CVPRW_2021_paper.html\",\n",
       "      \"title\": \"NTIRE 2021 challenge on image deblurring\",\n",
       "      \"authors\": \"S Nah, S Son, S Lee, R Timofte\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 60,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=6999508588420552953&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Maxim: Multi-axis mlp for image processing\",\n",
       "      \"id\": \"18275282813589182456\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Tu_MAXIM_Multi-Axis_MLP_for_Image_Processing_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Maxim: Multi-axis mlp for image processing\",\n",
       "      \"authors\": \"Z Tu, H Talebi, H Zhang, F Yang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 188,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=18275282813589182456&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Rethinking coarse-to-fine approach in single image deblurring\",\n",
       "      \"id\": \"11948207786179445379\",\n",
       "      \"url\": \"https://openaccess.thecvf.com/content/ICCV2021/html/Cho_Rethinking_Coarse-To-Fine_Approach_in_Single_Image_Deblurring_ICCV_2021_paper.html?ref=https://githubhelp.com\",\n",
       "      \"title\": \"Rethinking coarse-to-fine approach in single image deblurring\",\n",
       "      \"authors\": \"SJ Cho, SW Ji, JP Hong, SW Jung\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 271,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=11948207786179445379&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Hinet: Half instance normalization network for image restoration\",\n",
       "      \"id\": \"2731814227384441305\",\n",
       "      \"url\": \"https://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Chen_HINet_Half_Instance_Normalization_Network_for_Image_Restoration_CVPRW_2021_paper.html\",\n",
       "      \"title\": \"Hinet: Half instance normalization network for image restoration\",\n",
       "      \"authors\": \"L Chen, X Lu, J Zhang, X Chu\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 240,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=2731814227384441305&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Transweather: Transformer-based restoration of images degraded by adverse weather conditions\",\n",
       "      \"id\": \"563900006853828372\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Valanarasu_TransWeather_Transformer-Based_Restoration_of_Images_Degraded_by_Adverse_Weather_Conditions_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Transweather: Transformer-based restoration of images degraded by adverse weather conditions\",\n",
       "      \"authors\": \"JMJ Valanarasu, R Yasarla\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 90,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=563900006853828372&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Deblurring via stochastic refinement\",\n",
       "      \"id\": \"2892557376887066282\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Whang_Deblurring_via_Stochastic_Refinement_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Deblurring via stochastic refinement\",\n",
       "      \"authors\": \"J Whang, M Delbracio, H Talebi\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 88,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=2892557376887066282&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"All-in-one image restoration for unknown corruption\",\n",
       "      \"id\": \"10807347079650485654\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Li_All-in-One_Image_Restoration_for_Unknown_Corruption_CVPR_2022_paper.html\",\n",
       "      \"title\": \"All-in-one image restoration for unknown corruption\",\n",
       "      \"authors\": \"B Li, X Liu, P Hu, Z Wu, J Lv\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 72,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=10807347079650485654&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Images speak in images: A generalist painter for in-context visual learning\",\n",
       "      \"id\": \"8278982060432150337\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2023/html/Wang_Images_Speak_in_Images_A_Generalist_Painter_for_In-Context_Visual_CVPR_2023_paper.html\",\n",
       "      \"title\": \"Images speak in images: A generalist painter for in-context visual learning\",\n",
       "      \"authors\": \"X Wang, W Wang, Y Cao, C Shen\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 31,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=8278982060432150337&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Vrt: A video restoration transformer\",\n",
       "      \"id\": \"9248261167097334152\",\n",
       "      \"url\": \"https://arxiv.org/abs/2201.12288\",\n",
       "      \"title\": \"Vrt: A video restoration transformer\",\n",
       "      \"authors\": \"J Liang, J Cao, Y Fan, K Zhang, R Ranjan, Y Li\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 99,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=9248261167097334152&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Deep generalized unfolding networks for image restoration\",\n",
       "      \"id\": \"7494856167361955847\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Mou_Deep_Generalized_Unfolding_Networks_for_Image_Restoration_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Deep generalized unfolding networks for image restoration\",\n",
       "      \"authors\": \"C Mou, Q Wang, J Zhang\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 68,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=7494856167361955847&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Learning multiple adverse weather removal via two-stage knowledge learning and multi-contrastive regularization: Toward a unified model\",\n",
       "      \"id\": \"6389162819004784674\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Chen_Learning_Multiple_Adverse_Weather_Removal_via_Two-Stage_Knowledge_Learning_and_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Learning multiple adverse weather removal via two-stage knowledge learning and multi-contrastive regularization: Toward a unified model\",\n",
       "      \"authors\": \"WT Chen, ZK Huang, CC Tsai\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 39,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=6389162819004784674&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Cross Aggregation Transformer for Image Restoration\",\n",
       "      \"id\": \"10294262589674995487\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper_files/paper/2022/hash/a37fea8e67f907311826bc1ba2654d97-Abstract-Conference.html\",\n",
       "      \"title\": \"Cross Aggregation Transformer for Image Restoration\",\n",
       "      \"authors\": \"Z Chen, Y Zhang, J Gu, L Kong\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 19,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=10294262589674995487&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Improving image restoration by revisiting global information aggregation\",\n",
       "      \"id\": \"7561375020407203939\",\n",
       "      \"url\": \"https://link.springer.com/chapter/10.1007/978-3-031-20071-7_4\",\n",
       "      \"title\": \"Improving image restoration by revisiting global information aggregation\",\n",
       "      \"authors\": \"X Chu, L Chen, C Chen, X Lu\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 39,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=7561375020407203939&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 0\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Transreid: Transformer-based object re-identification\",\n",
       "      \"id\": \"15635397108812213817\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/ICCV2021/html/He_TransReID_Transformer-Based_Object_Re-Identification_ICCV_2021_paper.html\",\n",
       "      \"title\": \"Transreid: Transformer-based object re-identification\",\n",
       "      \"authors\": \"S He, H Luo, P Wang, F Wang, H Li\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 495,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=15635397108812213817&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Multiscale vision transformers\",\n",
       "      \"id\": \"7329647594369932315\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/ICCV2021/html/Fan_Multiscale_Vision_Transformers_ICCV_2021_paper.html\",\n",
       "      \"title\": \"Multiscale vision transformers\",\n",
       "      \"authors\": \"H Fan, B Xiong, K Mangalam, Y Li\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 1596,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=7329647594369932315&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Cswin transformer: A general vision transformer backbone with cross-shaped windows\",\n",
       "      \"id\": \"4431453089685809340\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Dong_CSWin_Transformer_A_General_Vision_Transformer_Backbone_With_Cross-Shaped_Windows_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Cswin transformer: A general vision transformer backbone with cross-shaped windows\",\n",
       "      \"authors\": \"X Dong, J Bao, D Chen, W Zhang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 479,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=4431453089685809340&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"A survey on vision transformer\",\n",
       "      \"id\": \"4278610892084589339\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/9716741/\",\n",
       "      \"title\": \"A survey on vision transformer\",\n",
       "      \"authors\": \"K Han, Y Wang, H Chen, X Chen, J Guo\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 684,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=4278610892084589339&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Transfg: A transformer architecture for fine-grained recognition\",\n",
       "      \"id\": \"601129416962130879\",\n",
       "      \"url\": \"https://ojs.aaai.org/index.php/AAAI/article/view/19967\",\n",
       "      \"title\": \"Transfg: A transformer architecture for fine-grained recognition\",\n",
       "      \"authors\": \"J He, JN Chen, S Liu, A Kortylewski, C Yang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 212,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=601129416962130879&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Multi-animal pose estimation, identification and tracking with DeepLabCut\",\n",
       "      \"id\": \"4326704403467340422\",\n",
       "      \"url\": \"https://www.nature.com/articles/s41592-022-01443-0\",\n",
       "      \"title\": \"Multi-animal pose estimation, identification and tracking with DeepLabCut\",\n",
       "      \"authors\": \"J Lauer, M Zhou, S Ye, W Menegas, S Schneider\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 167,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=4326704403467340422&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Mhformer: Multi-hypothesis transformer for 3d human pose estimation\",\n",
       "      \"id\": \"18177167198432349205\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Li_MHFormer_Multi-Hypothesis_Transformer_for_3D_Human_Pose_Estimation_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Mhformer: Multi-hypothesis transformer for 3d human pose estimation\",\n",
       "      \"authors\": \"W Li, H Liu, H Tang, P Wang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 120,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=18177167198432349205&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Uniformer: Unifying convolution and self-attention for visual recognition\",\n",
       "      \"id\": \"342346550438939327\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/10143709/\",\n",
       "      \"title\": \"Uniformer: Unifying convolution and self-attention for visual recognition\",\n",
       "      \"authors\": \"K Li, Y Wang, J Zhang, P Gao, G Song\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 100,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=342346550438939327&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition\",\n",
       "      \"id\": \"9632085609200326616\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper_files/paper/2021/hash/64517d8435994992e682b3e4aa0a0661-Abstract.html\",\n",
       "      \"title\": \"Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition\",\n",
       "      \"authors\": \"Y Wang, R Huang, S Song\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 99,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=9632085609200326616&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Adavit: Adaptive vision transformers for efficient image recognition\",\n",
       "      \"id\": \"14716201437512299394\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Meng_AdaViT_Adaptive_Vision_Transformers_for_Efficient_Image_Recognition_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Adavit: Adaptive vision transformers for efficient image recognition\",\n",
       "      \"authors\": \"L Meng, H Li, BC Chen, S Lan, Z Wu\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 69,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=14716201437512299394&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Accelerating DETR convergence via semantic-aligned matching\",\n",
       "      \"id\": \"6658129475220097194\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Accelerating_DETR_Convergence_via_Semantic-Aligned_Matching_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Accelerating DETR convergence via semantic-aligned matching\",\n",
       "      \"authors\": \"G Zhang, Z Luo, Y Yu, K Cui\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 51,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=6658129475220097194&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Cdtrans: Cross-domain transformer for unsupervised domain adaptation\",\n",
       "      \"id\": \"9897783945226246229\",\n",
       "      \"url\": \"https://arxiv.org/abs/2109.06165\",\n",
       "      \"title\": \"Cdtrans: Cross-domain transformer for unsupervised domain adaptation\",\n",
       "      \"authors\": \"T Xu, W Chen, P Wang, F Wang, H Li, R Jin\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 114,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=9897783945226246229&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Dual cross-attention learning for fine-grained visual categorization and object re-identification\",\n",
       "      \"id\": \"5598507831422168925\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Dual_Cross-Attention_Learning_for_Fine-Grained_Visual_Categorization_and_Object_Re-Identification_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Dual cross-attention learning for fine-grained visual categorization and object re-identification\",\n",
       "      \"authors\": \"H Zhu, W Ke, D Li, J Liu, L Tian\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 48,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=5598507831422168925&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Towards discriminative representation learning for unsupervised person re-identification\",\n",
       "      \"id\": \"3707506564686588016\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/ICCV2021/html/Isobe_Towards_Discriminative_Representation_Learning_for_Unsupervised_Person_Re-Identification_ICCV_2021_paper.html\",\n",
       "      \"title\": \"Towards discriminative representation learning for unsupervised person re-identification\",\n",
       "      \"authors\": \"T Isobe, D Li, L Tian, W Chen\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 54,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=3707506564686588016&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search\",\n",
       "      \"id\": \"4988594281116353083\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/ICCV2021/html/Li_BossNAS_Exploring_Hybrid_CNN-Transformers_With_Block-Wisely_Self-Supervised_Neural_Architecture_Search_ICCV_2021_paper.html\",\n",
       "      \"title\": \"Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search\",\n",
       "      \"authors\": \"C Li, T Tang, G Wang, J Peng\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 81,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=4988594281116353083&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Exploiting temporal contexts with strided transformer for 3d human pose estimation\",\n",
       "      \"id\": \"16448186114991458904\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/9674785/\",\n",
       "      \"title\": \"Exploiting temporal contexts with strided transformer for 3d human pose estimation\",\n",
       "      \"authors\": \"W Li, H Liu, R Ding, M Liu, P Wang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 92,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=16448186114991458904&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Recent advances in vision transformer: A survey and outlook of recent work\",\n",
       "      \"id\": \"10189561822678692220\",\n",
       "      \"url\": \"https://arxiv.org/abs/2203.01536\",\n",
       "      \"title\": \"Recent advances in vision transformer: A survey and outlook of recent work\",\n",
       "      \"authors\": \"K Islam\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 17,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=10189561822678692220&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Structure-aware positional transformer for visible-infrared person re-identification\",\n",
       "      \"id\": \"12518043648515496043\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/9725265/\",\n",
       "      \"title\": \"Structure-aware positional transformer for visible-infrared person re-identification\",\n",
       "      \"authors\": \"C Chen, M Ye, M Qi, J Wu, J Jiang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 58,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=12518043648515496043&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 3\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Intriguing properties of vision transformers\",\n",
       "      \"id\": \"15172072370662904150\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper_files/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html\",\n",
       "      \"title\": \"Intriguing properties of vision transformers\",\n",
       "      \"authors\": \"MM Naseer, K Ranasinghe, SH Khan\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 359,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=15172072370662904150&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"ibot: Image bert pre-training with online tokenizer\",\n",
       "      \"id\": \"6668235945473015803\",\n",
       "      \"url\": \"https://arxiv.org/abs/2111.07832\",\n",
       "      \"title\": \"ibot: Image bert pre-training with online tokenizer\",\n",
       "      \"authors\": \"J Zhou, C Wei, H Wang, W Shen, C Xie, A Yuille\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 372,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=6668235945473015803&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Are transformers more robust than cnns?\",\n",
       "      \"id\": \"2316302132679082774\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper/2021/hash/e19347e1c3ca0c0b97de5fb3b690855a-Abstract.html\",\n",
       "      \"title\": \"Are transformers more robust than cnns?\",\n",
       "      \"authors\": \"Y Bai, J Mei, AL Yuille, C Xie\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 167,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=2316302132679082774&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation\",\n",
       "      \"id\": \"4388759310460601633\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Hoyer_DAFormer_Improving_Network_Architectures_and_Training_Strategies_for_Domain-Adaptive_Semantic_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation\",\n",
       "      \"authors\": \"L Hoyer, D Dai, L Van Gool\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 177,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=4388759310460601633&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Understanding the robustness in vision transformers\",\n",
       "      \"id\": \"3041067607452518927\",\n",
       "      \"url\": \"https://proceedings.mlr.press/v162/zhou22m.html\",\n",
       "      \"title\": \"Understanding the robustness in vision transformers\",\n",
       "      \"authors\": \"D Zhou, Z Yu, E Xie, C Xiao\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 82,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=3041067607452518927&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Partial success in closing the gap between human and machine vision\",\n",
       "      \"id\": \"875131557547078483\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper/2021/hash/c8877cff22082a16395a57e97232bb6f-Abstract.html\",\n",
       "      \"title\": \"Partial success in closing the gap between human and machine vision\",\n",
       "      \"authors\": \"R Geirhos, K Narayanappa, B Mitzkus\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 110,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=875131557547078483&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Efficient training of visual transformers with small datasets\",\n",
       "      \"id\": \"17891879498080154736\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper/2021/hash/c81e155d85dae5430a8cee6f2242e82c-Abstract.html\",\n",
       "      \"title\": \"Efficient training of visual transformers with small datasets\",\n",
       "      \"authors\": \"Y Liu, E Sangineto, W Bi, N Sebe\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 106,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=17891879498080154736&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Assaying out-of-distribution generalization in transfer learning\",\n",
       "      \"id\": \"2028336304446280911\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper_files/paper/2022/hash/2f5acc925919209370a3af4eac5cad4a-Abstract-Conference.html\",\n",
       "      \"title\": \"Assaying out-of-distribution generalization in transfer learning\",\n",
       "      \"authors\": \"F Wenzel, A Dittadi, P Gehler\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 29,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=2028336304446280911&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Ow-detr: Open-world detection transformer\",\n",
       "      \"id\": \"5601871542106060008\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Gupta_OW-DETR_Open-World_Detection_Transformer_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Ow-detr: Open-world detection transformer\",\n",
       "      \"authors\": \"A Gupta, S Narayan, KJ Joseph\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 67,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=5601871542106060008&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"ViT-YOLO: Transformer-based YOLO for object detection\",\n",
       "      \"id\": \"14672720829595281606\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Zhang_ViT-YOLOTransformer-Based_YOLO_for_Object_Detection_ICCVW_2021_paper.html\",\n",
       "      \"title\": \"ViT-YOLO: Transformer-based YOLO for object detection\",\n",
       "      \"authors\": \"Z Zhang, X Lu, G Cao, Y Yang\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 82,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=14672720829595281606&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Not all patches are what you need: Expediting vision transformers via token reorganizations\",\n",
       "      \"id\": \"13367059770507522630\",\n",
       "      \"url\": \"https://arxiv.org/abs/2202.07800\",\n",
       "      \"title\": \"Not all patches are what you need: Expediting vision transformers via token reorganizations\",\n",
       "      \"authors\": \"Y Liang, C Ge, Z Tong, Y Song, J Wang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 111,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=13367059770507522630&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Msft-yolo: Improved yolov5 based on transformer for detecting defects of steel surface\",\n",
       "      \"id\": \"4241401890946035983\",\n",
       "      \"url\": \"https://www.mdpi.com/1424-8220/22/9/3467\",\n",
       "      \"title\": \"Msft-yolo: Improved yolov5 based on transformer for detecting defects of steel surface\",\n",
       "      \"authors\": \"Z Guo, C Wang, G Yang, Z Huang, G Li\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 68,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=4241401890946035983&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives\",\n",
       "      \"id\": \"6243645967630982889\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S1361841523000233\",\n",
       "      \"title\": \"Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives\",\n",
       "      \"authors\": \"J Li, J Chen, Y Tang, C Wang, BA Landman\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 56,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=6243645967630982889&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Viewfool: Evaluating the robustness of visual recognition to adversarial viewpoints\",\n",
       "      \"id\": \"4486454263174539234\",\n",
       "      \"url\": \"https://proceedings.neurips.cc/paper_files/paper/2022/hash/eee7ae5cf0c4356c2aeca400771791aa-Abstract-Conference.html\",\n",
       "      \"title\": \"Viewfool: Evaluating the robustness of visual recognition to adversarial viewpoints\",\n",
       "      \"authors\": \"Y Dong, S Ruan, H Su, C Kang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 18,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=4486454263174539234&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Efficient training of audio transformers with patchout\",\n",
       "      \"id\": \"1157620601786084092\",\n",
       "      \"url\": \"https://arxiv.org/abs/2110.05069\",\n",
       "      \"title\": \"Efficient training of audio transformers with patchout\",\n",
       "      \"authors\": \"K Koutini, J Schl\\u00fcter, H Eghbal-Zadeh\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 106,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=1157620601786084092&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Panoptic segformer: Delving deeper into panoptic segmentation with transformers\",\n",
       "      \"id\": \"11135517644142739642\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Li_Panoptic_SegFormer_Delving_Deeper_Into_Panoptic_Segmentation_With_Transformers_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Panoptic segformer: Delving deeper into panoptic segmentation with transformers\",\n",
       "      \"authors\": \"Z Li, W Wang, E Xie, Z Yu\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 47,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=11135517644142739642&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Localizing objects with self-supervised transformers and no labels\",\n",
       "      \"id\": \"13429345883781912849\",\n",
       "      \"url\": \"https://arxiv.org/abs/2109.14279\",\n",
       "      \"title\": \"Localizing objects with self-supervised transformers and no labels\",\n",
       "      \"authors\": \"O Sim\\u00e9oni, G Puy, HV Vo, S Roburin, S Gidaris\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 92,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=13429345883781912849&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 4\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"A survey of modern deep learning based object detection models\",\n",
       "      \"id\": \"14311400318178337111\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S1051200422001312\",\n",
       "      \"title\": \"A survey of modern deep learning based object detection models\",\n",
       "      \"authors\": \"SSA Zaidi, MS Ansari, A Aslam, N Kanwal\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 425,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=14311400318178337111&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Deep learning methods for object detection in smart manufacturing: A survey\",\n",
       "      \"id\": \"11789051068432887660\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S0278612522001066\",\n",
       "      \"title\": \"Deep learning methods for object detection in smart manufacturing: A survey\",\n",
       "      \"authors\": \"HM Ahmad, A Rahimi\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 20,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=11789051068432887660&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Mammogram breast cancer CAD systems for mass detection and classification: a review\",\n",
       "      \"id\": \"15295068953900215294\",\n",
       "      \"url\": \"https://link.springer.com/article/10.1007/s11042-022-12332-1\",\n",
       "      \"title\": \"Mammogram breast cancer CAD systems for mass detection and classification: a review\",\n",
       "      \"authors\": \"NM Hassan, S Hamad, K Mahar\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 32,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=15295068953900215294&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Precise single-stage detector\",\n",
       "      \"id\": \"15549291371117213871\",\n",
       "      \"url\": \"https://arxiv.org/abs/2210.04252\",\n",
       "      \"title\": \"Precise single-stage detector\",\n",
       "      \"authors\": \"A Chandio, G Gui, T Kumar, I Ullah\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 41,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=15549291371117213871&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"CE-FPN: Enhancing channel information for object detection\",\n",
       "      \"id\": \"9884544045603971658\",\n",
       "      \"url\": \"https://link.springer.com/article/10.1007/s11042-022-11940-1\",\n",
       "      \"title\": \"CE-FPN: Enhancing channel information for object detection\",\n",
       "      \"authors\": \"Y Luo, X Cao, J Zhang, J Guo, H Shen, T Wang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 68,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=9884544045603971658&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"A survey of self-supervised and few-shot object detection\",\n",
       "      \"id\": \"9591435289724766439\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/9860087/\",\n",
       "      \"title\": \"A survey of self-supervised and few-shot object detection\",\n",
       "      \"authors\": \"G Huang, I Laradji, D Vazquez\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 42,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=9591435289724766439&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Feature split\\u2013merge\\u2013enhancement network for remote sensing object detection\",\n",
       "      \"id\": \"10628215061802232868\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/9673713/\",\n",
       "      \"title\": \"Feature split\\u2013merge\\u2013enhancement network for remote sensing object detection\",\n",
       "      \"authors\": \"W Ma, N Li, H Zhu, L Jiao, X Tang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 42,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=10628215061802232868&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Guiding pretraining in reinforcement learning with large language models\",\n",
       "      \"id\": \"9532821550175512200\",\n",
       "      \"url\": \"https://arxiv.org/abs/2302.06692\",\n",
       "      \"title\": \"Guiding pretraining in reinforcement learning with large language models\",\n",
       "      \"authors\": \"Y Du, O Watkins, Z Wang, C Colas, T Darrell\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 19,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=9532821550175512200&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Integrating deep learning-based iot and fog computing with software-defined networking for detecting weapons in video surveillance systems\",\n",
       "      \"id\": \"12985976504909847163\",\n",
       "      \"url\": \"https://www.mdpi.com/1424-8220/22/14/5075\",\n",
       "      \"title\": \"Integrating deep learning-based iot and fog computing with software-defined networking for detecting weapons in video surveillance systems\",\n",
       "      \"authors\": \"C Fathy, SN Saleh\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 18,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=12985976504909847163&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Backbones-review: Feature extraction networks for deep learning and deep reinforcement learning approaches\",\n",
       "      \"id\": \"2812153438552156646\",\n",
       "      \"url\": \"https://arxiv.org/abs/2206.08016\",\n",
       "      \"title\": \"Backbones-review: Feature extraction networks for deep learning and deep reinforcement learning approaches\",\n",
       "      \"authors\": \"O Elharrouss, Y Akbari, N Almaadeed\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 28,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=2812153438552156646&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Small-object detection based on YOLOv5 in autonomous driving systems\",\n",
       "      \"id\": \"14349697475909471320\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S0167865523000727\",\n",
       "      \"title\": \"Small-object detection based on YOLOv5 in autonomous driving systems\",\n",
       "      \"authors\": \"B Mahaur, KK Mishra\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 14,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=14349697475909471320&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Single-stage uav detection and classification with yolov5: Mosaic data augmentation and panet\",\n",
       "      \"id\": \"18351357225093508985\",\n",
       "      \"url\": \"https://ieeexplore.ieee.org/abstract/document/9663841/\",\n",
       "      \"title\": \"Single-stage uav detection and classification with yolov5: Mosaic data augmentation and panet\",\n",
       "      \"authors\": \"F Dadboud, V Patel, V Mehta, M Bolic\\u2026\",\n",
       "      \"year\": \"2021\",\n",
       "      \"cited_by\": 25,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=18351357225093508985&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"FPGA-based accelerator for object detection: A comprehensive survey\",\n",
       "      \"id\": \"11021564807628327569\",\n",
       "      \"url\": \"https://link.springer.com/article/10.1007/s11227-022-04415-5\",\n",
       "      \"title\": \"FPGA-based accelerator for object detection: A comprehensive survey\",\n",
       "      \"authors\": \"K Zeng, Q Ma, JW Wu, Z Chen, T Shen\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 14,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=11021564807628327569&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Rail wheel tread defect detection using improved YOLOv3\",\n",
       "      \"id\": \"4655434626952320958\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S0263224122011551\",\n",
       "      \"title\": \"Rail wheel tread defect detection using improved YOLOv3\",\n",
       "      \"authors\": \"Z Xing, Z Zhang, X Yao, Y Qin, L Jia\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 11,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=4655434626952320958&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Development of a Low-Power IoMT Portable Pillbox for Medication Adherence Improvement and Remote Treatment Adjustment\",\n",
       "      \"id\": \"16079747206913991174\",\n",
       "      \"url\": \"https://www.mdpi.com/1424-8220/22/15/5818\",\n",
       "      \"title\": \"Development of a Low-Power IoMT Portable Pillbox for Medication Adherence Improvement and Remote Treatment Adjustment\",\n",
       "      \"authors\": \"D Karagiannis, K Mitsis, KS Nikita\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 8,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=16079747206913991174&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"A comprehensive review of object detection with deep learning\",\n",
       "      \"id\": \"16432688928939217038\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S1051200422004298\",\n",
       "      \"title\": \"A comprehensive review of object detection with deep learning\",\n",
       "      \"authors\": \"R Kaur, S Singh\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 11,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=16432688928939217038&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications\",\n",
       "      \"id\": \"6456248456066525600\",\n",
       "      \"url\": \"https://link.springer.com/article/10.1186/s40537-023-00727-2\",\n",
       "      \"title\": \"A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications\",\n",
       "      \"authors\": \"L Alzubaidi, J Bai, A Al-Sabaawi, J Santamar\\u00eda\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 25,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=6456248456066525600&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Video surveillance using deep transfer learning and deep domain adaptation: Towards better generalization\",\n",
       "      \"id\": \"8123745338600640152\",\n",
       "      \"url\": \"https://www.sciencedirect.com/science/article/pii/S0952197622006881\",\n",
       "      \"title\": \"Video surveillance using deep transfer learning and deep domain adaptation: Towards better generalization\",\n",
       "      \"authors\": \"Y Himeur, S Al-Maadeed, H Kheddar\\u2026\",\n",
       "      \"year\": \"2023\",\n",
       "      \"cited_by\": 18,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=8123745338600640152&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Review of recent automated pothole-detection methods\",\n",
       "      \"id\": \"10334732198267289555\",\n",
       "      \"url\": \"https://www.mdpi.com/2076-3417/12/11/5320\",\n",
       "      \"title\": \"Review of recent automated pothole-detection methods\",\n",
       "      \"authors\": \"YM Kim, YG Kim, SY Son, SY Lim, BY Choi, DH Choi\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 15,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=10334732198267289555&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Towards domain generalization in object detection\",\n",
       "      \"id\": \"5740479134802475221\",\n",
       "      \"url\": \"https://arxiv.org/abs/2203.14387\",\n",
       "      \"title\": \"Towards domain generalization in object detection\",\n",
       "      \"authors\": \"X Zhang, Z Xu, R Xu, J Liu, P Cui, W Wan\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 15,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=5740479134802475221&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Allergen30: detecting food items with possible allergens using deep learning-based computer vision\",\n",
       "      \"id\": \"6447041552955227244\",\n",
       "      \"url\": \"https://link.springer.com/article/10.1007/s12161-022-02353-9\",\n",
       "      \"title\": \"Allergen30: detecting food items with possible allergens using deep learning-based computer vision\",\n",
       "      \"authors\": \"M Mishra, T Sarkar, T Choudhury, N Bansal\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 7,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=6447041552955227244&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 1\n",
       "    },\n",
       "    {\n",
       "      \"label\": \"Point-bert: Pre-training 3d point cloud transformers with masked point modeling\",\n",
       "      \"id\": \"17327663970405370182\",\n",
       "      \"url\": \"http://openaccess.thecvf.com/content/CVPR2022/html/Yu_Point-BERT_Pre-Training_3D_Point_Cloud_Transformers_With_Masked_Point_Modeling_CVPR_2022_paper.html\",\n",
       "      \"title\": \"Point-bert: Pre-training 3d point cloud transformers with masked point modeling\",\n",
       "      \"authors\": \"X Yu, L Tang, Y Rao, T Huang\\u2026\",\n",
       "      \"year\": \"2022\",\n",
       "      \"cited_by\": 215,\n",
       "      \"cited_by_url\": \"https://scholar.google.com/scholar?cites=17327663970405370182&as_sdt=40005&sciodt=0,10&hl=en\",\n",
       "      \"modularity\": 7\n",
       "    }\n",
       "  ],\n",
       "  \"links\": [\n",
       "    {\n",
       "      \"source\": 0,\n",
       "      \"target\": 1\n",
       "    },\n",
       "    {\n",
       "      \"source\": 0,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 0,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 0,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 0,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 2,\n",
       "      \"target\": 0\n",
       "    },\n",
       "    {\n",
       "      \"source\": 2,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 3,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 3,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 3,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 4,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 5,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 5,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 6,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 7,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 8,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 9,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 10,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 10,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 11,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 12,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 13,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 14,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 15,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 16,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 17,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 18,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 19,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 20,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 21,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 22,\n",
       "      \"target\": 2\n",
       "    },\n",
       "    {\n",
       "      \"source\": 23,\n",
       "      \"target\": 0\n",
       "    },\n",
       "    {\n",
       "      \"source\": 24,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 25,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 26,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 26,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 27,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 28,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 29,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 30,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 31,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 31,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 32,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 33,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 34,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 35,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 36,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 37,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 38,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 39,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 40,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 41,\n",
       "      \"target\": 23\n",
       "    },\n",
       "    {\n",
       "      \"source\": 42,\n",
       "      \"target\": 0\n",
       "    },\n",
       "    {\n",
       "      \"source\": 42,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 43,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 44,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 45,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 46,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 47,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 48,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 49,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 50,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 51,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 52,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 52,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 53,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 53,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 54,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 55,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 56,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 57,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 58,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 58,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 59,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 59,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 60,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 60,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 61,\n",
       "      \"target\": 42\n",
       "    },\n",
       "    {\n",
       "      \"source\": 62,\n",
       "      \"target\": 0\n",
       "    },\n",
       "    {\n",
       "      \"source\": 63,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 64,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 65,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 66,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 67,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 68,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 69,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 70,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 71,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 72,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 73,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 74,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 75,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 76,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 77,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 78,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 79,\n",
       "      \"target\": 62\n",
       "    },\n",
       "    {\n",
       "      \"source\": 80,\n",
       "      \"target\": 0\n",
       "    },\n",
       "    {\n",
       "      \"source\": 81,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 82,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 83,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 84,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 85,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 86,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 87,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 88,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 89,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 90,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 91,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 92,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 93,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 94,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 95,\n",
       "      \"target\": 80\n",
       "    },\n",
       "    {\n",
       "      \"source\": 96,\n",
       "      \"target\": 0\n",
       "    },\n",
       "    {\n",
       "      \"source\": 97,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 98,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 99,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 100,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 101,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 102,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 103,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 104,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 105,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 106,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 107,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 107,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 108,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 109,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 110,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 111,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 112,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 113,\n",
       "      \"target\": 96\n",
       "    },\n",
       "    {\n",
       "      \"source\": 114,\n",
       "      \"target\": 0\n",
       "    },\n",
       "    {\n",
       "      \"source\": 115,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 116,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 117,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 118,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 119,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 120,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 121,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 122,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 123,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 124,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 125,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 126,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 127,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 128,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 129,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 130,\n",
       "      \"target\": 114\n",
       "    },\n",
       "    {\n",
       "      \"source\": 131,\n",
       "      \"target\": 0\n",
       "    },\n",
       "    {\n",
       "      \"source\": 132,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 133,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 134,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 135,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 136,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 137,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 138,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 139,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 140,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 141,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 142,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 143,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 144,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 145,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 146,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 147,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 148,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 149,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 150,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 151,\n",
       "      \"target\": 131\n",
       "    },\n",
       "    {\n",
       "      \"source\": 152,\n",
       "      \"target\": 0\n",
       "    }\n",
       "  ]\n",
       "};\n",
       "      var w = window.innerWidth;\n",
       "      var h = window.innerHeight;\n",
       "\n",
       "      var focusNode = null;\n",
       "      var highlightNode = null;\n",
       "\n",
       "      var textCenter = false;\n",
       "      var outline = false;\n",
       "\n",
       "      var minScore = Math.min(...graph.nodes.map(n => n.modularity));\n",
       "      var maxScore = Math.max(...graph.nodes.map(n => n.modularity));\n",
       "\n",
       "      var color = d3.scale\n",
       "        .linear()\n",
       "        .domain([\n",
       "          minScore,\n",
       "          (minScore + maxScore) / 4,\n",
       "          (minScore + maxScore) / 2,\n",
       "          ((minScore + maxScore) * 3) / 4,\n",
       "          maxScore,\n",
       "        ])\n",
       "        .range([\"lime\", \"yellow\", \"red\", \"deepskyblue\"]);\n",
       "\n",
       "      var highlightColor = \"blue\";\n",
       "      var highlightTrans = 0.1;\n",
       "\n",
       "      const citedBy = graph.nodes\n",
       "        .map(n => n.cited_by)\n",
       "        .filter(n => n != null)\n",
       "\n",
       "      const maxCitedBy = Math.max(...citedBy)\n",
       "      const minCitedBy = Math.min(...citedBy)\n",
       "\n",
       "      var size = d3.scale\n",
       "        .pow()\n",
       "        .exponent(1)\n",
       "        .domain([minCitedBy, maxCitedBy])\n",
       "        .range([8, 24]);\n",
       "\n",
       "      var force = d3.layout\n",
       "        .force()\n",
       "        .linkDistance(h / (graph.nodes.length / 10))\n",
       "        .charge(-300)\n",
       "        .size([w, h]);\n",
       "\n",
       "      var defaultNodeColor = \"#ccc\";\n",
       "      var defaultLinkColor = \"#888\";\n",
       "      var nominalBaseNodeSize = 8;\n",
       "      var nominalTextSize = 10;\n",
       "      var maxTextSize = 24;\n",
       "      var nominalStroke = 1.5;\n",
       "      var maxStroke = 4.5;\n",
       "      var maxBaseNodeSize = 36;\n",
       "      var minZoom = 0.1;\n",
       "      var maxZoom = 7;\n",
       "      var svg = d3.select(\"body\").append(\"svg\");\n",
       "      var zoom = d3.behavior.zoom().scaleExtent([minZoom, maxZoom]);\n",
       "      var g = svg.append(\"g\");\n",
       "      svg.style(\"cursor\", \"move\");\n",
       "\n",
       "      var linkedByIndex = {};\n",
       "      graph.links.forEach(function (d) {\n",
       "        linkedByIndex[d.source + \",\" + d.target] = true;\n",
       "      });\n",
       "\n",
       "      function isConnected(a, b) {\n",
       "        return (\n",
       "          linkedByIndex[a.index + \",\" + b.index] ||\n",
       "          linkedByIndex[b.index + \",\" + a.index] ||\n",
       "          a.index == b.index\n",
       "        );\n",
       "      }\n",
       "\n",
       "      force.size([w, h]);\n",
       "\n",
       "      force\n",
       "        .nodes(graph.nodes)\n",
       "        .links(graph.links)\n",
       "        .start();\n",
       "\n",
       "      function getLine(data) {\n",
       "\n",
       "        const x1 = data.source.x;\n",
       "        const y1= data.source.y;\n",
       "        const x2 = data.target.x;\n",
       "        const y2 = data.target.y;\n",
       "\n",
       "        const r = size(data.target.cited_by) + 1;\n",
       "\n",
       "        const m = (y2 - y1) / (x2 - x1);\n",
       "        const b = y1 - m * x1;\n",
       "\n",
       "        const c = Math.sqrt(Math.pow((y2 - y1), 2) + Math.pow((x2 - x1), 2))\n",
       "        const a = y2 - y1\n",
       "        const cos = a / c\n",
       "\n",
       "        const a2 = cos * r\n",
       "        const b2 = Math.sqrt(Math.pow(r, 2) - Math.pow(a2, 2))\n",
       "\n",
       "        const x = x2 > x1 ? x2 - b2 : x2 + b2;\n",
       "        const y = y2 - a2;\n",
       "\n",
       "        const path = 'M ' + data.source.x + ',' + data.source.y + ' L ' + x + ',' + y;\n",
       "        return path;\n",
       "      }\n",
       "\n",
       "      var link = g\n",
       "        .selectAll(\".link\")\n",
       "        .data(graph.links)\n",
       "        .enter()\n",
       "        .append(\"svg:path\")\n",
       "        .attr(\"d\", getLine) \n",
       "        .attr(\"stroke\", defaultLinkColor)\n",
       "        .attr(\"fill\", \"red\")\n",
       "        .style(\"stroke-width\", nominalStroke)\n",
       "        .style(\"marker-end\", \"url(#end)\")\n",
       "\n",
       "      var node = g\n",
       "        .selectAll(\".node\")\n",
       "        .data(graph.nodes)\n",
       "        .enter()\n",
       "        .append(\"g\")\n",
       "        .attr(\"class\", \"node\")\n",
       "        .call(force.drag);\n",
       "\n",
       "      var timeout = null;\n",
       "\n",
       "      node.on(\"dblclick\", function (d) {\n",
       "        clearTimeout(timeout);\n",
       "\n",
       "        timeout = setTimeout(function () {\n",
       "          window.open(d.url, \"_blank\");\n",
       "          d3.event.stopPropagation();\n",
       "        }, 300);\n",
       "      });\n",
       "\n",
       "      var tocolor = \"fill\";\n",
       "      var towhite = \"stroke\";\n",
       "      if (outline) {\n",
       "        tocolor = \"stroke\";\n",
       "        towhite = \"fill\";\n",
       "      }\n",
       "\n",
       "      var circle = node\n",
       "        .append(\"path\")\n",
       "        .attr(\n",
       "          \"d\",\n",
       "          d3.svg\n",
       "            .symbol()\n",
       "            .size(function (d) {\n",
       "              return (\n",
       "                Math.PI * Math.pow(size(d.cited_by) || nominalBaseNodeSize, 2)\n",
       "              );\n",
       "            })\n",
       "            .type(function (d) {\n",
       "              return d.type;\n",
       "            })\n",
       "        )\n",
       "        .style(tocolor, function (d) {\n",
       "          if (isNumber(d.modularity) && d.modularity >= 0) return color(d.modularity);\n",
       "          else return defaultNodeColor;\n",
       "        })\n",
       "        .style(\"stroke-width\", nominalStroke)\n",
       "        .style(towhite, \"white\");\n",
       "\n",
       "      svg.append(\"svg:defs\").selectAll(\"marker\")\n",
       "\t  .data([\"end\"])\n",
       "\t.enter().append(\"svg:marker\")\n",
       "\t  .attr(\"id\", String)\n",
       "\t  .attr(\"viewBox\", \"0 -5 10 10\")\n",
       "\t  .attr(\"refX\", 10)\n",
       "\t  .attr(\"refY\", 0)\n",
       "\t  .attr(\"markerWidth\", 6)\n",
       "\t  .attr(\"markerHeight\", 6)\n",
       "\t  .attr(\"orient\", \"auto\")\n",
       "          .style(\"fill\", defaultLinkColor)\n",
       "\t.append(\"svg:path\")\n",
       "\t  .attr(\"d\", \"M 0,-5 L 10,0 L 0,5\")\n",
       "          .style(\"stroke\", defaultLinkColor);\n",
       "\n",
       "      var text = g\n",
       "        .selectAll(\".text\")\n",
       "        .data(graph.nodes)\n",
       "        .enter()\n",
       "        .append(\"text\")\n",
       "        .attr(\"dy\", \".35em\")\n",
       "        .style(\"font-size\", nominalTextSize + \"px\");\n",
       "\n",
       "      node\n",
       "        .on(\"mouseover\", function (d) {\n",
       "          setHighlight(d);\n",
       "        })\n",
       "        .on(\"mousedown\", function (d) {\n",
       "          d3.event.stopPropagation();\n",
       "          focusNode = d;\n",
       "          setFocus(d);\n",
       "          if (highlightNode === null) setHighlight(d);\n",
       "        })\n",
       "        .on(\"mouseout\", function (d) {\n",
       "          exitHighlight();\n",
       "        });\n",
       "\n",
       "      d3.select(window).on(\"mouseup\", function () {\n",
       "        if (focusNode !== null) {\n",
       "          focusNode = null;\n",
       "          if (highlightTrans < 1) {\n",
       "            circle.style(\"opacity\", 1);\n",
       "            text.style(\"opacity\", 1);\n",
       "            link.style(\"opacity\", 1);\n",
       "          }\n",
       "        }\n",
       "\n",
       "        if (highlightNode === null) exitHighlight();\n",
       "      });\n",
       "\n",
       "      function exitHighlight() {\n",
       "        highlightNode = null;\n",
       "        if (focusNode === null) {\n",
       "          svg.style(\"cursor\", \"move\");\n",
       "          if (highlightColor != \"white\") {\n",
       "            circle.style(towhite, \"white\");\n",
       "            text.text('')\n",
       "            link.style(\"stroke\", function (o) {\n",
       "              return isNumber(o.score) && o.score >= 0\n",
       "                ? color(o.score)\n",
       "                : defaultLinkColor;\n",
       "            });\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "\n",
       "      function setFocus(d) {\n",
       "        if (highlightTrans < 1) {\n",
       "          circle.style(\"opacity\", function (o) {\n",
       "            return isConnected(d, o) ? 1 : highlightTrans;\n",
       "          });\n",
       "\n",
       "          text.style(\"opacity\", function (o) {\n",
       "            return isConnected(d, o) ? 1 : highlightTrans;\n",
       "          });\n",
       "\n",
       "          link.style(\"opacity\", function (o) {\n",
       "            return o.source.index == d.index || o.target.index == d.index\n",
       "              ? 1\n",
       "              : highlightTrans;\n",
       "          });\n",
       "        }\n",
       "      }\n",
       "\n",
       "      function setHighlight(d) {\n",
       "        svg.style(\"cursor\", \"pointer\");\n",
       "        if (focusNode !== null) d = focusNode;\n",
       "        highlightNode = d;\n",
       "\n",
       "        if (highlightColor != \"white\") {\n",
       "\n",
       "          circle.style(towhite, function (o) {\n",
       "            return isConnected(d, o) ? highlightColor : \"white\";\n",
       "          });\n",
       "          \n",
       "          text.attr(\"dx\", function (d) {\n",
       "            return size(d.cited_by)\n",
       "          });\n",
       "\n",
       "          text.text(function (o) {\n",
       "            if (isConnected(d, o)) {\n",
       "              let title = o.title;\n",
       "              if (o.year) title = title + \" (\" + o.year + \")\";\n",
       "              if (o.authors) title = title + \" - \" + o.authors;\n",
       "              return title\n",
       "            } else {\n",
       "              return \"\"\n",
       "            }\n",
       "          });\n",
       "\n",
       "        }\n",
       "      }\n",
       "\n",
       "      zoom.on(\"zoom\", function () {\n",
       "        var stroke = nominalStroke;\n",
       "        if (nominalStroke * zoom.scale() > maxStroke)\n",
       "          stroke = maxStroke / zoom.scale();\n",
       "        link.style(\"stroke-width\", stroke);\n",
       "        circle.style(\"stroke-width\", stroke);\n",
       "\n",
       "        var baseRadius = nominalBaseNodeSize;\n",
       "        if (nominalBaseNodeSize * zoom.scale() > maxBaseNodeSize)\n",
       "          baseRadius = maxBaseNodeSize / zoom.scale();\n",
       "        circle.attr(\n",
       "          \"d\",\n",
       "          d3.svg\n",
       "            .symbol()\n",
       "            .size(function (d) {\n",
       "              return (\n",
       "                Math.PI *\n",
       "                Math.pow(\n",
       "                  (size(d.cited_by) * baseRadius) / nominalBaseNodeSize ||\n",
       "                    baseRadius,\n",
       "                  2\n",
       "                )\n",
       "              );\n",
       "            })\n",
       "        );\n",
       "\n",
       "        if (!textCenter)\n",
       "          text.attr(\"dx\", function (d) {\n",
       "            return (\n",
       "              (size(d.cited_by) * baseRadius) / nominalBaseNodeSize ||\n",
       "              baseRadius\n",
       "            );\n",
       "          });\n",
       "\n",
       "        var textSize = nominalTextSize;\n",
       "        if (nominalTextSize * zoom.scale() > maxTextSize)\n",
       "          textSize = maxTextSize / zoom.scale();\n",
       "        text.style(\"font-size\", textSize + \"px\");\n",
       "\n",
       "        g.attr(\n",
       "          \"transform\",\n",
       "          \"translate(\" + d3.event.translate + \")scale(\" + d3.event.scale + \")\"\n",
       "        );\n",
       "      });\n",
       "\n",
       "      svg.call(zoom);\n",
       "\n",
       "      resize();\n",
       "      d3.select(window).on(\"resize\", resize);\n",
       "\n",
       "      force.on(\"tick\", function () {\n",
       "        node.attr(\"transform\", function (d) {\n",
       "          return \"translate(\" + d.x + \",\" + d.y + \")\";\n",
       "        });\n",
       "        text.attr(\"transform\", function (d) {\n",
       "          return \"translate(\" + d.x + \",\" + d.y + \")\";\n",
       "        });\n",
       "\n",
       "        link.attr(\"d\", getLine)\n",
       "\n",
       "        node\n",
       "          .attr(\"cx\", function (d) {\n",
       "            return d.x;\n",
       "          })\n",
       "          .attr(\"cy\", function (d) {\n",
       "            return d.y;\n",
       "          });\n",
       "      });\n",
       "\n",
       "      function resize() {\n",
       "        var width = window.innerWidth,\n",
       "          height = window.innerHeight;\n",
       "        svg.attr(\"width\", width).attr(\"height\", height);\n",
       "\n",
       "        force\n",
       "          .size([\n",
       "            force.size()[0] + (width - w) / zoom.scale(),\n",
       "            force.size()[1] + (height - h) / zoom.scale(),\n",
       "          ])\n",
       "          .resume();\n",
       "        w = width;\n",
       "        h = height;\n",
       "      }\n",
       "\n",
       "      function isNumber(n) {\n",
       "        return !isNaN(parseFloat(n)) && isFinite(n);\n",
       "      }\n",
       "\n",
       "    </script>\n",
       "  </body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 4: Load and view the HTML output within the notebook\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "with open(\"output.html\", \"r\") as f:\n",
    "    content = f.read()\n",
    "    display(HTML(content))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a2b32b51519e300773af616676efd90f79de773c4c7227e31450943f683ca7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
