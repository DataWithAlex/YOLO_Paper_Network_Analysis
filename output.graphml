<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d8" for="node" attr.name="modularity" attr.type="long"/>
<key id="d7" for="node" attr.name="cited_by_url" attr.type="string"/>
<key id="d6" for="node" attr.name="cited_by" attr.type="long"/>
<key id="d5" for="node" attr.name="year" attr.type="string"/>
<key id="d4" for="node" attr.name="authors" attr.type="string"/>
<key id="d3" for="node" attr.name="title" attr.type="string"/>
<key id="d2" for="node" attr.name="url" attr.type="string"/>
<key id="d1" for="node" attr.name="id" attr.type="string"/>
<key id="d0" for="node" attr.name="label" attr.type="string"/>
<graph edgedefault="directed"><node id="7522504961268153944">
  <data key="d0">Transformers in vision: A survey</data>
  <data key="d1">7522504961268153944</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3505244</data>
  <data key="d3">Transformers in vision: A survey</data>
  <data key="d4">S Khan, M Naseer, M Hayat, SW Zamir…</data>
  <data key="d5">2022</data>
  <data key="d6">1327</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7522504961268153944&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="6382612685700818764">
  <data key="d0">You only look once: Unified, real-time object detection</data>
  <data key="d1">6382612685700818764</data>
  <data key="d3">You only look once: Unified, real-time object detection</data>
  <data key="d6">39552</data>
  <data key="d8">0</data>
</node>
<node id="15456065911372617945">
  <data key="d0">Attention mechanisms in computer vision: A survey</data>
  <data key="d1">15456065911372617945</data>
  <data key="d2">https://link.springer.com/article/10.1007/s41095-022-0271-y</data>
  <data key="d3">Attention mechanisms in computer vision: A survey</data>
  <data key="d4">MH Guo, TX Xu, JJ Liu, ZN Liu, PT Jiang, TJ Mu…</data>
  <data key="d5">2022</data>
  <data key="d6">644</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15456065911372617945&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="7749897961068121501">
  <data key="d0">A survey of transformers</data>
  <data key="d1">7749897961068121501</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2666651022000146</data>
  <data key="d3">A survey of transformers</data>
  <data key="d4">T Lin, Y Wang, X Liu, X Qiu</data>
  <data key="d5">2022</data>
  <data key="d6">494</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7749897961068121501&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="16431204865977056518">
  <data key="d0">Restormer: Efficient transformer for high-resolution image restoration</data>
  <data key="d1">16431204865977056518</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zamir_Restormer_Efficient_Transformer_for_High-Resolution_Image_Restoration_CVPR_2022_paper.html</data>
  <data key="d3">Restormer: Efficient transformer for high-resolution image restoration</data>
  <data key="d4">SW Zamir, A Arora, S Khan, M Hayat…</data>
  <data key="d5">2022</data>
  <data key="d6">742</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16431204865977056518&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="10110104334022835757">
  <data key="d0">Coatnet: Marrying convolution and attention for all data sizes</data>
  <data key="d1">10110104334022835757</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/20568692db622456cc42a2e853ca21f8-Abstract.html</data>
  <data key="d3">Coatnet: Marrying convolution and attention for all data sizes</data>
  <data key="d4">Z Dai, H Liu, QV Le, M Tan</data>
  <data key="d5">2021</data>
  <data key="d6">736</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10110104334022835757&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="14988305211504802629">
  <data key="d0">Multi-stage progressive image restoration</data>
  <data key="d1">14988305211504802629</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Zamir_Multi-Stage_Progressive_Image_Restoration_CVPR_2021_paper.html</data>
  <data key="d3">Multi-stage progressive image restoration</data>
  <data key="d4">SW Zamir, A Arora, S Khan, M Hayat…</data>
  <data key="d5">2021</data>
  <data key="d6">906</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14988305211504802629&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="15635397108812213817">
  <data key="d0">Transreid: Transformer-based object re-identification</data>
  <data key="d1">15635397108812213817</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/He_TransReID_Transformer-Based_Object_Re-Identification_ICCV_2021_paper.html</data>
  <data key="d3">Transreid: Transformer-based object re-identification</data>
  <data key="d4">S He, H Luo, P Wang, F Wang, H Li…</data>
  <data key="d5">2021</data>
  <data key="d6">506</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15635397108812213817&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="15172072370662904150">
  <data key="d0">Intriguing properties of vision transformers</data>
  <data key="d1">15172072370662904150</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html</data>
  <data key="d3">Intriguing properties of vision transformers</data>
  <data key="d4">MM Naseer, K Ranasinghe, SH Khan…</data>
  <data key="d5">2021</data>
  <data key="d6">377</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15172072370662904150&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="14311400318178337111">
  <data key="d0">A survey of modern deep learning based object detection models</data>
  <data key="d1">14311400318178337111</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1051200422001312</data>
  <data key="d3">A survey of modern deep learning based object detection models</data>
  <data key="d4">SSA Zaidi, MS Ansari, A Aslam, N Kanwal…</data>
  <data key="d5">2022</data>
  <data key="d6">445</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14311400318178337111&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="17327663970405370182">
  <data key="d0">Point-bert: Pre-training 3d point cloud transformers with masked point modeling</data>
  <data key="d1">17327663970405370182</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Yu_Point-BERT_Pre-Training_3D_Point_Cloud_Transformers_With_Masked_Point_Modeling_CVPR_2022_paper.html</data>
  <data key="d3">Point-bert: Pre-training 3d point cloud transformers with masked point modeling</data>
  <data key="d4">X Yu, L Tang, Y Rao, T Huang…</data>
  <data key="d5">2022</data>
  <data key="d6">227</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17327663970405370182&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="6784655767122395745">
  <data key="d0">Maxvit: Multi-axis vision transformer</data>
  <data key="d1">6784655767122395745</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20053-3_27</data>
  <data key="d3">Maxvit: Multi-axis vision transformer</data>
  <data key="d4">Z Tu, H Talebi, H Zhang, F Yang, P Milanfar…</data>
  <data key="d5">2022</data>
  <data key="d6">174</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6784655767122395745&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="10445431151558976110">
  <data key="d0">Dynamic neural networks: A survey</data>
  <data key="d1">10445431151558976110</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9560049/</data>
  <data key="d3">Dynamic neural networks: A survey</data>
  <data key="d4">Y Han, G Huang, S Song, L Yang…</data>
  <data key="d5">2021</data>
  <data key="d6">368</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10445431151558976110&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="17272204730668948165">
  <data key="d0">Focal self-attention for local-global interactions in vision transformers</data>
  <data key="d1">17272204730668948165</data>
  <data key="d2">https://arxiv.org/abs/2107.00641</data>
  <data key="d3">Focal self-attention for local-global interactions in vision transformers</data>
  <data key="d4">J Yang, C Li, P Zhang, X Dai, B Xiao, L Yuan…</data>
  <data key="d5">2021</data>
  <data key="d6">301</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17272204730668948165&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="4397240948250887492">
  <data key="d0">Threat of adversarial attacks on deep learning in computer vision: A survey</data>
  <data key="d1">4397240948250887492</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/8294186/</data>
  <data key="d3">Threat of adversarial attacks on deep learning in computer vision: A survey</data>
  <data key="d4">N Akhtar, A Mian</data>
  <data key="d5">2018</data>
  <data key="d6">1955</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4397240948250887492&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="982391967541643955">
  <data key="d0">Transformers in medical imaging: A survey</data>
  <data key="d1">982391967541643955</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841523000634</data>
  <data key="d3">Transformers in medical imaging: A survey</data>
  <data key="d4">F Shamshad, S Khan, SW Zamir, MH Khan…</data>
  <data key="d5">2023</data>
  <data key="d6">190</data>
  <data key="d7">https://scholar.google.com/scholar?cites=982391967541643955&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="2524690293389739454">
  <data key="d0">Understanding robustness of transformers for image classification</data>
  <data key="d1">2524690293389739454</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Bhojanapalli_Understanding_Robustness_of_Transformers_for_Image_Classification_ICCV_2021_paper.html</data>
  <data key="d3">Understanding robustness of transformers for image classification</data>
  <data key="d4">S Bhojanapalli, A Chakrabarti…</data>
  <data key="d5">2021</data>
  <data key="d6">255</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2524690293389739454&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="8226119387953108124">
  <data key="d0">Deep neural networks and tabular data: A survey</data>
  <data key="d1">8226119387953108124</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9998482/</data>
  <data key="d3">Deep neural networks and tabular data: A survey</data>
  <data key="d4">V Borisov, T Leemann, K Seßler, J Haug…</data>
  <data key="d5">2022</data>
  <data key="d6">256</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8226119387953108124&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="4109211049740650760">
  <data key="d0">3d human pose estimation with spatial and temporal transformers</data>
  <data key="d1">4109211049740650760</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Zheng_3D_Human_Pose_Estimation_With_Spatial_and_Temporal_Transformers_ICCV_2021_paper.html</data>
  <data key="d3">3d human pose estimation with spatial and temporal transformers</data>
  <data key="d4">C Zheng, S Zhu, M Mendieta, T Yang…</data>
  <data key="d5">2021</data>
  <data key="d6">276</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4109211049740650760&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="13405798017275933263">
  <data key="d0">A survey of human-in-the-loop for machine learning</data>
  <data key="d1">13405798017275933263</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0167739X22001790</data>
  <data key="d3">A survey of human-in-the-loop for machine learning</data>
  <data key="d4">X Wu, L Xiao, Y Sun, J Zhang, T Ma, L He</data>
  <data key="d5">2022</data>
  <data key="d6">221</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13405798017275933263&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="7207323120779432545">
  <data key="d0">Transformers in time series: A survey</data>
  <data key="d1">7207323120779432545</data>
  <data key="d2">https://arxiv.org/abs/2202.07125</data>
  <data key="d3">Transformers in time series: A survey</data>
  <data key="d4">Q Wen, T Zhou, C Zhang, W Chen, Z Ma, J Yan…</data>
  <data key="d5">2022</data>
  <data key="d6">223</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7207323120779432545&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="572712567324091714">
  <data key="d0">An attentive survey of attention models</data>
  <data key="d1">572712567324091714</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3465055</data>
  <data key="d3">An attentive survey of attention models</data>
  <data key="d4">S Chaudhari, V Mithal, G Polatkan…</data>
  <data key="d5">2021</data>
  <data key="d6">635</data>
  <data key="d7">https://scholar.google.com/scholar?cites=572712567324091714&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="12569071048746881465">
  <data key="d0">Autoformer: Searching transformers for visual recognition</data>
  <data key="d1">12569071048746881465</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Chen_AutoFormer_Searching_Transformers_for_Visual_Recognition_ICCV_2021_paper.html</data>
  <data key="d3">Autoformer: Searching transformers for visual recognition</data>
  <data key="d4">M Chen, H Peng, J Fu, H Ling</data>
  <data key="d5">2021</data>
  <data key="d6">159</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12569071048746881465&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="10894599314289744077">
  <data key="d0">Mesh graphormer</data>
  <data key="d1">10894599314289744077</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Lin_Mesh_Graphormer_ICCV_2021_paper.html</data>
  <data key="d3">Mesh graphormer</data>
  <data key="d4">K Lin, L Wang, Z Liu</data>
  <data key="d5">2021</data>
  <data key="d6">186</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10894599314289744077&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="13410997068826787348">
  <data key="d0">Deep learning-based human pose estimation: A survey</data>
  <data key="d1">13410997068826787348</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3603618</data>
  <data key="d3">Deep learning-based human pose estimation: A survey</data>
  <data key="d4">C Zheng, W Wu, C Chen, T Yang, S Zhu, J Shen…</data>
  <data key="d5">2023</data>
  <data key="d6">179</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13410997068826787348&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="8678112700060654133">
  <data key="d0">A generalist framework for panoptic segmentation of images and videos</data>
  <data key="d1">8678112700060654133</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Chen_A_Generalist_Framework_for_Panoptic_Segmentation_of_Images_and_Videos_ICCV_2023_paper.html</data>
  <data key="d3">A generalist framework for panoptic segmentation of images and videos</data>
  <data key="d4">T Chen, L Li, S Saxena, G Hinton…</data>
  <data key="d5">2023</data>
  <data key="d6">27</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8678112700060654133&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="14580693445961229120">
  <data key="d0">Styleswin: Transformer-based gan for high-resolution image generation</data>
  <data key="d1">14580693445961229120</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_StyleSwin_Transformer-Based_GAN_for_High-Resolution_Image_Generation_CVPR_2022_paper.html</data>
  <data key="d3">Styleswin: Transformer-based gan for high-resolution image generation</data>
  <data key="d4">B Zhang, S Gu, B Zhang, J Bao…</data>
  <data key="d5">2022</data>
  <data key="d6">100</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14580693445961229120&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="8840944912676876934">
  <data key="d0">Focal attention for long-range interactions in vision transformers</data>
  <data key="d1">8840944912676876934</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/fc1a36821b02abbd2503fd949bfc9131-Abstract.html</data>
  <data key="d3">Focal attention for long-range interactions in vision transformers</data>
  <data key="d4">J Yang, C Li, P Zhang, X Dai, B Xiao…</data>
  <data key="d5">2021</data>
  <data key="d6">75</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8840944912676876934&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="9207086502844896554">
  <data key="d0">Multi-class token transformer for weakly supervised semantic segmentation</data>
  <data key="d1">9207086502844896554</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Xu_Multi-Class_Token_Transformer_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2022_paper.html</data>
  <data key="d3">Multi-class token transformer for weakly supervised semantic segmentation</data>
  <data key="d4">L Xu, W Ouyang, M Bennamoun…</data>
  <data key="d5">2022</data>
  <data key="d6">77</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9207086502844896554&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="10761248177036470713">
  <data key="d0">Multimodal learning with transformers: A survey</data>
  <data key="d1">10761248177036470713</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10123038/</data>
  <data key="d3">Multimodal learning with transformers: A survey</data>
  <data key="d4">P Xu, X Zhu, DA Clifton</data>
  <data key="d5">2023</data>
  <data key="d6">114</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10761248177036470713&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="7877485587962972033">
  <data key="d0">Deep contextual video compression</data>
  <data key="d1">7877485587962972033</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/96b250a90d3cf0868c83f8c965142d2a-Abstract.html</data>
  <data key="d3">Deep contextual video compression</data>
  <data key="d4">J Li, B Li, Y Lu</data>
  <data key="d5">2021</data>
  <data key="d6">103</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7877485587962972033&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="10471521790809284341">
  <data key="d0">Curriculum learning: A survey</data>
  <data key="d1">10471521790809284341</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11263-022-01611-x</data>
  <data key="d3">Curriculum learning: A survey</data>
  <data key="d4">P Soviany, RT Ionescu, P Rota, N Sebe</data>
  <data key="d5">2022</data>
  <data key="d6">167</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10471521790809284341&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="12867511582517934835">
  <data key="d0">Focal modulation networks</data>
  <data key="d1">12867511582517934835</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/1b08f585b0171b74d1401a5195e986f1-Abstract-Conference.html</data>
  <data key="d3">Focal modulation networks</data>
  <data key="d4">J Yang, C Li, X Dai, J Gao</data>
  <data key="d5">2022</data>
  <data key="d6">68</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12867511582517934835&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="2898208138417725333">
  <data key="d0">Human action recognition from various data modalities: A review</data>
  <data key="d1">2898208138417725333</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9795869/</data>
  <data key="d3">Human action recognition from various data modalities: A review</data>
  <data key="d4">Z Sun, Q Ke, H Rahmani, M Bennamoun…</data>
  <data key="d5">2022</data>
  <data key="d6">183</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2898208138417725333&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="5446507817827610731">
  <data key="d0">Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis</data>
  <data key="d1">5446507817827610731</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/1cdf14d1e3699d61d237cf76ce1c2dca-Abstract.html</data>
  <data key="d3">Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis</data>
  <data key="d4">P Esser, R Rombach, A Blattmann…</data>
  <data key="d5">2021</data>
  <data key="d6">84</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5446507817827610731&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="14136709172791920331">
  <data key="d0">A survey of visual transformers</data>
  <data key="d1">14136709172791920331</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10088164/</data>
  <data key="d3">A survey of visual transformers</data>
  <data key="d4">Y Liu, Y Zhang, Y Wang, F Hou, J Yuan…</data>
  <data key="d5">2023</data>
  <data key="d6">110</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14136709172791920331&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="568699136349469348">
  <data key="d0">Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations</data>
  <data key="d1">568699136349469348</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Sajjadi_Scene_Representation_Transformer_Geometry-Free_Novel_View_Synthesis_Through_Set-Latent_Scene_CVPR_2022_paper.html</data>
  <data key="d3">Scene representation transformer: Geometry-free novel view synthesis through set-latent scene representations</data>
  <data key="d4">MSM Sajjadi, H Meyer, E Pot…</data>
  <data key="d5">2022</data>
  <data key="d6">66</data>
  <data key="d7">https://scholar.google.com/scholar?cites=568699136349469348&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="4431578198915484435">
  <data key="d0">Ammus: A survey of transformer-based pretrained models in natural language processing</data>
  <data key="d1">4431578198915484435</data>
  <data key="d2">https://arxiv.org/abs/2108.05542</data>
  <data key="d3">Ammus: A survey of transformer-based pretrained models in natural language processing</data>
  <data key="d4">KS Kalyan, A Rajasekharan, S Sangeetha</data>
  <data key="d5">2021</data>
  <data key="d6">146</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4431578198915484435&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="9280402251304953591">
  <data key="d0">Simpleclick: Interactive image segmentation with simple vision transformers</data>
  <data key="d1">9280402251304953591</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Liu_SimpleClick_Interactive_Image_Segmentation_with_Simple_Vision_Transformers_ICCV_2023_paper.html</data>
  <data key="d3">Simpleclick: Interactive image segmentation with simple vision transformers</data>
  <data key="d4">Q Liu, Z Xu, G Bertasius…</data>
  <data key="d5">2023</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9280402251304953591&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="5601871542106060008">
  <data key="d0">Ow-detr: Open-world detection transformer</data>
  <data key="d1">5601871542106060008</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Gupta_OW-DETR_Open-World_Detection_Transformer_CVPR_2022_paper.html</data>
  <data key="d3">Ow-detr: Open-world detection transformer</data>
  <data key="d4">A Gupta, S Narayan, KJ Joseph…</data>
  <data key="d5">2022</data>
  <data key="d6">76</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5601871542106060008&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="10835602930051801712">
  <data key="d0">Advances in adversarial attacks and defenses in computer vision: A survey</data>
  <data key="d1">10835602930051801712</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9614158/</data>
  <data key="d3">Advances in adversarial attacks and defenses in computer vision: A survey</data>
  <data key="d4">N Akhtar, A Mian, N Kardan, M Shah</data>
  <data key="d5">2021</data>
  <data key="d6">121</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10835602930051801712&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="12895029264791091547">
  <data key="d0">Transformerfusion: Monocular rgb scene reconstruction using transformers</data>
  <data key="d1">12895029264791091547</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/0a87257e5308197df43230edf4ad1dae-Abstract.html</data>
  <data key="d3">Transformerfusion: Monocular rgb scene reconstruction using transformers</data>
  <data key="d4">A Bozic, P Palafox, J Thies, A Dai…</data>
  <data key="d5">2021</data>
  <data key="d6">80</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12895029264791091547&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="4988077150684599939">
  <data key="d0">GAN-based anomaly detection: A review</data>
  <data key="d1">4988077150684599939</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0925231221019482</data>
  <data key="d3">GAN-based anomaly detection: A review</data>
  <data key="d4">X Xia, X Pan, N Li, X He, L Ma, X Zhang, N Ding</data>
  <data key="d5">2022</data>
  <data key="d6">89</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4988077150684599939&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="5670723070535415123">
  <data key="d0">U-net transformer: Self and cross attention for medical image segmentation</data>
  <data key="d1">5670723070535415123</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-87589-3_28</data>
  <data key="d3">U-net transformer: Self and cross attention for medical image segmentation</data>
  <data key="d4">O Petit, N Thome, C Rambour, L Themyr…</data>
  <data key="d5">2021</data>
  <data key="d6">148</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5670723070535415123&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="7525849990631181493">
  <data key="d0">A transformer-based siamese network for change detection</data>
  <data key="d1">7525849990631181493</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9883686/</data>
  <data key="d3">A transformer-based siamese network for change detection</data>
  <data key="d4">WGC Bandara, VM Patel</data>
  <data key="d5">2022</data>
  <data key="d6">158</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7525849990631181493&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="2246956610590692796">
  <data key="d0">SNR-aware low-light image enhancement</data>
  <data key="d1">2246956610590692796</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Xu_SNR-Aware_Low-Light_Image_Enhancement_CVPR_2022_paper.html</data>
  <data key="d3">SNR-aware low-light image enhancement</data>
  <data key="d4">X Xu, R Wang, CW Fu, J Jia</data>
  <data key="d5">2022</data>
  <data key="d6">69</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2246956610590692796&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="10743434876113045449">
  <data key="d0">GasHis-Transformer: A multi-scale visual transformer approach for gastric histopathological image detection</data>
  <data key="d1">10743434876113045449</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0031320322003089</data>
  <data key="d3">GasHis-Transformer: A multi-scale visual transformer approach for gastric histopathological image detection</data>
  <data key="d4">H Chen, C Li, G Wang, X Li, MM Rahaman, H Sun…</data>
  <data key="d5">2022</data>
  <data key="d6">88</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10743434876113045449&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="9897783945226246229">
  <data key="d0">Cdtrans: Cross-domain transformer for unsupervised domain adaptation</data>
  <data key="d1">9897783945226246229</data>
  <data key="d2">https://arxiv.org/abs/2109.06165</data>
  <data key="d3">Cdtrans: Cross-domain transformer for unsupervised domain adaptation</data>
  <data key="d4">T Xu, W Chen, P Wang, F Wang, H Li, R Jin</data>
  <data key="d5">2021</data>
  <data key="d6">126</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9897783945226246229&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="278490733091580759">
  <data key="d0">Learning enriched features for fast image restoration and enhancement</data>
  <data key="d1">278490733091580759</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9756908/</data>
  <data key="d3">Learning enriched features for fast image restoration and enhancement</data>
  <data key="d4">SW Zamir, A Arora, S Khan, M Hayat…</data>
  <data key="d5">2022</data>
  <data key="d6">58</data>
  <data key="d7">https://scholar.google.com/scholar?cites=278490733091580759&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="12384531505899273984">
  <data key="d0">Perceptual image quality assessment with transformers</data>
  <data key="d1">12384531505899273984</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Cheon_Perceptual_Image_Quality_Assessment_With_Transformers_CVPRW_2021_paper.html</data>
  <data key="d3">Perceptual image quality assessment with transformers</data>
  <data key="d4">M Cheon, SJ Yoon, B Kang…</data>
  <data key="d5">2021</data>
  <data key="d6">84</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12384531505899273984&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="8053588590478703627">
  <data key="d0">Physformer: Facial video-based physiological measurement with temporal difference transformer</data>
  <data key="d1">8053588590478703627</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Yu_PhysFormer_Facial_Video-Based_Physiological_Measurement_With_Temporal_Difference_Transformer_CVPR_2022_paper.html</data>
  <data key="d3">Physformer: Facial video-based physiological measurement with temporal difference transformer</data>
  <data key="d4">Z Yu, Y Shen, J Shi, H Zhao…</data>
  <data key="d5">2022</data>
  <data key="d6">55</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8053588590478703627&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="2012981774095049918">
  <data key="d0">Query2label: A simple transformer way to multi-label classification</data>
  <data key="d1">2012981774095049918</data>
  <data key="d2">https://arxiv.org/abs/2107.10834</data>
  <data key="d3">Query2label: A simple transformer way to multi-label classification</data>
  <data key="d4">S Liu, L Zhang, X Yang, H Su, J Zhu</data>
  <data key="d5">2021</data>
  <data key="d6">105</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2012981774095049918&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="17245415599430658769">
  <data key="d0">Efficient transformer for remote sensing image segmentation</data>
  <data key="d1">17245415599430658769</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/18/3585</data>
  <data key="d3">Efficient transformer for remote sensing image segmentation</data>
  <data key="d4">Z Xu, W Zhang, T Zhang, Z Yang, J Li</data>
  <data key="d5">2021</data>
  <data key="d6">83</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17245415599430658769&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="10019448087059497602">
  <data key="d0">Sotr: Segmenting objects with transformers</data>
  <data key="d1">10019448087059497602</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Guo_SOTR_Segmenting_Objects_With_Transformers_ICCV_2021_paper.html</data>
  <data key="d3">Sotr: Segmenting objects with transformers</data>
  <data key="d4">R Guo, D Niu, L Qu, Z Li</data>
  <data key="d5">2021</data>
  <data key="d6">71</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10019448087059497602&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="11308628305789992240">
  <data key="d0">Self-supervised video transformer</data>
  <data key="d1">11308628305789992240</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Ranasinghe_Self-Supervised_Video_Transformer_CVPR_2022_paper.html</data>
  <data key="d3">Self-supervised video transformer</data>
  <data key="d4">K Ranasinghe, M Naseer, S Khan…</data>
  <data key="d5">2022</data>
  <data key="d6">52</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11308628305789992240&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="14281222646840020216">
  <data key="d0">Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications</data>
  <data key="d1">14281222646840020216</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-25082-8_1</data>
  <data key="d3">Edgenext: efficiently amalgamated cnn-transformer architecture for mobile vision applications</data>
  <data key="d4">M Maaz, A Shaker, H Cholakkal, S Khan…</data>
  <data key="d5">2022</data>
  <data key="d6">56</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14281222646840020216&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="350651523509452369">
  <data key="d0">Is it time to replace cnns with transformers for medical images?</data>
  <data key="d1">350651523509452369</data>
  <data key="d2">https://arxiv.org/abs/2108.09038</data>
  <data key="d3">Is it time to replace cnns with transformers for medical images?</data>
  <data key="d4">C Matsoukas, JF Haslum, M Söderberg…</data>
  <data key="d5">2021</data>
  <data key="d6">94</data>
  <data key="d7">https://scholar.google.com/scholar?cites=350651523509452369&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="14640072730760349377">
  <data key="d0">NTIRE 2022 challenge on perceptual image quality assessment</data>
  <data key="d1">14640072730760349377</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Gu_NTIRE_2022_Challenge_on_Perceptual_Image_Quality_Assessment_CVPRW_2022_paper.html</data>
  <data key="d3">NTIRE 2022 challenge on perceptual image quality assessment</data>
  <data key="d4">J Gu, H Cai, C Dong, JS Ren…</data>
  <data key="d5">2022</data>
  <data key="d6">71</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14640072730760349377&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="16207837178250254474">
  <data key="d0">Transformer and CNN hybrid deep neural network for semantic segmentation of very-high-resolution remote sensing imagery</data>
  <data key="d1">16207837178250254474</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9686732/</data>
  <data key="d3">Transformer and CNN hybrid deep neural network for semantic segmentation of very-high-resolution remote sensing imagery</data>
  <data key="d4">C Zhang, W Jiang, Y Zhang, W Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">76</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16207837178250254474&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="13079493271785323872">
  <data key="d0">Machine learning and deep learning based predictive quality in manufacturing: a systematic review</data>
  <data key="d1">13079493271785323872</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10845-022-01963-8</data>
  <data key="d3">Machine learning and deep learning based predictive quality in manufacturing: a systematic review</data>
  <data key="d4">H Tercan, T Meisen</data>
  <data key="d5">2022</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13079493271785323872&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="7347143883946966195">
  <data key="d0">Continual learning with lifelong vision transformer</data>
  <data key="d1">7347143883946966195</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Continual_Learning_With_Lifelong_Vision_Transformer_CVPR_2022_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">Continual learning with lifelong vision transformer</data>
  <data key="d4">Z Wang, L Liu, Y Duan, Y Kong…</data>
  <data key="d5">2022</data>
  <data key="d6">31</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7347143883946966195&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="10437338697213367930">
  <data key="d0">Sit: Self-supervised vision transformer</data>
  <data key="d1">10437338697213367930</data>
  <data key="d2">https://arxiv.org/abs/2104.03602</data>
  <data key="d3">Sit: Self-supervised vision transformer</data>
  <data key="d4">S Atito, M Awais, J Kittler</data>
  <data key="d5">2021</data>
  <data key="d6">80</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10437338697213367930&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="17121643192375450290">
  <data key="d0">Multi-task vision transformer using low-level chest X-ray feature corpus for COVID-19 diagnosis and severity quantification</data>
  <data key="d1">17121643192375450290</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841521003443</data>
  <data key="d3">Multi-task vision transformer using low-level chest X-ray feature corpus for COVID-19 diagnosis and severity quantification</data>
  <data key="d4">S Park, G Kim, Y Oh, JB Seo, SM Lee, JH Kim…</data>
  <data key="d5">2022</data>
  <data key="d6">75</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17121643192375450290&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="906783918798836468">
  <data key="d0">Transformer neural network for weed and crop classification of high resolution UAV images</data>
  <data key="d1">906783918798836468</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/3/592</data>
  <data key="d3">Transformer neural network for weed and crop classification of high resolution UAV images</data>
  <data key="d4">R Reedha, E Dericquebourg, R Canals, A Hafiane</data>
  <data key="d5">2022</data>
  <data key="d6">65</data>
  <data key="d7">https://scholar.google.com/scholar?cites=906783918798836468&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="17970566394936146777">
  <data key="d0">Attention mechanism in intelligent fault diagnosis of machinery: A review of technique and application</data>
  <data key="d1">17970566394936146777</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0263224122008077</data>
  <data key="d3">Attention mechanism in intelligent fault diagnosis of machinery: A review of technique and application</data>
  <data key="d4">H Lv, J Chen, T Pan, T Zhang, Y Feng, S Liu</data>
  <data key="d5">2022</data>
  <data key="d6">39</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17970566394936146777&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="1797517254712068026">
  <data key="d0">Retrieval augmented classification for long-tail visual recognition</data>
  <data key="d1">1797517254712068026</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Long_Retrieval_Augmented_Classification_for_Long-Tail_Visual_Recognition_CVPR_2022_paper.html</data>
  <data key="d3">Retrieval augmented classification for long-tail visual recognition</data>
  <data key="d4">A Long, W Yin, T Ajanthan, V Nguyen…</data>
  <data key="d5">2022</data>
  <data key="d6">33</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1797517254712068026&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="2971359940394370469">
  <data key="d0">Mil-vt: Multiple instance learning enhanced vision transformer for fundus image classification</data>
  <data key="d1">2971359940394370469</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-87237-3_5</data>
  <data key="d3">Mil-vt: Multiple instance learning enhanced vision transformer for fundus image classification</data>
  <data key="d4">S Yu, K Ma, Q Bi, C Bian, M Ning, N He, Y Li…</data>
  <data key="d5">2021</data>
  <data key="d6">75</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2971359940394370469&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="13482994153555216848">
  <data key="d0">A robust volumetric transformer for accurate 3D tumor segmentation</data>
  <data key="d1">13482994153555216848</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-16443-9_16</data>
  <data key="d3">A robust volumetric transformer for accurate 3D tumor segmentation</data>
  <data key="d4">H Peiris, M Hayat, Z Chen, G Egan…</data>
  <data key="d5">2022</data>
  <data key="d6">75</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13482994153555216848&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="3062420136055973965">
  <data key="d0">End-to-end deep learning framework for printed circuit board manufacturing defect classification</data>
  <data key="d1">3062420136055973965</data>
  <data key="d2">https://www.nature.com/articles/s41598-022-16302-3</data>
  <data key="d3">End-to-end deep learning framework for printed circuit board manufacturing defect classification</data>
  <data key="d4">A Bhattacharya, SG Cloutier</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3062420136055973965&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="12187111677452062889">
  <data key="d0">Convolution-free medical image segmentation using transformers</data>
  <data key="d1">12187111677452062889</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-87193-2_8</data>
  <data key="d3">Convolution-free medical image segmentation using transformers</data>
  <data key="d4">D Karimi, SD Vasylechko, A Gholipour</data>
  <data key="d5">2021</data>
  <data key="d6">80</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12187111677452062889&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="9999087805969499080">
  <data key="d0">Facial expression recognition with grid-wise attention and visual transformer</data>
  <data key="d1">9999087805969499080</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0020025521008495</data>
  <data key="d3">Facial expression recognition with grid-wise attention and visual transformer</data>
  <data key="d4">Q Huang, C Huang, X Wang, F Jiang</data>
  <data key="d5">2021</data>
  <data key="d6">63</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9999087805969499080&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="3774217736601927222">
  <data key="d0">Translation between molecules and natural language</data>
  <data key="d1">3774217736601927222</data>
  <data key="d2">https://arxiv.org/abs/2204.11817</data>
  <data key="d3">Translation between molecules and natural language</data>
  <data key="d4">C Edwards, T Lai, K Ros, G Honke, K Cho…</data>
  <data key="d5">2022</data>
  <data key="d6">41</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3774217736601927222&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="12443165844301288148">
  <data key="d0">Transzero: Attribute-guided transformer for zero-shot learning</data>
  <data key="d1">12443165844301288148</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/19909</data>
  <data key="d3">Transzero: Attribute-guided transformer for zero-shot learning</data>
  <data key="d4">S Chen, Z Hong, Y Liu, GS Xie, B Sun, H Li…</data>
  <data key="d5">2022</data>
  <data key="d6">49</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12443165844301288148&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="8750779121680802066">
  <data key="d0">Online continual learning with contrastive vision transformer</data>
  <data key="d1">8750779121680802066</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20044-1_36</data>
  <data key="d3">Online continual learning with contrastive vision transformer</data>
  <data key="d4">Z Wang, L Liu, Y Kong, J Guo, D Tao</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8750779121680802066&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="8737500900907252911">
  <data key="d0">EGDE-Net: A building change detection method for high-resolution remote sensing imagery based on edge guidance and differential enhancement</data>
  <data key="d1">8737500900907252911</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0924271622001940</data>
  <data key="d3">EGDE-Net: A building change detection method for high-resolution remote sensing imagery based on edge guidance and differential enhancement</data>
  <data key="d4">Z Chen, Y Zhou, B Wang, X Xu, N He, S Jin…</data>
  <data key="d5">2022</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8737500900907252911&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="15425036751013855629">
  <data key="d0">IL-MCAM: An interactive learning and multi-channel attention mechanism-based weakly supervised colorectal histopathology image classification approach</data>
  <data key="d1">15425036751013855629</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0010482522000579</data>
  <data key="d3">IL-MCAM: An interactive learning and multi-channel attention mechanism-based weakly supervised colorectal histopathology image classification approach</data>
  <data key="d4">H Chen, C Li, X Li, MM Rahaman, W Hu, Y Li…</data>
  <data key="d5">2022</data>
  <data key="d6">44</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15425036751013855629&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="6637879260771394229">
  <data key="d0">UTRAD: Anomaly detection and localization with U-transformer</data>
  <data key="d1">6637879260771394229</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0893608021004810</data>
  <data key="d3">UTRAD: Anomaly detection and localization with U-transformer</data>
  <data key="d4">L Chen, Z You, N Zhang, J Xi, X Le</data>
  <data key="d5">2022</data>
  <data key="d6">40</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6637879260771394229&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="17981792349028675992">
  <data key="d0">Towards the fully automated monitoring of ecological communities</data>
  <data key="d1">17981792349028675992</data>
  <data key="d2">https://onlinelibrary.wiley.com/doi/abs/10.1111/ele.14123</data>
  <data key="d3">Towards the fully automated monitoring of ecological communities</data>
  <data key="d4">M Besson, J Alison, K Bjerge, TE Gorochowski…</data>
  <data key="d5">2022</data>
  <data key="d6">36</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17981792349028675992&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="18080630766028543102">
  <data key="d0">Robustifying token attention for vision transformers</data>
  <data key="d1">18080630766028543102</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Guo_Robustifying_Token_Attention_for_Vision_Transformers_ICCV_2023_paper.html</data>
  <data key="d3">Robustifying token attention for vision transformers</data>
  <data key="d4">Y Guo, D Stutz, B Schiele</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18080630766028543102&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="15758326914658017418">
  <data key="d0">Istr: End-to-end instance segmentation with transformers</data>
  <data key="d1">15758326914658017418</data>
  <data key="d2">https://arxiv.org/abs/2105.00637</data>
  <data key="d3">Istr: End-to-end instance segmentation with transformers</data>
  <data key="d4">J Hu, L Cao, Y Lu, SC Zhang, Y Wang, K Li…</data>
  <data key="d5">2021</data>
  <data key="d6">72</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15758326914658017418&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="12478683589701856519">
  <data key="d0">Pointmixer: Mlp-mixer for point cloud understanding</data>
  <data key="d1">12478683589701856519</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19812-0_36</data>
  <data key="d3">Pointmixer: Mlp-mixer for point cloud understanding</data>
  <data key="d4">J Choe, C Park, F Rameau, J Park…</data>
  <data key="d5">2022</data>
  <data key="d6">33</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12478683589701856519&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="8623435992513029102">
  <data key="d0">Countering malicious deepfakes: Survey, battleground, and horizon</data>
  <data key="d1">8623435992513029102</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11263-022-01606-8</data>
  <data key="d3">Countering malicious deepfakes: Survey, battleground, and horizon</data>
  <data key="d4">F Juefei-Xu, R Wang, Y Huang, Q Guo, L Ma…</data>
  <data key="d5">2022</data>
  <data key="d6">65</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8623435992513029102&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="9927551479269048485">
  <data key="d0">On improving adversarial transferability of vision transformers</data>
  <data key="d1">9927551479269048485</data>
  <data key="d2">https://arxiv.org/abs/2106.04169</data>
  <data key="d3">On improving adversarial transferability of vision transformers</data>
  <data key="d4">M Naseer, K Ranasinghe, S Khan, FS Khan…</data>
  <data key="d5">2021</data>
  <data key="d6">58</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9927551479269048485&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="6583545798277065500">
  <data key="d0">Deep hierarchical vision transformer for hyperspectral and LiDAR data classification</data>
  <data key="d1">6583545798277065500</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9755059/</data>
  <data key="d3">Deep hierarchical vision transformer for hyperspectral and LiDAR data classification</data>
  <data key="d4">Z Xue, X Tan, X Yu, B Liu, A Yu…</data>
  <data key="d5">2022</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6583545798277065500&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="2547364804279732726">
  <data key="d0">Avoiding negative transfer for semantic segmentation of remote sensing images</data>
  <data key="d1">2547364804279732726</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9866807/</data>
  <data key="d3">Avoiding negative transfer for semantic segmentation of remote sensing images</data>
  <data key="d4">H Wang, C Tao, J Qi, R Xiao, H Li</data>
  <data key="d5">2022</data>
  <data key="d6">67</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2547364804279732726&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="3241342143336277180">
  <data key="d0">Federated split task-agnostic vision transformer for COVID-19 CXR diagnosis</data>
  <data key="d1">3241342143336277180</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/ceb0595112db2513b9325a85761b7310-Abstract.html</data>
  <data key="d3">Federated split task-agnostic vision transformer for COVID-19 CXR diagnosis</data>
  <data key="d4">S Park, G Kim, J Kim, B Kim…</data>
  <data key="d5">2021</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3241342143336277180&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="1309943264288419550">
  <data key="d0">Image harmonization with transformer</data>
  <data key="d1">1309943264288419550</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Guo_Image_Harmonization_With_Transformer_ICCV_2021_paper.html</data>
  <data key="d3">Image harmonization with transformer</data>
  <data key="d4">Z Guo, D Guo, H Zheng, Z Gu…</data>
  <data key="d5">2021</data>
  <data key="d6">42</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1309943264288419550&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="10901034885667795659">
  <data key="d0">Visual attention methods in deep learning: An in-depth survey</data>
  <data key="d1">10901034885667795659</data>
  <data key="d2">https://arxiv.org/abs/2204.07756</data>
  <data key="d3">Visual attention methods in deep learning: An in-depth survey</data>
  <data key="d4">M Hassanin, S Anwar, I Radwan, FS Khan…</data>
  <data key="d5">2022</data>
  <data key="d6">42</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10901034885667795659&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="15926311935982020340">
  <data key="d0">Video transformers: A survey</data>
  <data key="d1">15926311935982020340</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10041724/</data>
  <data key="d3">Video transformers: A survey</data>
  <data key="d4">J Selva, AS Johansen, S Escalera…</data>
  <data key="d5">2023</data>
  <data key="d6">44</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15926311935982020340&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="5301766735590672032">
  <data key="d0">FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer</data>
  <data key="d1">5301766735590672032</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Liu_FlatFormer_Flattened_Window_Attention_for_Efficient_Point_Cloud_Transformer_CVPR_2023_paper.html</data>
  <data key="d3">FlatFormer: Flattened Window Attention for Efficient Point Cloud Transformer</data>
  <data key="d4">Z Liu, X Yang, H Tang, S Yang…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5301766735590672032&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="16583630453825645332">
  <data key="d0">Wildfire segmentation using deep vision transformers</data>
  <data key="d1">16583630453825645332</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/17/3527</data>
  <data key="d3">Wildfire segmentation using deep vision transformers</data>
  <data key="d4">R Ghali, MA Akhloufi, M Jmal, W Souidene Mseddi…</data>
  <data key="d5">2021</data>
  <data key="d6">49</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16583630453825645332&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="3738295254278174876">
  <data key="d0">Transformers in remote sensing: A survey</data>
  <data key="d1">3738295254278174876</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/7/1860</data>
  <data key="d3">Transformers in remote sensing: A survey</data>
  <data key="d4">AA Aleissaee, A Kumar, RM Anwer, S Khan…</data>
  <data key="d5">2023</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3738295254278174876&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="9591435289724766439">
  <data key="d0">A survey of self-supervised and few-shot object detection</data>
  <data key="d1">9591435289724766439</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9860087/</data>
  <data key="d3">A survey of self-supervised and few-shot object detection</data>
  <data key="d4">G Huang, I Laradji, D Vazquez…</data>
  <data key="d5">2022</data>
  <data key="d6">45</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9591435289724766439&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="3333668025330401477">
  <data key="d0">A hybrid explainable ensemble transformer encoder for pneumonia identification from chest X-ray images</data>
  <data key="d1">3333668025330401477</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2090123222002028</data>
  <data key="d3">A hybrid explainable ensemble transformer encoder for pneumonia identification from chest X-ray images</data>
  <data key="d4">CC Ukwuoma, Z Qin, MBB Heyat, F Akhtar…</data>
  <data key="d5">2023</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3333668025330401477&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="5952922111315863760">
  <data key="d0">Keypoint transformer: Solving joint identification in challenging hands and object interactions for accurate 3d pose estimation</data>
  <data key="d1">5952922111315863760</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Hampali_Keypoint_Transformer_Solving_Joint_Identification_in_Challenging_Hands_and_Object_CVPR_2022_paper.html</data>
  <data key="d3">Keypoint transformer: Solving joint identification in challenging hands and object interactions for accurate 3d pose estimation</data>
  <data key="d4">S Hampali, SD Sarkar, M Rad…</data>
  <data key="d5">2022</data>
  <data key="d6">56</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5952922111315863760&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="1101546884380916291">
  <data key="d0">AMMU: a survey of transformer-based biomedical pretrained language models</data>
  <data key="d1">1101546884380916291</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1532046421003117</data>
  <data key="d3">AMMU: a survey of transformer-based biomedical pretrained language models</data>
  <data key="d4">KS Kalyan, A Rajasekharan, S Sangeetha</data>
  <data key="d5">2022</data>
  <data key="d6">58</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1101546884380916291&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="8994048266291075499">
  <data key="d0">CCTNet: Coupled CNN and transformer network for crop segmentation of remote sensing images</data>
  <data key="d1">8994048266291075499</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/9/1956</data>
  <data key="d3">CCTNet: Coupled CNN and transformer network for crop segmentation of remote sensing images</data>
  <data key="d4">H Wang, X Chen, T Zhang, Z Xu, J Li</data>
  <data key="d5">2022</data>
  <data key="d6">32</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8994048266291075499&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="15220408395767693757">
  <data key="d0">M3T: three-dimensional Medical image classifier using Multi-plane and Multi-slice Transformer</data>
  <data key="d1">15220408395767693757</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Jang_M3T_Three-Dimensional_Medical_Image_Classifier_Using_Multi-Plane_and_Multi-Slice_Transformer_CVPR_2022_paper.html</data>
  <data key="d3">M3T: three-dimensional Medical image classifier using Multi-plane and Multi-slice Transformer</data>
  <data key="d4">J Jang, D Hwang</data>
  <data key="d5">2022</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15220408395767693757&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="5423828611786306174">
  <data key="d0">Few-shot class-incremental learning by sampling multi-phase tasks</data>
  <data key="d1">5423828611786306174</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9864267/</data>
  <data key="d3">Few-shot class-incremental learning by sampling multi-phase tasks</data>
  <data key="d4">DW Zhou, HJ Ye, L Ma, D Xie, S Pu…</data>
  <data key="d5">2022</data>
  <data key="d6">31</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5423828611786306174&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="5888510936612783579">
  <data key="d0">Uniform masking: Enabling mae pre-training for pyramid-based vision transformers with locality</data>
  <data key="d1">5888510936612783579</data>
  <data key="d2">https://arxiv.org/abs/2205.10063</data>
  <data key="d3">Uniform masking: Enabling mae pre-training for pyramid-based vision transformers with locality</data>
  <data key="d4">X Li, W Wang, L Yang, J Yang</data>
  <data key="d5">2022</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5888510936612783579&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="17171842121702147403">
  <data key="d0">Searching the search space of vision transformer</data>
  <data key="d1">17171842121702147403</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/48e95c45c8217961bf6cd7696d80d238-Abstract.html</data>
  <data key="d3">Searching the search space of vision transformer</data>
  <data key="d4">M Chen, K Wu, B Ni, H Peng, B Liu…</data>
  <data key="d5">2021</data>
  <data key="d6">24</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17171842121702147403&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="8071202295861113716">
  <data key="d0">Deep learning based computer vision approaches for smart agricultural applications</data>
  <data key="d1">8071202295861113716</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2589721722000174</data>
  <data key="d3">Deep learning based computer vision approaches for smart agricultural applications</data>
  <data key="d4">VG Dhanya, A Subeesh, NL Kushwaha…</data>
  <data key="d5">2022</data>
  <data key="d6">40</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8071202295861113716&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="4041041901496425203">
  <data key="d0">A survey on image data augmentation for deep learning</data>
  <data key="d1">4041041901496425203</data>
  <data key="d2">https://journalofbigdata.springeropen.com/track/pdf/10.1186/s40537-019-0197-0.pdf</data>
  <data key="d3">A survey on image data augmentation for deep learning</data>
  <data key="d4">C Shorten, TM Khoshgoftaar</data>
  <data key="d5">2019</data>
  <data key="d6">7767</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4041041901496425203&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="8136644755673586417">
  <data key="d0">Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions</data>
  <data key="d1">8136644755673586417</data>
  <data key="d2">https://link.springer.com/article/10.1186/s40537-021-00444-8</data>
  <data key="d3">Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions</data>
  <data key="d4">L Alzubaidi, J Zhang, AJ Humaidi, A Al-Dujaili…</data>
  <data key="d5">2021</data>
  <data key="d6">2459</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8136644755673586417&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="6141939658324331542">
  <data key="d0">Review on Convolutional Neural Networks (CNN) in vegetation remote sensing</data>
  <data key="d1">6141939658324331542</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0924271620303488</data>
  <data key="d3">Review on Convolutional Neural Networks (CNN) in vegetation remote sensing</data>
  <data key="d4">T Kattenborn, J Leitloff, F Schiefer, S Hinz</data>
  <data key="d5">2021</data>
  <data key="d6">615</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6141939658324331542&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="7770442917120891581">
  <data key="d0">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation</data>
  <data key="d1">7770442917120891581</data>
  <data key="d2">https://proceedings.mlr.press/v162/li22n.html</data>
  <data key="d3">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation</data>
  <data key="d4">J Li, D Li, C Xiong, S Hoi</data>
  <data key="d5">2022</data>
  <data key="d6">835</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7770442917120891581&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="8793029896395507010">
  <data key="d0">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</data>
  <data key="d1">8793029896395507010</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.html</data>
  <data key="d3">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</data>
  <data key="d4">W Wang, E Xie, X Li, DP Fan, K Song…</data>
  <data key="d5">2021</data>
  <data key="d6">2381</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8793029896395507010&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="9595110325981705564">
  <data key="d0">On the opportunities and risks of foundation models</data>
  <data key="d1">9595110325981705564</data>
  <data key="d2">https://arxiv.org/abs/2108.07258</data>
  <data key="d3">On the opportunities and risks of foundation models</data>
  <data key="d4">R Bommasani, DA Hudson, E Adeli, R Altman…</data>
  <data key="d5">2021</data>
  <data key="d6">1551</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9595110325981705564&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="12987945369444025427">
  <data key="d0">Vilt: Vision-and-language transformer without convolution or region supervision</data>
  <data key="d1">12987945369444025427</data>
  <data key="d2">https://proceedings.mlr.press/v139/kim21k.html</data>
  <data key="d3">Vilt: Vision-and-language transformer without convolution or region supervision</data>
  <data key="d4">W Kim, B Son, I Kim</data>
  <data key="d5">2021</data>
  <data key="d6">851</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12987945369444025427&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="16770513324417061228">
  <data key="d0">A survey of uncertainty in deep neural networks</data>
  <data key="d1">16770513324417061228</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10462-023-10562-9</data>
  <data key="d3">A survey of uncertainty in deep neural networks</data>
  <data key="d4">J Gawlikowski, CRN Tassi, M Ali, J Lee, M Humt…</data>
  <data key="d5">2023</data>
  <data key="d6">450</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16770513324417061228&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="9063880872255850171">
  <data key="d0">Training generative adversarial networks with limited data</data>
  <data key="d1">9063880872255850171</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2020/hash/8d30aa96e72440759f74bd2306c1fa3d-Abstract.html</data>
  <data key="d3">Training generative adversarial networks with limited data</data>
  <data key="d4">T Karras, M Aittala, J Hellsten, S Laine…</data>
  <data key="d5">2020</data>
  <data key="d6">1343</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9063880872255850171&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="2321980635951558135">
  <data key="d0">Simple copy-paste is a strong data augmentation method for instance segmentation</data>
  <data key="d1">2321980635951558135</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2021/html/Ghiasi_Simple_Copy-Paste_Is_a_Strong_Data_Augmentation_Method_for_Instance_CVPR_2021_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">Simple copy-paste is a strong data augmentation method for instance segmentation</data>
  <data key="d4">G Ghiasi, Y Cui, A Srinivas, R Qian…</data>
  <data key="d5">2021</data>
  <data key="d6">684</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2321980635951558135&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="862169174991977666">
  <data key="d0">A survey of data augmentation approaches for NLP</data>
  <data key="d1">862169174991977666</data>
  <data key="d2">https://arxiv.org/abs/2105.03075</data>
  <data key="d3">A survey of data augmentation approaches for NLP</data>
  <data key="d4">SY Feng, V Gangal, J Wei, S Chandar…</data>
  <data key="d5">2021</data>
  <data key="d6">496</data>
  <data key="d7">https://scholar.google.com/scholar?cites=862169174991977666&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="7227117659365205945">
  <data key="d0">Generalizing to unseen domains: A survey on domain generalization</data>
  <data key="d1">7227117659365205945</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9782500/</data>
  <data key="d3">Generalizing to unseen domains: A survey on domain generalization</data>
  <data key="d4">J Wang, C Lan, C Liu, Y Ouyang, T Qin…</data>
  <data key="d5">2022</data>
  <data key="d6">498</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7227117659365205945&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="16564777441869762048">
  <data key="d0">Learning from noisy labels with deep neural networks: A survey</data>
  <data key="d1">16564777441869762048</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9729424/</data>
  <data key="d3">Learning from noisy labels with deep neural networks: A survey</data>
  <data key="d4">H Song, M Kim, D Park, Y Shin…</data>
  <data key="d5">2022</data>
  <data key="d6">608</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16564777441869762048&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="9003206964118171143">
  <data key="d0">Machine learning: new ideas and tools in environmental science and engineering</data>
  <data key="d1">9003206964118171143</data>
  <data key="d2">https://pubs.acs.org/doi/abs/10.1021/acs.est.1c01339</data>
  <data key="d3">Machine learning: new ideas and tools in environmental science and engineering</data>
  <data key="d4">S Zhong, K Zhang, M Bagheri, JG Burken…</data>
  <data key="d5">2021</data>
  <data key="d6">280</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9003206964118171143&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="10054528338033032937">
  <data key="d0">Out-of-distribution generalization via risk extrapolation (rex)</data>
  <data key="d1">10054528338033032937</data>
  <data key="d2">http://proceedings.mlr.press/v139/krueger21a.html</data>
  <data key="d3">Out-of-distribution generalization via risk extrapolation (rex)</data>
  <data key="d4">D Krueger, E Caballero, JH Jacobsen…</data>
  <data key="d5">2021</data>
  <data key="d6">545</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10054528338033032937&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="7358070883597795569">
  <data key="d0">Multi-sensor information fusion based on machine learning for real applications in human activity recognition: State-of-the-art and research challenges</data>
  <data key="d1">7358070883597795569</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1566253521002311</data>
  <data key="d3">Multi-sensor information fusion based on machine learning for real applications in human activity recognition: State-of-the-art and research challenges</data>
  <data key="d4">S Qiu, H Zhao, N Jiang, Z Wang, L Liu, Y An, H Zhao…</data>
  <data key="d5">2022</data>
  <data key="d6">224</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7358070883597795569&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="13927538846757401282">
  <data key="d0">Albumentations: fast and flexible image augmentations</data>
  <data key="d1">13927538846757401282</data>
  <data key="d2">https://www.mdpi.com/2078-2489/11/2/125?ref=https://coder.social</data>
  <data key="d3">Albumentations: fast and flexible image augmentations</data>
  <data key="d4">A Buslaev, VI Iglovikov, E Khvedchenya, A Parinov…</data>
  <data key="d5">2020</data>
  <data key="d6">1446</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13927538846757401282&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="11310550941481896506">
  <data key="d0">Embracing imperfect datasets: A review of deep learning solutions for medical image segmentation</data>
  <data key="d1">11310550941481896506</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S136184152030058X</data>
  <data key="d3">Embracing imperfect datasets: A review of deep learning solutions for medical image segmentation</data>
  <data key="d4">N Tajbakhsh, L Jeyaseelan, Q Li, JN Chiang, Z Wu…</data>
  <data key="d5">2020</data>
  <data key="d6">609</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11310550941481896506&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="13436034306365145381">
  <data key="d0">A novel transfer learning based approach for pneumonia detection in chest X-ray images</data>
  <data key="d1">13436034306365145381</data>
  <data key="d2">https://www.mdpi.com/2076-3417/10/2/559</data>
  <data key="d3">A novel transfer learning based approach for pneumonia detection in chest X-ray images</data>
  <data key="d4">V Chouhan, SK Singh, A Khamparia, D Gupta…</data>
  <data key="d5">2020</data>
  <data key="d6">549</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13436034306365145381&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="11885254696629287118">
  <data key="d0">A review of medical image data augmentation techniques for deep learning applications</data>
  <data key="d1">11885254696629287118</data>
  <data key="d2">https://onlinelibrary.wiley.com/doi/abs/10.1111/1754-9485.13261</data>
  <data key="d3">A review of medical image data augmentation techniques for deep learning applications</data>
  <data key="d4">P Chlap, H Min, N Vandenberg…</data>
  <data key="d5">2021</data>
  <data key="d6">292</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11885254696629287118&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="7309961819207216748">
  <data key="d0">Deep long-tailed learning: A survey</data>
  <data key="d1">7309961819207216748</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10105457/</data>
  <data key="d3">Deep long-tailed learning: A survey</data>
  <data key="d4">Y Zhang, B Kang, B Hooi, S Yan…</data>
  <data key="d5">2023</data>
  <data key="d6">237</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7309961819207216748&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="4984340879175316146">
  <data key="d0">Deep semantic segmentation of natural and medical images: a review</data>
  <data key="d1">4984340879175316146</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10462-020-09854-1</data>
  <data key="d3">Deep semantic segmentation of natural and medical images: a review</data>
  <data key="d4">S Asgari Taghanaki, K Abhishek, JP Cohen…</data>
  <data key="d5">2021</data>
  <data key="d6">534</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4984340879175316146&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="8302576934258228524">
  <data key="d0">Time series data augmentation for deep learning: A survey</data>
  <data key="d1">8302576934258228524</data>
  <data key="d2">https://arxiv.org/abs/2002.12478</data>
  <data key="d3">Time series data augmentation for deep learning: A survey</data>
  <data key="d4">Q Wen, L Sun, F Yang, X Song, J Gao, X Wang…</data>
  <data key="d5">2020</data>
  <data key="d6">502</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8302576934258228524&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="10911957018595078026">
  <data key="d0">An empirical survey of data augmentation for time series classification with neural networks</data>
  <data key="d1">10911957018595078026</data>
  <data key="d2">https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0254841</data>
  <data key="d3">An empirical survey of data augmentation for time series classification with neural networks</data>
  <data key="d4">BK Iwana, S Uchida</data>
  <data key="d5">2021</data>
  <data key="d6">356</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10911957018595078026&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="17777612268280748847">
  <data key="d0">Deep Learning applications for COVID-19</data>
  <data key="d1">17777612268280748847</data>
  <data key="d2">https://link.springer.com/article/10.1186/s40537-020-00392-9%23auth-Taghi_M_-Khoshgoftaar</data>
  <data key="d3">Deep Learning applications for COVID-19</data>
  <data key="d4">C Shorten, TM Khoshgoftaar, B Furht</data>
  <data key="d5">2021</data>
  <data key="d6">253</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17777612268280748847&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="4325500781447230872">
  <data key="d0">Continual test-time domain adaptation</data>
  <data key="d1">4325500781447230872</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Wang_Continual_Test-Time_Domain_Adaptation_CVPR_2022_paper.html</data>
  <data key="d3">Continual test-time domain adaptation</data>
  <data key="d4">Q Wang, O Fink, L Van Gool…</data>
  <data key="d5">2022</data>
  <data key="d6">135</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4325500781447230872&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="13909256571563279522">
  <data key="d0">Text data augmentation for deep learning</data>
  <data key="d1">13909256571563279522</data>
  <data key="d2">https://link.springer.com/article/10.1186/s40537-021-00492-0</data>
  <data key="d3">Text data augmentation for deep learning</data>
  <data key="d4">C Shorten, TM Khoshgoftaar, B Furht</data>
  <data key="d5">2021</data>
  <data key="d6">218</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13909256571563279522&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="14260054837347093009">
  <data key="d0">CNN-based transfer learning–BiLSTM network: A novel approach for COVID-19 infection detection</data>
  <data key="d1">14260054837347093009</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1568494620308504</data>
  <data key="d3">CNN-based transfer learning–BiLSTM network: A novel approach for COVID-19 infection detection</data>
  <data key="d4">MF Aslan, MF Unlersen, K Sabanci, A Durdu</data>
  <data key="d5">2021</data>
  <data key="d6">275</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14260054837347093009&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="2022703968544749835">
  <data key="d0">AI applications to medical images: From machine learning to deep learning</data>
  <data key="d1">2022703968544749835</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1120179721000946</data>
  <data key="d3">AI applications to medical images: From machine learning to deep learning</data>
  <data key="d4">I Castiglioni, L Rundo, M Codari, G Di Leo, C Salvatore…</data>
  <data key="d5">2021</data>
  <data key="d6">254</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2022703968544749835&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="18421653793757811360">
  <data key="d0">Label-only membership inference attacks</data>
  <data key="d1">18421653793757811360</data>
  <data key="d2">https://proceedings.mlr.press/v139/choquette-choo21a.html</data>
  <data key="d3">Label-only membership inference attacks</data>
  <data key="d4">CA Choquette-Choo, F Tramer…</data>
  <data key="d5">2021</data>
  <data key="d6">278</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18421653793757811360&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="639736007842008588">
  <data key="d0">Designing deep learning studies in cancer diagnostics</data>
  <data key="d1">639736007842008588</data>
  <data key="d2">https://www.nature.com/articles/s41568-020-00327-9</data>
  <data key="d3">Designing deep learning studies in cancer diagnostics</data>
  <data key="d4">A Kleppe, OJ Skrede, S De Raedt, K Liestøl…</data>
  <data key="d5">2021</data>
  <data key="d6">178</data>
  <data key="d7">https://scholar.google.com/scholar?cites=639736007842008588&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="11748945572016694012">
  <data key="d0">Data augmentation approaches in natural language processing: A survey</data>
  <data key="d1">11748945572016694012</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2666651022000080</data>
  <data key="d3">Data augmentation approaches in natural language processing: A survey</data>
  <data key="d4">B Li, Y Hou, W Che</data>
  <data key="d5">2022</data>
  <data key="d6">119</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11748945572016694012&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="18253746969450725752">
  <data key="d0">A survey of deep learning techniques for weed detection from images</data>
  <data key="d1">18253746969450725752</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169921000855</data>
  <data key="d3">A survey of deep learning techniques for weed detection from images</data>
  <data key="d4">ASMM Hasan, F Sohel, D Diepeveen, H Laga…</data>
  <data key="d5">2021</data>
  <data key="d6">207</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18253746969450725752&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="1091474511097006762">
  <data key="d0">Openood: Benchmarking generalized out-of-distribution detection</data>
  <data key="d1">1091474511097006762</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/d201587e3a84fc4761eadc743e9b3f35-Abstract-Datasets_and_Benchmarks.html</data>
  <data key="d3">Openood: Benchmarking generalized out-of-distribution detection</data>
  <data key="d4">J Yang, P Wang, D Zou, Z Zhou…</data>
  <data key="d5">2022</data>
  <data key="d6">57</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1091474511097006762&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="3667217905602891315">
  <data key="d0">A survey on data augmentation for text classification</data>
  <data key="d1">3667217905602891315</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3544558</data>
  <data key="d3">A survey on data augmentation for text classification</data>
  <data key="d4">M Bayer, MA Kaufhold, C Reuter</data>
  <data key="d5">2022</data>
  <data key="d6">165</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3667217905602891315&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="12913547344242152073">
  <data key="d0">An enhanced technique of skin cancer classification using deep convolutional neural network with transfer learning models</data>
  <data key="d1">12913547344242152073</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2666827021000177</data>
  <data key="d3">An enhanced technique of skin cancer classification using deep convolutional neural network with transfer learning models</data>
  <data key="d4">MS Ali, MS Miah, J Haque, MM Rahman…</data>
  <data key="d5">2021</data>
  <data key="d6">184</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12913547344242152073&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="10803661783624345607">
  <data key="d0">Enhanced data security of communication system using combined encryption and steganography</data>
  <data key="d1">10803661783624345607</data>
  <data key="d2">https://www.academia.edu/download/81991120/9765.pdf</data>
  <data key="d3">Enhanced data security of communication system using combined encryption and steganography</data>
  <data key="d4">HTS ALRikabi, HT Hazim</data>
  <data key="d5">2021</data>
  <data key="d6">135</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10803661783624345607&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="6953811760252391041">
  <data key="d0">Predictive maintenance enabled by machine learning: Use cases and challenges in the automotive industry</data>
  <data key="d1">6953811760252391041</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0951832021003835</data>
  <data key="d3">Predictive maintenance enabled by machine learning: Use cases and challenges in the automotive industry</data>
  <data key="d4">A Theissler, J Pérez-Velázquez, M Kettelgerdes…</data>
  <data key="d5">2021</data>
  <data key="d6">164</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6953811760252391041&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="7183832294030210754">
  <data key="d0">A survey on deep learning in medicine: Why, how and when?</data>
  <data key="d1">7183832294030210754</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1566253520303651</data>
  <data key="d3">A survey on deep learning in medicine: Why, how and when?</data>
  <data key="d4">F Piccialli, V Di Somma, F Giampaolo, S Cuomo…</data>
  <data key="d5">2021</data>
  <data key="d6">236</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7183832294030210754&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="7119350279654084346">
  <data key="d0">Data augmentation for graph neural networks</data>
  <data key="d1">7119350279654084346</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/17315</data>
  <data key="d3">Data augmentation for graph neural networks</data>
  <data key="d4">T Zhao, Y Liu, L Neves, O Woodford, M Jiang…</data>
  <data key="d5">2021</data>
  <data key="d6">263</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7119350279654084346&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="4506444976781600964">
  <data key="d0">COVID-19 open source data sets: a comprehensive survey</data>
  <data key="d1">4506444976781600964</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10489-020-01862-6</data>
  <data key="d3">COVID-19 open source data sets: a comprehensive survey</data>
  <data key="d4">J Shuja, E Alanazi, W Alasmary, A Alashaikh</data>
  <data key="d5">2021</data>
  <data key="d6">250</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4506444976781600964&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="10215776754955167080">
  <data key="d0">A scoping review of transfer learning research on medical image analysis using ImageNet</data>
  <data key="d1">10215776754955167080</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0010482520304467</data>
  <data key="d3">A scoping review of transfer learning research on medical image analysis using ImageNet</data>
  <data key="d4">MA Morid, A Borjali, G Del Fiol</data>
  <data key="d5">2021</data>
  <data key="d6">263</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10215776754955167080&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="6018697739026892882">
  <data key="d0">Transparency and reproducibility in artificial intelligence</data>
  <data key="d1">6018697739026892882</data>
  <data key="d2">https://www.nature.com/articles/s41586-020-2766-y</data>
  <data key="d3">Transparency and reproducibility in artificial intelligence</data>
  <data key="d4">B Haibe-Kains, GA Adam, A Hosny, F Khodakarami…</data>
  <data key="d5">2020</data>
  <data key="d6">269</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6018697739026892882&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="14158261027181687094">
  <data key="d0">Memo: Test time robustness via adaptation and augmentation</data>
  <data key="d1">14158261027181687094</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/fc28053a08f59fccb48b11f2e31e81c7-Abstract-Conference.html</data>
  <data key="d3">Memo: Test time robustness via adaptation and augmentation</data>
  <data key="d4">M Zhang, S Levine, C Finn</data>
  <data key="d5">2022</data>
  <data key="d6">89</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14158261027181687094&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="5892850021726448686">
  <data key="d0">Deblurring by realistic blurring</data>
  <data key="d1">5892850021726448686</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Deblurring_by_Realistic_Blurring_CVPR_2020_paper.html</data>
  <data key="d3">Deblurring by realistic blurring</data>
  <data key="d4">K Zhang, W Luo, Y Zhong, L Ma…</data>
  <data key="d5">2020</data>
  <data key="d6">250</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5892850021726448686&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="14228395058672942618">
  <data key="d0">Deep learning tools for the measurement of animal behavior in neuroscience</data>
  <data key="d1">14228395058672942618</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0959438819301151</data>
  <data key="d3">Deep learning tools for the measurement of animal behavior in neuroscience</data>
  <data key="d4">MW Mathis, A Mathis</data>
  <data key="d5">2020</data>
  <data key="d6">298</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14228395058672942618&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="18043841524050979844">
  <data key="d0">Data augmentation for deep-learning-based electroencephalography</data>
  <data key="d1">18043841524050979844</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0165027020303083</data>
  <data key="d3">Data augmentation for deep-learning-based electroencephalography</data>
  <data key="d4">E Lashgari, D Liang, U Maoz</data>
  <data key="d5">2020</data>
  <data key="d6">210</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18043841524050979844&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="16629143072867731417">
  <data key="d0">The risks of invariant risk minimization</data>
  <data key="d1">16629143072867731417</data>
  <data key="d2">https://arxiv.org/abs/2010.05761</data>
  <data key="d3">The risks of invariant risk minimization</data>
  <data key="d4">E Rosenfeld, P Ravikumar, A Risteski</data>
  <data key="d5">2020</data>
  <data key="d6">209</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16629143072867731417&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="370769816745103063">
  <data key="d0">Graph neural networks: foundation, frontiers and applications</data>
  <data key="d1">370769816745103063</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3534678.3542609</data>
  <data key="d3">Graph neural networks: foundation, frontiers and applications</data>
  <data key="d4">L Wu, P Cui, J Pei, L Zhao, X Guo</data>
  <data key="d5">2022</data>
  <data key="d6">140</data>
  <data key="d7">https://scholar.google.com/scholar?cites=370769816745103063&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="1555581554615315719">
  <data key="d0">A machine learning-based framework for diagnosis of COVID-19 from chest X-ray images</data>
  <data key="d1">1555581554615315719</data>
  <data key="d2">https://link.springer.com/article/10.1007/s12539-020-00403-6</data>
  <data key="d3">A machine learning-based framework for diagnosis of COVID-19 from chest X-ray images</data>
  <data key="d4">J Rasheed, AA Hameed, C Djeddi, A Jamil…</data>
  <data key="d5">2021</data>
  <data key="d6">168</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1555581554615315719&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="7294969416203662369">
  <data key="d0">[CITATION][C]</data>
  <data key="d1">7294969416203662369</data>
  <data key="d3">[CITATION][C]</data>
  <data key="d4">HA Khan, W Jue, M Mushtaq, MU Mushtaq</data>
  <data key="d5">2021</data>
  <data key="d6">225</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7294969416203662369&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="9634990485617283026">
  <data key="d0">Data augmentation for deep graph learning: A survey</data>
  <data key="d1">9634990485617283026</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3575637.3575646</data>
  <data key="d3">Data augmentation for deep graph learning: A survey</data>
  <data key="d4">K Ding, Z Xu, H Tong, H Liu</data>
  <data key="d5">2022</data>
  <data key="d6">92</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9634990485617283026&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="5055262884129010690">
  <data key="d0">An enhanced deep learning approach for brain cancer MRI images classification using residual networks</data>
  <data key="d1">5055262884129010690</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0933365719306177</data>
  <data key="d3">An enhanced deep learning approach for brain cancer MRI images classification using residual networks</data>
  <data key="d4">SAA Ismael, A Mohammed, H Hefny</data>
  <data key="d5">2020</data>
  <data key="d6">259</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5055262884129010690&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="3398859238063529749">
  <data key="d0">Humble teachers teach better students for semi-supervised object detection</data>
  <data key="d1">3398859238063529749</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Tang_Humble_Teachers_Teach_Better_Students_for_Semi-Supervised_Object_Detection_CVPR_2021_paper.html</data>
  <data key="d3">Humble teachers teach better students for semi-supervised object detection</data>
  <data key="d4">Y Tang, W Chen, Y Luo…</data>
  <data key="d5">2021</data>
  <data key="d6">112</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3398859238063529749&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="14585779787909883755">
  <data key="d0">3d human action representation learning via cross-view consistency pursuit</data>
  <data key="d1">14585779787909883755</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Li_3D_Human_Action_Representation_Learning_via_Cross-View_Consistency_Pursuit_CVPR_2021_paper.html</data>
  <data key="d3">3d human action representation learning via cross-view consistency pursuit</data>
  <data key="d4">L Li, M Wang, B Ni, H Wang, J Yang…</data>
  <data key="d5">2021</data>
  <data key="d6">116</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14585779787909883755&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="4956847805201710311">
  <data key="d0">State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis</data>
  <data key="d1">4956847805201710311</data>
  <data key="d2">https://www.nature.com/articles/s41467-020-19266-y</data>
  <data key="d3">State-of-the-art augmented NLP transformer models for direct and single-step retrosynthesis</data>
  <data key="d4">IV Tetko, P Karpov, R Van Deursen, G Godin</data>
  <data key="d5">2020</data>
  <data key="d6">203</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4956847805201710311&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="6542176543643613250">
  <data key="d0">A comprehensive survey of recent trends in deep learning for digital images augmentation</data>
  <data key="d1">6542176543643613250</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10462-021-10066-4</data>
  <data key="d3">A comprehensive survey of recent trends in deep learning for digital images augmentation</data>
  <data key="d4">NE Khalifa, M Loey, S Mirjalili</data>
  <data key="d5">2022</data>
  <data key="d6">119</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6542176543643613250&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="13844417272486663442">
  <data key="d0">Variability and reproducibility in deep learning for medical image segmentation</data>
  <data key="d1">13844417272486663442</data>
  <data key="d2">https://www.nature.com/articles/s41598-020-69920-0</data>
  <data key="d3">Variability and reproducibility in deep learning for medical image segmentation</data>
  <data key="d4">F Renard, S Guedria, ND Palma, N Vuillerme</data>
  <data key="d5">2020</data>
  <data key="d6">148</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13844417272486663442&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="1995042310766600523">
  <data key="d0">A survey of public datasets for computer vision tasks in precision agriculture</data>
  <data key="d1">1995042310766600523</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169920312709</data>
  <data key="d3">A survey of public datasets for computer vision tasks in precision agriculture</data>
  <data key="d4">Y Lu, S Young</data>
  <data key="d5">2020</data>
  <data key="d6">168</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1995042310766600523&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="17455667949400521995">
  <data key="d0">Statistical and machine learning models in credit scoring: A systematic literature survey</data>
  <data key="d1">17455667949400521995</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1568494620302039</data>
  <data key="d3">Statistical and machine learning models in credit scoring: A systematic literature survey</data>
  <data key="d4">X Dastile, T Celik, M Potsane</data>
  <data key="d5">2020</data>
  <data key="d6">219</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17455667949400521995&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="1824518314930278626">
  <data key="d0">Generative adversarial networks (GANs) for image augmentation in agriculture: A systematic review</data>
  <data key="d1">1824518314930278626</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169922005233</data>
  <data key="d3">Generative adversarial networks (GANs) for image augmentation in agriculture: A systematic review</data>
  <data key="d4">Y Lu, D Chen, E Olaniyi, Y Huang</data>
  <data key="d5">2022</data>
  <data key="d6">57</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1824518314930278626&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="16973471166771569958">
  <data key="d0">Detection of coronavirus (COVID-19) associated pneumonia based on generative adversarial networks and a fine-tuned deep transfer learning model using chest X …</data>
  <data key="d1">16973471166771569958</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20601-6_22</data>
  <data key="d3">Detection of coronavirus (COVID-19) associated pneumonia based on generative adversarial networks and a fine-tuned deep transfer learning model using chest X …</data>
  <data key="d4">NEM Khalifa, MHN Taha, AE Hassanien…</data>
  <data key="d5">2022</data>
  <data key="d6">205</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16973471166771569958&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="11854062718992351025">
  <data key="d0">Data augmentation for brain-tumor segmentation: a review</data>
  <data key="d1">11854062718992351025</data>
  <data key="d2">https://www.frontiersin.org/articles/10.3389/fncom.2019.00083/full</data>
  <data key="d3">Data augmentation for brain-tumor segmentation: a review</data>
  <data key="d4">J Nalepa, M Marcinkiewicz, M Kawulok</data>
  <data key="d5">2019</data>
  <data key="d6">229</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11854062718992351025&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="14217745962583788184">
  <data key="d0">Machine learning in aerodynamic shape optimization</data>
  <data key="d1">14217745962583788184</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0376042122000410</data>
  <data key="d3">Machine learning in aerodynamic shape optimization</data>
  <data key="d4">J Li, X Du, JRRA Martins</data>
  <data key="d5">2022</data>
  <data key="d6">59</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14217745962583788184&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="5951026065039086409">
  <data key="d0">Deep learning in medical imaging</data>
  <data key="d1">5951026065039086409</data>
  <data key="d2">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6945006/</data>
  <data key="d3">Deep learning in medical imaging</data>
  <data key="d4">M Kim, J Yun, Y Cho, K Shin, R Jang, H Bae, N Kim</data>
  <data key="d5">2019</data>
  <data key="d6">225</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5951026065039086409&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="7826621618102468948">
  <data key="d0">Advances in data preprocessing for biomedical data fusion: An overview of the methods, challenges, and prospects</data>
  <data key="d1">7826621618102468948</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1566253521001354</data>
  <data key="d3">Advances in data preprocessing for biomedical data fusion: An overview of the methods, challenges, and prospects</data>
  <data key="d4">S Wang, ME Celebi, YD Zhang, X Yu, S Lu, X Yao…</data>
  <data key="d5">2021</data>
  <data key="d6">98</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7826621618102468948&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="7802246301550866837">
  <data key="d0">A survey of generalisation in deep reinforcement learning</data>
  <data key="d1">7802246301550866837</data>
  <data key="d2">https://ui.adsabs.harvard.edu/abs/2021arXiv211109794K/abstract</data>
  <data key="d3">A survey of generalisation in deep reinforcement learning</data>
  <data key="d4">R Kirk, A Zhang, E Grefenstette…</data>
  <data key="d5">2021</data>
  <data key="d6">157</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7802246301550866837&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="10296237619596505984">
  <data key="d0">COVID-19 detection and disease progression visualization: Deep learning on chest X-rays for classification and coarse localization</data>
  <data key="d1">10296237619596505984</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10489-020-01867-1</data>
  <data key="d3">COVID-19 detection and disease progression visualization: Deep learning on chest X-rays for classification and coarse localization</data>
  <data key="d4">T Zebin, S Rezvy</data>
  <data key="d5">2021</data>
  <data key="d6">171</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10296237619596505984&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="2178700053782973603">
  <data key="d0">Deep building footprint update network: A semi-supervised method for updating existing building footprint from bi-temporal remote sensing images</data>
  <data key="d1">2178700053782973603</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0034425721003096</data>
  <data key="d3">Deep building footprint update network: A semi-supervised method for updating existing building footprint from bi-temporal remote sensing images</data>
  <data key="d4">H Guo, Q Shi, A Marinoni, B Du, L Zhang</data>
  <data key="d5">2021</data>
  <data key="d6">88</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2178700053782973603&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="10604152332073933497">
  <data key="d0">Artificial intelligence and machine learning for medical imaging: A technology review</data>
  <data key="d1">10604152332073933497</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1120179721001733</data>
  <data key="d3">Artificial intelligence and machine learning for medical imaging: A technology review</data>
  <data key="d4">A Barragán-Montero, U Javaid, G Valdés, D Nguyen…</data>
  <data key="d5">2021</data>
  <data key="d6">128</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10604152332073933497&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="7188590031243869377">
  <data key="d0">Models genesis</data>
  <data key="d1">7188590031243869377</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841520302048</data>
  <data key="d3">Models genesis</data>
  <data key="d4">Z Zhou, V Sodha, J Pang, MB Gotway, J Liang</data>
  <data key="d5">2021</data>
  <data key="d6">184</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7188590031243869377&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="15803341694212540345">
  <data key="d0">IoT and interpretable machine learning based framework for disease prediction in pearl millet</data>
  <data key="d1">15803341694212540345</data>
  <data key="d2">https://www.mdpi.com/1424-8220/21/16/5386</data>
  <data key="d3">IoT and interpretable machine learning based framework for disease prediction in pearl millet</data>
  <data key="d4">N Kundu, G Rani, VS Dhaka, K Gupta, SC Nayak…</data>
  <data key="d5">2021</data>
  <data key="d6">107</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15803341694212540345&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="6256455976929564913">
  <data key="d0">Directional graph networks</data>
  <data key="d1">6256455976929564913</data>
  <data key="d2">https://proceedings.mlr.press/v139/beaini21a.html</data>
  <data key="d3">Directional graph networks</data>
  <data key="d4">D Beaini, S Passaro, V Létourneau…</data>
  <data key="d5">2021</data>
  <data key="d6">120</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6256455976929564913&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="15580172266410736369">
  <data key="d0">A comprehensive survey of image augmentation techniques for deep learning</data>
  <data key="d1">15580172266410736369</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0031320323000481</data>
  <data key="d3">A comprehensive survey of image augmentation techniques for deep learning</data>
  <data key="d4">M Xu, S Yoon, A Fuentes, DS Park</data>
  <data key="d5">2023</data>
  <data key="d6">75</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15580172266410736369&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="6628585655867076014">
  <data key="d0">Avoiding overfitting: A survey on regularization methods for convolutional neural networks</data>
  <data key="d1">6628585655867076014</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3510413</data>
  <data key="d3">Avoiding overfitting: A survey on regularization methods for convolutional neural networks</data>
  <data key="d4">CFGD Santos, JP Papa</data>
  <data key="d5">2022</data>
  <data key="d6">70</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6628585655867076014&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="8885814579435600701">
  <data key="d0">COVID-19 salivary Raman fingerprint: innovative approach for the detection of current and past SARS-CoV-2 infections</data>
  <data key="d1">8885814579435600701</data>
  <data key="d2">https://www.nature.com/articles/s41598-021-84565-3</data>
  <data key="d3">COVID-19 salivary Raman fingerprint: innovative approach for the detection of current and past SARS-CoV-2 infections</data>
  <data key="d4">C Carlomagno, D Bertazioli, A Gualerzi, S Picciolini…</data>
  <data key="d5">2021</data>
  <data key="d6">96</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8885814579435600701&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="1656744800823975080">
  <data key="d0">The effects of regularization and data augmentation are class dependent</data>
  <data key="d1">1656744800823975080</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/f73c04538a5e1cad40ba5586b4b517d3-Abstract-Conference.html</data>
  <data key="d3">The effects of regularization and data augmentation are class dependent</data>
  <data key="d4">R Balestriero, L Bottou…</data>
  <data key="d5">2022</data>
  <data key="d6">52</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1656744800823975080&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="10374132085208051787">
  <data key="d0">A survey on deep learning-based change detection from high-resolution remote sensing images</data>
  <data key="d1">10374132085208051787</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/7/1552</data>
  <data key="d3">A survey on deep learning-based change detection from high-resolution remote sensing images</data>
  <data key="d4">H Jiang, M Peng, Y Zhong, H Xie, Z Hao, J Lin, X Ma…</data>
  <data key="d5">2022</data>
  <data key="d6">67</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10374132085208051787&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="10940775338620708972">
  <data key="d0">How variability shapes learning and generalization</data>
  <data key="d1">10940775338620708972</data>
  <data key="d2">https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(22)00065-1</data>
  <data key="d3">How variability shapes learning and generalization</data>
  <data key="d4">L Raviv, G Lupyan, SC Green</data>
  <data key="d5">2022</data>
  <data key="d6">53</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10940775338620708972&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="17945544379535649211">
  <data key="d0">CovidXrayNet: Optimizing data augmentation and CNN hyperparameters for improved COVID-19 detection from CXR</data>
  <data key="d1">17945544379535649211</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0010482521001694</data>
  <data key="d3">CovidXrayNet: Optimizing data augmentation and CNN hyperparameters for improved COVID-19 detection from CXR</data>
  <data key="d4">MMA Monshi, J Poon, V Chung, FM Monshi</data>
  <data key="d5">2021</data>
  <data key="d6">94</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17945544379535649211&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="16412654433518658547">
  <data key="d0">Insect classification and detection in field crops using modern machine learning techniques</data>
  <data key="d1">16412654433518658547</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2214317320302067</data>
  <data key="d3">Insect classification and detection in field crops using modern machine learning techniques</data>
  <data key="d4">T Kasinathan, D Singaraju, SR Uyyala</data>
  <data key="d5">2021</data>
  <data key="d6">142</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16412654433518658547&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="804636322301015973">
  <data key="d0">A survey on data‐efficient algorithms in big data era</data>
  <data key="d1">804636322301015973</data>
  <data key="d2">https://link.springer.com/article/10.1186/s40537-021-00419-9</data>
  <data key="d3">A survey on data‐efficient algorithms in big data era</data>
  <data key="d4">A Adadi</data>
  <data key="d5">2021</data>
  <data key="d6">132</data>
  <data key="d7">https://scholar.google.com/scholar?cites=804636322301015973&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="6794503273897899990">
  <data key="d0">Stabilizing deep q-learning with convnets and vision transformers under data augmentation</data>
  <data key="d1">6794503273897899990</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/1e0f65eb20acbfb27ee05ddc000b50ec-Abstract.html</data>
  <data key="d3">Stabilizing deep q-learning with convnets and vision transformers under data augmentation</data>
  <data key="d4">N Hansen, H Su, X Wang</data>
  <data key="d5">2021</data>
  <data key="d6">63</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6794503273897899990&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="5744336008744333374">
  <data key="d0">Applications of explainable artificial intelligence in diagnosis and surgery</data>
  <data key="d1">5744336008744333374</data>
  <data key="d2">https://www.mdpi.com/2075-4418/12/2/237</data>
  <data key="d3">Applications of explainable artificial intelligence in diagnosis and surgery</data>
  <data key="d4">Y Zhang, Y Weng, J Lund</data>
  <data key="d5">2022</data>
  <data key="d6">94</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5744336008744333374&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="11739296977766500505">
  <data key="d0">Channel augmented joint learning for visible-infrared recognition</data>
  <data key="d1">11739296977766500505</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Ye_Channel_Augmented_Joint_Learning_for_Visible-Infrared_Recognition_ICCV_2021_paper.html</data>
  <data key="d3">Channel augmented joint learning for visible-infrared recognition</data>
  <data key="d4">M Ye, W Ruan, B Du, MZ Shou</data>
  <data key="d5">2021</data>
  <data key="d6">79</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11739296977766500505&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="9595578074718261885">
  <data key="d0">An efficient CNN model for COVID-19 disease detection based on X-ray image classification</data>
  <data key="d1">9595578074718261885</data>
  <data key="d2">https://www.hindawi.com/journals/complexity/2021/6621607/</data>
  <data key="d3">An efficient CNN model for COVID-19 disease detection based on X-ray image classification</data>
  <data key="d4">AA Reshi, F Rustam, A Mehmood, A Alhossan…</data>
  <data key="d5">2021</data>
  <data key="d6">107</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9595578074718261885&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="7437652212406869439">
  <data key="d0">Plant diseases recognition on images using convolutional neural networks: A systematic review</data>
  <data key="d1">7437652212406869439</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169921001435</data>
  <data key="d3">Plant diseases recognition on images using convolutional neural networks: A systematic review</data>
  <data key="d4">A Abade, PA Ferreira, F de Barros Vidal</data>
  <data key="d5">2021</data>
  <data key="d6">121</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7437652212406869439&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="14190759885589230911">
  <data key="d0">Comprehensive survey of recent drug discovery using deep learning</data>
  <data key="d1">14190759885589230911</data>
  <data key="d2">https://www.mdpi.com/1422-0067/22/18/9983</data>
  <data key="d3">Comprehensive survey of recent drug discovery using deep learning</data>
  <data key="d4">J Kim, S Park, D Min, W Kim</data>
  <data key="d5">2021</data>
  <data key="d6">75</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14190759885589230911&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="15052277191896050356">
  <data key="d0">A literature review on one-class classification and its potential applications in big data</data>
  <data key="d1">15052277191896050356</data>
  <data key="d2">https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00514-x</data>
  <data key="d3">A literature review on one-class classification and its potential applications in big data</data>
  <data key="d4">N Seliya, A Abdollah Zadeh…</data>
  <data key="d5">2021</data>
  <data key="d6">67</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15052277191896050356&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="8770863820400864485">
  <data key="d0">Differentiable biology: using deep learning for biophysics-based and data-driven modeling of molecular mechanisms</data>
  <data key="d1">8770863820400864485</data>
  <data key="d2">https://www.nature.com/articles/s41592-021-01283-4</data>
  <data key="d3">Differentiable biology: using deep learning for biophysics-based and data-driven modeling of molecular mechanisms</data>
  <data key="d4">M AlQuraishi, PK Sorger</data>
  <data key="d5">2021</data>
  <data key="d6">49</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8770863820400864485&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="11914760532013868738">
  <data key="d0">Cassava disease recognition from low‐quality images using enhanced data augmentation model and deep learning</data>
  <data key="d1">11914760532013868738</data>
  <data key="d2">https://onlinelibrary.wiley.com/doi/abs/10.1111/exsy.12746</data>
  <data key="d3">Cassava disease recognition from low‐quality images using enhanced data augmentation model and deep learning</data>
  <data key="d4">OO Abayomi‐Alli, R Damaševičius, S Misra…</data>
  <data key="d5">2021</data>
  <data key="d6">84</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11914760532013868738&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="8884460723513624148">
  <data key="d0">Unlocking high-accuracy differentially private image classification through scale</data>
  <data key="d1">8884460723513624148</data>
  <data key="d2">https://arxiv.org/abs/2204.13650</data>
  <data key="d3">Unlocking high-accuracy differentially private image classification through scale</data>
  <data key="d4">S De, L Berrada, J Hayes, SL Smith, B Balle</data>
  <data key="d5">2022</data>
  <data key="d6">77</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8884460723513624148&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="16779378878512616320">
  <data key="d0">Avoiding a replication crisis in deep-learning-based bioimage analysis</data>
  <data key="d1">16779378878512616320</data>
  <data key="d2">https://www.nature.com/articles/s41592-021-01284-3</data>
  <data key="d3">Avoiding a replication crisis in deep-learning-based bioimage analysis</data>
  <data key="d4">RF Laine, I Arganda-Carreras, R Henriques…</data>
  <data key="d5">2021</data>
  <data key="d6">52</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16779378878512616320&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="17666447766930405698">
  <data key="d0">Machine learning for metal additive manufacturing: Towards a physics-informed data-driven paradigm</data>
  <data key="d1">17666447766930405698</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0278612521002259</data>
  <data key="d3">Machine learning for metal additive manufacturing: Towards a physics-informed data-driven paradigm</data>
  <data key="d4">S Guo, M Agarwal, C Cooper, Q Tian, RX Gao…</data>
  <data key="d5">2022</data>
  <data key="d6">66</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17666447766930405698&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="18148219906827393870">
  <data key="d0">Image data augmentation for deep learning: A survey</data>
  <data key="d1">18148219906827393870</data>
  <data key="d2">https://arxiv.org/abs/2204.08610</data>
  <data key="d3">Image data augmentation for deep learning: A survey</data>
  <data key="d4">S Yang, W Xiao, M Zhang, S Guo, J Zhao…</data>
  <data key="d5">2022</data>
  <data key="d6">101</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18148219906827393870&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="2194257343717484563">
  <data key="d0">Augmenting organizational decision-making with deep learning algorithms: Principles, promises, and challenges</data>
  <data key="d1">2194257343717484563</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0148296320306512</data>
  <data key="d3">Augmenting organizational decision-making with deep learning algorithms: Principles, promises, and challenges</data>
  <data key="d4">YR Shrestha, V Krishna, G von Krogh</data>
  <data key="d5">2021</data>
  <data key="d6">115</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2194257343717484563&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="9750378231570611023">
  <data key="d0">Towards a better understanding of transfer learning for medical imaging: a case study</data>
  <data key="d1">9750378231570611023</data>
  <data key="d2">https://www.mdpi.com/2076-3417/10/13/4523</data>
  <data key="d3">Towards a better understanding of transfer learning for medical imaging: a case study</data>
  <data key="d4">L Alzubaidi, MA Fadhel, O Al-Shamma, J Zhang…</data>
  <data key="d5">2020</data>
  <data key="d6">152</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9750378231570611023&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="8336676790606133743">
  <data key="d0">Hyperspectral image classification—Traditional to deep models: A survey for future prospects</data>
  <data key="d1">8336676790606133743</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9645266/</data>
  <data key="d3">Hyperspectral image classification—Traditional to deep models: A survey for future prospects</data>
  <data key="d4">M Ahmad, S Shabbir, SK Roy, D Hong…</data>
  <data key="d5">2021</data>
  <data key="d6">107</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8336676790606133743&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="9023365447299077306">
  <data key="d0">Real time pear fruit detection and counting using YOLOv4 models and deep SORT</data>
  <data key="d1">9023365447299077306</data>
  <data key="d2">https://www.mdpi.com/1424-8220/21/14/4803</data>
  <data key="d3">Real time pear fruit detection and counting using YOLOv4 models and deep SORT</data>
  <data key="d4">AIB Parico, T Ahamed</data>
  <data key="d5">2021</data>
  <data key="d6">96</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9023365447299077306&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="448552700743310645">
  <data key="d0">A comprehensive study on classification of COVID-19 on computed tomography with pretrained convolutional neural networks</data>
  <data key="d1">448552700743310645</data>
  <data key="d2">https://www.nature.com/articles/s41598-020-74164-z</data>
  <data key="d3">A comprehensive study on classification of COVID-19 on computed tomography with pretrained convolutional neural networks</data>
  <data key="d4">TD Pham</data>
  <data key="d5">2020</data>
  <data key="d6">137</data>
  <data key="d7">https://scholar.google.com/scholar?cites=448552700743310645&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="2026726248513085794">
  <data key="d0">YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</data>
  <data key="d1">2026726248513085794</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Wang_YOLOv7_Trainable_Bag-of-Freebies_Sets_New_State-of-the-Art_for_Real-Time_Object_Detectors_CVPR_2023_paper.html</data>
  <data key="d3">YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</data>
  <data key="d4">CY Wang, A Bochkovskiy…</data>
  <data key="d5">2023</data>
  <data key="d6">1921</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2026726248513085794&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="9850512646184180167">
  <data key="d0">Object detection in 20 years: A survey</data>
  <data key="d1">9850512646184180167</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10028728/</data>
  <data key="d3">Object detection in 20 years: A survey</data>
  <data key="d4">Z Zou, K Chen, Z Shi, Y Guo, J Ye</data>
  <data key="d5">2023</data>
  <data key="d6">1626</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9850512646184180167&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14638466021176544465">
  <data key="d0">Bytetrack: Multi-object tracking by associating every detection box</data>
  <data key="d1">14638466021176544465</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20047-2_1</data>
  <data key="d3">Bytetrack: Multi-object tracking by associating every detection box</data>
  <data key="d4">Y Zhang, P Sun, Y Jiang, D Yu, F Weng, Z Yuan…</data>
  <data key="d5">2022</data>
  <data key="d6">538</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14638466021176544465&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="13702720529764835843">
  <data key="d0">YOLOv6: A single-stage object detection framework for industrial applications</data>
  <data key="d1">13702720529764835843</data>
  <data key="d2">https://arxiv.org/abs/2209.02976</data>
  <data key="d3">YOLOv6: A single-stage object detection framework for industrial applications</data>
  <data key="d4">C Li, L Li, H Jiang, K Weng, Y Geng, L Li, Z Ke…</data>
  <data key="d5">2022</data>
  <data key="d6">477</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13702720529764835843&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="17646032300901507453">
  <data key="d0">Optimization strategies of fruit detection to overcome the challenge of unstructured background in field orchard environment: A review</data>
  <data key="d1">17646032300901507453</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11119-023-10009-9</data>
  <data key="d3">Optimization strategies of fruit detection to overcome the challenge of unstructured background in field orchard environment: A review</data>
  <data key="d4">Y Tang, J Qiu, Y Zhang, D Wu, Y Cao, K Zhao…</data>
  <data key="d5">2023</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17646032300901507453&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="17127958062872744853">
  <data key="d0">YOLOWeeds: a novel benchmark of YOLO object detectors for multi-class weed detection in cotton production systems</data>
  <data key="d1">17127958062872744853</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923000431</data>
  <data key="d3">YOLOWeeds: a novel benchmark of YOLO object detectors for multi-class weed detection in cotton production systems</data>
  <data key="d4">F Dang, D Chen, Y Lu, Z Li</data>
  <data key="d5">2023</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17127958062872744853&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="13222462680960111198">
  <data key="d0">An attention mechanism-improved YOLOv7 object detection algorithm for hemp duck count estimation</data>
  <data key="d1">13222462680960111198</data>
  <data key="d2">https://www.mdpi.com/2077-0472/12/10/1659</data>
  <data key="d3">An attention mechanism-improved YOLOv7 object detection algorithm for hemp duck count estimation</data>
  <data key="d4">K Jiang, T Xie, R Yan, X Wen, D Li, H Jiang, N Jiang…</data>
  <data key="d5">2022</data>
  <data key="d6">39</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13222462680960111198&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="8594852448598418616">
  <data key="d0">Zenseact open dataset: A large-scale and diverse multimodal dataset for autonomous driving</data>
  <data key="d1">8594852448598418616</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Alibeigi_Zenseact_Open_Dataset_A_Large-Scale_and_Diverse_Multimodal_Dataset_for_ICCV_2023_paper.html</data>
  <data key="d3">Zenseact open dataset: A large-scale and diverse multimodal dataset for autonomous driving</data>
  <data key="d4">M Alibeigi, W Ljungbergh, A Tonderski…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8594852448598418616&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="16827683962441657873">
  <data key="d0">Sf-yolov5: A lightweight small object detection algorithm based on improved feature fusion mode</data>
  <data key="d1">16827683962441657873</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/15/5817</data>
  <data key="d3">Sf-yolov5: A lightweight small object detection algorithm based on improved feature fusion mode</data>
  <data key="d4">H Liu, F Sun, J Gu, L Deng</data>
  <data key="d5">2022</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16827683962441657873&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="11112203079270473115">
  <data key="d0">Detection of Camellia oleifera Fruit in Complex Scenes by Using YOLOv7 and Data Augmentation</data>
  <data key="d1">11112203079270473115</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/22/11318</data>
  <data key="d3">Detection of Camellia oleifera Fruit in Complex Scenes by Using YOLOv7 and Data Augmentation</data>
  <data key="d4">D Wu, S Jiang, E Zhao, Y Liu, H Zhu, W Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">42</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11112203079270473115&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="17690753104303819861">
  <data key="d0">Adaptive Active Positioning of Camellia oleifera Fruit Picking Points: Classical Image Processing and YOLOv7 Fusion Algorithm</data>
  <data key="d1">17690753104303819861</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/24/12959</data>
  <data key="d3">Adaptive Active Positioning of Camellia oleifera Fruit Picking Points: Classical Image Processing and YOLOv7 Fusion Algorithm</data>
  <data key="d4">Y Zhou, Y Tang, X Zou, M Wu, W Tang, F Meng…</data>
  <data key="d5">2022</data>
  <data key="d6">24</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17690753104303819861&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="10264464849626080964">
  <data key="d0">Domain feature mapping with YOLOv7 for automated edge-based pallet racking inspections</data>
  <data key="d1">10264464849626080964</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/18/6927</data>
  <data key="d3">Domain feature mapping with YOLOv7 for automated edge-based pallet racking inspections</data>
  <data key="d4">M Hussain, H Al-Aqrabi, M Munawar, R Hill, T Alsboui</data>
  <data key="d5">2022</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10264464849626080964&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="17785166320928157068">
  <data key="d0">Rtmdet: An empirical study of designing real-time object detectors</data>
  <data key="d1">17785166320928157068</data>
  <data key="d2">https://arxiv.org/abs/2212.07784</data>
  <data key="d3">Rtmdet: An empirical study of designing real-time object detectors</data>
  <data key="d4">C Lyu, W Zhang, H Huang, Y Zhou, Y Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">39</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17785166320928157068&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="7665772481323515367">
  <data key="d0">Target detection method of UAV aerial imagery based on improved YOLOv5</data>
  <data key="d1">7665772481323515367</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/19/5063</data>
  <data key="d3">Target detection method of UAV aerial imagery based on improved YOLOv5</data>
  <data key="d4">X Luo, Y Wu, F Wang</data>
  <data key="d5">2022</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7665772481323515367&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="5875186414038374408">
  <data key="d0">Hybrid-YOLO for classification of insulators defects in transmission lines based on UAV</data>
  <data key="d1">5875186414038374408</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S014206152300039X</data>
  <data key="d3">Hybrid-YOLO for classification of insulators defects in transmission lines based on UAV</data>
  <data key="d4">BJ Souza, SF Stefenon, G Singh, RZ Freire</data>
  <data key="d5">2023</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5875186414038374408&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="9420485880557840183">
  <data key="d0">Real-time multi-class helmet violation detection using few-shot data sampling technique and yolov8</data>
  <data key="d1">9420485880557840183</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Aboah_Real-Time_Multi-Class_Helmet_Violation_Detection_Using_Few-Shot_Data_Sampling_Technique_CVPRW_2023_paper.html</data>
  <data key="d3">Real-time multi-class helmet violation detection using few-shot data sampling technique and yolov8</data>
  <data key="d4">A Aboah, B Wang, U Bagci…</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9420485880557840183&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14658447611882211088">
  <data key="d0">Deep learning for detecting macroplastic litter in water bodies: A review</data>
  <data key="d1">14658447611882211088</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0043135423000672</data>
  <data key="d3">Deep learning for detecting macroplastic litter in water bodies: A review</data>
  <data key="d4">T Jia, Z Kapelan, R de Vries, P Vriend, EC Peereboom…</data>
  <data key="d5">2023</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14658447611882211088&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="10221363205500346797">
  <data key="d0">Lite DETR: An interleaved multi-scale encoder for efficient detr</data>
  <data key="d1">10221363205500346797</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Li_Lite_DETR_An_Interleaved_Multi-Scale_Encoder_for_Efficient_DETR_CVPR_2023_paper.html</data>
  <data key="d3">Lite DETR: An interleaved multi-scale encoder for efficient detr</data>
  <data key="d4">F Li, A Zeng, S Liu, H Zhang, H Li…</data>
  <data key="d5">2023</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10221363205500346797&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="16559564215385655731">
  <data key="d0">A multiscale lightweight and efficient model based on YOLOv7: Applied to citrus orchard</data>
  <data key="d1">16559564215385655731</data>
  <data key="d2">https://www.mdpi.com/2223-7747/11/23/3260</data>
  <data key="d3">A multiscale lightweight and efficient model based on YOLOv7: Applied to citrus orchard</data>
  <data key="d4">J Chen, H Liu, Y Zhang, D Zhang, H Ouyang, X Chen</data>
  <data key="d5">2022</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16559564215385655731&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="958587697277086390">
  <data key="d0">Yolov6 v3. 0: A full-scale reloading</data>
  <data key="d1">958587697277086390</data>
  <data key="d2">https://arxiv.org/abs/2301.05586</data>
  <data key="d3">Yolov6 v3. 0: A full-scale reloading</data>
  <data key="d4">C Li, L Li, Y Geng, H Jiang, M Cheng, B Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">42</data>
  <data key="d7">https://scholar.google.com/scholar?cites=958587697277086390&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="17898472391686934449">
  <data key="d0">Deep object detection of crop weeds: Performance of YOLOv7 on a real case dataset from UAV images</data>
  <data key="d1">17898472391686934449</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/2/539?ref=blog.roboflow.com</data>
  <data key="d3">Deep object detection of crop weeds: Performance of YOLOv7 on a real case dataset from UAV images</data>
  <data key="d4">I Gallo, AU Rehman, RH Dehkordi, N Landro…</data>
  <data key="d5">2023</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17898472391686934449&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="6352382094902298941">
  <data key="d0">Deepsegmenter: Temporal action localization for detecting anomalies in untrimmed naturalistic driving videos</data>
  <data key="d1">6352382094902298941</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Aboah_DeepSegmenter_Temporal_Action_Localization_for_Detecting_Anomalies_in_Untrimmed_Naturalistic_CVPRW_2023_paper.html</data>
  <data key="d3">Deepsegmenter: Temporal action localization for detecting anomalies in untrimmed naturalistic driving videos</data>
  <data key="d4">A Aboah, U Bagci, AR Mussah…</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6352382094902298941&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14183845217069566705">
  <data key="d0">Improved YOLOv3 model with feature map cropping for multi-scale road object detection</data>
  <data key="d1">14183845217069566705</data>
  <data key="d2">https://iopscience.iop.org/article/10.1088/1361-6501/acb075/meta</data>
  <data key="d3">Improved YOLOv3 model with feature map cropping for multi-scale road object detection</data>
  <data key="d4">L Shen, H Tao, Y Ni, Y Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">40</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14183845217069566705&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14052692408086479292">
  <data key="d0">Development of YOLOv5-based real-time smart monitoring system for increasing lab safety awareness in educational institutions</data>
  <data key="d1">14052692408086479292</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/22/8820</data>
  <data key="d3">Development of YOLOv5-based real-time smart monitoring system for increasing lab safety awareness in educational institutions</data>
  <data key="d4">L Ali, F Alnajjar, MMA Parambil, MI Younes…</data>
  <data key="d5">2022</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14052692408086479292&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="16133934620382970642">
  <data key="d0">Stall number detection of cow teats key frames</data>
  <data key="d1">16133934620382970642</data>
  <data key="d2">https://arxiv.org/abs/2303.10444</data>
  <data key="d3">Stall number detection of cow teats key frames</data>
  <data key="d4">Y Zhang</data>
  <data key="d5">2023</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16133934620382970642&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="15271212594475018431">
  <data key="d0">Adversarial patch attack on multi-scale object detection for uav remote sensing images</data>
  <data key="d1">15271212594475018431</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/21/5298</data>
  <data key="d3">Adversarial patch attack on multi-scale object detection for uav remote sensing images</data>
  <data key="d4">Y Zhang, Y Zhang, J Qi, K Bin, H Wen, X Tong…</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15271212594475018431&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="8819196546794680544">
  <data key="d0">A comprehensive review of YOLO: From YOLOv1 to YOLOv8 and beyond</data>
  <data key="d1">8819196546794680544</data>
  <data key="d2">https://arxiv.org/abs/2304.00501</data>
  <data key="d3">A comprehensive review of YOLO: From YOLOv1 to YOLOv8 and beyond</data>
  <data key="d4">J Terven, D Cordova-Esparza</data>
  <data key="d5">2023</data>
  <data key="d6">92</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8819196546794680544&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="12345969996209231071">
  <data key="d0">Insulator-defect detection algorithm based on improved YOLOv7</data>
  <data key="d1">12345969996209231071</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/22/8801</data>
  <data key="d3">Insulator-defect detection algorithm based on improved YOLOv7</data>
  <data key="d4">J Zheng, H Wu, H Zhang, Z Wang, W Xu</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12345969996209231071&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="13723544815291086075">
  <data key="d0">1st workshop on maritime computer vision (macvi) 2023: Challenge results</data>
  <data key="d1">13723544815291086075</data>
  <data key="d2">https://openaccess.thecvf.com/content/WACV2023W/MaCVi/html/Kiefer_1st_Workshop_on_Maritime_Computer_Vision_MaCVi_2023_Challenge_Results_WACVW_2023_paper.html</data>
  <data key="d3">1st workshop on maritime computer vision (macvi) 2023: Challenge results</data>
  <data key="d4">B Kiefer, M Kristan, J Perš, L Žust…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13723544815291086075&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="7938131059737931281">
  <data key="d0">Mislaying behavior detection in cage-free hens with deep learning technologies</data>
  <data key="d1">7938131059737931281</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0032579123002511</data>
  <data key="d3">Mislaying behavior detection in cage-free hens with deep learning technologies</data>
  <data key="d4">RB Bist, X Yang, S Subedi, L Chai</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7938131059737931281&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="4384676216153265937">
  <data key="d0">Yolov7-sea: Object detection of maritime uav images based on improved yolov7</data>
  <data key="d1">4384676216153265937</data>
  <data key="d2">https://openaccess.thecvf.com/content/WACV2023W/MaCVi/html/Zhao_YOLOv7-Sea_Object_Detection_of_Maritime_UAV_Images_Based_on_Improved_WACVW_2023_paper.html</data>
  <data key="d3">Yolov7-sea: Object detection of maritime uav images based on improved yolov7</data>
  <data key="d4">H Zhao, H Zhang, Y Zhao</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4384676216153265937&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="10116304010765610972">
  <data key="d0">Gbh-yolov5: Ghost convolution with bottleneckcsp and tiny target prediction head incorporating yolov5 for pv panel defect detection</data>
  <data key="d1">10116304010765610972</data>
  <data key="d2">https://www.mdpi.com/2079-9292/12/3/561</data>
  <data key="d3">Gbh-yolov5: Ghost convolution with bottleneckcsp and tiny target prediction head incorporating yolov5 for pv panel defect detection</data>
  <data key="d4">L Li, Z Wang, T Zhang</data>
  <data key="d5">2023</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10116304010765610972&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="2663039362134560569">
  <data key="d0">A reinforcement learning paradigm of configuring visual enhancement for object detection in underwater scenes</data>
  <data key="d1">2663039362134560569</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10058092/</data>
  <data key="d3">A reinforcement learning paradigm of configuring visual enhancement for object detection in underwater scenes</data>
  <data key="d4">H Wang, S Sun, X Bai, J Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2663039362134560569&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3281046445393215005">
  <data key="d0">CEAM-YOLOv7: Improved YOLOv7 based on channel expansion and attention mechanism for driver distraction behavior detection</data>
  <data key="d1">3281046445393215005</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9980374/</data>
  <data key="d3">CEAM-YOLOv7: Improved YOLOv7 based on channel expansion and attention mechanism for driver distraction behavior detection</data>
  <data key="d4">S Liu, Y Wang, Q Yu, H Liu, Z Peng</data>
  <data key="d5">2022</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3281046445393215005&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="176548951834254449">
  <data key="d0">GGT-YOLO: a novel object detection algorithm for drone-based maritime cruising</data>
  <data key="d1">176548951834254449</data>
  <data key="d2">https://www.mdpi.com/2504-446X/6/11/335</data>
  <data key="d3">GGT-YOLO: a novel object detection algorithm for drone-based maritime cruising</data>
  <data key="d4">Y Li, H Yuan, Y Wang, C Xiao</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=176548951834254449&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="7090138214741207408">
  <data key="d0">One-to-Few Label Assignment for End-to-End Dense Detection</data>
  <data key="d1">7090138214741207408</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Li_One-to-Few_Label_Assignment_for_End-to-End_Dense_Detection_CVPR_2023_paper.html</data>
  <data key="d3">One-to-Few Label Assignment for End-to-End Dense Detection</data>
  <data key="d4">S Li, M Li, R Li, C He, L Zhang</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7090138214741207408&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14151669589159930339">
  <data key="d0">Using lightweight deep learning algorithm for real-time detection of apple flowers in natural environments</data>
  <data key="d1">14151669589159930339</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923001539</data>
  <data key="d3">Using lightweight deep learning algorithm for real-time detection of apple flowers in natural environments</data>
  <data key="d4">Y Shang, X Xu, Y Jiao, Z Wang, Z Hua…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14151669589159930339&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="6535827589324635187">
  <data key="d0">Efficient detection model of steel strip surface defects based on YOLO-V7</data>
  <data key="d1">6535827589324635187</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9994733/</data>
  <data key="d3">Efficient detection model of steel strip surface defects based on YOLO-V7</data>
  <data key="d4">Y Wang, H Wang, Z Xin</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6535827589324635187&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="12472359179257688469">
  <data key="d0">TCPMFNet: An infrared and visible image fusion network with composite auto encoder and transformer–convolutional parallel mixed fusion strategy</data>
  <data key="d1">12472359179257688469</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1350449522003863</data>
  <data key="d3">TCPMFNet: An infrared and visible image fusion network with composite auto encoder and transformer–convolutional parallel mixed fusion strategy</data>
  <data key="d4">S Yi, G Jiang, X Liu, J Li, L Chen</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12472359179257688469&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="9925896961448568187">
  <data key="d0">YOLOv5s-FP: a novel method for in-field pear detection using a transformer encoder and multi-scale collaboration perception</data>
  <data key="d1">9925896961448568187</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/1/30</data>
  <data key="d3">YOLOv5s-FP: a novel method for in-field pear detection using a transformer encoder and multi-scale collaboration perception</data>
  <data key="d4">Y Li, Y Rao, X Jin, Z Jiang, Y Wang, T Wang, F Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9925896961448568187&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14205155098487362799">
  <data key="d0">YOLO-HR: Improved YOLOv5 for Object Detection in High-Resolution Optical Remote Sensing Images</data>
  <data key="d1">14205155098487362799</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/3/614</data>
  <data key="d3">YOLO-HR: Improved YOLOv5 for Object Detection in High-Resolution Optical Remote Sensing Images</data>
  <data key="d4">D Wan, R Lu, S Wang, S Shen, T Xu, X Lang</data>
  <data key="d5">2023</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14205155098487362799&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="7644674706913288304">
  <data key="d0">Cross-modal text and visual generation: A systematic review. Part 1—Image to text</data>
  <data key="d1">7644674706913288304</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1566253523000143</data>
  <data key="d3">Cross-modal text and visual generation: A systematic review. Part 1—Image to text</data>
  <data key="d4">M Żelaszczyk, J Mańdziuk</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7644674706913288304&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="15845207782709112329">
  <data key="d0">Automated optical inspection of FAST's reflector surface using drones and computer vision</data>
  <data key="d1">15845207782709112329</data>
  <data key="d2">https://www.light-am.com/article/doi/10.37188/lam.2023.001</data>
  <data key="d3">Automated optical inspection of FAST's reflector surface using drones and computer vision</data>
  <data key="d4">J Li, S Jiang, L Song, P Peng, F Mu, H Li…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15845207782709112329&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="2749712196276682618">
  <data key="d0">Automated vehicle counting from pre-recorded video using you only look once (YOLO) object detection model</data>
  <data key="d1">2749712196276682618</data>
  <data key="d2">https://www.mdpi.com/2313-433X/9/7/131</data>
  <data key="d3">Automated vehicle counting from pre-recorded video using you only look once (YOLO) object detection model</data>
  <data key="d4">M Majumder, C Wilmot</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2749712196276682618&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="8263847837619800805">
  <data key="d0">Research on safety helmet detection algorithm based on improved YOLOv5s</data>
  <data key="d1">8263847837619800805</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/13/5824</data>
  <data key="d3">Research on safety helmet detection algorithm based on improved YOLOv5s</data>
  <data key="d4">Q An, Y Xu, J Yu, M Tang, T Liu, F Xu</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8263847837619800805&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="7690447212830750827">
  <data key="d0">Brain tumor detection based on deep learning approaches and magnetic resonance imaging</data>
  <data key="d1">7690447212830750827</data>
  <data key="d2">https://www.mdpi.com/2072-6694/15/16/4172</data>
  <data key="d3">Brain tumor detection based on deep learning approaches and magnetic resonance imaging</data>
  <data key="d4">AB Abdusalomov, M Mukhiddinov, TK Whangbo</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7690447212830750827&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="10608878509050050554">
  <data key="d0">Multi-Scale Ship Detection Algorithm Based on YOLOv7 for Complex Scene SAR Images</data>
  <data key="d1">10608878509050050554</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/8/2071</data>
  <data key="d3">Multi-Scale Ship Detection Algorithm Based on YOLOv7 for Complex Scene SAR Images</data>
  <data key="d4">Z Chen, C Liu, VF Filaretov, DA Yukhimets</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10608878509050050554&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="7076299391791953686">
  <data key="d0">Three-stage pavement crack localization and segmentation algorithm based on digital image processing and deep learning techniques</data>
  <data key="d1">7076299391791953686</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/21/8459</data>
  <data key="d3">Three-stage pavement crack localization and segmentation algorithm based on digital image processing and deep learning techniques</data>
  <data key="d4">Z Yang, C Ni, L Li, W Luo, Y Qin</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7076299391791953686&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="7291687913883151332">
  <data key="d0">Machine learning in solar plants inspection automation</data>
  <data key="d1">7291687913883151332</data>
  <data key="d2">https://www.mdpi.com/1996-1073/15/16/5966</data>
  <data key="d3">Machine learning in solar plants inspection automation</data>
  <data key="d4">J Starzyński, P Zawadzki, D Harańczyk</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7291687913883151332&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="7099002288627031897">
  <data key="d0">A cooperative vehicle-infrastructure system for road hazards detection with edge intelligence</data>
  <data key="d1">7099002288627031897</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10038653/</data>
  <data key="d3">A cooperative vehicle-infrastructure system for road hazards detection with edge intelligence</data>
  <data key="d4">C Chen, G Yao, L Liu, Q Pei, H Song…</data>
  <data key="d5">2023</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7099002288627031897&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="15184762675201395065">
  <data key="d0">Classification of diabetic retinopathy severity in fundus images using the vision transformer and residual attention</data>
  <data key="d1">15184762675201395065</data>
  <data key="d2">https://www.hindawi.com/journals/cin/2023/1305583/</data>
  <data key="d3">Classification of diabetic retinopathy severity in fundus images using the vision transformer and residual attention</data>
  <data key="d4">Z Gu, Y Li, Z Wang, J Kan, J Shu, Q Wang</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15184762675201395065&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="6327480080772807927">
  <data key="d0">YOLOv7-RAR for Urban Vehicle Detection</data>
  <data key="d1">6327480080772807927</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/4/1801</data>
  <data key="d3">YOLOv7-RAR for Urban Vehicle Detection</data>
  <data key="d4">Y Zhang, Y Sun, Z Wang, Y Jiang</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6327480080772807927&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="8866059787183806571">
  <data key="d0">Lightweight SM-YOLOv5 tomato fruit detection algorithm for plant factory</data>
  <data key="d1">8866059787183806571</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/6/3336</data>
  <data key="d3">Lightweight SM-YOLOv5 tomato fruit detection algorithm for plant factory</data>
  <data key="d4">X Wang, Z Wu, M Jia, T Xu, C Pan, X Qi, M Zhao</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8866059787183806571&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="13803925855903140317">
  <data key="d0">Feature-enhanced centernet for small object detection in remote sensing images</data>
  <data key="d1">13803925855903140317</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/21/5488</data>
  <data key="d3">Feature-enhanced centernet for small object detection in remote sensing images</data>
  <data key="d4">T Shi, J Gong, J Hu, X Zhi, W Zhang, Y Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13803925855903140317&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="16632032221621362841">
  <data key="d0">Video object tracking based on YOLOv7 and DeepSORT</data>
  <data key="d1">16632032221621362841</data>
  <data key="d2">https://arxiv.org/abs/2207.12202</data>
  <data key="d3">Video object tracking based on YOLOv7 and DeepSORT</data>
  <data key="d4">F Yang, X Zhang, B Liu</data>
  <data key="d5">2022</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16632032221621362841&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="18391486949977622277">
  <data key="d0">Learning emotion representations from verbal and nonverbal communication</data>
  <data key="d1">18391486949977622277</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Learning_Emotion_Representations_From_Verbal_and_Nonverbal_Communication_CVPR_2023_paper.html</data>
  <data key="d3">Learning emotion representations from verbal and nonverbal communication</data>
  <data key="d4">S Zhang, Y Pan, JZ Wang</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18391486949977622277&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="10392802398675002497">
  <data key="d0">Two-stream regression network for dental implant position prediction</data>
  <data key="d1">10392802398675002497</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417423016378</data>
  <data key="d3">Two-stream regression network for dental implant position prediction</data>
  <data key="d4">X Yang, X Li, X Li, W Chen, L Shen, X Li…</data>
  <data key="d5">2024</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10392802398675002497&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="9903488889701687950">
  <data key="d0">Semantic scene understanding with large language models on unmanned aerial vehicles</data>
  <data key="d1">9903488889701687950</data>
  <data key="d2">https://www.mdpi.com/2504-446X/7/2/114</data>
  <data key="d3">Semantic scene understanding with large language models on unmanned aerial vehicles</data>
  <data key="d4">J de Curtò, I de Zarzà, CT Calafate</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9903488889701687950&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="8851123877575968338">
  <data key="d0">Recognition and Counting of Apples in a Dynamic State Using a 3D Camera and Deep Learning Algorithms for Robotic Harvesting Systems</data>
  <data key="d1">8851123877575968338</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/8/3810</data>
  <data key="d3">Recognition and Counting of Apples in a Dynamic State Using a 3D Camera and Deep Learning Algorithms for Robotic Harvesting Systems</data>
  <data key="d4">RMRD Abeyrathna, VM Nakaguchi, A Minn, T Ahamed</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8851123877575968338&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="1702973244756968520">
  <data key="d0">Neural Architecture Search Survey: A Computer Vision Perspective</data>
  <data key="d1">1702973244756968520</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/3/1713</data>
  <data key="d3">Neural Architecture Search Survey: A Computer Vision Perspective</data>
  <data key="d4">JS Kang, JK Kang, JJ Kim, KW Jeon, HJ Chung…</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1702973244756968520&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14037392503982667044">
  <data key="d0">Comparison of pre-trained YOLO models on steel surface defects detector based on transfer learning with GPU-based embedded devices</data>
  <data key="d1">14037392503982667044</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/24/9926</data>
  <data key="d3">Comparison of pre-trained YOLO models on steel surface defects detector based on transfer learning with GPU-based embedded devices</data>
  <data key="d4">HV Nguyen, JH Bae, YE Lee, HS Lee, KR Kwon</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14037392503982667044&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="2410476445007210626">
  <data key="d0">Road Damage Detection and Classification with YOLOv7</data>
  <data key="d1">2410476445007210626</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10020856/</data>
  <data key="d3">Road Damage Detection and Classification with YOLOv7</data>
  <data key="d4">V Pham, D Nguyen, C Donan</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2410476445007210626&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="17386293721154871155">
  <data key="d0">IDOD-YOLOV7: Image-Dehazing YOLOV7 for Object Detection in Low-Light Foggy Traffic Environments</data>
  <data key="d1">17386293721154871155</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/3/1347</data>
  <data key="d3">IDOD-YOLOV7: Image-Dehazing YOLOV7 for Object Detection in Low-Light Foggy Traffic Environments</data>
  <data key="d4">Y Qiu, Y Lu, Y Wang, H Jiang</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17386293721154871155&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="9015347870265085472">
  <data key="d0">Creating a dynamic quadrupedal robotic goalkeeper with reinforcement learning</data>
  <data key="d1">9015347870265085472</data>
  <data key="d2">https://arxiv.org/abs/2210.04435</data>
  <data key="d3">Creating a dynamic quadrupedal robotic goalkeeper with reinforcement learning</data>
  <data key="d4">X Huang, Z Li, Y Xiang, Y Ni, Y Chi, Y Li, L Yang…</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9015347870265085472&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12337878440731894992">
  <data key="d0">Fire detection and notification method in ship areas using deep learning and computer vision approaches</data>
  <data key="d1">12337878440731894992</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/16/7078</data>
  <data key="d3">Fire detection and notification method in ship areas using deep learning and computer vision approaches</data>
  <data key="d4">K Avazov, MK Jamil, B Muminov, AB Abdusalomov…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12337878440731894992&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="6147088247534052612">
  <data key="d0">Rip Current Segmentation: A Novel Benchmark and YOLOv8 Baseline Results</data>
  <data key="d1">6147088247534052612</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Dumitriu_Rip_Current_Segmentation_A_Novel_Benchmark_and_YOLOv8_Baseline_Results_CVPRW_2023_paper.html</data>
  <data key="d3">Rip Current Segmentation: A Novel Benchmark and YOLOv8 Baseline Results</data>
  <data key="d4">A Dumitriu, F Tatui, F Miron…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6147088247534052612&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="4526718419682475090">
  <data key="d0">Underwater Object Detection Using TC-YOLO with Attention Mechanisms</data>
  <data key="d1">4526718419682475090</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/5/2567</data>
  <data key="d3">Underwater Object Detection Using TC-YOLO with Attention Mechanisms</data>
  <data key="d4">K Liu, L Peng, S Tang</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4526718419682475090&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="6128611593700706618">
  <data key="d0">Yolopv2: Better, faster, stronger for panoptic driving perception</data>
  <data key="d1">6128611593700706618</data>
  <data key="d2">https://arxiv.org/abs/2208.11434</data>
  <data key="d3">Yolopv2: Better, faster, stronger for panoptic driving perception</data>
  <data key="d4">C Han, Q Zhao, S Zhang, Y Chen, Z Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6128611593700706618&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="17237114188030762025">
  <data key="d0">Towards Single Camera Human 3D-Kinematics</data>
  <data key="d1">17237114188030762025</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/1/341</data>
  <data key="d3">Towards Single Camera Human 3D-Kinematics</data>
  <data key="d4">M Bittner, WT Yang, X Zhang, A Seth, J van Gemert…</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17237114188030762025&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="15542278339082031875">
  <data key="d0">Improved ship detection algorithm from satellite images using YOLOv7 and graph neural network</data>
  <data key="d1">15542278339082031875</data>
  <data key="d2">https://www.mdpi.com/1999-4893/15/12/473</data>
  <data key="d3">Improved ship detection algorithm from satellite images using YOLOv7 and graph neural network</data>
  <data key="d4">K Patel, C Bhatt, PL Mazzeo</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15542278339082031875&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="8569975234173607623">
  <data key="d0">Deformable convolution and coordinate attention for fast cattle detection</data>
  <data key="d1">8569975234173607623</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923003940</data>
  <data key="d3">Deformable convolution and coordinate attention for fast cattle detection</data>
  <data key="d4">W Yang, J Wu, J Zhang, K Gao, R Du, Z Wu…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8569975234173607623&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14110793700557908078">
  <data key="d0">Detection of Power Poles in Orchards Based on Improved Yolov5s Model</data>
  <data key="d1">14110793700557908078</data>
  <data key="d2">https://www.mdpi.com/2073-4395/13/7/1705</data>
  <data key="d3">Detection of Power Poles in Orchards Based on Improved Yolov5s Model</data>
  <data key="d4">Y Zhang, X Lu, W Li, K Yan, Z Mo, Y Lan, L Wang</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14110793700557908078&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="388784122271071816">
  <data key="d0">Yolo-former: Marrying yolo and transformer for foreign object detection</data>
  <data key="d1">388784122271071816</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9953039/</data>
  <data key="d3">Yolo-former: Marrying yolo and transformer for foreign object detection</data>
  <data key="d4">Y Dai, W Liu, H Wang, W Xie…</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=388784122271071816&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="8471323315563528709">
  <data key="d0">Special Vehicle Detection from UAV Perspective via YOLO-GNS Based Deep Learning Network</data>
  <data key="d1">8471323315563528709</data>
  <data key="d2">https://www.mdpi.com/2504-446X/7/2/117</data>
  <data key="d3">Special Vehicle Detection from UAV Perspective via YOLO-GNS Based Deep Learning Network</data>
  <data key="d4">Z Qiu, H Bai, T Chen</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8471323315563528709&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="13825486577349279839">
  <data key="d0">DynamicDet: A Unified Dynamic Architecture for Object Detection</data>
  <data key="d1">13825486577349279839</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Lin_DynamicDet_A_Unified_Dynamic_Architecture_for_Object_Detection_CVPR_2023_paper.html</data>
  <data key="d3">DynamicDet: A Unified Dynamic Architecture for Object Detection</data>
  <data key="d4">Z Lin, Y Wang, J Zhang, X Chu</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13825486577349279839&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="4419171281200457557">
  <data key="d0">KPE-YOLOv5: An Improved Small Target Detection Algorithm Based on YOLOv5</data>
  <data key="d1">4419171281200457557</data>
  <data key="d2">https://www.mdpi.com/2079-9292/12/4/817</data>
  <data key="d3">KPE-YOLOv5: An Improved Small Target Detection Algorithm Based on YOLOv5</data>
  <data key="d4">R Yang, W Li, X Shang, D Zhu, X Man</data>
  <data key="d5">2023</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4419171281200457557&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="4043189257734549657">
  <data key="d0">Dense Text-to-Image Generation with Attention Modulation</data>
  <data key="d1">4043189257734549657</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Kim_Dense_Text-to-Image_Generation_with_Attention_Modulation_ICCV_2023_paper.html</data>
  <data key="d3">Dense Text-to-Image Generation with Attention Modulation</data>
  <data key="d4">Y Kim, J Lee, JH Kim, JW Ha…</data>
  <data key="d5">2023</data>
  <data key="d8">1</data>
</node>
<node id="12115561072014802341">
  <data key="d0">3d-net: Monocular 3d object recognition for traffic monitoring</data>
  <data key="d1">12115561072014802341</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417423007558</data>
  <data key="d3">3d-net: Monocular 3d object recognition for traffic monitoring</data>
  <data key="d4">M Rezaei, M Azarmi, FMP Mir</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12115561072014802341&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="5444293888769597917">
  <data key="d0">Damage Detection and Localization of Bridge Deck Pavement Based on Deep Learning</data>
  <data key="d1">5444293888769597917</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/11/5138</data>
  <data key="d3">Damage Detection and Localization of Bridge Deck Pavement Based on Deep Learning</data>
  <data key="d4">Y Ni, J Mao, Y Fu, H Wang, H Zong, K Luo</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5444293888769597917&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="13381092205432192534">
  <data key="d0">SafeFac: Video-based smart safety monitoring for preventing industrial work accidents</data>
  <data key="d1">13381092205432192534</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417422024150</data>
  <data key="d3">SafeFac: Video-based smart safety monitoring for preventing industrial work accidents</data>
  <data key="d4">J Ahn, JY Park, SS Lee, KH Lee, H Do, JG Ko</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13381092205432192534&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14213127820813811957">
  <data key="d0">SegLoc: Learning Segmentation-Based Representations for Privacy-Preserving Visual Localization</data>
  <data key="d1">14213127820813811957</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Pietrantoni_SegLoc_Learning_Segmentation-Based_Representations_for_Privacy-Preserving_Visual_Localization_CVPR_2023_paper.html</data>
  <data key="d3">SegLoc: Learning Segmentation-Based Representations for Privacy-Preserving Visual Localization</data>
  <data key="d4">M Pietrantoni, M Humenberger…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14213127820813811957&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3527582028524441499">
  <data key="d0">RGB-D datasets for robotic perception in site-specific agricultural operations—A survey</data>
  <data key="d1">3527582028524441499</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923004234</data>
  <data key="d3">RGB-D datasets for robotic perception in site-specific agricultural operations—A survey</data>
  <data key="d4">P Kurtser, S Lowry</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3527582028524441499&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="310549142066083798">
  <data key="d0">SegDetector: A Deep Learning Model for Detecting Small and Overlapping Damaged Buildings in Satellite Images</data>
  <data key="d1">310549142066083798</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/23/6136</data>
  <data key="d3">SegDetector: A Deep Learning Model for Detecting Small and Overlapping Damaged Buildings in Satellite Images</data>
  <data key="d4">Z Yu, Z Chen, Z Sun, H Guo, B Leng, Z He, J Yang…</data>
  <data key="d5">2022</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=310549142066083798&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="2795686517415303571">
  <data key="d0">Instruction-vit: Multi-modal prompts for instruction learning in vit</data>
  <data key="d1">2795686517415303571</data>
  <data key="d2">https://arxiv.org/abs/2305.00201</data>
  <data key="d3">Instruction-vit: Multi-modal prompts for instruction learning in vit</data>
  <data key="d4">Z Xiao, Y Chen, L Zhang, J Yao, Z Wu, X Yu…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2795686517415303571&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12602640083300493507">
  <data key="d0">Deep Learning Based Object Detection for Resource Constrained Devices-Systematic Review, Future Trends and Challenges Ahead</data>
  <data key="d1">12602640083300493507</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0925231223001388</data>
  <data key="d3">Deep Learning Based Object Detection for Resource Constrained Devices-Systematic Review, Future Trends and Challenges Ahead</data>
  <data key="d4">V Kamath, A Renuka</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12602640083300493507&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="2199006774138818312">
  <data key="d0">Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism</data>
  <data key="d1">2199006774138818312</data>
  <data key="d2">https://arxiv.org/abs/2301.10051</data>
  <data key="d3">Wise-IoU: Bounding Box Regression Loss with Dynamic Focusing Mechanism</data>
  <data key="d4">Z Tong, Y Chen, Z Xu, R Yu</data>
  <data key="d5">2023</data>
  <data key="d6">37</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2199006774138818312&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="1068983787928302396">
  <data key="d0">Deep-worm-tracker: Deep learning methods for accurate detection and tracking for behavioral studies in C. elegans</data>
  <data key="d1">1068983787928302396</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S016815912300196X</data>
  <data key="d3">Deep-worm-tracker: Deep learning methods for accurate detection and tracking for behavioral studies in C. elegans</data>
  <data key="d4">SC Banerjee, KA Khan, R Sharma</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1068983787928302396&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="13704777355910523310">
  <data key="d0">Study on Lightweight Model of Maize Seedling Object Detection Based on YOLOv7</data>
  <data key="d1">13704777355910523310</data>
  <data key="d2">https://www.mdpi.com/2076-3417/13/13/7731</data>
  <data key="d3">Study on Lightweight Model of Maize Seedling Object Detection Based on YOLOv7</data>
  <data key="d4">K Zhao, L Zhao, Y Zhao, H Deng</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13704777355910523310&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="8849990741625315400">
  <data key="d0">TIA-YOLOv5: An improved YOLOv5 network for real-time detection of crop and weed in the field</data>
  <data key="d1">8849990741625315400</data>
  <data key="d2">https://www.frontiersin.org/articles/10.3389/fpls.2022.1091655/full</data>
  <data key="d3">TIA-YOLOv5: An improved YOLOv5 network for real-time detection of crop and weed in the field</data>
  <data key="d4">A Wang, T Peng, H Cao, Y Xu, X Wei…</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8849990741625315400&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14014258077591706555">
  <data key="d0">Deep Learning-Based Pine Nematode Trees' Identification Using Multispectral and Visible UAV Imagery</data>
  <data key="d1">14014258077591706555</data>
  <data key="d2">https://www.mdpi.com/2504-446X/7/3/183</data>
  <data key="d3">Deep Learning-Based Pine Nematode Trees' Identification Using Multispectral and Visible UAV Imagery</data>
  <data key="d4">B Qin, F Sun, W Shen, B Dong, S Ma, X Huo, P Lan</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14014258077591706555&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="10603972393777456896">
  <data key="d0">A Lightweight Object Detection Algorithm for Remote Sensing Images Based on Attention Mechanism and YOLOv5s</data>
  <data key="d1">10603972393777456896</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/9/2429</data>
  <data key="d3">A Lightweight Object Detection Algorithm for Remote Sensing Images Based on Attention Mechanism and YOLOv5s</data>
  <data key="d4">P Liu, Q Wang, H Zhang, J Mi, Y Liu</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10603972393777456896&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="5046561428623654374">
  <data key="d0">Predicting highly dynamic traffic noise using rotating mobile monitoring and machine learning method</data>
  <data key="d1">5046561428623654374</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0013935123006886</data>
  <data key="d3">Predicting highly dynamic traffic noise using rotating mobile monitoring and machine learning method</data>
  <data key="d4">Y Zhang, H Zhao, Y Li, Y Long, W Liang</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5046561428623654374&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3550218616048655433">
  <data key="d0">Edge AI-based tree trunk detection for forestry monitoring robotics</data>
  <data key="d1">3550218616048655433</data>
  <data key="d2">https://www.mdpi.com/2218-6581/11/6/136</data>
  <data key="d3">Edge AI-based tree trunk detection for forestry monitoring robotics</data>
  <data key="d4">DQ da Silva, FN dos Santos, V Filipe, AJ Sousa…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3550218616048655433&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="5900725104936277094">
  <data key="d0">Fast-Yolo-Rec: incorporating yolo-base detection and recurrent-base prediction networks for fast vehicle detection in consecutive images</data>
  <data key="d1">5900725104936277094</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9950239/</data>
  <data key="d3">Fast-Yolo-Rec: incorporating yolo-base detection and recurrent-base prediction networks for fast vehicle detection in consecutive images</data>
  <data key="d4">N Zarei, P Moallem, M Shams</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5900725104936277094&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12974822052652996066">
  <data key="d0">Advanced crack detection and segmentation on bridge decks using deep learning</data>
  <data key="d1">12974822052652996066</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0950061823025552</data>
  <data key="d3">Advanced crack detection and segmentation on bridge decks using deep learning</data>
  <data key="d4">TS Tran, SD Nguyen, HJ Lee, VP Tran</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12974822052652996066&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="f2PZtE-BhD0J">
  <data key="d0">Object Detection With Self-Supervised Scene Adaptation</data>
  <data key="d1">f2PZtE-BhD0J</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Object_Detection_With_Self-Supervised_Scene_Adaptation_CVPR_2023_paper.html</data>
  <data key="d3">Object Detection With Self-Supervised Scene Adaptation</data>
  <data key="d4">Z Zhang, M Hoai</data>
  <data key="d5">2023</data>
  <data key="d8">1</data>
</node>
<node id="2868755694884485530">
  <data key="d0">Crowdsensing-based Road Damage Detection Challenge (CRDDC'2022)</data>
  <data key="d1">2868755694884485530</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10021040/</data>
  <data key="d3">Crowdsensing-based Road Damage Detection Challenge (CRDDC'2022)</data>
  <data key="d4">D Arya, H Maeda, SK Ghosh…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2868755694884485530&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="5981499027157381733">
  <data key="d0">Swin-Transformer-Based YOLOv5 for Small-Object Detection in Remote Sensing Images</data>
  <data key="d1">5981499027157381733</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/7/3634</data>
  <data key="d3">Swin-Transformer-Based YOLOv5 for Small-Object Detection in Remote Sensing Images</data>
  <data key="d4">X Cao, Y Zhang, S Lang, Y Gong</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5981499027157381733&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="16205265336717732975">
  <data key="d0">Sim-YOLOv5s: A method for detecting defects on the end face of lithium battery steel shells</data>
  <data key="d1">16205265336717732975</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1474034622002828</data>
  <data key="d3">Sim-YOLOv5s: A method for detecting defects on the end face of lithium battery steel shells</data>
  <data key="d4">H Hu, Z Zhu</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16205265336717732975&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="7572686774487370911">
  <data key="d0">A Survey on Waste Detection and Classification Using Deep Learning</data>
  <data key="d1">7572686774487370911</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9970346/</data>
  <data key="d3">A Survey on Waste Detection and Classification Using Deep Learning</data>
  <data key="d4">H Abdu, MHM Noor</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7572686774487370911&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3244016085706305630">
  <data key="d0">Skew class-balanced re-weighting for unbiased scene graph generation</data>
  <data key="d1">3244016085706305630</data>
  <data key="d2">https://www.mdpi.com/2504-4990/5/1/18</data>
  <data key="d3">Skew class-balanced re-weighting for unbiased scene graph generation</data>
  <data key="d4">H Kang, CD Yoo</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3244016085706305630&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="11531242419091815801">
  <data key="d0">Yolox: Exceeding yolo series in 2021</data>
  <data key="d1">11531242419091815801</data>
  <data key="d2">https://arxiv.org/abs/2107.08430</data>
  <data key="d3">Yolox: Exceeding yolo series in 2021</data>
  <data key="d4">Z Ge, S Liu, F Wang, Z Li, J Sun</data>
  <data key="d5">2021</data>
  <data key="d6">2281</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11531242419091815801&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="18210588638302847093">
  <data key="d0">TPH-YOLOv5: Improved YOLOv5 based on transformer prediction head for object detection on drone-captured scenarios</data>
  <data key="d1">18210588638302847093</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Zhu_TPH-YOLOv5_Improved_YOLOv5_Based_on_Transformer_Prediction_Head_for_Object_ICCVW_2021_paper.html</data>
  <data key="d3">TPH-YOLOv5: Improved YOLOv5 based on transformer prediction head for object detection on drone-captured scenarios</data>
  <data key="d4">X Zhu, S Lyu, X Wang, Q Zhao</data>
  <data key="d5">2021</data>
  <data key="d6">683</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18210588638302847093&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="11452477192554017570">
  <data key="d0">Diffusiondet: Diffusion model for object detection</data>
  <data key="d1">11452477192554017570</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Chen_DiffusionDet_Diffusion_Model_for_Object_Detection_ICCV_2023_paper.html</data>
  <data key="d3">Diffusiondet: Diffusion model for object detection</data>
  <data key="d4">S Chen, P Sun, Y Song, P Luo</data>
  <data key="d5">2023</data>
  <data key="d6">90</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11452477192554017570&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="7039269942427062691">
  <data key="d0">Dino: Detr with improved denoising anchor boxes for end-to-end object detection</data>
  <data key="d1">7039269942427062691</data>
  <data key="d2">https://arxiv.org/abs/2203.03605</data>
  <data key="d3">Dino: Detr with improved denoising anchor boxes for end-to-end object detection</data>
  <data key="d4">H Zhang, F Li, S Liu, L Zhang, H Su, J Zhu…</data>
  <data key="d5">2022</data>
  <data key="d6">361</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7039269942427062691&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4488471448867589825">
  <data key="d0">Comparing YOLOv3, YOLOv4 and YOLOv5 for autonomous landing spot detection in faulty UAVs</data>
  <data key="d1">4488471448867589825</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/2/464</data>
  <data key="d3">Comparing YOLOv3, YOLOv4 and YOLOv5 for autonomous landing spot detection in faulty UAVs</data>
  <data key="d4">U Nepal, H Eslamiat</data>
  <data key="d5">2022</data>
  <data key="d6">271</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4488471448867589825&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="5295462520886771746">
  <data key="d0">Key technologies of machine vision for weeding robots: A review and benchmark</data>
  <data key="d1">5295462520886771746</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169922001971</data>
  <data key="d3">Key technologies of machine vision for weeding robots: A review and benchmark</data>
  <data key="d4">Y Li, Z Guo, F Shuang, M Zhang, X Li</data>
  <data key="d5">2022</data>
  <data key="d6">33</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5295462520886771746&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="2093389731615411975">
  <data key="d0">Observation-centric sort: Rethinking sort for robust multi-object tracking</data>
  <data key="d1">2093389731615411975</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Cao_Observation-Centric_SORT_Rethinking_SORT_for_Robust_Multi-Object_Tracking_CVPR_2023_paper.html</data>
  <data key="d3">Observation-centric sort: Rethinking sort for robust multi-object tracking</data>
  <data key="d4">J Cao, J Pang, X Weng…</data>
  <data key="d5">2023</data>
  <data key="d6">142</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2093389731615411975&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="11789051068432887660">
  <data key="d0">Deep learning methods for object detection in smart manufacturing: A survey</data>
  <data key="d1">11789051068432887660</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0278612522001066</data>
  <data key="d3">Deep learning methods for object detection in smart manufacturing: A survey</data>
  <data key="d4">HM Ahmad, A Rahimi</data>
  <data key="d5">2022</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11789051068432887660&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="11838073149065061192">
  <data key="d0">Dab-detr: Dynamic anchor boxes are better queries for detr</data>
  <data key="d1">11838073149065061192</data>
  <data key="d2">https://arxiv.org/abs/2201.12329</data>
  <data key="d3">Dab-detr: Dynamic anchor boxes are better queries for detr</data>
  <data key="d4">S Liu, F Li, H Zhang, X Yang, X Qi, H Su, J Zhu…</data>
  <data key="d5">2022</data>
  <data key="d6">266</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11838073149065061192&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14151291256828319904">
  <data key="d0">Focal and global knowledge distillation for detectors</data>
  <data key="d1">14151291256828319904</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Yang_Focal_and_Global_Knowledge_Distillation_for_Detectors_CVPR_2022_paper.html</data>
  <data key="d3">Focal and global knowledge distillation for detectors</data>
  <data key="d4">Z Yang, Z Li, X Jiang, Y Gong, Z Yuan…</data>
  <data key="d5">2022</data>
  <data key="d6">116</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14151291256828319904&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="16069829188377130053">
  <data key="d0">In defense of online models for video instance segmentation</data>
  <data key="d1">16069829188377130053</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19815-1_34</data>
  <data key="d3">In defense of online models for video instance segmentation</data>
  <data key="d4">J Wu, Q Liu, Y Jiang, S Bai, A Yuille, X Bai</data>
  <data key="d5">2022</data>
  <data key="d6">53</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16069829188377130053&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="14300935760162828522">
  <data key="d0">Towards grand unification of object tracking</data>
  <data key="d1">14300935760162828522</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19803-8_43</data>
  <data key="d3">Towards grand unification of object tracking</data>
  <data key="d4">B Yan, Y Jiang, P Sun, D Wang, Z Yuan, P Luo…</data>
  <data key="d5">2022</data>
  <data key="d6">72</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14300935760162828522&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="7664894228759942842">
  <data key="d0">Group detr: Fast detr training with group-wise one-to-many assignment</data>
  <data key="d1">7664894228759942842</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Chen_Group_DETR_Fast_DETR_Training_with_Group-Wise_One-to-Many_Assignment_ICCV_2023_paper.html</data>
  <data key="d3">Group detr: Fast detr training with group-wise one-to-many assignment</data>
  <data key="d4">Q Chen, X Chen, J Wang, S Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7664894228759942842&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14167857935364818481">
  <data key="d0">Strongsort: Make deepsort great again</data>
  <data key="d1">14167857935364818481</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10032656/</data>
  <data key="d3">Strongsort: Make deepsort great again</data>
  <data key="d4">Y Du, Z Zhao, Y Song, Y Zhao, F Su…</data>
  <data key="d5">2023</data>
  <data key="d6">150</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14167857935364818481&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="13684672788136223420">
  <data key="d0">Alphapose: Whole-body regional multi-person pose estimation and tracking in real-time</data>
  <data key="d1">13684672788136223420</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9954214/</data>
  <data key="d3">Alphapose: Whole-body regional multi-person pose estimation and tracking in real-time</data>
  <data key="d4">HS Fang, J Li, H Tang, C Xu, H Zhu…</data>
  <data key="d5">2022</data>
  <data key="d6">89</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13684672788136223420&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="5598971766222052345">
  <data key="d0">Memot: Multi-object tracking with memory</data>
  <data key="d1">5598971766222052345</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Cai_MeMOT_Multi-Object_Tracking_With_Memory_CVPR_2022_paper.html</data>
  <data key="d3">Memot: Multi-object tracking with memory</data>
  <data key="d4">J Cai, M Xu, W Li, Y Xiong, W Xia…</data>
  <data key="d5">2022</data>
  <data key="d6">63</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5598971766222052345&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="9529319158101525799">
  <data key="d0">Dancetrack: Multi-object tracking in uniform appearance and diverse motion</data>
  <data key="d1">9529319158101525799</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Sun_DanceTrack_Multi-Object_Tracking_in_Uniform_Appearance_and_Diverse_Motion_CVPR_2022_paper.html</data>
  <data key="d3">Dancetrack: Multi-object tracking in uniform appearance and diverse motion</data>
  <data key="d4">P Sun, J Cao, Y Jiang, Z Yuan, S Bai…</data>
  <data key="d5">2022</data>
  <data key="d6">72</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9529319158101525799&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="7104781172538541114">
  <data key="d0">YOLOv5-Tassel: Detecting tassels in RGB UAV imagery with improved YOLOv5 based on transfer learning</data>
  <data key="d1">7104781172538541114</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9889182/</data>
  <data key="d3">YOLOv5-Tassel: Detecting tassels in RGB UAV imagery with improved YOLOv5 based on transfer learning</data>
  <data key="d4">W Liu, K Quijano, MM Crawford</data>
  <data key="d5">2022</data>
  <data key="d6">121</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7104781172538541114&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="3501401895928296969">
  <data key="d0">Cbnet: A composite backbone network architecture for object detection</data>
  <data key="d1">3501401895928296969</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9932281/</data>
  <data key="d3">Cbnet: A composite backbone network architecture for object detection</data>
  <data key="d4">T Liang, X Chu, Y Liu, Y Wang, Z Tang…</data>
  <data key="d5">2022</data>
  <data key="d6">119</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3501401895928296969&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="3753161556540928995">
  <data key="d0">Equalized focal loss for dense long-tailed object detection</data>
  <data key="d1">3753161556540928995</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Li_Equalized_Focal_Loss_for_Dense_Long-Tailed_Object_Detection_CVPR_2022_paper.html</data>
  <data key="d3">Equalized focal loss for dense long-tailed object detection</data>
  <data key="d4">B Li, Y Yao, J Tan, G Zhang, F Yu…</data>
  <data key="d5">2022</data>
  <data key="d6">55</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3753161556540928995&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="4241401890946035983">
  <data key="d0">Msft-yolo: Improved yolov5 based on transformer for detecting defects of steel surface</data>
  <data key="d1">4241401890946035983</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/9/3467</data>
  <data key="d3">Msft-yolo: Improved yolov5 based on transformer for detecting defects of steel surface</data>
  <data key="d4">Z Guo, C Wang, G Yang, Z Huang, G Li</data>
  <data key="d5">2022</data>
  <data key="d6">70</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4241401890946035983&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="10239175441885037567">
  <data key="d0">Tracking objects as pixel-wise distributions</data>
  <data key="d1">10239175441885037567</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20047-2_5</data>
  <data key="d3">Tracking objects as pixel-wise distributions</data>
  <data key="d4">Z Zhao, Z Wu, Y Zhuang, B Li, J Jia</data>
  <data key="d5">2022</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10239175441885037567&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="11442601300141569813">
  <data key="d0">Clrnet: Cross layer refinement network for lane detection</data>
  <data key="d1">11442601300141569813</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zheng_CLRNet_Cross_Layer_Refinement_Network_for_Lane_Detection_CVPR_2022_paper.html</data>
  <data key="d3">Clrnet: Cross layer refinement network for lane detection</data>
  <data key="d4">T Zheng, Y Huang, Y Liu, W Tang…</data>
  <data key="d5">2022</data>
  <data key="d6">57</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11442601300141569813&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="12358278621343852999">
  <data key="d0">PP-YOLOE: An evolved version of YOLO</data>
  <data key="d1">12358278621343852999</data>
  <data key="d2">https://arxiv.org/abs/2203.16250</data>
  <data key="d3">PP-YOLOE: An evolved version of YOLO</data>
  <data key="d4">S Xu, X Wang, W Lv, Q Chang, C Cui, K Deng…</data>
  <data key="d5">2022</data>
  <data key="d6">89</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12358278621343852999&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="14575314968965851624">
  <data key="d0">Motrv2: Bootstrapping end-to-end multi-object tracking by pretrained object detectors</data>
  <data key="d1">14575314968965851624</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_MOTRv2_Bootstrapping_End-to-End_Multi-Object_Tracking_by_Pretrained_Object_Detectors_CVPR_2023_paper.html</data>
  <data key="d3">Motrv2: Bootstrapping end-to-end multi-object tracking by pretrained object detectors</data>
  <data key="d4">Y Zhang, T Wang, X Zhang</data>
  <data key="d5">2023</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14575314968965851624&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="8851003302500796244">
  <data key="d0">Fast forest fire smoke detection using MVMNet</data>
  <data key="d1">8851003302500796244</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0950705122000612</data>
  <data key="d3">Fast forest fire smoke detection using MVMNet</data>
  <data key="d4">Y Hu, J Zhan, G Zhou, A Chen, W Cai, K Guo…</data>
  <data key="d5">2022</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8851003302500796244&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="10246639746398567269">
  <data key="d0">Extendable multiple nodes recurrent tracking framework with RTU++</data>
  <data key="d1">10246639746398567269</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9841421/</data>
  <data key="d3">Extendable multiple nodes recurrent tracking framework with RTU++</data>
  <data key="d4">S Wang, H Sheng, D Yang, Y Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">44</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10246639746398567269&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="9808171637715574951">
  <data key="d0">Qdtrack: Quasi-dense similarity learning for appearance-only multiple object tracking</data>
  <data key="d1">9808171637715574951</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10209207/</data>
  <data key="d3">Qdtrack: Quasi-dense similarity learning for appearance-only multiple object tracking</data>
  <data key="d4">T Fischer, TE Huang, J Pang, L Qiu…</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9808171637715574951&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="3318911338829069796">
  <data key="d0">BoT-SORT: Robust associations multi-pedestrian tracking</data>
  <data key="d1">3318911338829069796</data>
  <data key="d2">https://arxiv.org/abs/2206.14651</data>
  <data key="d3">BoT-SORT: Robust associations multi-pedestrian tracking</data>
  <data key="d4">N Aharon, R Orfaig, BZ Bobrovsky</data>
  <data key="d5">2022</data>
  <data key="d6">91</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3318911338829069796&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="10139464426827633103">
  <data key="d0">Real-time object detection for streaming perception</data>
  <data key="d1">10139464426827633103</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Yang_Real-Time_Object_Detection_for_Streaming_Perception_CVPR_2022_paper.html</data>
  <data key="d3">Real-time object detection for streaming perception</data>
  <data key="d4">J Yang, S Liu, Z Li, X Li, J Sun</data>
  <data key="d5">2022</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10139464426827633103&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="5177357115845082072">
  <data key="d0">PP-PicoDet: A better real-time object detector on mobile devices</data>
  <data key="d1">5177357115845082072</data>
  <data key="d2">https://arxiv.org/abs/2111.00902</data>
  <data key="d3">PP-PicoDet: A better real-time object detector on mobile devices</data>
  <data key="d4">G Yu, Q Chang, W Lv, C Xu, C Cui, W Ji…</data>
  <data key="d5">2021</data>
  <data key="d6">72</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5177357115845082072&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="503273143199070187">
  <data key="d0">Yolo-pose: Enhancing yolo for multi person pose estimation using object keypoint similarity loss</data>
  <data key="d1">503273143199070187</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022W/ECV/html/Maji_YOLO-Pose_Enhancing_YOLO_for_Multi_Person_Pose_Estimation_Using_Object_CVPRW_2022_paper.html</data>
  <data key="d3">Yolo-pose: Enhancing yolo for multi person pose estimation using object keypoint similarity loss</data>
  <data key="d4">D Maji, S Nagori, M Mathew…</data>
  <data key="d5">2022</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=503273143199070187&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="17928092202816562244">
  <data key="d0">Universal instance perception as object discovery and retrieval</data>
  <data key="d1">17928092202816562244</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Yan_Universal_Instance_Perception_As_Object_Discovery_and_Retrieval_CVPR_2023_paper.html</data>
  <data key="d3">Universal instance perception as object discovery and retrieval</data>
  <data key="d4">B Yan, Y Jiang, J Wu, D Wang, P Luo…</data>
  <data key="d5">2023</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17928092202816562244&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="17207953557216789132">
  <data key="d0">Simple multi-dataset detection</data>
  <data key="d1">17207953557216789132</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Simple_Multi-Dataset_Detection_CVPR_2022_paper.html</data>
  <data key="d3">Simple multi-dataset detection</data>
  <data key="d4">X Zhou, V Koltun, P Krähenbühl</data>
  <data key="d5">2022</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17207953557216789132&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="1858398396401784628">
  <data key="d0">High-accuracy detection of maize leaf diseases CNN based on multi-pathway activation function module</data>
  <data key="d1">1858398396401784628</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/21/4218</data>
  <data key="d3">High-accuracy detection of maize leaf diseases CNN based on multi-pathway activation function module</data>
  <data key="d4">Y Zhang, S Wa, Y Liu, X Zhou, P Sun, Q Ma</data>
  <data key="d5">2021</data>
  <data key="d6">44</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1858398396401784628&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="12280368052734223898">
  <data key="d0">Soccernet-tracking: Multiple object tracking dataset and benchmark in soccer videos</data>
  <data key="d1">12280368052734223898</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022W/CVSports/html/Cioppa_SoccerNet-Tracking_Multiple_Object_Tracking_Dataset_and_Benchmark_in_Soccer_Videos_CVPRW_2022_paper.html</data>
  <data key="d3">Soccernet-tracking: Multiple object tracking dataset and benchmark in soccer videos</data>
  <data key="d4">A Cioppa, S Giancola, A Deliege…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12280368052734223898&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="5901510358391583323">
  <data key="d0">Surface defect detection of steel strips based on improved YOLOv4</data>
  <data key="d1">5901510358391583323</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0045790622004499</data>
  <data key="d3">Surface defect detection of steel strips based on improved YOLOv4</data>
  <data key="d4">M Li, H Wang, Z Wan</data>
  <data key="d5">2022</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5901510358391583323&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="7280187709203663599">
  <data key="d0">Autoalignv2: Deformable feature aggregation for dynamic multi-modal 3d object detection</data>
  <data key="d1">7280187709203663599</data>
  <data key="d2">https://arxiv.org/abs/2207.10316</data>
  <data key="d3">Autoalignv2: Deformable feature aggregation for dynamic multi-modal 3d object detection</data>
  <data key="d4">Z Chen, Z Li, S Zhang, L Fang, Q Jiang…</data>
  <data key="d5">2022</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7280187709203663599&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="8918294861982063782">
  <data key="d0">TRACE: 5D temporal regression of avatars with dynamic cameras in 3D environments</data>
  <data key="d1">8918294861982063782</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Sun_TRACE_5D_Temporal_Regression_of_Avatars_With_Dynamic_Cameras_in_CVPR_2023_paper.html</data>
  <data key="d3">TRACE: 5D temporal regression of avatars with dynamic cameras in 3D environments</data>
  <data key="d4">Y Sun, Q Bao, W Liu, T Mei…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8918294861982063782&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="9002295544688722648">
  <data key="d0">Regioncl: exploring contrastive region pairs for self-supervised representation learning</data>
  <data key="d1">9002295544688722648</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19827-4_28</data>
  <data key="d3">Regioncl: exploring contrastive region pairs for self-supervised representation learning</data>
  <data key="d4">Y Xu, Q Zhang, J Zhang, D Tao</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9002295544688722648&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="4172110649540745529">
  <data key="d0">ECAP-YOLO: Efficient channel attention pyramid YOLO for small object detection in aerial image</data>
  <data key="d1">4172110649540745529</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/23/4851</data>
  <data key="d3">ECAP-YOLO: Efficient channel attention pyramid YOLO for small object detection in aerial image</data>
  <data key="d4">M Kim, J Jeong, S Kim</data>
  <data key="d5">2021</data>
  <data key="d6">42</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4172110649540745529&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="1182214878703167432">
  <data key="d0">Fully convolutional one-stage 3d object detection on lidar range images</data>
  <data key="d1">1182214878703167432</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/e1f418450107c4a0ddc16d008d131573-Abstract-Conference.html</data>
  <data key="d3">Fully convolutional one-stage 3d object detection on lidar range images</data>
  <data key="d4">Z Tian, X Chu, X Wang, X Wei…</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1182214878703167432&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="5085325784414234300">
  <data key="d0">Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition</data>
  <data key="d1">5085325784414234300</data>
  <data key="d2">https://arxiv.org/abs/2205.14756</data>
  <data key="d3">Efficientvit: Enhanced linear attention for high-resolution low-computation visual recognition</data>
  <data key="d4">H Cai, C Gan, S Han</data>
  <data key="d5">2022</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5085325784414234300&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="8537681662983932237">
  <data key="d0">Bop challenge 2022 on detection, segmentation and pose estimation of specific rigid objects</data>
  <data key="d1">8537681662983932237</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/CV4MR/html/Sundermeyer_BOP_Challenge_2022_on_Detection_Segmentation_and_Pose_Estimation_of_CVPRW_2023_paper.html</data>
  <data key="d3">Bop challenge 2022 on detection, segmentation and pose estimation of specific rigid objects</data>
  <data key="d4">M Sundermeyer, T Hodaň, Y Labbe…</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8537681662983932237&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="1803748338102694409">
  <data key="d0">Improved ship detection algorithm based on YOLOX for SAR outline enhancement image</data>
  <data key="d1">1803748338102694409</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/16/4070</data>
  <data key="d3">Improved ship detection algorithm based on YOLOX for SAR outline enhancement image</data>
  <data key="d4">S Li, X Fu, J Dong</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1803748338102694409&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="16254437278639517990">
  <data key="d0">Active teacher for semi-supervised object detection</data>
  <data key="d1">16254437278639517990</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Mi_Active_Teacher_for_Semi-Supervised_Object_Detection_CVPR_2022_paper.html</data>
  <data key="d3">Active teacher for semi-supervised object detection</data>
  <data key="d4">P Mi, J Lin, Y Zhou, Y Shen, G Luo…</data>
  <data key="d5">2022</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16254437278639517990&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="9097004020017827588">
  <data key="d0">A real-time apple targets detection method for picking robot based on ShufflenetV2-YOLOX</data>
  <data key="d1">9097004020017827588</data>
  <data key="d2">https://www.mdpi.com/2077-0472/12/6/856</data>
  <data key="d3">A real-time apple targets detection method for picking robot based on ShufflenetV2-YOLOX</data>
  <data key="d4">W Ji, Y Pan, B Xu, J Wang</data>
  <data key="d5">2022</data>
  <data key="d6">27</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9097004020017827588&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="12877767864993853066">
  <data key="d0">No more strided convolutions or pooling: A new CNN building block for low-resolution images and small objects</data>
  <data key="d1">12877767864993853066</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-26409-2_27</data>
  <data key="d3">No more strided convolutions or pooling: A new CNN building block for low-resolution images and small objects</data>
  <data key="d4">R Sunkara, T Luo</data>
  <data key="d5">2022</data>
  <data key="d6">58</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12877767864993853066&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="2089165535494063327">
  <data key="d0">Automated bridge surface crack detection and segmentation using computer vision-based deep learning model</data>
  <data key="d1">2089165535494063327</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0952197622003050</data>
  <data key="d3">Automated bridge surface crack detection and segmentation using computer vision-based deep learning model</data>
  <data key="d4">J Zhang, S Qian, C Tan</data>
  <data key="d5">2022</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2089165535494063327&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="3119590478708025935">
  <data key="d0">Vision-based anti-uav detection and tracking</data>
  <data key="d1">3119590478708025935</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9785379/</data>
  <data key="d3">Vision-based anti-uav detection and tracking</data>
  <data key="d4">J Zhao, J Zhang, D Li, D Wang</data>
  <data key="d5">2022</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3119590478708025935&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="679650553160689765">
  <data key="d0">Pear defect detection method based on resnet and dcgan</data>
  <data key="d1">679650553160689765</data>
  <data key="d2">https://www.mdpi.com/2078-2489/12/10/397</data>
  <data key="d3">Pear defect detection method based on resnet and dcgan</data>
  <data key="d4">Y Zhang, S Wa, P Sun, Y Wang</data>
  <data key="d5">2021</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=679650553160689765&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="1907607333359949293">
  <data key="d0">{RECL}: Responsive {Resource-Efficient} Continuous Learning for Video Analytics</data>
  <data key="d1">1907607333359949293</data>
  <data key="d2">https://www.usenix.org/conference/nsdi23/presentation/khani</data>
  <data key="d3">{RECL}: Responsive {Resource-Efficient} Continuous Learning for Video Analytics</data>
  <data key="d4">M Khani, G Ananthanarayanan, K Hsieh…</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1907607333359949293&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="18256102101708736597">
  <data key="d0">SwinGD: A robust grape bunch detection model based on swin transformer in complex vineyard environment</data>
  <data key="d1">18256102101708736597</data>
  <data key="d2">https://www.mdpi.com/2311-7524/7/11/492</data>
  <data key="d3">SwinGD: A robust grape bunch detection model based on swin transformer in complex vineyard environment</data>
  <data key="d4">J Wang, Z Zhang, L Luo, W Zhu, J Chen, W Wang</data>
  <data key="d5">2021</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18256102101708736597&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="9917302194767651380">
  <data key="d0">An improved apple object detection method based on lightweight YOLOv4 in complex backgrounds</data>
  <data key="d1">9917302194767651380</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/17/4150</data>
  <data key="d3">An improved apple object detection method based on lightweight YOLOv4 in complex backgrounds</data>
  <data key="d4">C Zhang, F Kang, Y Wang</data>
  <data key="d5">2022</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9917302194767651380&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="7439091361445822478">
  <data key="d0">Mum: Mix image tiles and unmix feature tiles for semi-supervised object detection</data>
  <data key="d1">7439091361445822478</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Kim_MUM_Mix_Image_Tiles_and_UnMix_Feature_Tiles_for_Semi-Supervised_CVPR_2022_paper.html</data>
  <data key="d3">Mum: Mix image tiles and unmix feature tiles for semi-supervised object detection</data>
  <data key="d4">JM Kim, JY Jang, S Seo, J Jeong…</data>
  <data key="d5">2022</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7439091361445822478&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="13668398880115037252">
  <data key="d0">Intelligent edge-enabled efficient multi-source data fusion for autonomous surface vehicles in maritime internet of things</data>
  <data key="d1">13668398880115037252</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9731523/</data>
  <data key="d3">Intelligent edge-enabled efficient multi-source data fusion for autonomous surface vehicles in maritime internet of things</data>
  <data key="d4">RW Liu, Y Guo, J Nie, Q Hu, Z Xiong…</data>
  <data key="d5">2022</data>
  <data key="d6">24</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13668398880115037252&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="9399655475680081198">
  <data key="d0">Adversarially-aware robust object detector</data>
  <data key="d1">9399655475680081198</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20077-9_18</data>
  <data key="d3">Adversarially-aware robust object detector</data>
  <data key="d4">Z Dong, P Wei, L Lin</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9399655475680081198&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="3905038671402584496">
  <data key="d0">Low-cost mobile mapping system solution for traffic sign segmentation using Azure Kinect</data>
  <data key="d1">3905038671402584496</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1569843222000978</data>
  <data key="d3">Low-cost mobile mapping system solution for traffic sign segmentation using Azure Kinect</data>
  <data key="d4">Z Qiu, J Martínez-Sánchez, VM Brea, P López…</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3905038671402584496&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="4526443549077206496">
  <data key="d0">Semi-DETR: Semi-Supervised Object Detection With Detection Transformers</data>
  <data key="d1">4526443549077206496</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Semi-DETR_Semi-Supervised_Object_Detection_With_Detection_Transformers_CVPR_2023_paper.html</data>
  <data key="d3">Semi-DETR: Semi-Supervised Object Detection With Detection Transformers</data>
  <data key="d4">J Zhang, X Lin, W Zhang, K Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4526443549077206496&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="7753649110726260014">
  <data key="d0">Yolov: Making still image object detectors great at video object detection</data>
  <data key="d1">7753649110726260014</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/25320</data>
  <data key="d3">Yolov: Making still image object detectors great at video object detection</data>
  <data key="d4">Y Shi, N Wang, X Guo</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7753649110726260014&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="48995825530336826">
  <data key="d0">Improved Mask R-CNN for obstacle detection of rail transit</data>
  <data key="d1">48995825530336826</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S026322412200032X</data>
  <data key="d3">Improved Mask R-CNN for obstacle detection of rail transit</data>
  <data key="d4">D He, Y Qiu, J Miao, Z Zou, K Li, C Ren, G Shen</data>
  <data key="d5">2022</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=48995825530336826&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="16929187721477555329">
  <data key="d0">Cross-modality attentive feature fusion for object detection in multispectral remote sensing imagery</data>
  <data key="d1">16929187721477555329</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0031320322002679</data>
  <data key="d3">Cross-modality attentive feature fusion for object detection in multispectral remote sensing imagery</data>
  <data key="d4">F Qingyun, W Zhaokui</data>
  <data key="d5">2022</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16929187721477555329&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="13672856354810391005">
  <data key="d0">Centralized feature pyramid for object detection</data>
  <data key="d1">13672856354810391005</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10194544/</data>
  <data key="d3">Centralized feature pyramid for object detection</data>
  <data key="d4">Y Quan, D Zhang, L Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13672856354810391005&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="7436725887660345544">
  <data key="d0">You should look at all objects</data>
  <data key="d1">7436725887660345544</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20077-9_20</data>
  <data key="d3">You should look at all objects</data>
  <data key="d4">Z Jin, D Yu, L Song, Z Yuan, L Yu</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7436725887660345544&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="8487537285655290697">
  <data key="d0">Normalizing flows for human pose anomaly detection</data>
  <data key="d1">8487537285655290697</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Hirschorn_Normalizing_Flows_for_Human_Pose_Anomaly_Detection_ICCV_2023_paper.html</data>
  <data key="d3">Normalizing flows for human pose anomaly detection</data>
  <data key="d4">O Hirschorn, S Avidan</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8487537285655290697&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="5327297247516466432">
  <data key="d0">TransVCL: attention-enhanced video copy localization network with flexible supervision</data>
  <data key="d1">5327297247516466432</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/25158</data>
  <data key="d3">TransVCL: attention-enhanced video copy localization network with flexible supervision</data>
  <data key="d4">S He, Y He, M Lu, C Jiang, X Yang, F Qian…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5327297247516466432&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="1845681316412466465">
  <data key="d0">A lightweight position-enhanced anchor-free algorithm for SAR ship detection</data>
  <data key="d1">1845681316412466465</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/8/1908</data>
  <data key="d3">A lightweight position-enhanced anchor-free algorithm for SAR ship detection</data>
  <data key="d4">Y Feng, J Chen, Z Huang, H Wan, R Xia, B Wu, L Sun…</data>
  <data key="d5">2022</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1845681316412466465&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="15573361464256666532">
  <data key="d0">TraCon: A novel dataset for real-time traffic cones detection using deep learning</data>
  <data key="d1">15573361464256666532</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-17601-2_37</data>
  <data key="d3">TraCon: A novel dataset for real-time traffic cones detection using deep learning</data>
  <data key="d4">I Katsamenis, EE Karolou, A Davradou…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15573361464256666532&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="1380225769636433327">
  <data key="d0">UTM: A Unified Multiple Object Tracking Model With Identity-Aware Feature Enhancement</data>
  <data key="d1">1380225769636433327</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/You_UTM_A_Unified_Multiple_Object_Tracking_Model_With_Identity-Aware_Feature_CVPR_2023_paper.html</data>
  <data key="d3">UTM: A Unified Multiple Object Tracking Model With Identity-Aware Feature Enhancement</data>
  <data key="d4">S You, H Yao, BK Bao, C Xu</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1380225769636433327&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="17206319611652371856">
  <data key="d0">Hard to track objects with irregular motions and similar appearances? make it easier by buffering the matching space</data>
  <data key="d1">17206319611652371856</data>
  <data key="d2">https://openaccess.thecvf.com/content/WACV2023/html/Yang_Hard_To_Track_Objects_With_Irregular_Motions_and_Similar_Appearances_WACV_2023_paper.html</data>
  <data key="d3">Hard to track objects with irregular motions and similar appearances? make it easier by buffering the matching space</data>
  <data key="d4">F Yang, S Odashima, S Masui…</data>
  <data key="d5">2023</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17206319611652371856&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="410585140321607722">
  <data key="d0">Lightweight detection algorithm of kiwifruit based on improved YOLOX-s</data>
  <data key="d1">410585140321607722</data>
  <data key="d2">https://www.mdpi.com/2077-0472/12/7/993</data>
  <data key="d3">Lightweight detection algorithm of kiwifruit based on improved YOLOX-s</data>
  <data key="d4">J Zhou, W Hu, A Zou, S Zhai, T Liu, W Yang, P Jiang</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=410585140321607722&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="2499885218349084470">
  <data key="d0">A vision-based detection and spatial localization scheme for forest fire inspection from uav</data>
  <data key="d1">2499885218349084470</data>
  <data key="d2">https://www.mdpi.com/1999-4907/13/3/383</data>
  <data key="d3">A vision-based detection and spatial localization scheme for forest fire inspection from uav</data>
  <data key="d4">K Lu, R Xu, J Li, Y Lv, H Lin, Y Liu</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2499885218349084470&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="15199713057680873086">
  <data key="d0">Improved MobileNetV2-SSDLite for automatic fabric defect detection system based on cloud-edge computing</data>
  <data key="d1">15199713057680873086</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0263224122008739</data>
  <data key="d3">Improved MobileNetV2-SSDLite for automatic fabric defect detection system based on cloud-edge computing</data>
  <data key="d4">J Zhang, J Jing, P Lu, S Song</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15199713057680873086&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="5063651843298506539">
  <data key="d0">A New Approach for Detecting Fundus Lesions Using Image Processing and Deep Neural Network Architecture Based on YOLO Model</data>
  <data key="d1">5063651843298506539</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/17/6441</data>
  <data key="d3">A New Approach for Detecting Fundus Lesions Using Image Processing and Deep Neural Network Architecture Based on YOLO Model</data>
  <data key="d4">C Santos, M Aguiar, D Welfer, B Belloni</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5063651843298506539&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="17116877368626144169">
  <data key="d0">Sap-detr: Bridging the gap between salient points and queries-based transformer detector for fast model convergency</data>
  <data key="d1">17116877368626144169</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Liu_SAP-DETR_Bridging_the_Gap_Between_Salient_Points_and_Queries-Based_Transformer_CVPR_2023_paper.html</data>
  <data key="d3">Sap-detr: Bridging the gap between salient points and queries-based transformer detector for fast model convergency</data>
  <data key="d4">Y Liu, Y Zhang, Y Wang, Y Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17116877368626144169&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="11206281819202318260">
  <data key="d0">CenterNet++ for object detection</data>
  <data key="d1">11206281819202318260</data>
  <data key="d2">https://arxiv.org/abs/2204.08394</data>
  <data key="d3">CenterNet++ for object detection</data>
  <data key="d4">K Duan, S Bai, L Xie, H Qi, Q Huang, Q Tian</data>
  <data key="d5">2022</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11206281819202318260&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="12932718969286495303">
  <data key="d0">A review of convolutional neural network architectures and their optimizations</data>
  <data key="d1">12932718969286495303</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10462-022-10213-5</data>
  <data key="d3">A review of convolutional neural network architectures and their optimizations</data>
  <data key="d4">S Cong, Y Zhou</data>
  <data key="d5">2023</data>
  <data key="d6">27</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12932718969286495303&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="3883959449895642650">
  <data key="d0">Dense Oil Tank Detection and Classification via YOLOX-TR Network in Large-Scale SAR Images</data>
  <data key="d1">3883959449895642650</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/14/3246</data>
  <data key="d3">Dense Oil Tank Detection and Classification via YOLOX-TR Network in Large-Scale SAR Images</data>
  <data key="d4">Q Wu, B Zhang, C Xu, H Zhang, C Wang</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3883959449895642650&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="1435298664216281195">
  <data key="d0">Field-matching attention network for object detection</data>
  <data key="d1">1435298664216281195</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S092523122300262X</data>
  <data key="d3">Field-matching attention network for object detection</data>
  <data key="d4">Y Dong, L Shen, Y Pei, H Yang, X Li</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1435298664216281195&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="5279305686836034362">
  <data key="d0">PDAM–STPNNet: A small target detection approach for wildland fire smoke through remote sensing images</data>
  <data key="d1">5279305686836034362</data>
  <data key="d2">https://www.mdpi.com/2073-8994/13/12/2260</data>
  <data key="d3">PDAM–STPNNet: A small target detection approach for wildland fire smoke through remote sensing images</data>
  <data key="d4">J Zhan, Y Hu, W Cai, G Zhou, L Li</data>
  <data key="d5">2021</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5279305686836034362&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="2900647541574704523">
  <data key="d0">Dist-YOLO: fast object detection with distance estimation</data>
  <data key="d1">2900647541574704523</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/3/1354</data>
  <data key="d3">Dist-YOLO: fast object detection with distance estimation</data>
  <data key="d4">M Vajgl, P Hurtik, T Nejezchleba</data>
  <data key="d5">2022</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2900647541574704523&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="9358192350302074225">
  <data key="d0">Detection of river plastic using UAV sensor data and deep learning</data>
  <data key="d1">9358192350302074225</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/13/3049</data>
  <data key="d3">Detection of river plastic using UAV sensor data and deep learning</data>
  <data key="d4">N Maharjan, H Miyazaki, BM Pati, MN Dailey…</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9358192350302074225&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="7333075876299418187">
  <data key="d0">RegionCL: Can simple region swapping contribute to contrastive learning?</data>
  <data key="d1">7333075876299418187</data>
  <data key="d2">https://arxiv.org/abs/2111.12309</data>
  <data key="d3">RegionCL: Can simple region swapping contribute to contrastive learning?</data>
  <data key="d4">Y Xu, Q Zhang, J Zhang, D Tao</data>
  <data key="d5">2021</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7333075876299418187&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="41036872912386481">
  <data key="d0">Pcbnet: A lightweight convolutional neural network for defect inspection in surface mount technology</data>
  <data key="d1">41036872912386481</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9837457/</data>
  <data key="d3">Pcbnet: A lightweight convolutional neural network for defect inspection in surface mount technology</data>
  <data key="d4">H Wu, R Lei, Y Peng</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=41036872912386481&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="3142941948497874730">
  <data key="d0">A domestic trash detection model based on improved YOLOX</data>
  <data key="d1">3142941948497874730</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/18/6974</data>
  <data key="d3">A domestic trash detection model based on improved YOLOX</data>
  <data key="d4">C Liu, N Xie, X Yang, R Chen, X Chang, RY Zhong…</data>
  <data key="d5">2022</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3142941948497874730&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="12459243040456605693">
  <data key="d0">Efficient one-stage video object detection by exploiting temporal consistency</data>
  <data key="d1">12459243040456605693</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19833-5_1</data>
  <data key="d3">Efficient one-stage video object detection by exploiting temporal consistency</data>
  <data key="d4">G Sun, Y Hua, G Hu, N Robertson</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12459243040456605693&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="6585235751137549645">
  <data key="d0">YOLOv5 with convMixer prediction heads for precise object detection in drone imagery</data>
  <data key="d1">6585235751137549645</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/21/8424</data>
  <data key="d3">YOLOv5 with convMixer prediction heads for precise object detection in drone imagery</data>
  <data key="d4">R Baidya, H Jeong</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6585235751137549645&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="16425348670839745439">
  <data key="d0">A systematic review of drone based road traffic monitoring system</data>
  <data key="d1">16425348670839745439</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9893814/</data>
  <data key="d3">A systematic review of drone based road traffic monitoring system</data>
  <data key="d4">I Bisio, C Garibotto, H Haleem, F Lavagetto…</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16425348670839745439&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="16435089417528679481">
  <data key="d0">YOLOD: A target detection method for UAV aerial imagery</data>
  <data key="d1">16435089417528679481</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/14/3240</data>
  <data key="d3">YOLOD: A target detection method for UAV aerial imagery</data>
  <data key="d4">X Luo, Y Wu, L Zhao</data>
  <data key="d5">2022</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16435089417528679481&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="7917884940240812522">
  <data key="d0">Fbnetv5: Neural architecture search for multiple tasks in one run</data>
  <data key="d1">7917884940240812522</data>
  <data key="d2">https://arxiv.org/abs/2111.10007</data>
  <data key="d3">Fbnetv5: Neural architecture search for multiple tasks in one run</data>
  <data key="d4">B Wu, C Li, H Zhang, X Dai, P Zhang, M Yu…</data>
  <data key="d5">2021</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7917884940240812522&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="14459465598979164145">
  <data key="d0">Object detection in autonomous vehicles: Status and open challenges</data>
  <data key="d1">14459465598979164145</data>
  <data key="d2">https://arxiv.org/abs/2201.07706</data>
  <data key="d3">Object detection in autonomous vehicles: Status and open challenges</data>
  <data key="d4">A Balasubramaniam, S Pasricha</data>
  <data key="d5">2022</data>
  <data key="d6">27</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14459465598979164145&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="4490918786976048296">
  <data key="d0">Exploring plain vision transformer backbones for object detection</data>
  <data key="d1">4490918786976048296</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20077-9_17</data>
  <data key="d3">Exploring plain vision transformer backbones for object detection</data>
  <data key="d4">Y Li, H Mao, R Girshick, K He</data>
  <data key="d5">2022</data>
  <data key="d6">294</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4490918786976048296&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="6243645967630982889">
  <data key="d0">Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives</data>
  <data key="d1">6243645967630982889</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841523000233</data>
  <data key="d3">Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives</data>
  <data key="d4">J Li, J Chen, Y Tang, C Wang, BA Landman…</data>
  <data key="d5">2023</data>
  <data key="d6">57</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6243645967630982889&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="9370947851501467347">
  <data key="d0">Adding conditional control to text-to-image diffusion models</data>
  <data key="d1">9370947851501467347</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Adding_Conditional_Control_to_Text-to-Image_Diffusion_Models_ICCV_2023_paper.html</data>
  <data key="d3">Adding conditional control to text-to-image diffusion models</data>
  <data key="d4">L Zhang, A Rao, M Agrawala</data>
  <data key="d5">2023</data>
  <data key="d6">314</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9370947851501467347&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="15741444728855576863">
  <data key="d0">Segment anything</data>
  <data key="d1">15741444728855576863</data>
  <data key="d2">https://arxiv.org/abs/2304.02643</data>
  <data key="d3">Segment anything</data>
  <data key="d4">A Kirillov, E Mintun, N Ravi, H Mao, C Rolland…</data>
  <data key="d5">2023</data>
  <data key="d6">703</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15741444728855576863&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="10588342779298269046">
  <data key="d0">Eva: Exploring the limits of masked visual representation learning at scale</data>
  <data key="d1">10588342779298269046</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.html</data>
  <data key="d3">Eva: Exploring the limits of masked visual representation learning at scale</data>
  <data key="d4">Y Fang, W Wang, B Xie, Q Sun, L Wu…</data>
  <data key="d5">2023</data>
  <data key="d6">114</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10588342779298269046&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="6118595289890500680">
  <data key="d0">Internimage: Exploring large-scale vision foundation models with deformable convolutions</data>
  <data key="d1">6118595289890500680</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Wang_InternImage_Exploring_Large-Scale_Vision_Foundation_Models_With_Deformable_Convolutions_CVPR_2023_paper.html</data>
  <data key="d3">Internimage: Exploring large-scale vision foundation models with deformable convolutions</data>
  <data key="d4">W Wang, J Dai, Z Chen, Z Huang, Z Li…</data>
  <data key="d5">2023</data>
  <data key="d6">128</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6118595289890500680&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="17997891498068622933">
  <data key="d0">Mixformer: End-to-end tracking with iterative mixed attention</data>
  <data key="d1">17997891498068622933</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Cui_MixFormer_End-to-End_Tracking_With_Iterative_Mixed_Attention_CVPR_2022_paper.html</data>
  <data key="d3">Mixformer: End-to-end tracking with iterative mixed attention</data>
  <data key="d4">Y Cui, C Jiang, L Wang, G Wu</data>
  <data key="d5">2022</data>
  <data key="d6">170</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17997891498068622933&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="9439766841533136382">
  <data key="d0">Vitpose: Simple vision transformer baselines for human pose estimation</data>
  <data key="d1">9439766841533136382</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/fbb10d319d44f8c3b4720873e4177c65-Abstract-Conference.html</data>
  <data key="d3">Vitpose: Simple vision transformer baselines for human pose estimation</data>
  <data key="d4">Y Xu, J Zhang, Q Zhang, D Tao</data>
  <data key="d5">2022</data>
  <data key="d6">140</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9439766841533136382&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="12938213222665733645">
  <data key="d0">Hornet: Efficient high-order spatial interactions with recursive gated convolutions</data>
  <data key="d1">12938213222665733645</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/436d042b2dd81214d23ae43eb196b146-Abstract-Conference.html</data>
  <data key="d3">Hornet: Efficient high-order spatial interactions with recursive gated convolutions</data>
  <data key="d4">Y Rao, W Zhao, Y Tang, J Zhou…</data>
  <data key="d5">2022</data>
  <data key="d6">102</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12938213222665733645&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="14316138733173377966">
  <data key="d0">Vision transformer adapter for dense predictions</data>
  <data key="d1">14316138733173377966</data>
  <data key="d2">https://arxiv.org/abs/2205.08534</data>
  <data key="d3">Vision transformer adapter for dense predictions</data>
  <data key="d4">Z Chen, Y Duan, W Wang, J He, T Lu, J Dai…</data>
  <data key="d5">2022</data>
  <data key="d6">172</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14316138733173377966&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="12550135891789567985">
  <data key="d0">Detrs with hybrid matching</data>
  <data key="d1">12550135891789567985</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Jia_DETRs_With_Hybrid_Matching_CVPR_2023_paper.html</data>
  <data key="d3">Detrs with hybrid matching</data>
  <data key="d4">D Jia, Y Yuan, H He, X Wu, H Yu…</data>
  <data key="d5">2023</data>
  <data key="d6">59</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12550135891789567985&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="4555711365675635781">
  <data key="d0">Convmae: Masked convolution meets masked autoencoders</data>
  <data key="d1">4555711365675635781</data>
  <data key="d2">https://arxiv.org/abs/2205.03892</data>
  <data key="d3">Convmae: Masked convolution meets masked autoencoders</data>
  <data key="d4">P Gao, T Ma, H Li, Z Lin, J Dai, Y Qiao</data>
  <data key="d5">2022</data>
  <data key="d6">69</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4555711365675635781&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="14680303082655356082">
  <data key="d0">A unified sequence interface for vision tasks</data>
  <data key="d1">14680303082655356082</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/cb0f9020c00fc52a9f6c9dbfacc6ac58-Abstract-Conference.html</data>
  <data key="d3">A unified sequence interface for vision tasks</data>
  <data key="d4">T Chen, S Saxena, L Li, TY Lin…</data>
  <data key="d5">2022</data>
  <data key="d6">45</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14680303082655356082&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="640071900436442539">
  <data key="d0">Segment anything model for medical image analysis: an experimental study</data>
  <data key="d1">640071900436442539</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841523001780</data>
  <data key="d3">Segment anything model for medical image analysis: an experimental study</data>
  <data key="d4">MA Mazurowski, H Dong, H Gu, J Yang, N Konz…</data>
  <data key="d5">2023</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=640071900436442539&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="9830572971874050892">
  <data key="d0">Siamese image modeling for self-supervised vision representation learning</data>
  <data key="d1">9830572971874050892</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Tao_Siamese_Image_Modeling_for_Self-Supervised_Vision_Representation_Learning_CVPR_2023_paper.html</data>
  <data key="d3">Siamese image modeling for self-supervised vision representation learning</data>
  <data key="d4">C Tao, X Zhu, W Su, G Huang, B Li…</data>
  <data key="d5">2023</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9830572971874050892&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="18387677115510147109">
  <data key="d0">Detrs with collaborative hybrid assignments training</data>
  <data key="d1">18387677115510147109</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Zong_DETRs_with_Collaborative_Hybrid_Assignments_Training_ICCV_2023_paper.html</data>
  <data key="d3">Detrs with collaborative hybrid assignments training</data>
  <data key="d4">Z Zong, G Song, Y Liu</data>
  <data key="d5">2023</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18387677115510147109&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="7734774232157964979">
  <data key="d0">MCMAE: Masked convolution meets masked autoencoders</data>
  <data key="d1">7734774232157964979</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/e7938ede51225b490bb69f7b361a9259-Abstract-Conference.html</data>
  <data key="d3">MCMAE: Masked convolution meets masked autoencoders</data>
  <data key="d4">P Gao, T Ma, H Li, Z Lin, J Dai…</data>
  <data key="d5">2022</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7734774232157964979&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="14613261815949086918">
  <data key="d0">Advancing plain vision transformer toward remote sensing foundation model</data>
  <data key="d1">14613261815949086918</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9956816/</data>
  <data key="d3">Advancing plain vision transformer toward remote sensing foundation model</data>
  <data key="d4">D Wang, Q Zhang, Y Xu, J Zhang, B Du…</data>
  <data key="d5">2022</data>
  <data key="d6">47</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14613261815949086918&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="6979440938086217563">
  <data key="d0">Decoupling human and camera motion from videos in the wild</data>
  <data key="d1">6979440938086217563</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Ye_Decoupling_Human_and_Camera_Motion_From_Videos_in_the_Wild_CVPR_2023_paper.html</data>
  <data key="d3">Decoupling human and camera motion from videos in the wild</data>
  <data key="d4">V Ye, G Pavlakos, J Malik…</data>
  <data key="d5">2023</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6979440938086217563&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="5337226914153811562">
  <data key="d0">Towards all-in-one pre-training via maximizing multi-modal mutual information</data>
  <data key="d1">5337226914153811562</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Su_Towards_All-in-One_Pre-Training_via_Maximizing_Multi-Modal_Mutual_Information_CVPR_2023_paper.html</data>
  <data key="d3">Towards all-in-one pre-training via maximizing multi-modal mutual information</data>
  <data key="d4">W Su, X Zhu, C Tao, L Lu, B Li…</data>
  <data key="d5">2023</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5337226914153811562&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="7134900495559356797">
  <data key="d0">Vsa: Learning varied-size window attention in vision transformers</data>
  <data key="d1">7134900495559356797</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19806-9_27</data>
  <data key="d3">Vsa: Learning varied-size window attention in vision transformers</data>
  <data key="d4">Q Zhang, Y Xu, J Zhang, D Tao</data>
  <data key="d5">2022</data>
  <data key="d6">37</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7134900495559356797&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="5030683813836303943">
  <data key="d0">Masked autoencoders enable efficient knowledge distillers</data>
  <data key="d1">5030683813836303943</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Bai_Masked_Autoencoders_Enable_Efficient_Knowledge_Distillers_CVPR_2023_paper.html</data>
  <data key="d3">Masked autoencoders enable efficient knowledge distillers</data>
  <data key="d4">Y Bai, Z Wang, J Xiao, C Wei, H Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5030683813836303943&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="11522546362437060713">
  <data key="d0">Unleashing vanilla vision transformer with masked image modeling for object detection</data>
  <data key="d1">11522546362437060713</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Fang_Unleashing_Vanilla_Vision_Transformer_with_Masked_Image_Modeling_for_Object_ICCV_2023_paper.html</data>
  <data key="d3">Unleashing vanilla vision transformer with masked image modeling for object detection</data>
  <data key="d4">Y Fang, S Yang, S Wang, Y Ge…</data>
  <data key="d5">2023</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11522546362437060713&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="12195536569534646845">
  <data key="d0">Hydra attention: Efficient attention with many heads</data>
  <data key="d1">12195536569534646845</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-25082-8_3</data>
  <data key="d3">Hydra attention: Efficient attention with many heads</data>
  <data key="d4">D Bolya, CY Fu, X Dai, P Zhang, J Hoffman</data>
  <data key="d5">2022</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12195536569534646845&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="5962552400166695847">
  <data key="d0">Understanding masked image modeling via learning occlusion invariant feature</data>
  <data key="d1">5962552400166695847</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Kong_Understanding_Masked_Image_Modeling_via_Learning_Occlusion_Invariant_Feature_CVPR_2023_paper.html</data>
  <data key="d3">Understanding masked image modeling via learning occlusion invariant feature</data>
  <data key="d4">X Kong, X Zhang</data>
  <data key="d5">2023</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5962552400166695847&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="14300246766552093616">
  <data key="d0">Flatten transformer: Vision transformer using focused linear attention</data>
  <data key="d1">14300246766552093616</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Han_FLatten_Transformer_Vision_Transformer_using_Focused_Linear_Attention_ICCV_2023_paper.html</data>
  <data key="d3">Flatten transformer: Vision transformer using focused linear attention</data>
  <data key="d4">D Han, X Pan, Y Han, S Song…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14300246766552093616&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="8055277125890470910">
  <data key="d0">F-vlm: Open-vocabulary object detection upon frozen vision and language models</data>
  <data key="d1">8055277125890470910</data>
  <data key="d2">https://arxiv.org/abs/2209.15639</data>
  <data key="d3">F-vlm: Open-vocabulary object detection upon frozen vision and language models</data>
  <data key="d4">W Kuo, Y Cui, X Gu, AJ Piergiovanni…</data>
  <data key="d5">2022</data>
  <data key="d6">39</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8055277125890470910&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="7111056020821706590">
  <data key="d0">Vision transformers are good mask auto-labelers</data>
  <data key="d1">7111056020821706590</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Lan_Vision_Transformers_Are_Good_Mask_Auto-Labelers_CVPR_2023_paper.html</data>
  <data key="d3">Vision transformers are good mask auto-labelers</data>
  <data key="d4">S Lan, X Yang, Z Yu, Z Wu…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7111056020821706590&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="1052062078621162321">
  <data key="d0">Paco: Parts and attributes of common objects</data>
  <data key="d1">1052062078621162321</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Ramanathan_PACO_Parts_and_Attributes_of_Common_Objects_CVPR_2023_paper.html</data>
  <data key="d3">Paco: Parts and attributes of common objects</data>
  <data key="d4">V Ramanathan, A Kalia, V Petrovic…</data>
  <data key="d5">2023</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1052062078621162321&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="7380604299331262465">
  <data key="d0">Resformer: Scaling vits with multi-resolution training</data>
  <data key="d1">7380604299331262465</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Tian_ResFormer_Scaling_ViTs_With_Multi-Resolution_Training_CVPR_2023_paper.html</data>
  <data key="d3">Resformer: Scaling vits with multi-resolution training</data>
  <data key="d4">R Tian, Z Wu, Q Dai, H Hu, Y Qiao…</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7380604299331262465&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="17776221386292352341">
  <data key="d0">Better plain ViT baselines for ImageNet-1k</data>
  <data key="d1">17776221386292352341</data>
  <data key="d2">https://arxiv.org/abs/2205.01580</data>
  <data key="d3">Better plain ViT baselines for ImageNet-1k</data>
  <data key="d4">L Beyer, X Zhai, A Kolesnikov</data>
  <data key="d5">2022</data>
  <data key="d6">31</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17776221386292352341&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="8429597570092570925">
  <data key="d0">Stare at what you see: Masked image modeling without reconstruction</data>
  <data key="d1">8429597570092570925</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Xue_Stare_at_What_You_See_Masked_Image_Modeling_Without_Reconstruction_CVPR_2023_paper.html</data>
  <data key="d3">Stare at what you see: Masked image modeling without reconstruction</data>
  <data key="d4">H Xue, P Gao, H Li, Y Qiao, H Sun…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8429597570092570925&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="8815839713273959723">
  <data key="d0">Dilated neighborhood attention transformer</data>
  <data key="d1">8815839713273959723</data>
  <data key="d2">https://arxiv.org/abs/2209.15001</data>
  <data key="d3">Dilated neighborhood attention transformer</data>
  <data key="d4">A Hassani, H Shi</data>
  <data key="d5">2022</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8815839713273959723&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="4731455256244956465">
  <data key="d0">BiFormer: Vision Transformer with Bi-Level Routing Attention</data>
  <data key="d1">4731455256244956465</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhu_BiFormer_Vision_Transformer_With_Bi-Level_Routing_Attention_CVPR_2023_paper.html</data>
  <data key="d3">BiFormer: Vision Transformer with Bi-Level Routing Attention</data>
  <data key="d4">L Zhu, X Wang, Z Ke, W Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4731455256244956465&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="8388230239628727224">
  <data key="d0">Bridgetower: Building bridges between encoders in vision-language representation learning</data>
  <data key="d1">8388230239628727224</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/26263</data>
  <data key="d3">Bridgetower: Building bridges between encoders in vision-language representation learning</data>
  <data key="d4">X Xu, C Wu, S Rosenman, V Lal, W Che…</data>
  <data key="d5">2023</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8388230239628727224&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="13876321258274905912">
  <data key="d0">Lightvit: Towards light-weight convolution-free vision transformers</data>
  <data key="d1">13876321258274905912</data>
  <data key="d2">https://arxiv.org/abs/2207.05557</data>
  <data key="d3">Lightvit: Towards light-weight convolution-free vision transformers</data>
  <data key="d4">T Huang, L Huang, S You, F Wang, C Qian…</data>
  <data key="d5">2022</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13876321258274905912&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="3429308917664707403">
  <data key="d0">Expediting large-scale vision transformer for dense prediction without fine-tuning</data>
  <data key="d1">3429308917664707403</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/e6c2e85db1f1039177c4495ccd399ac4-Abstract-Conference.html</data>
  <data key="d3">Expediting large-scale vision transformer for dense prediction without fine-tuning</data>
  <data key="d4">W Liang, Y Yuan, H Ding, X Luo…</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3429308917664707403&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="16202141712210963294">
  <data key="d0">CroCo: Self-Supervised Pre-training for 3D Vision Tasks by Cross-View Completion</data>
  <data key="d1">16202141712210963294</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/16e71d1a24b98a02c17b1be1f634f979-Abstract-Conference.html</data>
  <data key="d3">CroCo: Self-Supervised Pre-training for 3D Vision Tasks by Cross-View Completion</data>
  <data key="d4">P Weinzaepfel, V Leroy, T Lucas…</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16202141712210963294&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="3360913791146424633">
  <data key="d0">Grit: A generative region-to-text transformer for object understanding</data>
  <data key="d1">3360913791146424633</data>
  <data key="d2">https://arxiv.org/abs/2212.00280</data>
  <data key="d3">Grit: A generative region-to-text transformer for object understanding</data>
  <data key="d4">J Wu, J Wang, Z Yang, Z Gan, Z Liu, J Yuan…</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3360913791146424633&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="11213926060739194818">
  <data key="d0">SAM Fails to Segment Anything?--SAM-Adapter: Adapting SAM in Underperformed Scenes: Camouflage, Shadow, and More</data>
  <data key="d1">11213926060739194818</data>
  <data key="d2">https://arxiv.org/abs/2304.09148</data>
  <data key="d3">SAM Fails to Segment Anything?--SAM-Adapter: Adapting SAM in Underperformed Scenes: Camouflage, Shadow, and More</data>
  <data key="d4">T Chen, L Zhu, C Ding, R Cao, S Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">24</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11213926060739194818&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="352322076591852020">
  <data key="d0">Affordance grounding from demonstration video to target image</data>
  <data key="d1">352322076591852020</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Affordance_Grounding_From_Demonstration_Video_To_Target_Image_CVPR_2023_paper.html</data>
  <data key="d3">Affordance grounding from demonstration video to target image</data>
  <data key="d4">J Chen, D Gao, KQ Lin…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=352322076591852020&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="13906564416939487459">
  <data key="d0">Language adaptive weight generation for multi-task visual grounding</data>
  <data key="d1">13906564416939487459</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Su_Language_Adaptive_Weight_Generation_for_Multi-Task_Visual_Grounding_CVPR_2023_paper.html</data>
  <data key="d3">Language adaptive weight generation for multi-task visual grounding</data>
  <data key="d4">W Su, P Miao, H Dou, G Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13906564416939487459&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="12626688018988097647">
  <data key="d0">Region-aware pretraining for open-vocabulary object detection with vision transformers</data>
  <data key="d1">12626688018988097647</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Kim_Region-Aware_Pretraining_for_Open-Vocabulary_Object_Detection_With_Vision_Transformers_CVPR_2023_paper.html</data>
  <data key="d3">Region-aware pretraining for open-vocabulary object detection with vision transformers</data>
  <data key="d4">D Kim, A Angelova, W Kuo</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12626688018988097647&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="15264491751168338591">
  <data key="d0">2D and 3D object detection algorithms from images: A Survey</data>
  <data key="d1">15264491751168338591</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2590005623000309</data>
  <data key="d3">2D and 3D object detection algorithms from images: A Survey</data>
  <data key="d4">W Chen, Y Li, Z Tian, F Zhang</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15264491751168338591&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="1684205177621612046">
  <data key="d0">Cat: Localization and identification cascade detection transformer for open-world object detection</data>
  <data key="d1">1684205177621612046</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Ma_CAT_LoCalization_and_IdentificAtion_Cascade_Detection_Transformer_for_Open-World_Object_CVPR_2023_paper.html</data>
  <data key="d3">Cat: Localization and identification cascade detection transformer for open-world object detection</data>
  <data key="d4">S Ma, Y Wang, Y Wei, J Fan, TH Li…</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1684205177621612046&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="9289927810203044884">
  <data key="d0">A closer look at self-supervised lightweight vision transformers</data>
  <data key="d1">9289927810203044884</data>
  <data key="d2">https://proceedings.mlr.press/v202/wang23e.html</data>
  <data key="d3">A closer look at self-supervised lightweight vision transformers</data>
  <data key="d4">S Wang, J Gao, Z Li, X Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9289927810203044884&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="4028732696004816605">
  <data key="d0">The effectiveness of MAE pre-pretraining for billion-scale pretraining</data>
  <data key="d1">4028732696004816605</data>
  <data key="d2">https://arxiv.org/abs/2303.13496</data>
  <data key="d3">The effectiveness of MAE pre-pretraining for billion-scale pretraining</data>
  <data key="d4">M Singh, Q Duval, KV Alwala, H Fan…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4028732696004816605&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="2922049850680781929">
  <data key="d0">BiViT: Extremely Compressed Binary Vision Transformers</data>
  <data key="d1">2922049850680781929</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/He_BiViT_Extremely_Compressed_Binary_Vision_Transformers_ICCV_2023_paper.html</data>
  <data key="d3">BiViT: Extremely Compressed Binary Vision Transformers</data>
  <data key="d4">Y He, Z Lou, L Zhang, J Liu, W Wu…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2922049850680781929&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="6877116560747276387">
  <data key="d0">Hivit: A simpler and more efficient design of hierarchical vision transformer</data>
  <data key="d1">6877116560747276387</data>
  <data key="d2">https://openreview.net/forum?id=3F6I-0-57SC</data>
  <data key="d3">Hivit: A simpler and more efficient design of hierarchical vision transformer</data>
  <data key="d4">X Zhang, Y Tian, L Xie, W Huang, Q Dai…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6877116560747276387&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="4504943264737055020">
  <data key="d0">Vitkd: Practical guidelines for vit feature knowledge distillation</data>
  <data key="d1">4504943264737055020</data>
  <data key="d2">https://arxiv.org/abs/2209.02432</data>
  <data key="d3">Vitkd: Practical guidelines for vit feature knowledge distillation</data>
  <data key="d4">Z Yang, Z Li, A Zeng, Z Li, C Yuan, Y Li</data>
  <data key="d5">2022</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4504943264737055020&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="8057404930177375775">
  <data key="d0">Efficient self-supervised vision pretraining with local masked reconstruction</data>
  <data key="d1">8057404930177375775</data>
  <data key="d2">https://arxiv.org/abs/2206.00790</data>
  <data key="d3">Efficient self-supervised vision pretraining with local masked reconstruction</data>
  <data key="d4">J Chen, M Hu, B Li, M Elhoseiny</data>
  <data key="d5">2022</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8057404930177375775&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="8100725342405011358">
  <data key="d0">Eva-02: A visual representation for neon genesis</data>
  <data key="d1">8100725342405011358</data>
  <data key="d2">https://arxiv.org/abs/2303.11331</data>
  <data key="d3">Eva-02: A visual representation for neon genesis</data>
  <data key="d4">Y Fang, Q Sun, X Wang, T Huang, X Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8100725342405011358&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="1673777623717824835">
  <data key="d0">Less is More: Focus Attention for Efficient DETR</data>
  <data key="d1">1673777623717824835</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_Less_is_More_Focus_Attention_for_Efficient_DETR_ICCV_2023_paper.html</data>
  <data key="d3">Less is More: Focus Attention for Efficient DETR</data>
  <data key="d4">D Zheng, W Dong, H Hu, X Chen…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1673777623717824835&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="526538470553837028">
  <data key="d0">More than encoder: Introducing transformer decoder to upsample</data>
  <data key="d1">526538470553837028</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9995378/</data>
  <data key="d3">More than encoder: Introducing transformer decoder to upsample</data>
  <data key="d4">Y Li, W Cai, Y Gao, C Li, X Hu</data>
  <data key="d5">2022</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=526538470553837028&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="11594648656724394854">
  <data key="d0">Group detr v2: Strong object detector with encoder-decoder pretraining</data>
  <data key="d1">11594648656724394854</data>
  <data key="d2">https://arxiv.org/abs/2211.03594</data>
  <data key="d3">Group detr v2: Strong object detector with encoder-decoder pretraining</data>
  <data key="d4">Q Chen, J Wang, C Han, S Zhang, Z Li, X Chen…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11594648656724394854&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="4578573813216045589">
  <data key="d0">UniHCP: A Unified Model for Human-Centric Perceptions</data>
  <data key="d1">4578573813216045589</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Ci_UniHCP_A_Unified_Model_for_Human-Centric_Perceptions_CVPR_2023_paper.html</data>
  <data key="d3">UniHCP: A Unified Model for Human-Centric Perceptions</data>
  <data key="d4">Y Ci, Y Wang, M Chen, S Tang, L Bai…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4578573813216045589&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="9793040987893379450">
  <data key="d0">Swin-Transformer-YOLOv5 for real-time wine grape bunch detection</data>
  <data key="d1">9793040987893379450</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/22/5853</data>
  <data key="d3">Swin-Transformer-YOLOv5 for real-time wine grape bunch detection</data>
  <data key="d4">S Lu, X Liu, Z He, X Zhang, W Liu, M Karkee</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9793040987893379450&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="10898983420399357710">
  <data key="d0">Exploring long-sequence masked autoencoders</data>
  <data key="d1">10898983420399357710</data>
  <data key="d2">https://arxiv.org/abs/2210.07224</data>
  <data key="d3">Exploring long-sequence masked autoencoders</data>
  <data key="d4">R Hu, S Debnath, S Xie, X Chen</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10898983420399357710&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="11821895939908774894">
  <data key="d0">STMixer: A One-Stage Sparse Action Detector</data>
  <data key="d1">11821895939908774894</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Wu_STMixer_A_One-Stage_Sparse_Action_Detector_CVPR_2023_paper.html</data>
  <data key="d3">STMixer: A One-Stage Sparse Action Detector</data>
  <data key="d4">T Wu, M Cao, Z Gao, G Wu…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11821895939908774894&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="10214171113959087100">
  <data key="d0">Good helper is around you: Attention-driven masked image modeling</data>
  <data key="d1">10214171113959087100</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/25269</data>
  <data key="d3">Good helper is around you: Attention-driven masked image modeling</data>
  <data key="d4">Z Liu, J Gui, H Luo</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10214171113959087100&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="6493028014197162041">
  <data key="d0">Pidray: A large-scale x-ray benchmark for real-world prohibited item detection</data>
  <data key="d1">6493028014197162041</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11263-023-01855-1</data>
  <data key="d3">Pidray: A large-scale x-ray benchmark for real-world prohibited item detection</data>
  <data key="d4">L Zhang, L Jiang, R Ji, H Fan</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6493028014197162041&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="14312905429445779769">
  <data key="d0">A domain specific knowledge extraction transformer method for multisource satellite-borne SAR images ship detection</data>
  <data key="d1">14312905429445779769</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0924271623000515</data>
  <data key="d3">A domain specific knowledge extraction transformer method for multisource satellite-borne SAR images ship detection</data>
  <data key="d4">S Zhao, Y Luo, T Zhang, W Guo, Z Zhang</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14312905429445779769&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="4706041545647768300">
  <data key="d0">Mvsformer: Learning robust image representations via transformers and temperature-based depth for multi-view stereo</data>
  <data key="d1">4706041545647768300</data>
  <data key="d2">https://arxiv.org/abs/2208.02541</data>
  <data key="d3">Mvsformer: Learning robust image representations via transformers and temperature-based depth for multi-view stereo</data>
  <data key="d4">C Cao, X Ren, Y Fu</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4706041545647768300&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="11889722873208015946">
  <data key="d0">Streaming Video Model</data>
  <data key="d1">11889722873208015946</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Streaming_Video_Model_CVPR_2023_paper.html</data>
  <data key="d3">Streaming Video Model</data>
  <data key="d4">Y Zhao, C Luo, C Tang, D Chen…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11889722873208015946&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="RR4ndOlcdmUJ">
  <data key="d0">Improving CLIP Fine-tuning Performance</data>
  <data key="d1">RR4ndOlcdmUJ</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Wei_Improving_CLIP_Fine-tuning_Performance_ICCV_2023_paper.html</data>
  <data key="d3">Improving CLIP Fine-tuning Performance</data>
  <data key="d4">Y Wei, H Hu, Z Xie, Z Liu, Z Zhang…</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="7490603673765775284">
  <data key="d0">Transvg++: End-to-end visual grounding with language conditioned vision transformer</data>
  <data key="d1">7490603673765775284</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10187690/</data>
  <data key="d3">Transvg++: End-to-end visual grounding with language conditioned vision transformer</data>
  <data key="d4">J Deng, Z Yang, D Liu, T Chen, W Zhou…</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7490603673765775284&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="117882505525518941">
  <data key="d0">Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks</data>
  <data key="d1">117882505525518941</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Wang_Image_as_a_Foreign_Language_BEiT_Pretraining_for_Vision_and_CVPR_2023_paper.html</data>
  <data key="d3">Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks</data>
  <data key="d4">W Wang, H Bao, L Dong, J Bjorck…</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=117882505525518941&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="16343611144020544659">
  <data key="d0">Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference</data>
  <data key="d1">16343611144020544659</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/You_Castling-ViT_Compressing_Self-Attention_via_Switching_Towards_Linear-Angular_Attention_at_Vision_CVPR_2023_paper.html</data>
  <data key="d3">Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference</data>
  <data key="d4">H You, Y Xiong, X Dai, B Wu, P Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16343611144020544659&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="11178483353620714191">
  <data key="d0">Cascade-DETR: Delving into High-Quality Universal Object Detection</data>
  <data key="d1">11178483353620714191</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Cascade-DETR_Delving_into_High-Quality_Universal_Object_Detection_ICCV_2023_paper.html</data>
  <data key="d3">Cascade-DETR: Delving into High-Quality Universal Object Detection</data>
  <data key="d4">M Ye, L Ke, S Li, YW Tai, CK Tang…</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="vTRbp0ZWND4J">
  <data key="d0">Integrally Migrating Pre-trained Transformer Encoder-decoders for Visual Object Detection</data>
  <data key="d1">vTRbp0ZWND4J</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Integrally_Migrating_Pre-trained_Transformer_Encoder-decoders_for_Visual_Object_Detection_ICCV_2023_paper.html</data>
  <data key="d3">Integrally Migrating Pre-trained Transformer Encoder-decoders for Visual Object Detection</data>
  <data key="d4">F Liu, X Zhang, Z Peng, Z Guo…</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="17890971807751741818">
  <data key="d0">SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer</data>
  <data key="d1">17890971807751741818</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Chen_SparseViT_Revisiting_Activation_Sparsity_for_Efficient_High-Resolution_Vision_Transformer_CVPR_2023_paper.html</data>
  <data key="d3">SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer</data>
  <data key="d4">X Chen, Z Liu, H Tang, L Yi…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17890971807751741818&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="7816124690949941974">
  <data key="d0">Mammut: A simple architecture for joint learning for multimodal tasks</data>
  <data key="d1">7816124690949941974</data>
  <data key="d2">https://arxiv.org/abs/2303.16839</data>
  <data key="d3">Mammut: A simple architecture for joint learning for multimodal tasks</data>
  <data key="d4">W Kuo, AJ Piergiovanni, D Kim, X Luo, B Caine…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7816124690949941974&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="7897280292740363837">
  <data key="d0">Vision transformer with quadrangle attention</data>
  <data key="d1">7897280292740363837</data>
  <data key="d2">https://arxiv.org/abs/2303.15105</data>
  <data key="d3">Vision transformer with quadrangle attention</data>
  <data key="d4">Q Zhang, J Zhang, Y Xu, D Tao</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7897280292740363837&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="306282026774466293">
  <data key="d0">Budgeted Training for Vision Transformer</data>
  <data key="d1">306282026774466293</data>
  <data key="d2">https://openreview.net/forum?id=sVzBN-DlJRi</data>
  <data key="d3">Budgeted Training for Vision Transformer</data>
  <data key="d4">X Pan, X Jin, Y He, S Song, G Huang</data>
  <data key="d5">2022</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=306282026774466293&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="3818577016458576609">
  <data key="d0">Where should i spend my flops? efficiency evaluations of visual pre-training methods</data>
  <data key="d1">3818577016458576609</data>
  <data key="d2">https://arxiv.org/abs/2209.15589</data>
  <data key="d3">Where should i spend my flops? efficiency evaluations of visual pre-training methods</data>
  <data key="d4">S Koppula, Y Li, E Shelhamer, A Jaegle…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3818577016458576609&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="2272102423598794653">
  <data key="d0">Contrastive Feature Masking Open-Vocabulary Vision Transformer</data>
  <data key="d1">2272102423598794653</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Kim_Contrastive_Feature_Masking_Open-Vocabulary_Vision_Transformer_ICCV_2023_paper.html</data>
  <data key="d3">Contrastive Feature Masking Open-Vocabulary Vision Transformer</data>
  <data key="d4">D Kim, A Angelova, W Kuo</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="15646051846564298047">
  <data key="d0">Self-distillation augmented masked autoencoders for histopathological image classification</data>
  <data key="d1">15646051846564298047</data>
  <data key="d2">https://arxiv.org/abs/2203.16983</data>
  <data key="d3">Self-distillation augmented masked autoencoders for histopathological image classification</data>
  <data key="d4">Y Luo, Z Chen, S Zhou, X Gao</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15646051846564298047&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="647200095634493159">
  <data key="d0">How can objects help action recognition?</data>
  <data key="d1">647200095634493159</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhou_How_Can_Objects_Help_Action_Recognition_CVPR_2023_paper.html</data>
  <data key="d3">How can objects help action recognition?</data>
  <data key="d4">X Zhou, A Arnab, C Sun…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=647200095634493159&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="8153900683485623660">
  <data key="d0">Integrally Pre-Trained Transformer Pyramid Networks</data>
  <data key="d1">8153900683485623660</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Tian_Integrally_Pre-Trained_Transformer_Pyramid_Networks_CVPR_2023_paper.html</data>
  <data key="d3">Integrally Pre-Trained Transformer Pyramid Networks</data>
  <data key="d4">Y Tian, L Xie, Z Wang, L Wei, X Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8153900683485623660&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="mT3lyH5QtvIJ">
  <data key="d0">RILS: Masked Visual Reconstruction in Language Semantic Space</data>
  <data key="d1">mT3lyH5QtvIJ</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023/html/Yang_RILS_Masked_Visual_Reconstruction_in_Language_Semantic_Space_CVPR_2023_paper.html</data>
  <data key="d3">RILS: Masked Visual Reconstruction in Language Semantic Space</data>
  <data key="d4">S Yang, Y Ge, K Yi, D Li, Y Shan…</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="15577220087848933491">
  <data key="d0">Vitpose+: Vision transformer foundation model for generic body pose estimation</data>
  <data key="d1">15577220087848933491</data>
  <data key="d2">https://arxiv.org/abs/2212.04246</data>
  <data key="d3">Vitpose+: Vision transformer foundation model for generic body pose estimation</data>
  <data key="d4">Y Xu, J Zhang, Q Zhang, D Tao</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15577220087848933491&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="cTJB_-Obg3YJ">
  <data key="d0">Learning Geometric-Aware Properties in 2D Representation Using Lightweight CAD Models, or Zero Real 3D Pairs</data>
  <data key="d1">cTJB_-Obg3YJ</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Arsomngern_Learning_Geometric-Aware_Properties_in_2D_Representation_Using_Lightweight_CAD_Models_CVPR_2023_paper.html</data>
  <data key="d3">Learning Geometric-Aware Properties in 2D Representation Using Lightweight CAD Models, or Zero Real 3D Pairs</data>
  <data key="d4">P Arsomngern, S Nutanong…</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="5523520554880035051">
  <data key="d0">A billion-scale foundation model for remote sensing images</data>
  <data key="d1">5523520554880035051</data>
  <data key="d2">https://arxiv.org/abs/2304.05215</data>
  <data key="d3">A billion-scale foundation model for remote sensing images</data>
  <data key="d4">K Cha, J Seo, T Lee</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5523520554880035051&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="7501011475036280832">
  <data key="d0">Cumulative Spatial Knowledge Distillation for Vision Transformers</data>
  <data key="d1">7501011475036280832</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Cumulative_Spatial_Knowledge_Distillation_for_Vision_Transformers_ICCV_2023_paper.html</data>
  <data key="d3">Cumulative Spatial Knowledge Distillation for Vision Transformers</data>
  <data key="d4">B Zhao, R Song, J Liang</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="7424513001270707708">
  <data key="d0">Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception</data>
  <data key="d1">7424513001270707708</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Pan_Aria_Digital_Twin_A_New_Benchmark_Dataset_for_Egocentric_3D_ICCV_2023_paper.html</data>
  <data key="d3">Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception</data>
  <data key="d4">X Pan, N Charron, Y Yang, S Peters…</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="10014472325439300361">
  <data key="d0">SparseMAE: Sparse Training Meets Masked Autoencoders</data>
  <data key="d1">10014472325439300361</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Zhou_SparseMAE_Sparse_Training_Meets_Masked_Autoencoders_ICCV_2023_paper.html</data>
  <data key="d3">SparseMAE: Sparse Training Meets Masked Autoencoders</data>
  <data key="d4">A Zhou, Y Li, Z Qin, J Liu, J Pan…</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="16089508268500048635">
  <data key="d0">UnLoc: A Unified Framework for Video Localization Tasks</data>
  <data key="d1">16089508268500048635</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Yan_UnLoc_A_Unified_Framework_for_Video_Localization_Tasks_ICCV_2023_paper.html</data>
  <data key="d3">UnLoc: A Unified Framework for Video Localization Tasks</data>
  <data key="d4">S Yan, X Xiong, A Nagrani, A Arnab…</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="14381170671853204535">
  <data key="d0">Improving Pixel-based MIM by Reducing Wasted Modeling Capability</data>
  <data key="d1">14381170671853204535</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Liu_Improving_Pixel-based_MIM_by_Reducing_Wasted_Modeling_Capability_ICCV_2023_paper.html</data>
  <data key="d3">Improving Pixel-based MIM by Reducing Wasted Modeling Capability</data>
  <data key="d4">Y Liu, S Zhang, J Chen, Z Yu…</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="7564591903506430655">
  <data key="d0">Generic-to-Specific Distillation of Masked Autoencoders</data>
  <data key="d1">7564591903506430655</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Huang_Generic-to-Specific_Distillation_of_Masked_Autoencoders_CVPR_2023_paper.html</data>
  <data key="d3">Generic-to-Specific Distillation of Masked Autoencoders</data>
  <data key="d4">W Huang, Z Peng, L Dong, F Wei…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7564591903506430655&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="16928381517970009960">
  <data key="d0">Learning Distortion Invariant Representation for Image Restoration from A Causality Perspective</data>
  <data key="d1">16928381517970009960</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Li_Learning_Distortion_Invariant_Representation_for_Image_Restoration_From_a_Causality_CVPR_2023_paper.html</data>
  <data key="d3">Learning Distortion Invariant Representation for Image Restoration from A Causality Perspective</data>
  <data key="d4">X Li, B Li, X Jin, C Lan, Z Chen</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16928381517970009960&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="11996337183205116538">
  <data key="d0">Vision Transformer Adapters for Generalizable Multitask Learning</data>
  <data key="d1">11996337183205116538</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Bhattacharjee_Vision_Transformer_Adapters_for_Generalizable_Multitask_Learning_ICCV_2023_paper.html</data>
  <data key="d3">Vision Transformer Adapters for Generalizable Multitask Learning</data>
  <data key="d4">D Bhattacharjee, S Süsstrunk…</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="18182863446124159795">
  <data key="d0">Stitchable Neural Networks</data>
  <data key="d1">18182863446124159795</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Pan_Stitchable_Neural_Networks_CVPR_2023_paper.html</data>
  <data key="d3">Stitchable Neural Networks</data>
  <data key="d4">Z Pan, J Cai, B Zhuang</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18182863446124159795&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="56664849184651828">
  <data key="d0">DETR Does Not Need Multi-Scale or Locality Design</data>
  <data key="d1">56664849184651828</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Lin_DETR_Does_Not_Need_Multi-Scale_or_Locality_Design_ICCV_2023_paper.html</data>
  <data key="d3">DETR Does Not Need Multi-Scale or Locality Design</data>
  <data key="d4">Y Lin, Y Yuan, Z Zhang, C Li…</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="1672665553767281734">
  <data key="d0">End-to-end object detection with transformers</data>
  <data key="d1">1672665553767281734</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-58452-8_13</data>
  <data key="d3">End-to-end object detection with transformers</data>
  <data key="d4">N Carion, F Massa, G Synnaeve, N Usunier…</data>
  <data key="d5">2020</data>
  <data key="d6">7987</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1672665553767281734&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="2325917221075842848">
  <data key="d0">Flamingo: a visual language model for few-shot learning</data>
  <data key="d1">2325917221075842848</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html</data>
  <data key="d3">Flamingo: a visual language model for few-shot learning</data>
  <data key="d4">JB Alayrac, J Donahue, P Luc…</data>
  <data key="d5">2022</data>
  <data key="d6">807</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2325917221075842848&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="11165298458048562314">
  <data key="d0">SegFormer: Simple and efficient design for semantic segmentation with transformers</data>
  <data key="d1">11165298458048562314</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/64f1f27bf1b4ec22924fd0acb550c235-Abstract.html</data>
  <data key="d3">SegFormer: Simple and efficient design for semantic segmentation with transformers</data>
  <data key="d4">E Xie, W Wang, Z Yu, A Anandkumar…</data>
  <data key="d5">2021</data>
  <data key="d6">1809</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11165298458048562314&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="15816136068893942524">
  <data key="d0">Swinir: Image restoration using swin transformer</data>
  <data key="d1">15816136068893942524</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html</data>
  <data key="d3">Swinir: Image restoration using swin transformer</data>
  <data key="d4">J Liang, J Cao, G Sun, K Zhang…</data>
  <data key="d5">2021</data>
  <data key="d6">1295</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15816136068893942524&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="3458396398389387877">
  <data key="d0">Swin transformer: Hierarchical vision transformer using shifted windows</data>
  <data key="d1">3458396398389387877</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper</data>
  <data key="d3">Swin transformer: Hierarchical vision transformer using shifted windows</data>
  <data key="d4">Z Liu, Y Lin, Y Cao, H Hu, Y Wei…</data>
  <data key="d5">2021</data>
  <data key="d6">10712</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3458396398389387877&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="7329647594369932315">
  <data key="d0">Multiscale vision transformers</data>
  <data key="d1">7329647594369932315</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Fan_Multiscale_Vision_Transformers_ICCV_2021_paper.html</data>
  <data key="d3">Multiscale vision transformers</data>
  <data key="d4">H Fan, B Xiong, K Mangalam, Y Li…</data>
  <data key="d5">2021</data>
  <data key="d6">1670</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7329647594369932315&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="16235705232339507184">
  <data key="d0">Training data-efficient image transformers &amp; distillation through attention</data>
  <data key="d1">16235705232339507184</data>
  <data key="d2">https://proceedings.mlr.press/v139/touvron21a</data>
  <data key="d3">Training data-efficient image transformers &amp; distillation through attention</data>
  <data key="d4">H Touvron, M Cord, M Douze, F Massa…</data>
  <data key="d5">2021</data>
  <data key="d6">4101</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16235705232339507184&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="2013933719074368496">
  <data key="d0">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</data>
  <data key="d1">2013933719074368496</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.html</data>
  <data key="d3">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</data>
  <data key="d4">S Zheng, J Lu, H Zhao, X Zhu, Z Luo…</data>
  <data key="d5">2021</data>
  <data key="d6">2077</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2013933719074368496&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="11517447940529951525">
  <data key="d0">Cvt: Introducing convolutions to vision transformers</data>
  <data key="d1">11517447940529951525</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Wu_CvT_Introducing_Convolutions_to_Vision_Transformers_ICCV_2021_paper.html</data>
  <data key="d3">Cvt: Introducing convolutions to vision transformers</data>
  <data key="d4">H Wu, B Xiao, N Codella, M Liu, X Dai…</data>
  <data key="d5">2021</data>
  <data key="d6">1237</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11517447940529951525&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="4461602603986165987">
  <data key="d0">Swin-unet: Unet-like pure transformer for medical image segmentation</data>
  <data key="d1">4461602603986165987</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-25066-8_9</data>
  <data key="d3">Swin-unet: Unet-like pure transformer for medical image segmentation</data>
  <data key="d4">H Cao, Y Wang, J Chen, D Jiang, X Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">1219</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4461602603986165987&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="1788827408361087894">
  <data key="d0">Vivit: A video vision transformer</data>
  <data key="d1">1788827408361087894</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021/html/Arnab_ViViT_A_Video_Vision_Transformer_ICCV_2021_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">Vivit: A video vision transformer</data>
  <data key="d4">A Arnab, M Dehghani, G Heigold…</data>
  <data key="d5">2021</data>
  <data key="d6">1222</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1788827408361087894&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="3844087685498854388">
  <data key="d0">Tokens-to-token vit: Training vision transformers from scratch on imagenet</data>
  <data key="d1">3844087685498854388</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021/html/Yuan_Tokens-to-Token_ViT_Training_Vision_Transformers_From_Scratch_on_ImageNet_ICCV_2021_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">Tokens-to-token vit: Training vision transformers from scratch on imagenet</data>
  <data key="d4">L Yuan, Y Chen, T Wang, W Yu, Y Shi…</data>
  <data key="d5">2021</data>
  <data key="d6">1397</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3844087685498854388&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="10375739191012965737">
  <data key="d0">Masked-attention mask transformer for universal image segmentation</data>
  <data key="d1">10375739191012965737</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.html</data>
  <data key="d3">Masked-attention mask transformer for universal image segmentation</data>
  <data key="d4">B Cheng, I Misra, AG Schwing…</data>
  <data key="d5">2022</data>
  <data key="d6">652</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10375739191012965737&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="17465500328468068642">
  <data key="d0">Segmenter: Transformer for semantic segmentation</data>
  <data key="d1">17465500328468068642</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Strudel_Segmenter_Transformer_for_Semantic_Segmentation_ICCV_2021_paper.html</data>
  <data key="d3">Segmenter: Transformer for semantic segmentation</data>
  <data key="d4">R Strudel, R Garcia, I Laptev…</data>
  <data key="d5">2021</data>
  <data key="d6">886</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17465500328468068642&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="7911999856845003856">
  <data key="d0">Deformable detr: Deformable transformers for end-to-end object detection</data>
  <data key="d1">7911999856845003856</data>
  <data key="d2">https://arxiv.org/abs/2010.04159</data>
  <data key="d3">Deformable detr: Deformable transformers for end-to-end object detection</data>
  <data key="d4">X Zhu, W Su, L Lu, B Li, X Wang, J Dai</data>
  <data key="d5">2020</data>
  <data key="d6">2795</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7911999856845003856&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="8508636578765152299">
  <data key="d0">Per-pixel classification is not all you need for semantic segmentation</data>
  <data key="d1">8508636578765152299</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/950a4152c2b4aa3ad78bdd6b366cc179-Abstract.html</data>
  <data key="d3">Per-pixel classification is not all you need for semantic segmentation</data>
  <data key="d4">B Cheng, A Schwing, A Kirillov</data>
  <data key="d5">2021</data>
  <data key="d6">666</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8508636578765152299&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="6828425192739736056">
  <data key="d0">Is space-time attention all you need for video understanding?</data>
  <data key="d1">6828425192739736056</data>
  <data key="d2">http://proceedings.mlr.press/v139/bertasius21a/bertasius21a-supp.pdf</data>
  <data key="d3">Is space-time attention all you need for video understanding?</data>
  <data key="d4">G Bertasius, H Wang, L Torresani</data>
  <data key="d5">2021</data>
  <data key="d6">1190</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6828425192739736056&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="1018521690946850362">
  <data key="d0">Do vision transformers see like convolutional neural networks?</data>
  <data key="d1">1018521690946850362</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/652cf38361a209088302ba2b8b7f51e0-Abstract.html</data>
  <data key="d3">Do vision transformers see like convolutional neural networks?</data>
  <data key="d4">M Raghu, T Unterthiner, S Kornblith…</data>
  <data key="d5">2021</data>
  <data key="d6">535</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1018521690946850362&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="15154755818357511167">
  <data key="d0">Transformer in transformer</data>
  <data key="d1">15154755818357511167</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/854d9fca60b4bd07f9bb215d59ef5561-Abstract.html</data>
  <data key="d3">Transformer in transformer</data>
  <data key="d4">K Han, A Xiao, E Wu, J Guo, C Xu…</data>
  <data key="d5">2021</data>
  <data key="d6">931</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15154755818357511167&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="7704492432415173786">
  <data key="d0">Decision transformer: Reinforcement learning via sequence modeling</data>
  <data key="d1">7704492432415173786</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/7f489f642a0ddb10272b5c31057f0663-Abstract.html</data>
  <data key="d3">Decision transformer: Reinforcement learning via sequence modeling</data>
  <data key="d4">L Chen, K Lu, A Rajeswaran, K Lee…</data>
  <data key="d5">2021</data>
  <data key="d6">718</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7704492432415173786&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="13209348926291080860">
  <data key="d0">Unsupervised learning of visual features by contrasting cluster assignments</data>
  <data key="d1">13209348926291080860</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2020/hash/70feb62b69f16e0238f741fab228fec2-Abstract.html</data>
  <data key="d3">Unsupervised learning of visual features by contrasting cluster assignments</data>
  <data key="d4">M Caron, I Misra, J Mairal, P Goyal…</data>
  <data key="d5">2020</data>
  <data key="d6">2695</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13209348926291080860&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="5512802662340022027">
  <data key="d0">Pre-trained image processing transformer</data>
  <data key="d1">5512802662340022027</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Chen_Pre-Trained_Image_Processing_Transformer_CVPR_2021_paper.html</data>
  <data key="d3">Pre-trained image processing transformer</data>
  <data key="d4">H Chen, Y Wang, T Guo, C Xu…</data>
  <data key="d5">2021</data>
  <data key="d6">1152</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5512802662340022027&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="14421942083121350206">
  <data key="d0">Visual prompt tuning</data>
  <data key="d1">14421942083121350206</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19827-4_41</data>
  <data key="d3">Visual prompt tuning</data>
  <data key="d4">M Jia, L Tang, BC Chen, C Cardie, S Belongie…</data>
  <data key="d5">2022</data>
  <data key="d6">405</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14421942083121350206&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="13501013621324561884">
  <data key="d0">Scaling vision transformers</data>
  <data key="d1">13501013621324561884</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhai_Scaling_Vision_Transformers_CVPR_2022_paper.html</data>
  <data key="d3">Scaling vision transformers</data>
  <data key="d4">X Zhai, A Kolesnikov, N Houlsby…</data>
  <data key="d5">2022</data>
  <data key="d6">590</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13501013621324561884&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="13810965122904729211">
  <data key="d0">Going deeper with image transformers</data>
  <data key="d1">13810965122904729211</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Touvron_Going_Deeper_With_Image_Transformers_ICCV_2021_paper.html</data>
  <data key="d3">Going deeper with image transformers</data>
  <data key="d4">H Touvron, M Cord, A Sablayrolles…</data>
  <data key="d5">2021</data>
  <data key="d6">689</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13810965122904729211&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="5060121065165184210">
  <data key="d0">Twins: Revisiting the design of spatial attention in vision transformers</data>
  <data key="d1">5060121065165184210</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/4e0928de075538c593fbdabb0c5ef2c3-Abstract.html</data>
  <data key="d3">Twins: Revisiting the design of spatial attention in vision transformers</data>
  <data key="d4">X Chu, Z Tian, Y Wang, B Zhang…</data>
  <data key="d5">2021</data>
  <data key="d6">592</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5060121065165184210&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="15783325521916683415">
  <data key="d0">Bottleneck transformers for visual recognition</data>
  <data key="d1">15783325521916683415</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Srinivas_Bottleneck_Transformers_for_Visual_Recognition_CVPR_2021_paper.html</data>
  <data key="d3">Bottleneck transformers for visual recognition</data>
  <data key="d4">A Srinivas, TY Lin, N Parmar, J Shlens…</data>
  <data key="d5">2021</data>
  <data key="d6">795</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15783325521916683415&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="10722665686456251008">
  <data key="d0">Transformer tracking</data>
  <data key="d1">10722665686456251008</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Chen_Transformer_Tracking_CVPR_2021_paper.html</data>
  <data key="d3">Transformer tracking</data>
  <data key="d4">X Chen, B Yan, J Zhu, D Wang…</data>
  <data key="d5">2021</data>
  <data key="d6">677</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10722665686456251008&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="8140812159859442226">
  <data key="d0">Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training</data>
  <data key="d1">8140812159859442226</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/416f9cb3276121c42eebb86352a4354a-Abstract-Conference.html</data>
  <data key="d3">Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training</data>
  <data key="d4">Z Tong, Y Song, J Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">321</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8140812159859442226&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="4072300244718161964">
  <data key="d0">Unetr: Transformers for 3d medical image segmentation</data>
  <data key="d1">4072300244718161964</data>
  <data key="d2">http://openaccess.thecvf.com/content/WACV2022/html/Hatamizadeh_UNETR_Transformers_for_3D_Medical_Image_Segmentation_WACV_2022_paper.html</data>
  <data key="d3">Unetr: Transformers for 3d medical image segmentation</data>
  <data key="d4">A Hatamizadeh, Y Tang, V Nath…</data>
  <data key="d5">2022</data>
  <data key="d6">756</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4072300244718161964&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="5447961710166142858">
  <data key="d0">Bevformer: Learning bird's-eye-view representation from multi-camera images via spatiotemporal transformers</data>
  <data key="d1">5447961710166142858</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20077-9_1</data>
  <data key="d3">Bevformer: Learning bird's-eye-view representation from multi-camera images via spatiotemporal transformers</data>
  <data key="d4">Z Li, W Wang, H Li, E Xie, C Sima, T Lu, Y Qiao…</data>
  <data key="d5">2022</data>
  <data key="d6">347</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5447961710166142858&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="12908708535020454138">
  <data key="d0">Pct: Point cloud transformer</data>
  <data key="d1">12908708535020454138</data>
  <data key="d2">https://link.springer.com/article/10.1007/s41095-021-0229-5</data>
  <data key="d3">Pct: Point cloud transformer</data>
  <data key="d4">MH Guo, JX Cai, ZN Liu, TJ Mu, RR Martin…</data>
  <data key="d5">2021</data>
  <data key="d6">867</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12908708535020454138&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="817698272872287436">
  <data key="d0">Convit: Improving vision transformers with soft convolutional inductive biases</data>
  <data key="d1">817698272872287436</data>
  <data key="d2">https://proceedings.mlr.press/v139/d-ascoli21a</data>
  <data key="d3">Convit: Improving vision transformers with soft convolutional inductive biases</data>
  <data key="d4">S d'Ascoli, H Touvron, ML Leavitt…</data>
  <data key="d5">2021</data>
  <data key="d6">533</data>
  <data key="d7">https://scholar.google.com/scholar?cites=817698272872287436&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="8704237515510088771">
  <data key="d0">Perceiver: General perception with iterative attention</data>
  <data key="d1">8704237515510088771</data>
  <data key="d2">http://proceedings.mlr.press/v139/jaegle21a.html</data>
  <data key="d3">Perceiver: General perception with iterative attention</data>
  <data key="d4">A Jaegle, F Gimeno, A Brock…</data>
  <data key="d5">2021</data>
  <data key="d6">531</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8704237515510088771&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="4431453089685809340">
  <data key="d0">Cswin transformer: A general vision transformer backbone with cross-shaped windows</data>
  <data key="d1">4431453089685809340</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Dong_CSWin_Transformer_A_General_Vision_Transformer_Backbone_With_Cross-Shaped_Windows_CVPR_2022_paper.html</data>
  <data key="d3">Cswin transformer: A general vision transformer backbone with cross-shaped windows</data>
  <data key="d4">X Dong, J Bao, D Chen, W Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">512</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4431453089685809340&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="2104521862399993019">
  <data key="d0">Sparse r-cnn: End-to-end object detection with learnable proposals</data>
  <data key="d1">2104521862399993019</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Sun_Sparse_R-CNN_End-to-End_Object_Detection_With_Learnable_Proposals_CVPR_2021_paper.html</data>
  <data key="d3">Sparse r-cnn: End-to-end object detection with learnable proposals</data>
  <data key="d4">P Sun, R Zhang, Y Jiang, T Kong…</data>
  <data key="d5">2021</data>
  <data key="d6">730</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2104521862399993019&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="14911027486580949315">
  <data key="d0">Conversational agents in therapeutic interventions for neurodevelopmental disorders: a survey</data>
  <data key="d1">14911027486580949315</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3564269</data>
  <data key="d3">Conversational agents in therapeutic interventions for neurodevelopmental disorders: a survey</data>
  <data key="d4">F Catania, M Spitale, F Garzotto</data>
  <data key="d5">2023</data>
  <data key="d6">798</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14911027486580949315&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="4278610892084589339">
  <data key="d0">A survey on vision transformer</data>
  <data key="d1">4278610892084589339</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9716741/</data>
  <data key="d3">A survey on vision transformer</data>
  <data key="d4">K Han, Y Wang, H Chen, X Chen, J Guo…</data>
  <data key="d5">2022</data>
  <data key="d6">724</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4278610892084589339&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="3868256673952367025">
  <data key="d0">LoFTR: Detector-free local feature matching with transformers</data>
  <data key="d1">3868256673952367025</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Sun_LoFTR_Detector-Free_Local_Feature_Matching_With_Transformers_CVPR_2021_paper.html</data>
  <data key="d3">LoFTR: Detector-free local feature matching with transformers</data>
  <data key="d4">J Sun, Z Shen, Y Wang, H Bao…</data>
  <data key="d5">2021</data>
  <data key="d6">528</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3868256673952367025&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="915917310659134030">
  <data key="d0">Mdetr-modulated detection for end-to-end multi-modal understanding</data>
  <data key="d1">915917310659134030</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Kamath_MDETR_-_Modulated_Detection_for_End-to-End_Multi-Modal_Understanding_ICCV_2021_paper.html</data>
  <data key="d3">Mdetr-modulated detection for end-to-end multi-modal understanding</data>
  <data key="d4">A Kamath, M Singh, Y LeCun…</data>
  <data key="d5">2021</data>
  <data key="d6">462</data>
  <data key="d7">https://scholar.google.com/scholar?cites=915917310659134030&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="6004268348151288098">
  <data key="d0">Grounded language-image pre-training</data>
  <data key="d1">6004268348151288098</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2022/html/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.html?ref=blog.roboflow.com</data>
  <data key="d3">Grounded language-image pre-training</data>
  <data key="d4">LH Li, P Zhang, H Zhang, J Yang, C Li…</data>
  <data key="d5">2022</data>
  <data key="d6">332</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6004268348151288098&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="7327595990658945420">
  <data key="d0">Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text</data>
  <data key="d1">7327595990658945420</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/cb3213ada48302953cb0f166464ab356-Abstract.html</data>
  <data key="d3">Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text</data>
  <data key="d4">H Akbari, L Yuan, R Qian…</data>
  <data key="d5">2021</data>
  <data key="d6">412</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7327595990658945420&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="7623662567011711547">
  <data key="d0">Metaformer is actually what you need for vision</data>
  <data key="d1">7623662567011711547</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Yu_MetaFormer_Is_Actually_What_You_Need_for_Vision_CVPR_2022_paper.html</data>
  <data key="d3">Metaformer is actually what you need for vision</data>
  <data key="d4">W Yu, M Luo, P Zhou, C Si, Y Zhou…</data>
  <data key="d5">2022</data>
  <data key="d6">371</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7623662567011711547&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="10424310778620231784">
  <data key="d0">End-to-end video instance segmentation with transformers</data>
  <data key="d1">10424310778620231784</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Wang_End-to-End_Video_Instance_Segmentation_With_Transformers_CVPR_2021_paper.html</data>
  <data key="d3">End-to-end video instance segmentation with transformers</data>
  <data key="d4">Y Wang, Z Xu, X Wang, C Shen…</data>
  <data key="d5">2021</data>
  <data key="d6">584</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10424310778620231784&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="8367071398417962813">
  <data key="d0">You only look one-level feature</data>
  <data key="d1">8367071398417962813</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Chen_You_Only_Look_One-Level_Feature_CVPR_2021_paper.html</data>
  <data key="d3">You only look one-level feature</data>
  <data key="d4">Q Chen, Y Wang, T Yang, X Zhang…</data>
  <data key="d5">2021</data>
  <data key="d6">437</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8367071398417962813&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="5445650339708387168">
  <data key="d0">An end-to-end transformer model for 3d object detection</data>
  <data key="d1">5445650339708387168</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021/html/Misra_An_End-to-End_Transformer_Model_for_3D_Object_Detection_ICCV_2021_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">An end-to-end transformer model for 3d object detection</data>
  <data key="d4">I Misra, R Girdhar, A Joulin</data>
  <data key="d5">2021</data>
  <data key="d6">302</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5445650339708387168&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="8639836755629224484">
  <data key="d0">Pay attention to mlps</data>
  <data key="d1">8639836755629224484</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/4cc05b35c2f937c5bd9e7d41d3686fff-Abstract.html</data>
  <data key="d3">Pay attention to mlps</data>
  <data key="d4">H Liu, Z Dai, D So, QV Le</data>
  <data key="d5">2021</data>
  <data key="d6">334</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8639836755629224484&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="1440121271646678581">
  <data key="d0">Flava: A foundational language and vision alignment model</data>
  <data key="d1">1440121271646678581</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.html</data>
  <data key="d3">Flava: A foundational language and vision alignment model</data>
  <data key="d4">A Singh, R Hu, V Goswami…</data>
  <data key="d5">2022</data>
  <data key="d6">268</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1440121271646678581&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="3380151090492446607">
  <data key="d0">Rethinking spatial dimensions of vision transformers</data>
  <data key="d1">3380151090492446607</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Heo_Rethinking_Spatial_Dimensions_of_Vision_Transformers_ICCV_2021_paper.html</data>
  <data key="d3">Rethinking spatial dimensions of vision transformers</data>
  <data key="d4">B Heo, S Yun, D Han, S Chun…</data>
  <data key="d5">2021</data>
  <data key="d6">399</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3380151090492446607&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="17454749727813031882">
  <data key="d0">Learning spatio-temporal transformer for visual tracking</data>
  <data key="d1">17454749727813031882</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Yan_Learning_Spatio-Temporal_Transformer_for_Visual_Tracking_ICCV_2021_paper.html</data>
  <data key="d3">Learning spatio-temporal transformer for visual tracking</data>
  <data key="d4">B Yan, H Peng, J Fu, D Wang…</data>
  <data key="d5">2021</data>
  <data key="d6">427</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17454749727813031882&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="11808828478262249051">
  <data key="d0">Frozen in time: A joint video and image encoder for end-to-end retrieval</data>
  <data key="d1">11808828478262249051</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Bain_Frozen_in_Time_A_Joint_Video_and_Image_Encoder_for_ICCV_2021_paper.html</data>
  <data key="d3">Frozen in time: A joint video and image encoder for end-to-end retrieval</data>
  <data key="d4">M Bain, A Nagrani, G Varol…</data>
  <data key="d5">2021</data>
  <data key="d6">415</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11808828478262249051&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="11460907858590036405">
  <data key="d0">Trackformer: Multi-object tracking with transformers</data>
  <data key="d1">11460907858590036405</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Meinhardt_TrackFormer_Multi-Object_Tracking_With_Transformers_CVPR_2022_paper.html</data>
  <data key="d3">Trackformer: Multi-object tracking with transformers</data>
  <data key="d4">T Meinhardt, A Kirillov, L Leal-Taixe…</data>
  <data key="d5">2022</data>
  <data key="d6">467</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11460907858590036405&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="610621467807251926">
  <data key="d0">Inception transformer</data>
  <data key="d1">610621467807251926</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/94e85561a342de88b559b72c9b29f638-Abstract-Conference.html</data>
  <data key="d3">Inception transformer</data>
  <data key="d4">C Si, W Yu, P Zhou, Y Zhou…</data>
  <data key="d5">2022</data>
  <data key="d6">153</data>
  <data key="d7">https://scholar.google.com/scholar?cites=610621467807251926&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="9233219793813194550">
  <data key="d0">Transformer meets tracker: Exploiting temporal context for robust visual tracking</data>
  <data key="d1">9233219793813194550</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Wang_Transformer_Meets_Tracker_Exploiting_Temporal_Context_for_Robust_Visual_Tracking_CVPR_2021_paper.html</data>
  <data key="d3">Transformer meets tracker: Exploiting temporal context for robust visual tracking</data>
  <data key="d4">N Wang, W Zhou, J Wang, H Li</data>
  <data key="d5">2021</data>
  <data key="d6">405</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9233219793813194550&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="17780356579077510129">
  <data key="d0">Ego4d: Around the world in 3,000 hours of egocentric video</data>
  <data key="d1">17780356579077510129</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Grauman_Ego4D_Around_the_World_in_3000_Hours_of_Egocentric_Video_CVPR_2022_paper.html</data>
  <data key="d3">Ego4d: Around the world in 3,000 hours of egocentric video</data>
  <data key="d4">K Grauman, A Westbury, E Byrne…</data>
  <data key="d5">2022</data>
  <data key="d6">301</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17780356579077510129&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="18216985783540724512">
  <data key="d0">Cmt: Convolutional neural networks meet vision transformers</data>
  <data key="d1">18216985783540724512</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Guo_CMT_Convolutional_Neural_Networks_Meet_Vision_Transformers_CVPR_2022_paper.html</data>
  <data key="d3">Cmt: Convolutional neural networks meet vision transformers</data>
  <data key="d4">J Guo, K Han, H Wu, Y Tang, X Chen…</data>
  <data key="d5">2022</data>
  <data key="d6">344</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18216985783540724512&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="10842331006678867208">
  <data key="d0">Conditional detr for fast training convergence</data>
  <data key="d1">10842331006678867208</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Meng_Conditional_DETR_for_Fast_Training_Convergence_ICCV_2021_paper.html</data>
  <data key="d3">Conditional detr for fast training convergence</data>
  <data key="d4">D Meng, X Chen, Z Fan, G Zeng, H Li…</data>
  <data key="d5">2021</data>
  <data key="d6">299</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10842331006678867208&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="3320528631702187603">
  <data key="d0">Conformer: Local features coupling global representations for visual recognition</data>
  <data key="d1">3320528631702187603</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Peng_Conformer_Local_Features_Coupling_Global_Representations_for_Visual_Recognition_ICCV_2021_paper.html</data>
  <data key="d3">Conformer: Local features coupling global representations for visual recognition</data>
  <data key="d4">Z Peng, W Huang, S Gu, L Xie…</data>
  <data key="d5">2021</data>
  <data key="d6">356</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3320528631702187603&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="4773463079530656035">
  <data key="d0">Visual attention network</data>
  <data key="d1">4773463079530656035</data>
  <data key="d2">https://link.springer.com/article/10.1007/s41095-023-0364-2</data>
  <data key="d3">Visual attention network</data>
  <data key="d4">MH Guo, CZ Lu, ZN Liu, MM Cheng, SM Hu</data>
  <data key="d5">2023</data>
  <data key="d6">252</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4773463079530656035&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="3799744009906269739">
  <data key="d0">Petr: Position embedding transformation for multi-view 3d object detection</data>
  <data key="d1">3799744009906269739</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19812-0_31</data>
  <data key="d3">Petr: Position embedding transformation for multi-view 3d object detection</data>
  <data key="d4">Y Liu, T Wang, X Zhang, J Sun</data>
  <data key="d5">2022</data>
  <data key="d6">183</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3799744009906269739&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="11112701312974631954">
  <data key="d0">Adabins: Depth estimation using adaptive bins</data>
  <data key="d1">11112701312974631954</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Bhat_AdaBins_Depth_Estimation_Using_Adaptive_Bins_CVPR_2021_paper.html</data>
  <data key="d3">Adabins: Depth estimation using adaptive bins</data>
  <data key="d4">SF Bhat, I Alhashim, P Wonka</data>
  <data key="d5">2021</data>
  <data key="d6">483</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11112701312974631954&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="12911220675230413174">
  <data key="d0">Transfusion: Robust lidar-camera fusion for 3d object detection with transformers</data>
  <data key="d1">12911220675230413174</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Bai_TransFusion_Robust_LiDAR-Camera_Fusion_for_3D_Object_Detection_With_Transformers_CVPR_2022_paper.html</data>
  <data key="d3">Transfusion: Robust lidar-camera fusion for 3d object detection with transformers</data>
  <data key="d4">X Bai, Z Hu, X Zhu, Q Huang, Y Chen…</data>
  <data key="d5">2022</data>
  <data key="d6">228</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12911220675230413174&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="14185047449981394536">
  <data key="d0">Dynamicvit: Efficient vision transformers with dynamic token sparsification</data>
  <data key="d1">14185047449981394536</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/747d3443e319a22747fbb873e8b2f9f2-Abstract.html</data>
  <data key="d3">Dynamicvit: Efficient vision transformers with dynamic token sparsification</data>
  <data key="d4">Y Rao, W Zhao, B Liu, J Lu, J Zhou…</data>
  <data key="d5">2021</data>
  <data key="d6">316</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14185047449981394536&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="9393703958705125224">
  <data key="d0">Incorporating convolution designs into visual transformers</data>
  <data key="d1">9393703958705125224</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Yuan_Incorporating_Convolution_Designs_Into_Visual_Transformers_ICCV_2021_paper.html</data>
  <data key="d3">Incorporating convolution designs into visual transformers</data>
  <data key="d4">K Yuan, S Guo, Z Liu, A Zhou…</data>
  <data key="d5">2021</data>
  <data key="d6">354</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9393703958705125224&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="14030709993426908081">
  <data key="d0">Bevdepth: Acquisition of reliable depth for multi-view 3d object detection</data>
  <data key="d1">14030709993426908081</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/25233</data>
  <data key="d3">Bevdepth: Acquisition of reliable depth for multi-view 3d object detection</data>
  <data key="d4">Y Li, Z Ge, G Yu, J Yang, Z Wang, Y Shi…</data>
  <data key="d5">2023</data>
  <data key="d6">162</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14030709993426908081&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="12956114412627599893">
  <data key="d0">End-to-end human pose and mesh reconstruction with transformers</data>
  <data key="d1">12956114412627599893</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Lin_End-to-End_Human_Pose_and_Mesh_Reconstruction_with_Transformers_CVPR_2021_paper.html</data>
  <data key="d3">End-to-end human pose and mesh reconstruction with transformers</data>
  <data key="d4">K Lin, L Wang, Z Liu</data>
  <data key="d5">2021</data>
  <data key="d6">434</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12956114412627599893&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="17712252571307454824">
  <data key="d0">Clip-adapter: Better vision-language models with feature adapters</data>
  <data key="d1">17712252571307454824</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11263-023-01891-x</data>
  <data key="d3">Clip-adapter: Better vision-language models with feature adapters</data>
  <data key="d4">P Gao, S Geng, R Zhang, T Ma, R Fang…</data>
  <data key="d5">2023</data>
  <data key="d6">265</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17712252571307454824&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="9646853108847192407">
  <data key="d0">Dn-detr: Accelerate detr training by introducing query denoising</data>
  <data key="d1">9646853108847192407</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Li_DN-DETR_Accelerate_DETR_Training_by_Introducing_Query_DeNoising_CVPR_2022_paper.html</data>
  <data key="d3">Dn-detr: Accelerate detr training by introducing query denoising</data>
  <data key="d4">F Li, H Zhang, S Liu, J Guo, LM Ni…</data>
  <data key="d5">2022</data>
  <data key="d6">227</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9646853108847192407&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="5792560058636996973">
  <data key="d0">Max-deeplab: End-to-end panoptic segmentation with mask transformers</data>
  <data key="d1">5792560058636996973</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Wang_MaX-DeepLab_End-to-End_Panoptic_Segmentation_With_Mask_Transformers_CVPR_2021_paper.html</data>
  <data key="d3">Max-deeplab: End-to-end panoptic segmentation with mask transformers</data>
  <data key="d4">H Wang, Y Zhu, H Adam, A Yuille…</data>
  <data key="d5">2021</data>
  <data key="d6">399</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5792560058636996973&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="7843478121381660661">
  <data key="d0">Voxel transformer for 3d object detection</data>
  <data key="d1">7843478121381660661</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Mao_Voxel_Transformer_for_3D_Object_Detection_ICCV_2021_paper.html</data>
  <data key="d3">Voxel transformer for 3d object detection</data>
  <data key="d4">J Mao, Y Xue, M Niu, H Bai, J Feng…</data>
  <data key="d5">2021</data>
  <data key="d6">252</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7843478121381660661&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="8730617665338917129">
  <data key="d0">Transformer interpretability beyond attention visualization</data>
  <data key="d1">8730617665338917129</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.html</data>
  <data key="d3">Transformer interpretability beyond attention visualization</data>
  <data key="d4">H Chefer, S Gur, L Wolf</data>
  <data key="d5">2021</data>
  <data key="d6">393</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8730617665338917129&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="10828507269261066901">
  <data key="d0">Up-detr: Unsupervised pre-training for object detection with transformers</data>
  <data key="d1">10828507269261066901</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Dai_UP-DETR_Unsupervised_Pre-Training_for_Object_Detection_With_Transformers_CVPR_2021_paper.html</data>
  <data key="d3">Up-detr: Unsupervised pre-training for object detection with transformers</data>
  <data key="d4">Z Dai, B Cai, Y Lin, J Chen</data>
  <data key="d5">2021</data>
  <data key="d6">393</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10828507269261066901&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="1986765630077984342">
  <data key="d0">Detr3d: 3d object detection from multi-view images via 3d-to-2d queries</data>
  <data key="d1">1986765630077984342</data>
  <data key="d2">https://proceedings.mlr.press/v164/wang22b.html</data>
  <data key="d3">Detr3d: 3d object detection from multi-view images via 3d-to-2d queries</data>
  <data key="d4">Y Wang, VC Guizilini, T Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">286</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1986765630077984342&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="1319826694668830613">
  <data key="d0">Dynamic head: Unifying object detection heads with attentions</data>
  <data key="d1">1319826694668830613</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Dai_Dynamic_Head_Unifying_Object_Detection_Heads_With_Attentions_CVPR_2021_paper.html</data>
  <data key="d3">Dynamic head: Unifying object detection heads with attentions</data>
  <data key="d4">X Dai, Y Chen, B Xiao, D Chen, M Liu…</data>
  <data key="d5">2021</data>
  <data key="d6">282</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1319826694668830613&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4416527254888837916">
  <data key="d0">Localvit: Bringing locality to vision transformers</data>
  <data key="d1">4416527254888837916</data>
  <data key="d2">https://arxiv.org/abs/2104.05707</data>
  <data key="d3">Localvit: Bringing locality to vision transformers</data>
  <data key="d4">Y Li, K Zhang, J Cao, R Timofte, L Van Gool</data>
  <data key="d5">2021</data>
  <data key="d6">347</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4416527254888837916&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="12372357504406827550">
  <data key="d0">Object-centric learning with slot attention</data>
  <data key="d1">12372357504406827550</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2020/hash/8511df98c02ab60aea1b2356c013bc0f-Abstract.html</data>
  <data key="d3">Object-centric learning with slot attention</data>
  <data key="d4">F Locatello, D Weissenborn…</data>
  <data key="d5">2020</data>
  <data key="d6">483</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12372357504406827550&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="14889255110023347971">
  <data key="d0">You only learn one representation: Unified network for multiple tasks</data>
  <data key="d1">14889255110023347971</data>
  <data key="d2">https://arxiv.org/abs/2105.04206</data>
  <data key="d3">You only learn one representation: Unified network for multiple tasks</data>
  <data key="d4">CY Wang, IH Yeh, HYM Liao</data>
  <data key="d5">2021</data>
  <data key="d6">377</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14889255110023347971&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="8562591691593838404">
  <data key="d0">Deepvit: Towards deeper vision transformer</data>
  <data key="d1">8562591691593838404</data>
  <data key="d2">https://arxiv.org/abs/2103.11886</data>
  <data key="d3">Deepvit: Towards deeper vision transformer</data>
  <data key="d4">D Zhou, B Kang, X Jin, L Yang, X Lian, Z Jiang…</data>
  <data key="d5">2021</data>
  <data key="d6">383</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8562591691593838404&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="17870066505440679476">
  <data key="d0">Conditional positional encodings for vision transformers</data>
  <data key="d1">17870066505440679476</data>
  <data key="d2">https://arxiv.org/abs/2102.10882</data>
  <data key="d3">Conditional positional encodings for vision transformers</data>
  <data key="d4">X Chu, Z Tian, B Zhang, X Wang, X Wei, H Xia…</data>
  <data key="d5">2021</data>
  <data key="d6">380</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17870066505440679476&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="14766365221923861675">
  <data key="d0">Mobile-former: Bridging mobilenet and transformer</data>
  <data key="d1">14766365221923861675</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Chen_Mobile-Former_Bridging_MobileNet_and_Transformer_CVPR_2022_paper.html</data>
  <data key="d3">Mobile-former: Bridging mobilenet and transformer</data>
  <data key="d4">Y Chen, X Dai, D Chen, M Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">262</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14766365221923861675&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="966567457136989804">
  <data key="d0">Pre-trained models: Past, present and future</data>
  <data key="d1">966567457136989804</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2666651021000231</data>
  <data key="d3">Pre-trained models: Past, present and future</data>
  <data key="d4">X Han, Z Zhang, N Ding, Y Gu, X Liu, Y Huo, J Qiu…</data>
  <data key="d5">2021</data>
  <data key="d6">369</data>
  <data key="d7">https://scholar.google.com/scholar?cites=966567457136989804&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="14768862339001694574">
  <data key="d0">Video transformer network</data>
  <data key="d1">14768862339001694574</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021W/CVEU/html/Neimark_Video_Transformer_Network_ICCVW_2021_paper.html</data>
  <data key="d3">Video transformer network</data>
  <data key="d4">D Neimark, O Bar, M Zohar…</data>
  <data key="d5">2021</data>
  <data key="d6">343</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14768862339001694574&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="3435520687143059291">
  <data key="d0">Remote sensing image change detection with transformers</data>
  <data key="d1">3435520687143059291</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9491802/</data>
  <data key="d3">Remote sensing image change detection with transformers</data>
  <data key="d4">H Chen, Z Qi, Z Shi</data>
  <data key="d5">2021</data>
  <data key="d6">368</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3435520687143059291&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="12563424232401784595">
  <data key="d0">Scaled-yolov4: Scaling cross stage partial network</data>
  <data key="d1">12563424232401784595</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Scaled-YOLOv4_Scaling_Cross_Stage_Partial_Network_CVPR_2021_paper.html?ref=</data>
  <data key="d3">Scaled-yolov4: Scaling cross stage partial network</data>
  <data key="d4">CY Wang, A Bochkovskiy…</data>
  <data key="d5">2021</data>
  <data key="d6">1152</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12563424232401784595&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="1747429016739752835">
  <data key="d0">Review of image classification algorithms based on convolutional neural networks</data>
  <data key="d1">1747429016739752835</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/22/4712</data>
  <data key="d3">Review of image classification algorithms based on convolutional neural networks</data>
  <data key="d4">L Chen, S Li, Q Bai, J Yang, S Jiang, Y Miao</data>
  <data key="d5">2021</data>
  <data key="d6">164</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1747429016739752835&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="8455459026871994587">
  <data key="d0">You only look at one sequence: Rethinking transformer in vision through object detection</data>
  <data key="d1">8455459026871994587</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/dc912a253d1e9ba40e2c597ed2376640-Abstract.html</data>
  <data key="d3">You only look at one sequence: Rethinking transformer in vision through object detection</data>
  <data key="d4">Y Fang, B Liao, X Wang, J Fang, J Qi…</data>
  <data key="d5">2021</data>
  <data key="d6">163</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8455459026871994587&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="1529894457156192581">
  <data key="d0">Probabilistic two-stage detection</data>
  <data key="d1">1529894457156192581</data>
  <data key="d2">https://arxiv.org/abs/2103.07461</data>
  <data key="d3">Probabilistic two-stage detection</data>
  <data key="d4">X Zhou, V Koltun, P Krähenbühl</data>
  <data key="d5">2021</data>
  <data key="d6">185</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1529894457156192581&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="3460125397015656161">
  <data key="d0">Detection and tracking meet drones challenge</data>
  <data key="d1">3460125397015656161</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9573394/</data>
  <data key="d3">Detection and tracking meet drones challenge</data>
  <data key="d4">P Zhu, L Wen, D Du, X Bian, H Fan…</data>
  <data key="d5">2021</data>
  <data key="d6">242</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3460125397015656161&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="17632693958287984620">
  <data key="d0">The 7th AI City Challenge</data>
  <data key="d1">17632693958287984620</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Naphade_The_7th_AI_City_Challenge_CVPRW_2023_paper.html</data>
  <data key="d3">The 7th AI City Challenge</data>
  <data key="d4">M Naphade, S Wang, DC Anastasiu…</data>
  <data key="d5">2023</data>
  <data key="d6">189</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17632693958287984620&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="12528683203894514853">
  <data key="d0">Non-deep networks</data>
  <data key="d1">12528683203894514853</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/2d52879ef2ba487445ca2e143b104c3b-Abstract-Conference.html</data>
  <data key="d3">Non-deep networks</data>
  <data key="d4">A Goyal, A Bochkovskiy, J Deng…</data>
  <data key="d5">2022</data>
  <data key="d6">46</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12528683203894514853&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="7857274260714324520">
  <data key="d0">Deep learning-based waste detection in natural and urban environments</data>
  <data key="d1">7857274260714324520</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0956053X21006474</data>
  <data key="d3">Deep learning-based waste detection in natural and urban environments</data>
  <data key="d4">S Majchrowska, A Mikołajczyk, M Ferlin…</data>
  <data key="d5">2022</data>
  <data key="d6">65</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7857274260714324520&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="14032891689439695129">
  <data key="d0">Yolop: You only look once for panoptic driving perception</data>
  <data key="d1">14032891689439695129</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11633-022-1339-y</data>
  <data key="d3">Yolop: You only look once for panoptic driving perception</data>
  <data key="d4">D Wu, MW Liao, WT Zhang, XG Wang, X Bai…</data>
  <data key="d5">2022</data>
  <data key="d6">122</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14032891689439695129&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="14685699400456064504">
  <data key="d0">A robust real-time deep learning based automatic polyp detection system</data>
  <data key="d1">14685699400456064504</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0010482521003139</data>
  <data key="d3">A robust real-time deep learning based automatic polyp detection system</data>
  <data key="d4">I Pacal, D Karaboga</data>
  <data key="d5">2021</data>
  <data key="d6">73</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14685699400456064504&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="1836634166646364702">
  <data key="d0">Lightweight underwater object detection based on yolo v4 and multi-scale attentional feature fusion</data>
  <data key="d1">1836634166646364702</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/22/4706</data>
  <data key="d3">Lightweight underwater object detection based on yolo v4 and multi-scale attentional feature fusion</data>
  <data key="d4">M Zhang, S Xu, W Song, Q He, Q Wei</data>
  <data key="d5">2021</data>
  <data key="d6">69</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1836634166646364702&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="9800203968264557275">
  <data key="d0">Detection of objects in the images: From likelihood relationships towards scalable and efficient neural networks</data>
  <data key="d1">9800203968264557275</data>
  <data key="d2">https://www.computeroptics.ru/eng/KO/Annot/KO46-1/460117e.html</data>
  <data key="d3">Detection of objects in the images: From likelihood relationships towards scalable and efficient neural networks</data>
  <data key="d4">NA Andriyanov, VE Dementiev, AG Tashlinskii</data>
  <data key="d5">2022</data>
  <data key="d6">57</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9800203968264557275&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="4801397296227909937">
  <data key="d0">An efficient real-time colonic polyp detection with YOLO algorithms trained by using negative samples and large datasets</data>
  <data key="d1">4801397296227909937</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0010482521008258</data>
  <data key="d3">An efficient real-time colonic polyp detection with YOLO algorithms trained by using negative samples and large datasets</data>
  <data key="d4">I Pacal, A Karaman, D Karaboga, B Akay…</data>
  <data key="d5">2022</data>
  <data key="d6">59</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4801397296227909937&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="17733214079678161609">
  <data key="d0">Assessment of state-of-the-art deep learning based citrus disease detection techniques using annotated optical leaf images</data>
  <data key="d1">17733214079678161609</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S016816992100675X</data>
  <data key="d3">Assessment of state-of-the-art deep learning based citrus disease detection techniques using annotated optical leaf images</data>
  <data key="d4">S Dananjayan, Y Tang, J Zhuang, C Hou…</data>
  <data key="d5">2022</data>
  <data key="d6">48</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17733214079678161609&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="17275460023441273996">
  <data key="d0">A novel apple fruit detection and counting methodology based on deep learning and trunk tracking in modern orchard</data>
  <data key="d1">17275460023441273996</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169922003179</data>
  <data key="d3">A novel apple fruit detection and counting methodology based on deep learning and trunk tracking in modern orchard</data>
  <data key="d4">F Gao, W Fang, X Sun, Z Wu, G Zhao, G Li, R Li…</data>
  <data key="d5">2022</data>
  <data key="d6">42</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17275460023441273996&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="9099615620722636165">
  <data key="d0">Swin-transformer-enabled YOLOv5 with attention mechanism for small object detection on satellite images</data>
  <data key="d1">9099615620722636165</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/12/2861</data>
  <data key="d3">Swin-transformer-enabled YOLOv5 with attention mechanism for small object detection on satellite images</data>
  <data key="d4">H Gong, T Mu, Q Li, H Dai, C Li, Z He, W Wang, F Han…</data>
  <data key="d5">2022</data>
  <data key="d6">53</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9099615620722636165&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="15832601975079045141">
  <data key="d0">Slim-neck by GSConv: A better design paradigm of detector architectures for autonomous vehicles</data>
  <data key="d1">15832601975079045141</data>
  <data key="d2">https://arxiv.org/abs/2206.02424</data>
  <data key="d3">Slim-neck by GSConv: A better design paradigm of detector architectures for autonomous vehicles</data>
  <data key="d4">H Li, J Li, H Wei, Z Liu, Z Zhan, Q Ren</data>
  <data key="d5">2022</data>
  <data key="d6">70</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15832601975079045141&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="501800524296721407">
  <data key="d0">Machine learning for microcontroller-class hardware-a review</data>
  <data key="d1">501800524296721407</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9912325/</data>
  <data key="d3">Machine learning for microcontroller-class hardware-a review</data>
  <data key="d4">SS Saha, SS Sandha, M Srivastava</data>
  <data key="d5">2022</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=501800524296721407&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="13429408729973378271">
  <data key="d0">Designing network design strategies through gradient path analysis</data>
  <data key="d1">13429408729973378271</data>
  <data key="d2">https://arxiv.org/abs/2211.04800</data>
  <data key="d3">Designing network design strategies through gradient path analysis</data>
  <data key="d4">CY Wang, HYM Liao, IH Yeh</data>
  <data key="d5">2022</data>
  <data key="d6">36</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13429408729973378271&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="16749039856030657986">
  <data key="d0">Rethinking keypoint representations: Modeling keypoints and poses as objects for multi-person human pose estimation</data>
  <data key="d1">16749039856030657986</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20068-7_3</data>
  <data key="d3">Rethinking keypoint representations: Modeling keypoints and poses as objects for multi-person human pose estimation</data>
  <data key="d4">W McNally, K Vats, A Wong, J McPhee</data>
  <data key="d5">2022</data>
  <data key="d6">34</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16749039856030657986&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="11425593478470212697">
  <data key="d0">A real-time table grape detection method based on improved YOLOv4-tiny network in complex background</data>
  <data key="d1">11425593478470212697</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1537511021002786</data>
  <data key="d3">A real-time table grape detection method based on improved YOLOv4-tiny network in complex background</data>
  <data key="d4">H Li, C Li, G Li, L Chen</data>
  <data key="d5">2021</data>
  <data key="d6">47</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11425593478470212697&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="11107660116184346387">
  <data key="d0">Disentangle your dense object detector</data>
  <data key="d1">11107660116184346387</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3474085.3475351</data>
  <data key="d3">Disentangle your dense object detector</data>
  <data key="d4">Z Chen, C Yang, Q Li, F Zhao, ZJ Zha…</data>
  <data key="d5">2021</data>
  <data key="d6">56</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11107660116184346387&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="12615305976817605176">
  <data key="d0">Cross-domain object detection for autonomous driving: A stepwise domain adaptative YOLO approach</data>
  <data key="d1">12615305976817605176</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9750853/</data>
  <data key="d3">Cross-domain object detection for autonomous driving: A stepwise domain adaptative YOLO approach</data>
  <data key="d4">G Li, Z Ji, X Qu, R Zhou, D Cao</data>
  <data key="d5">2022</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12615305976817605176&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="15647727552153509563">
  <data key="d0">Application of deep learning and machine learning models to detect COVID-19 face masks-A review</data>
  <data key="d1">15647727552153509563</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2666412721000325</data>
  <data key="d3">Application of deep learning and machine learning models to detect COVID-19 face masks-A review</data>
  <data key="d4">E Mbunge, S Simelane, SG Fashoto…</data>
  <data key="d5">2021</data>
  <data key="d6">57</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15647727552153509563&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="15014850471536980328">
  <data key="d0">Towards goal-oriented semantic signal processing: Applications and future challenges</data>
  <data key="d1">15014850471536980328</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1051200421001731</data>
  <data key="d3">Towards goal-oriented semantic signal processing: Applications and future challenges</data>
  <data key="d4">M Kalfa, M Gok, A Atalik, B Tegin, TM Duman…</data>
  <data key="d5">2021</data>
  <data key="d6">40</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15014850471536980328&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="6434716222653444990">
  <data key="d0">Cat: Cross attention in vision transformer</data>
  <data key="d1">6434716222653444990</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9859720/</data>
  <data key="d3">Cat: Cross attention in vision transformer</data>
  <data key="d4">H Lin, X Cheng, X Wu, D Shen</data>
  <data key="d5">2022</data>
  <data key="d6">53</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6434716222653444990&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="17859848890141888541">
  <data key="d0">Robust multi-object tracking by marginal inference</data>
  <data key="d1">17859848890141888541</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20047-2_2</data>
  <data key="d3">Robust multi-object tracking by marginal inference</data>
  <data key="d4">Y Zhang, C Wang, X Wang, W Zeng, W Liu</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17859848890141888541&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="3847442534681122442">
  <data key="d0">Small object detection method based on adaptive spatial parallel convolution and fast multi-scale fusion</data>
  <data key="d1">3847442534681122442</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/2/420</data>
  <data key="d3">Small object detection method based on adaptive spatial parallel convolution and fast multi-scale fusion</data>
  <data key="d4">G Qi, Y Zhang, K Wang, N Mazur, Y Liu, D Malaviya</data>
  <data key="d5">2022</data>
  <data key="d6">35</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3847442534681122442&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="3175561385815325856">
  <data key="d0">Confidence score: The forgotten dimension of object detection performance evaluation</data>
  <data key="d1">3175561385815325856</data>
  <data key="d2">https://www.mdpi.com/1424-8220/21/13/4350</data>
  <data key="d3">Confidence score: The forgotten dimension of object detection performance evaluation</data>
  <data key="d4">S Wenkel, K Alhazmi, T Liiv, S Alrshoud, M Simon</data>
  <data key="d5">2021</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3175561385815325856&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="6897858680399954297">
  <data key="d0">Autonomous, onboard vision-based trash and litter detection in low altitude aerial images collected by an unmanned aerial vehicle</data>
  <data key="d1">6897858680399954297</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/5/965</data>
  <data key="d3">Autonomous, onboard vision-based trash and litter detection in low altitude aerial images collected by an unmanned aerial vehicle</data>
  <data key="d4">M Kraft, M Piechocki, B Ptak, K Walas</data>
  <data key="d5">2021</data>
  <data key="d6">48</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6897858680399954297&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="17212634534495365627">
  <data key="d0">Giaotracker: A comprehensive framework for mcmot with global information and optimizing strategies in visdrone 2021</data>
  <data key="d1">17212634534495365627</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Du_GIAOTracker_A_Comprehensive_Framework_for_MCMOT_With_Global_Information_and_ICCVW_2021_paper.html</data>
  <data key="d3">Giaotracker: A comprehensive framework for mcmot with global information and optimizing strategies in visdrone 2021</data>
  <data key="d4">Y Du, J Wan, Y Zhao, B Zhang…</data>
  <data key="d5">2021</data>
  <data key="d6">33</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17212634534495365627&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="5912943256082429583">
  <data key="d0">Generalized focal loss: Towards efficient representation learning for dense object detection</data>
  <data key="d1">5912943256082429583</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9792391/</data>
  <data key="d3">Generalized focal loss: Towards efficient representation learning for dense object detection</data>
  <data key="d4">X Li, C Lv, W Wang, G Li, L Yang…</data>
  <data key="d5">2022</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5912943256082429583&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="16607894208431157893">
  <data key="d0">CF2PN: A cross-scale feature fusion pyramid network based remote sensing target detection</data>
  <data key="d1">16607894208431157893</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/5/847</data>
  <data key="d3">CF2PN: A cross-scale feature fusion pyramid network based remote sensing target detection</data>
  <data key="d4">W Huang, G Li, Q Chen, M Ju, J Qu</data>
  <data key="d5">2021</data>
  <data key="d6">42</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16607894208431157893&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="14615147074360053768">
  <data key="d0">ES-Net: Efficient scale-aware network for tiny defect detection</data>
  <data key="d1">14615147074360053768</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9760388/</data>
  <data key="d3">ES-Net: Efficient scale-aware network for tiny defect detection</data>
  <data key="d4">X Yu, W Lyu, D Zhou, C Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">24</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14615147074360053768&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="8195426956859072339">
  <data key="d0">Tensorrt-based framework and optimization methodology for deep learning inference on jetson boards</data>
  <data key="d1">8195426956859072339</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3508391</data>
  <data key="d3">Tensorrt-based framework and optimization methodology for deep learning inference on jetson boards</data>
  <data key="d4">EJ Jeong, J Kim, S Ha</data>
  <data key="d5">2022</data>
  <data key="d6">34</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8195426956859072339&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="3657623424699616492">
  <data key="d0">K-Radar: 4D radar object detection for autonomous driving in various weather conditions</data>
  <data key="d1">3657623424699616492</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/185fdf627eaae2abab36205dcd19b817-Abstract-Datasets_and_Benchmarks.html</data>
  <data key="d3">K-Radar: 4D radar object detection for autonomous driving in various weather conditions</data>
  <data key="d4">DH Paek, SH Kong, KT Wijaya</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3657623424699616492&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="7400981380759084546">
  <data key="d0">EAutoDet: Efficient architecture search for object detection</data>
  <data key="d1">7400981380759084546</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20044-1_38</data>
  <data key="d3">EAutoDet: Efficient architecture search for object detection</data>
  <data key="d4">X Wang, J Lin, J Zhao, X Yang, J Yan</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7400981380759084546&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="3490367220831301861">
  <data key="d0">Autonomous path planning with obstacle avoidance for smart assistive systems</data>
  <data key="d1">3490367220831301861</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S095741742202067X</data>
  <data key="d3">Autonomous path planning with obstacle avoidance for smart assistive systems</data>
  <data key="d4">C Ntakolia, S Moustakidis, A Siouras</data>
  <data key="d5">2023</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3490367220831301861&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="15727602987444865635">
  <data key="d0">Hyper-parameter optimization of deep learning architectures using artificial bee colony (ABC) algorithm for high performance real-time automatic colorectal cancer …</data>
  <data key="d1">15727602987444865635</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10489-022-04299-1</data>
  <data key="d3">Hyper-parameter optimization of deep learning architectures using artificial bee colony (ABC) algorithm for high performance real-time automatic colorectal cancer …</data>
  <data key="d4">A Karaman, D Karaboga, I Pacal, B Akay, A Basturk…</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15727602987444865635&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="1805226328438105949">
  <data key="d0">Boxer: Box-attention for 2d and 3d transformers</data>
  <data key="d1">1805226328438105949</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Nguyen_BoxeR_Box-Attention_for_2D_and_3D_Transformers_CVPR_2022_paper.html</data>
  <data key="d3">Boxer: Box-attention for 2d and 3d transformers</data>
  <data key="d4">DK Nguyen, J Ju, O Booij…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1805226328438105949&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="14442074394232253245">
  <data key="d0">Bigdetection: A large-scale benchmark for improved object detector pre-training</data>
  <data key="d1">14442074394232253245</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022W/VDU/html/Cai_BigDetection_A_Large-Scale_Benchmark_for_Improved_Object_Detector_Pre-Training_CVPRW_2022_paper.html</data>
  <data key="d3">Bigdetection: A large-scale benchmark for improved object detector pre-training</data>
  <data key="d4">L Cai, Z Zhang, Y Zhu, L Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14442074394232253245&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="15900202697333596864">
  <data key="d0">Helmet wearing detection of motorcycle drivers using deep learning network with residual transformer-spatial attention</data>
  <data key="d1">15900202697333596864</data>
  <data key="d2">https://www.mdpi.com/2504-446X/6/12/415</data>
  <data key="d3">Helmet wearing detection of motorcycle drivers using deep learning network with residual transformer-spatial attention</data>
  <data key="d4">S Chen, J Lan, H Liu, C Chen, X Wang</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15900202697333596864&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="3843578070760729914">
  <data key="d0">Fast detection and location of longan fruits using UAV images</data>
  <data key="d1">3843578070760729914</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169921004828</data>
  <data key="d3">Fast detection and location of longan fruits using UAV images</data>
  <data key="d4">D Li, X Sun, H Elkhouchlaa, Y Jia, Z Yao, P Lin…</data>
  <data key="d5">2021</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3843578070760729914&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="7176502249404124990">
  <data key="d0">Deep learning-based detection of aluminum casting defects and their types</data>
  <data key="d1">7176502249404124990</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0952197622006261</data>
  <data key="d3">Deep learning-based detection of aluminum casting defects and their types</data>
  <data key="d4">IE Parlak, E Emel</data>
  <data key="d5">2023</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7176502249404124990&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="1170441202157905831">
  <data key="d0">Improved YOLOv4-tiny network for real-time electronic component detection</data>
  <data key="d1">1170441202157905831</data>
  <data key="d2">https://www.nature.com/articles/s41598-021-02225-y</data>
  <data key="d3">Improved YOLOv4-tiny network for real-time electronic component detection</data>
  <data key="d4">C Guo, X Lv, Y Zhang, M Zhang</data>
  <data key="d5">2021</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1170441202157905831&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="12063573584218762374">
  <data key="d0">YOLO-G: A lightweight network model for improving the performance of military targets detection</data>
  <data key="d1">12063573584218762374</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9780377/</data>
  <data key="d3">YOLO-G: A lightweight network model for improving the performance of military targets detection</data>
  <data key="d4">L Kong, J Wang, P Zhao</data>
  <data key="d5">2022</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12063573584218762374&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="2762494041692657615">
  <data key="d0">Real-time citywide reconstruction of traffic flow from moving cameras on lightweight edge devices</data>
  <data key="d1">2762494041692657615</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S092427162200199X</data>
  <data key="d3">Real-time citywide reconstruction of traffic flow from moving cameras on lightweight edge devices</data>
  <data key="d4">A Kumar, T Kashiyama, H Maeda, H Omata…</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2762494041692657615&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="5067198574099017621">
  <data key="d0">Identification and classification of mechanical damage during continuous harvesting of root crops using computer vision methods</data>
  <data key="d1">5067198574099017621</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9729819/</data>
  <data key="d3">Identification and classification of mechanical damage during continuous harvesting of root crops using computer vision methods</data>
  <data key="d4">A Osipov, V Shumaev, A Ekielski, T Gataullin…</data>
  <data key="d5">2022</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5067198574099017621&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="10920476727045815687">
  <data key="d0">Identifying images of dead chickens with a chicken removal system integrated with a deep learning algorithm</data>
  <data key="d1">10920476727045815687</data>
  <data key="d2">https://www.mdpi.com/1424-8220/21/11/3579</data>
  <data key="d3">Identifying images of dead chickens with a chicken removal system integrated with a deep learning algorithm</data>
  <data key="d4">HW Liu, CH Chen, YC Tsai, KW Hsieh, HT Lin</data>
  <data key="d5">2021</data>
  <data key="d6">33</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10920476727045815687&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="618726556668923706">
  <data key="d0">Research on recognition method of electrical components based on FEYOLOv4-tiny</data>
  <data key="d1">618726556668923706</data>
  <data key="d2">https://link.springer.com/article/10.1007/s42835-022-01124-0</data>
  <data key="d3">Research on recognition method of electrical components based on FEYOLOv4-tiny</data>
  <data key="d4">J Gao, H Sun, J Han, Q Sun, T Zhong</data>
  <data key="d5">2022</data>
  <data key="d6">31</data>
  <data key="d7">https://scholar.google.com/scholar?cites=618726556668923706&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="10392290641755897502">
  <data key="d0">Rail transit obstacle detection based on improved CNN</data>
  <data key="d1">10392290641755897502</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9552593/</data>
  <data key="d3">Rail transit obstacle detection based on improved CNN</data>
  <data key="d4">D He, Z Zou, Y Chen, B Liu…</data>
  <data key="d5">2021</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10392290641755897502&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="16867947394585746384">
  <data key="d0">Deep visual social distancing monitoring to combat COVID-19: A comprehensive survey</data>
  <data key="d1">16867947394585746384</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2210670722003821</data>
  <data key="d3">Deep visual social distancing monitoring to combat COVID-19: A comprehensive survey</data>
  <data key="d4">Y Himeur, S Al-Maadeed, N Almaadeed…</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16867947394585746384&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="7958224277211121064">
  <data key="d0">Multi-level knowledge distillation for low-resolution object detection and facial expression recognition</data>
  <data key="d1">7958224277211121064</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0950705122000144</data>
  <data key="d3">Multi-level knowledge distillation for low-resolution object detection and facial expression recognition</data>
  <data key="d4">T Ma, W Tian, Y Xie</data>
  <data key="d5">2022</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7958224277211121064&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="14994183852412960494">
  <data key="d0">A robust traffic-aware city-scale multi-camera vehicle tracking of vehicles</data>
  <data key="d1">14994183852412960494</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Tran_A_Robust_Traffic-Aware_City-Scale_Multi-Camera_Vehicle_Tracking_of_Vehicles_CVPRW_2022_paper.html</data>
  <data key="d3">A robust traffic-aware city-scale multi-camera vehicle tracking of vehicles</data>
  <data key="d4">DNN Tran, LH Pham, HJ Jeon…</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14994183852412960494&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="8743963594325005332">
  <data key="d0">An improved YOLOv5 method for large objects detection with multi-scale feature cross-layer fusion network</data>
  <data key="d1">8743963594325005332</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0262885622001470</data>
  <data key="d3">An improved YOLOv5 method for large objects detection with multi-scale feature cross-layer fusion network</data>
  <data key="d4">Z Qu, L Gao, S Wang, H Yin, T Yi</data>
  <data key="d5">2022</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8743963594325005332&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="3716397009677337300">
  <data key="d0">YOLO-ReT: Towards high accuracy real-time object detection on edge GPUs</data>
  <data key="d1">3716397009677337300</data>
  <data key="d2">https://openaccess.thecvf.com/content/WACV2022/html/Ganesh_YOLO-ReT_Towards_High_Accuracy_Real-Time_Object_Detection_on_Edge_GPUs_WACV_2022_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">YOLO-ReT: Towards high accuracy real-time object detection on edge GPUs</data>
  <data key="d4">P Ganesh, Y Chen, Y Yang, D Chen…</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3716397009677337300&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="11651296702682936875">
  <data key="d0">Fast and accurate multi-class geospatial object detection with large-size remote sensing imagery using CNN and Truncated NMS</data>
  <data key="d1">11651296702682936875</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0924271622001976</data>
  <data key="d3">Fast and accurate multi-class geospatial object detection with large-size remote sensing imagery using CNN and Truncated NMS</data>
  <data key="d4">Y Shen, D Liu, F Zhang, Q Zhang</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11651296702682936875&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="491588394875958513">
  <data key="d0">Real-time monitoring for manual operations with machine vision in smart manufacturing</data>
  <data key="d1">491588394875958513</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0278612522001868</data>
  <data key="d3">Real-time monitoring for manual operations with machine vision in smart manufacturing</data>
  <data key="d4">P Lou, J Li, YH Zeng, B Chen, X Zhang</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=491588394875958513&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="2613459347497379531">
  <data key="d0">Conformer: Local features coupling global representations for recognition and detection</data>
  <data key="d1">2613459347497379531</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10040235/</data>
  <data key="d3">Conformer: Local features coupling global representations for recognition and detection</data>
  <data key="d4">Z Peng, Z Guo, W Huang, Y Wang, L Xie…</data>
  <data key="d5">2023</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2613459347497379531&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="6340122554340223367">
  <data key="d0">Deep learning-based weed detection in turf: a review</data>
  <data key="d1">6340122554340223367</data>
  <data key="d2">https://www.mdpi.com/2073-4395/12/12/3051</data>
  <data key="d3">Deep learning-based weed detection in turf: a review</data>
  <data key="d4">X Jin, T Liu, Y Chen, J Yu</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6340122554340223367&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="6724179115405406423">
  <data key="d0">Fall prevention from scaffolding using computer vision and IoT-based monitoring</data>
  <data key="d1">6724179115405406423</data>
  <data key="d2">https://ascelibrary.org/doi/full/10.1061/(ASCE)CO.1943-7862.0002278</data>
  <data key="d3">Fall prevention from scaffolding using computer vision and IoT-based monitoring</data>
  <data key="d4">M Khan, R Khalid, S Anjum, SVT Tran…</data>
  <data key="d5">2022</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6724179115405406423&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="14372476051154245137">
  <data key="d0">Automatic weld type classification, tacked spot recognition and weld ROI determination for robotic welding based on modified YOLOv5</data>
  <data key="d1">14372476051154245137</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0736584522001727</data>
  <data key="d3">Automatic weld type classification, tacked spot recognition and weld ROI determination for robotic welding based on modified YOLOv5</data>
  <data key="d4">S Chen, D Yang, J Liu, Q Tian, F Zhou</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14372476051154245137&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="18310514353510448588">
  <data key="d0">Interspace pruning: Using adaptive filter representations to improve training of sparse cnns</data>
  <data key="d1">18310514353510448588</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Wimmer_Interspace_Pruning_Using_Adaptive_Filter_Representations_To_Improve_Training_of_CVPR_2022_paper.html</data>
  <data key="d3">Interspace pruning: Using adaptive filter representations to improve training of sparse cnns</data>
  <data key="d4">P Wimmer, J Mehnert…</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18310514353510448588&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="14757740639617181116">
  <data key="d0">Location-sensitive visual recognition with cross-iou loss</data>
  <data key="d1">14757740639617181116</data>
  <data key="d2">https://arxiv.org/abs/2104.04899</data>
  <data key="d3">Location-sensitive visual recognition with cross-iou loss</data>
  <data key="d4">K Duan, L Xie, H Qi, S Bai, Q Huang, Q Tian</data>
  <data key="d5">2021</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14757740639617181116&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="3625588465481963755">
  <data key="d0">AIR-YOLOv3: Aerial infrared pedestrian detection via an improved YOLOv3 with network pruning</data>
  <data key="d1">3625588465481963755</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/7/3627</data>
  <data key="d3">AIR-YOLOv3: Aerial infrared pedestrian detection via an improved YOLOv3 with network pruning</data>
  <data key="d4">Y Shao, X Zhang, H Chu, X Zhang, D Zhang, Y Rao</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3625588465481963755&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="1358307963221369570">
  <data key="d0">Detectorguard: Provably securing object detectors against localized patch hiding attacks</data>
  <data key="d1">1358307963221369570</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3460120.3484757</data>
  <data key="d3">Detectorguard: Provably securing object detectors against localized patch hiding attacks</data>
  <data key="d4">C Xiang, P Mittal</data>
  <data key="d5">2021</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1358307963221369570&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="17729334233569922217">
  <data key="d0">MITNET: a novel dataset and a two-stage deep learning approach for mitosis recognition in whole slide images of breast cancer tissue</data>
  <data key="d1">17729334233569922217</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00521-022-07441-9</data>
  <data key="d3">MITNET: a novel dataset and a two-stage deep learning approach for mitosis recognition in whole slide images of breast cancer tissue</data>
  <data key="d4">S Çayır, G Solmaz, H Kusetogullari, F Tokat…</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17729334233569922217&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="5997947092271436772">
  <data key="d0">Detection of bird species related to transmission line faults based on lightweight convolutional neural network</data>
  <data key="d1">5997947092271436772</data>
  <data key="d2">https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/gtd2.12333</data>
  <data key="d3">Detection of bird species related to transmission line faults based on lightweight convolutional neural network</data>
  <data key="d4">Z Qiu, X Zhu, C Liao, D Shi, Y Kuang…</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5997947092271436772&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="10928542084451319450">
  <data key="d0">Nms strikes back</data>
  <data key="d1">10928542084451319450</data>
  <data key="d2">https://arxiv.org/abs/2212.06137</data>
  <data key="d3">Nms strikes back</data>
  <data key="d4">J Ouyang-Zhang, JH Cho, X Zhou…</data>
  <data key="d5">2022</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10928542084451319450&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="14498566121599741556">
  <data key="d0">Liver tumor segmentation using 2.5 D UV-Net with multi-scale convolution</data>
  <data key="d1">14498566121599741556</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0010482521002183</data>
  <data key="d3">Liver tumor segmentation using 2.5 D UV-Net with multi-scale convolution</data>
  <data key="d4">C Zhang, Q Hua, Y Chu, P Wang</data>
  <data key="d5">2021</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14498566121599741556&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="10132132318136226957">
  <data key="d0">SHOMY: Detection of Small Hazardous Objects using the You Only Look Once Algorithm.</data>
  <data key="d1">10132132318136226957</data>
  <data key="d2">http://itiis.org/journals/tiis/digital-library/manuscript/file/25914/TIIS%20Vol%2016,%20No%208-12.pdf</data>
  <data key="d3">SHOMY: Detection of Small Hazardous Objects using the You Only Look Once Algorithm.</data>
  <data key="d4">E Kim, J Lee, H Jo, K Na, E Moon, G Gweon…</data>
  <data key="d5">2022</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10132132318136226957&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="4082625592026884096">
  <data key="d0">A fast aircraft detection method for SAR images based on efficient bidirectional path aggregated attention network</data>
  <data key="d1">4082625592026884096</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/15/2940</data>
  <data key="d3">A fast aircraft detection method for SAR images based on efficient bidirectional path aggregated attention network</data>
  <data key="d4">R Luo, L Chen, J Xing, Z Yuan, S Tan, X Cai, J Wang</data>
  <data key="d5">2021</data>
  <data key="d6">24</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4082625592026884096&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="6948486903139323825">
  <data key="d0">Revitalizing optimization for 3d human pose and shape estimation: A sparse constrained formulation</data>
  <data key="d1">6948486903139323825</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Fan_Revitalizing_Optimization_for_3D_Human_Pose_and_Shape_Estimation_A_ICCV_2021_paper.html</data>
  <data key="d3">Revitalizing optimization for 3d human pose and shape estimation: A sparse constrained formulation</data>
  <data key="d4">T Fan, KV Alwala, D Xiang, W Xu…</data>
  <data key="d5">2021</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6948486903139323825&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="18135864541368653593">
  <data key="d0">Augmentative contrastive learning for one-shot object detection</data>
  <data key="d1">18135864541368653593</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0925231222012152</data>
  <data key="d3">Augmentative contrastive learning for one-shot object detection</data>
  <data key="d4">Y Du, F Liu, L Jiao, Z Hao, S Li, X Liu, J Liu</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18135864541368653593&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="7748241482703179571">
  <data key="d0">LES-YOLO: A lightweight pinecone detection algorithm based on improved YOLOv4-Tiny network</data>
  <data key="d1">7748241482703179571</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923000017</data>
  <data key="d3">LES-YOLO: A lightweight pinecone detection algorithm based on improved YOLOv4-Tiny network</data>
  <data key="d4">M Cui, Y Lou, Y Ge, K Wang</data>
  <data key="d5">2023</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7748241482703179571&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="3294374866555392652">
  <data key="d0">A learning-based crack defect detection and 3D localization framework for automated fluorescent magnetic particle inspection</data>
  <data key="d1">3294374866555392652</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417422019844</data>
  <data key="d3">A learning-based crack defect detection and 3D localization framework for automated fluorescent magnetic particle inspection</data>
  <data key="d4">Q Wu, X Qin, K Dong, A Shi, Z Hu</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3294374866555392652&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="5386784097479583414">
  <data key="d0">ssFPN: Scale Sequence (S2) Feature-Based Feature Pyramid Network for Object Detection</data>
  <data key="d1">5386784097479583414</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/9/4432</data>
  <data key="d3">ssFPN: Scale Sequence (S2) Feature-Based Feature Pyramid Network for Object Detection</data>
  <data key="d4">HJ Park, JW Kang, BG Kim</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5386784097479583414&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="6661086806896725511">
  <data key="d0">TRC‐YOLO: A real‐time detection method for lightweight targets based on mobile devices</data>
  <data key="d1">6661086806896725511</data>
  <data key="d2">https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/cvi2.12072</data>
  <data key="d3">TRC‐YOLO: A real‐time detection method for lightweight targets based on mobile devices</data>
  <data key="d4">G Wang, H Ding, Z Yang, B Li, Y Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6661086806896725511&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="15115558022538513237">
  <data key="d0">Automatic Vehicle Identification and Classification Model Using the YOLOv3 Algorithm for a Toll Management System</data>
  <data key="d1">15115558022538513237</data>
  <data key="d2">https://www.mdpi.com/2071-1050/14/15/9163</data>
  <data key="d3">Automatic Vehicle Identification and Classification Model Using the YOLOv3 Algorithm for a Toll Management System</data>
  <data key="d4">SK Rajput, JC Patni, SS Alshamrani, V Chaudhari…</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15115558022538513237&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="14159121989210618633">
  <data key="d0">A lightweight framework for obstacle detection in the railway image based on fast region proposal and improved YOLO-tiny network</data>
  <data key="d1">14159121989210618633</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9715050/</data>
  <data key="d3">A lightweight framework for obstacle detection in the railway image based on fast region proposal and improved YOLO-tiny network</data>
  <data key="d4">L Guan, L Jia, Z Xie, C Yin</data>
  <data key="d5">2022</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14159121989210618633&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="15045952046135698930">
  <data key="d0">A comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities</data>
  <data key="d1">15045952046135698930</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3582688</data>
  <data key="d3">A comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities</data>
  <data key="d4">Y Song, T Wang, P Cai, SK Mondal…</data>
  <data key="d5">2023</data>
  <data key="d6">46</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15045952046135698930&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="1223563817431199857">
  <data key="d0">A lightweight vehicles detection network model based on YOLOv5</data>
  <data key="d1">1223563817431199857</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0952197622001415</data>
  <data key="d3">A lightweight vehicles detection network model based on YOLOv5</data>
  <data key="d4">X Dong, S Yan, C Duan</data>
  <data key="d5">2022</data>
  <data key="d6">80</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1223563817431199857&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="3743433833463586545">
  <data key="d0">Improved yolov5: Efficient object detection using drone images under various conditions</data>
  <data key="d1">3743433833463586545</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/14/7255</data>
  <data key="d3">Improved yolov5: Efficient object detection using drone images under various conditions</data>
  <data key="d4">HK Jung, GS Choi</data>
  <data key="d5">2022</data>
  <data key="d6">62</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3743433833463586545&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="2805320302601413924">
  <data key="d0">Underwater target detection algorithm based on improved YOLOv5</data>
  <data key="d1">2805320302601413924</data>
  <data key="d2">https://www.mdpi.com/2077-1312/10/3/310</data>
  <data key="d3">Underwater target detection algorithm based on improved YOLOv5</data>
  <data key="d4">F Lei, F Tang, S Li</data>
  <data key="d5">2022</data>
  <data key="d6">49</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2805320302601413924&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12502648671025662060">
  <data key="d0">Deep learning-based object detection techniques for remote sensing images: A survey</data>
  <data key="d1">12502648671025662060</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/10/2385</data>
  <data key="d3">Deep learning-based object detection techniques for remote sensing images: A survey</data>
  <data key="d4">Z Li, Y Wang, N Zhang, Y Zhang, Z Zhao, D Xu, G Ben…</data>
  <data key="d5">2022</data>
  <data key="d6">34</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12502648671025662060&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="11992929173642808953">
  <data key="d0">Concrete dam damage detection and localisation based on YOLOv5s-HSC and photogrammetric 3D reconstruction</data>
  <data key="d1">11992929173642808953</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0926580522004253</data>
  <data key="d3">Concrete dam damage detection and localisation based on YOLOv5s-HSC and photogrammetric 3D reconstruction</data>
  <data key="d4">S Zhao, F Kang, J Li</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11992929173642808953&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="6429502500328305488">
  <data key="d0">A deep learning model for detecting cage-free hens on the litter floor</data>
  <data key="d1">6429502500328305488</data>
  <data key="d2">https://www.mdpi.com/2076-2615/12/15/1983</data>
  <data key="d3">A deep learning model for detecting cage-free hens on the litter floor</data>
  <data key="d4">X Yang, L Chai, RB Bist, S Subedi, Z Wu</data>
  <data key="d5">2022</data>
  <data key="d6">24</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6429502500328305488&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="4628199491611022446">
  <data key="d0">Research on the coordinate attention mechanism fuse in a YOLOv5 deep learning detector for the SAR ship detection task</data>
  <data key="d1">4628199491611022446</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/9/3370</data>
  <data key="d3">Research on the coordinate attention mechanism fuse in a YOLOv5 deep learning detector for the SAR ship detection task</data>
  <data key="d4">F Xie, B Lin, Y Liu</data>
  <data key="d5">2022</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4628199491611022446&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="17059844040400317816">
  <data key="d0">YOLOv5-Fog: A multiobjective visual detection algorithm for fog driving scenes based on improved YOLOv5</data>
  <data key="d1">17059844040400317816</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9851677/</data>
  <data key="d3">YOLOv5-Fog: A multiobjective visual detection algorithm for fog driving scenes based on improved YOLOv5</data>
  <data key="d4">H Wang, Y Xu, Y He, Y Cai, L Chen, Y Li…</data>
  <data key="d5">2022</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17059844040400317816&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="2901021971175385417">
  <data key="d0">An improved Yolov5 for multi-rotor UAV detection</data>
  <data key="d1">2901021971175385417</data>
  <data key="d2">https://www.mdpi.com/2079-9292/11/15/2330</data>
  <data key="d3">An improved Yolov5 for multi-rotor UAV detection</data>
  <data key="d4">B Liu, H Luo</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2901021971175385417&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="5838860470680827023">
  <data key="d0">Real-time detection of cracks in tiled sidewalks using YOLO-based method applied to unmanned aerial vehicle (UAV) images</data>
  <data key="d1">5838860470680827023</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0926580523000055</data>
  <data key="d3">Real-time detection of cracks in tiled sidewalks using YOLO-based method applied to unmanned aerial vehicle (UAV) images</data>
  <data key="d4">Q Qiu, D Lau</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5838860470680827023&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="16864394826962881925">
  <data key="d0">Research on tiny target detection technology of fabric defects based on improved Yolo</data>
  <data key="d1">16864394826962881925</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/13/6823</data>
  <data key="d3">Research on tiny target detection technology of fabric defects based on improved Yolo</data>
  <data key="d4">X Yue, Q Wang, L He, Y Li, D Tang</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16864394826962881925&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="7269756958028651926">
  <data key="d0">A region-based deep learning approach to automated retail checkout</data>
  <data key="d1">7269756958028651926</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Shoman_A_Region-Based_Deep_Learning_Approach_to_Automated_Retail_Checkout_CVPRW_2022_paper.html</data>
  <data key="d3">A region-based deep learning approach to automated retail checkout</data>
  <data key="d4">M Shoman, A Aboah, A Morehead…</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7269756958028651926&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="6832920879012580854">
  <data key="d0">An end-to-end smart IoT-driven navigation for social distancing enforcement</data>
  <data key="d1">6832920879012580854</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9834931/</data>
  <data key="d3">An end-to-end smart IoT-driven navigation for social distancing enforcement</data>
  <data key="d4">H Friji, A Khanfor, H Ghazzai, Y Massoud</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6832920879012580854&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12145681576382105820">
  <data key="d0">Boosting target-level infrared and visible image fusion with regional information coordination</data>
  <data key="d1">12145681576382105820</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1566253522002536</data>
  <data key="d3">Boosting target-level infrared and visible image fusion with regional information coordination</data>
  <data key="d4">M Han, K Yu, J Qiu, H Li, D Wu, Y Rao, Y Yang, L Xing…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12145681576382105820&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="18188852084265258548">
  <data key="d0">Recognition of terminal buds of densely-planted Chinese fir seedlings using improved YOLOv5 by integrating attention mechanism</data>
  <data key="d1">18188852084265258548</data>
  <data key="d2">https://www.frontiersin.org/articles/10.3389/fpls.2022.991929/full</data>
  <data key="d3">Recognition of terminal buds of densely-planted Chinese fir seedlings using improved YOLOv5 by integrating attention mechanism</data>
  <data key="d4">Z Ye, Q Guo, J Wei, J Zhang, H Zhang, L Bian…</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18188852084265258548&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="795370128396508846">
  <data key="d0">Solar cell surface defect detection based on improved YOLO v5</data>
  <data key="d1">795370128396508846</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9847242/</data>
  <data key="d3">Solar cell surface defect detection based on improved YOLO v5</data>
  <data key="d4">M Zhang, L Yin</data>
  <data key="d5">2022</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=795370128396508846&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="17094049497238104059">
  <data key="d0">Detection of norway spruce trees (Picea Abies) infested by bark beetle in UAV images using YOLOs architectures</data>
  <data key="d1">17094049497238104059</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9684880/</data>
  <data key="d3">Detection of norway spruce trees (Picea Abies) infested by bark beetle in UAV images using YOLOs architectures</data>
  <data key="d4">A Safonova, Y Hamad, A Alekhina, D Kaplun</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17094049497238104059&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="8167187293724692556">
  <data key="d0">Object detection of road assets using transformer-based YOLOX with feature pyramid decoder on thai highway panorama</data>
  <data key="d1">8167187293724692556</data>
  <data key="d2">https://www.mdpi.com/2078-2489/13/1/5</data>
  <data key="d3">Object detection of road assets using transformer-based YOLOX with feature pyramid decoder on thai highway panorama</data>
  <data key="d4">T Panboonyuen, S Thongbai, W Wongweeranimit…</data>
  <data key="d5">2021</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8167187293724692556&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="1708442884848326195">
  <data key="d0">Automatic estimation of apple orchard blooming levels using the improved YOLOv5</data>
  <data key="d1">1708442884848326195</data>
  <data key="d2">https://www.mdpi.com/2073-4395/12/10/2483</data>
  <data key="d3">Automatic estimation of apple orchard blooming levels using the improved YOLOv5</data>
  <data key="d4">Z Chen, R Su, Y Wang, G Chen, Z Wang, P Yin, J Wang</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1708442884848326195&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="10621171041712014560">
  <data key="d0">Explainable Multitask Shapley Explanation Networks for Real-time Polyp Diagnosis in Videos</data>
  <data key="d1">10621171041712014560</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9896998/</data>
  <data key="d3">Explainable Multitask Shapley Explanation Networks for Real-time Polyp Diagnosis in Videos</data>
  <data key="d4">D Wang, X Wang, S Wang, Y Yin</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10621171041712014560&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="282841756216791105">
  <data key="d0">Last decade in vehicle detection and classification: a comprehensive survey</data>
  <data key="d1">282841756216791105</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11831-022-09764-1</data>
  <data key="d3">Last decade in vehicle detection and classification: a comprehensive survey</data>
  <data key="d4">S Maity, A Bhattacharyya, PK Singh, M Kumar…</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=282841756216791105&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="1020146708014614449">
  <data key="d0">An improved YOLOv5 crack detection method combined with transformer</data>
  <data key="d1">1020146708014614449</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9794770/</data>
  <data key="d3">An improved YOLOv5 crack detection method combined with transformer</data>
  <data key="d4">X Xiang, Z Wang, Y Qiao</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1020146708014614449&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="10294172557164487522">
  <data key="d0">Transvisdrone: Spatio-temporal transformer for vision-based drone-to-drone detection in aerial videos</data>
  <data key="d1">10294172557164487522</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10161433/</data>
  <data key="d3">Transvisdrone: Spatio-temporal transformer for vision-based drone-to-drone detection in aerial videos</data>
  <data key="d4">T Sangam, IR Dave, W Sultani…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10294172557164487522&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14493401188532558995">
  <data key="d0">Lightweight detection network for arbitrary-oriented vehicles in UAV imagery via global attentive relation and multi-path fusion</data>
  <data key="d1">14493401188532558995</data>
  <data key="d2">https://www.mdpi.com/2504-446X/6/5/108</data>
  <data key="d3">Lightweight detection network for arbitrary-oriented vehicles in UAV imagery via global attentive relation and multi-path fusion</data>
  <data key="d4">J Feng, C Yi</data>
  <data key="d5">2022</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14493401188532558995&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="915175193869878529">
  <data key="d0">Detection of river floating garbage based on improved YOLOv5</data>
  <data key="d1">915175193869878529</data>
  <data key="d2">https://www.mdpi.com/2227-7390/10/22/4366</data>
  <data key="d3">Detection of river floating garbage based on improved YOLOv5</data>
  <data key="d4">X Yang, J Zhao, L Zhao, H Zhang, L Li, Z Ji, I Ganchev</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=915175193869878529&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="4712339159260231874">
  <data key="d0">FiFoNet: Fine-Grained Target Focusing Network for Object Detection in UAV Images</data>
  <data key="d1">4712339159260231874</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/16/3919</data>
  <data key="d3">FiFoNet: Fine-Grained Target Focusing Network for Object Detection in UAV Images</data>
  <data key="d4">Y Xi, W Jia, Q Miao, X Liu, X Fan, H Li</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4712339159260231874&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="10634144542916924319">
  <data key="d0">Research on crack detection method of wind turbine blade based on a deep learning method</data>
  <data key="d1">10634144542916924319</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0306261922014982</data>
  <data key="d3">Research on crack detection method of wind turbine blade based on a deep learning method</data>
  <data key="d4">Z Xiaoxun, H Xinyu, G Xiaoxia, Y Xing, X Zixu, W Yu…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10634144542916924319&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="15151751976029410131">
  <data key="d0">Drone Detection Using YOLOv5</data>
  <data key="d1">15151751976029410131</data>
  <data key="d2">https://www.mdpi.com/2673-4117/4/1/25</data>
  <data key="d3">Drone Detection Using YOLOv5</data>
  <data key="d4">B Aydin, S Singha</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15151751976029410131&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3619283013073038983">
  <data key="d0">Pareto refocusing for drone-view object detection</data>
  <data key="d1">3619283013073038983</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9905640/</data>
  <data key="d3">Pareto refocusing for drone-view object detection</data>
  <data key="d4">J Leng, M Mo, Y Zhou, C Gao, W Li…</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3619283013073038983&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14298330466237469464">
  <data key="d0">Adverse Weather Target Detection Algorithm Based on Adaptive Color Levels and Improved YOLOv5</data>
  <data key="d1">14298330466237469464</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/21/8577</data>
  <data key="d3">Adverse Weather Target Detection Algorithm Based on Adaptive Color Levels and Improved YOLOv5</data>
  <data key="d4">J Yao, X Fan, B Li, W Qin</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14298330466237469464&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="17746736907900692263">
  <data key="d0">TPH-YOLOv5++: Boosting Object Detection on Drone-Captured Scenarios with Cross-Layer Asymmetric Transformer</data>
  <data key="d1">17746736907900692263</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/6/1687</data>
  <data key="d3">TPH-YOLOv5++: Boosting Object Detection on Drone-Captured Scenarios with Cross-Layer Asymmetric Transformer</data>
  <data key="d4">Q Zhao, B Liu, S Lyu, C Wang, H Zhang</data>
  <data key="d5">2023</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17746736907900692263&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="7490525040340169893">
  <data key="d0">EfficientLiteDet: a real-time pedestrian and vehicle detection algorithm</data>
  <data key="d1">7490525040340169893</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00138-022-01293-y</data>
  <data key="d3">EfficientLiteDet: a real-time pedestrian and vehicle detection algorithm</data>
  <data key="d4">CB Murthy, MF Hashmi, AG Keskar</data>
  <data key="d5">2022</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7490525040340169893&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="1646852206964174022">
  <data key="d0">SCFNet: Semantic correction and focus network for remote sensing image object detection</data>
  <data key="d1">1646852206964174022</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417423004827</data>
  <data key="d3">SCFNet: Semantic correction and focus network for remote sensing image object detection</data>
  <data key="d4">C Yue, J Yan, Y Zhang, Z Luo, Y Liu, P Guo</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1646852206964174022&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3566720882072534084">
  <data key="d0">An Efficient YOLO Algorithm with an Attention Mechanism for Vision-Based Defect Inspection Deployed on FPGA</data>
  <data key="d1">3566720882072534084</data>
  <data key="d2">https://www.mdpi.com/2072-666X/13/7/1058</data>
  <data key="d3">An Efficient YOLO Algorithm with an Attention Mechanism for Vision-Based Defect Inspection Deployed on FPGA</data>
  <data key="d4">L Yu, J Zhu, Q Zhao, Z Wang</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3566720882072534084&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="6060221142406415982">
  <data key="d0">Underwater Target Detection Lightweight Algorithm Based on Multi-Scale Feature Fusion</data>
  <data key="d1">6060221142406415982</data>
  <data key="d2">https://www.mdpi.com/2077-1312/11/2/320</data>
  <data key="d3">Underwater Target Detection Lightweight Algorithm Based on Multi-Scale Feature Fusion</data>
  <data key="d4">L Chen, Y Yang, Z Wang, J Zhang, S Zhou…</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6060221142406415982&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="7686098348117464607">
  <data key="d0">Research on the Application of Visual Recognition in the Engine Room of Intelligent Ships</data>
  <data key="d1">7686098348117464607</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/19/7261</data>
  <data key="d3">Research on the Application of Visual Recognition in the Engine Room of Intelligent Ships</data>
  <data key="d4">D Shang, J Zhang, K Zhou, T Wang, J Qi</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7686098348117464607&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="10112787672038528081">
  <data key="d0">MR-YOLO: an improved YOLOv5 network for detecting magnetic ring surface defects</data>
  <data key="d1">10112787672038528081</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/24/9897</data>
  <data key="d3">MR-YOLO: an improved YOLOv5 network for detecting magnetic ring surface defects</data>
  <data key="d4">X Lang, Z Ren, D Wan, Y Zhang, S Shu</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10112787672038528081&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12467383398842524346">
  <data key="d0">A pedestrian detection network model based on improved YOLOv5</data>
  <data key="d1">12467383398842524346</data>
  <data key="d2">https://www.mdpi.com/1099-4300/25/2/381</data>
  <data key="d3">A pedestrian detection network model based on improved YOLOv5</data>
  <data key="d4">ML Li, GB Sun, JX Yu</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12467383398842524346&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="17781373599130948810">
  <data key="d0">Small vessel detection based on adaptive dual-polarimetric feature fusion and sea–land segmentation in SAR images</data>
  <data key="d1">17781373599130948810</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9735422/</data>
  <data key="d3">Small vessel detection based on adaptive dual-polarimetric feature fusion and sea–land segmentation in SAR images</data>
  <data key="d4">Y Zhou, F Zhang, F Ma, D Xiang…</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17781373599130948810&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="10321528843008389282">
  <data key="d0">Deep learning in spatial transcriptomics: Learning from the next next-generation sequencing</data>
  <data key="d1">10321528843008389282</data>
  <data key="d2">https://www.biorxiv.org/content/10.1101/2022.02.28.482392.abstract</data>
  <data key="d3">Deep learning in spatial transcriptomics: Learning from the next next-generation sequencing</data>
  <data key="d4">AA Heydari, SS Sindi</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10321528843008389282&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3073101641983057594">
  <data key="d0">Safety helmet detection using deep learning: Implementation and comparative study using YOLOv5, YOLOv6, and YOLOv7</data>
  <data key="d1">3073101641983057594</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10010490/</data>
  <data key="d3">Safety helmet detection using deep learning: Implementation and comparative study using YOLOv5, YOLOv6, and YOLOv7</data>
  <data key="d4">NDT Yung, WK Wong, FH Juwono…</data>
  <data key="d5">2022</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3073101641983057594&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="15258815906138889567">
  <data key="d0">Rd-yolo: An effective and efficient object detector for roadside perception system</data>
  <data key="d1">15258815906138889567</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/21/8097</data>
  <data key="d3">Rd-yolo: An effective and efficient object detector for roadside perception system</data>
  <data key="d4">L Huang, W Huang</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15258815906138889567&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12958131567968188905">
  <data key="d0">Hidden dangerous object recognition in terahertz images using deep learning methods</data>
  <data key="d1">12958131567968188905</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/15/7354</data>
  <data key="d3">Hidden dangerous object recognition in terahertz images using deep learning methods</data>
  <data key="d4">SA Danso, L Shang, D Hu, J Odoom, Q Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12958131567968188905&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="11183708110410802287">
  <data key="d0">Classification of Tomato Fruit Using Yolov5 and Convolutional Neural Network Models</data>
  <data key="d1">11183708110410802287</data>
  <data key="d2">https://www.mdpi.com/2223-7747/12/4/790</data>
  <data key="d3">Classification of Tomato Fruit Using Yolov5 and Convolutional Neural Network Models</data>
  <data key="d4">QH Phan, VT Nguyen, CH Lien, TP Duong, MTK Hou…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11183708110410802287&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14682032714717312533">
  <data key="d0">Research on Chengdu Ma goat recognition based on computer vison</data>
  <data key="d1">14682032714717312533</data>
  <data key="d2">https://www.mdpi.com/2076-2615/12/14/1746</data>
  <data key="d3">Research on Chengdu Ma goat recognition based on computer vison</data>
  <data key="d4">J Pu, C Yu, X Chen, Y Zhang, X Yang, J Li</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14682032714717312533&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="11101126821023653833">
  <data key="d0">Fruit ripeness identification using transformers</data>
  <data key="d1">11101126821023653833</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10489-023-04799-8</data>
  <data key="d3">Fruit ripeness identification using transformers</data>
  <data key="d4">B Xiao, M Nguyen, WQ Yan</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11101126821023653833&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="2547097814727899492">
  <data key="d0">An Improved YOLOv5 Crack Detection Method Combined with a Bottleneck Transformer</data>
  <data key="d1">2547097814727899492</data>
  <data key="d2">https://www.mdpi.com/2227-7390/11/10/2377</data>
  <data key="d3">An Improved YOLOv5 Crack Detection Method Combined with a Bottleneck Transformer</data>
  <data key="d4">G Yu, X Zhou</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2547097814727899492&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="9478150375568593008">
  <data key="d0">YOLOSR-IST: A deep learning method for small target detection in infrared remote sensing images based on super-resolution and YOLO</data>
  <data key="d1">9478150375568593008</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0165168423000361</data>
  <data key="d3">YOLOSR-IST: A deep learning method for small target detection in infrared remote sensing images based on super-resolution and YOLO</data>
  <data key="d4">R Li, Y Shen</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9478150375568593008&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="1144856546777335713">
  <data key="d0">A video analytics system for person detection combined with edge computing</data>
  <data key="d1">1144856546777335713</data>
  <data key="d2">https://www.mdpi.com/2079-3197/10/3/35</data>
  <data key="d3">A video analytics system for person detection combined with edge computing</data>
  <data key="d4">E Maltezos, P Lioupis, A Dadoukis, L Karagiannidis…</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1144856546777335713&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="5373184172069649878">
  <data key="d0">Tiny vehicle detection for mid-to-high altitude UAV images based on visual attention and spatial-temporal information</data>
  <data key="d1">5373184172069649878</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/6/2354</data>
  <data key="d3">Tiny vehicle detection for mid-to-high altitude UAV images based on visual attention and spatial-temporal information</data>
  <data key="d4">R Yu, H Li, Y Jiang, B Zhang, Y Wang</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5373184172069649878&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="1630814355216789856">
  <data key="d0">Research on car license plate recognition based on improved YOLOv5m and LPRNet</data>
  <data key="d1">1630814355216789856</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9874789/</data>
  <data key="d3">Research on car license plate recognition based on improved YOLOv5m and LPRNet</data>
  <data key="d4">S Luo, J Liu</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1630814355216789856&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="5480963051895971527">
  <data key="d0">HematoNet: expert level classification of bone marrow cytology morphology in hematological malignancy with deep learning</data>
  <data key="d1">5480963051895971527</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2667318522000137</data>
  <data key="d3">HematoNet: expert level classification of bone marrow cytology morphology in hematological malignancy with deep learning</data>
  <data key="d4">S Tripathi, AI Augustin, R Sukumaran, S Dheer…</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5480963051895971527&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14031173575587899194">
  <data key="d0">Integrating Multi-Scale Remote-Sensing Data to Monitor Severe Forest Infestation in Response to Pine Wilt Disease</data>
  <data key="d1">14031173575587899194</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/20/5164</data>
  <data key="d3">Integrating Multi-Scale Remote-Sensing Data to Monitor Severe Forest Infestation in Response to Pine Wilt Disease</data>
  <data key="d4">X Li, Y Liu, P Huang, T Tong, L Li, Y Chen, T Hou…</data>
  <data key="d5">2022</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14031173575587899194&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="16620415112774447142">
  <data key="d0">A detection approach for late-autumn shoots of litchi based on unmanned aerial vehicle (UAV) remote sensing</data>
  <data key="d1">16620415112774447142</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169922008432</data>
  <data key="d3">A detection approach for late-autumn shoots of litchi based on unmanned aerial vehicle (UAV) remote sensing</data>
  <data key="d4">J Liang, X Chen, C Liang, T Long, X Tang, Z Shi…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16620415112774447142&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="16166479600523370143">
  <data key="d0">Automatic Mapping of Karez in Turpan Basin Based on Google Earth Images and the YOLOv5 Model</data>
  <data key="d1">16166479600523370143</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/14/3318</data>
  <data key="d3">Automatic Mapping of Karez in Turpan Basin Based on Google Earth Images and the YOLOv5 Model</data>
  <data key="d4">Q Li, H Guo, L Luo, X Wang</data>
  <data key="d5">2022</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16166479600523370143&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12327522076345093055">
  <data key="d0">Novel Recursive BiFPN Combining with Swin Transformer for Wildland Fire Smoke Detection</data>
  <data key="d1">12327522076345093055</data>
  <data key="d2">https://www.mdpi.com/1999-4907/13/12/2032</data>
  <data key="d3">Novel Recursive BiFPN Combining with Swin Transformer for Wildland Fire Smoke Detection</data>
  <data key="d4">A Li, Y Zhao, Z Zheng</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12327522076345093055&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14339464007846837517">
  <data key="d0">An Improved YOLOv5-Based Underwater Object-Detection Framework</data>
  <data key="d1">14339464007846837517</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/7/3693</data>
  <data key="d3">An Improved YOLOv5-Based Underwater Object-Detection Framework</data>
  <data key="d4">J Zhang, J Zhang, K Zhou, Y Zhang, H Chen, X Yan</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14339464007846837517&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="7455995568870583888">
  <data key="d0">Coupled Global–Local object detection for large VHR aerial images</data>
  <data key="d1">7455995568870583888</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0950705122011935</data>
  <data key="d3">Coupled Global–Local object detection for large VHR aerial images</data>
  <data key="d4">X Chen, C Wang, Z Li, M Liu, Q Li, H Qi, D Ma…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7455995568870583888&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="3242865566534850692">
  <data key="d0">YOLOv5s-CBAM-DMLHead: A lightweight identification algorithm for weedy rice (Oryza sativa f. spontanea) based on improved YOLOv5</data>
  <data key="d1">3242865566534850692</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0261219423001655</data>
  <data key="d3">YOLOv5s-CBAM-DMLHead: A lightweight identification algorithm for weedy rice (Oryza sativa f. spontanea) based on improved YOLOv5</data>
  <data key="d4">C Yuan, T Liu, F Gao, R Zhang, X Seng</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3242865566534850692&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="10190720705683005400">
  <data key="d0">ARSD: An Adaptive Region Selection Object Detection Framework for UAV Images</data>
  <data key="d1">10190720705683005400</data>
  <data key="d2">https://www.mdpi.com/2504-446X/6/9/228</data>
  <data key="d3">ARSD: An Adaptive Region Selection Object Detection Framework for UAV Images</data>
  <data key="d4">Y Wan, Y Zhong, Y Huang, Y Han, Y Cui, Q Yang, Z Li…</data>
  <data key="d5">2022</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10190720705683005400&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3066843940507119154">
  <data key="d0">Improved YOLOX Foreign Object Detection Algorithm for Transmission Lines</data>
  <data key="d1">3066843940507119154</data>
  <data key="d2">https://www.hindawi.com/journals/wcmc/2022/5835693/</data>
  <data key="d3">Improved YOLOX Foreign Object Detection Algorithm for Transmission Lines</data>
  <data key="d4">M Wu, L Guo, R Chen, W Du, J Wang, M Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3066843940507119154&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="6332829983840302405">
  <data key="d0">Artificial intelligence–based method for the rapid detection of fish parasites (Ichthyophthirius multifiliis, Gyrodactylus kobayashii, and Argulus japonicus)</data>
  <data key="d1">6332829983840302405</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0044848622009073</data>
  <data key="d3">Artificial intelligence–based method for the rapid detection of fish parasites (Ichthyophthirius multifiliis, Gyrodactylus kobayashii, and Argulus japonicus)</data>
  <data key="d4">J Li, Z Lian, Z Wu, L Zeng, L Mu, Y Yuan, H Bai, Z Guo…</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6332829983840302405&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="13500090513888098729">
  <data key="d0">Towards efficient detection for small objects via attention-guided detection network and data augmentation</data>
  <data key="d1">13500090513888098729</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/19/7663</data>
  <data key="d3">Towards efficient detection for small objects via attention-guided detection network and data augmentation</data>
  <data key="d4">X Wang, D Zhu, Y Yan</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13500090513888098729&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12621962626789350377">
  <data key="d0">Differential evolution based dual adversarial camouflage: Fooling human eyes and object detectors</data>
  <data key="d1">12621962626789350377</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0893608023001764</data>
  <data key="d3">Differential evolution based dual adversarial camouflage: Fooling human eyes and object detectors</data>
  <data key="d4">J Sun, W Yao, T Jiang, D Wang, X Chen</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12621962626789350377&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="16002571822077945928">
  <data key="d0">Lightweight and efficient neural network with SPSA attention for wheat ear detection</data>
  <data key="d1">16002571822077945928</data>
  <data key="d2">https://peerj.com/articles/cs-931/</data>
  <data key="d3">Lightweight and efficient neural network with SPSA attention for wheat ear detection</data>
  <data key="d4">Y Dong, Y Liu, H Kang, C Li, P Liu, Z Liu</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16002571822077945928&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3232219129862852847">
  <data key="d0">Deep Crowd Anomaly Detection: State-of-the-Art, Challenges, and Future Research Directions</data>
  <data key="d1">3232219129862852847</data>
  <data key="d2">https://arxiv.org/abs/2210.13927</data>
  <data key="d3">Deep Crowd Anomaly Detection: State-of-the-Art, Challenges, and Future Research Directions</data>
  <data key="d4">MH Sharif, L Jiao, CW Omlin</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3232219129862852847&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="5354452319526960123">
  <data key="d0">An improved YOLOv7 lightweight detection algorithm for obscured pedestrians</data>
  <data key="d1">5354452319526960123</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/13/5912</data>
  <data key="d3">An improved YOLOv7 lightweight detection algorithm for obscured pedestrians</data>
  <data key="d4">C Li, Y Wang, X Liu</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5354452319526960123&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="14918519107862734365">
  <data key="d0">YOLOv4 with Deformable-Embedding-Transformer Feature Extractor for Exact Object Detection in Aerial Imagery</data>
  <data key="d1">14918519107862734365</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/5/2522</data>
  <data key="d3">YOLOv4 with Deformable-Embedding-Transformer Feature Extractor for Exact Object Detection in Aerial Imagery</data>
  <data key="d4">Y Wu, J Li</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14918519107862734365&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="17165120566620849856">
  <data key="d0">An Intelligent Weighted Object Detector for Feature Extraction to Enrich Global Image Information</data>
  <data key="d1">17165120566620849856</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/15/7825</data>
  <data key="d3">An Intelligent Weighted Object Detector for Feature Extraction to Enrich Global Image Information</data>
  <data key="d4">L Yan, K Li, R Gao, C Wang, N Xiong</data>
  <data key="d5">2022</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17165120566620849856&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="4741106364027764952">
  <data key="d0">JR-TFViT: A Lightweight Efficient Radar Jamming Recognition Network Based on Global Representation of the Time–Frequency Domain</data>
  <data key="d1">4741106364027764952</data>
  <data key="d2">https://www.mdpi.com/2079-9292/11/17/2794</data>
  <data key="d3">JR-TFViT: A Lightweight Efficient Radar Jamming Recognition Network Based on Global Representation of the Time–Frequency Domain</data>
  <data key="d4">B Lang, J Gong</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4741106364027764952&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="11222483890632287401">
  <data key="d0">A Sidelobe-Aware Small Ship Detection Network for Synthetic Aperture Radar Imagery</data>
  <data key="d1">11222483890632287401</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10091564/</data>
  <data key="d3">A Sidelobe-Aware Small Ship Detection Network for Synthetic Aperture Radar Imagery</data>
  <data key="d4">Y Zhou, H Liu, F Ma, Z Pan…</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11222483890632287401&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12142856329509817511">
  <data key="d0">Object Detection for UAV Aerial Scenarios Based on Vectorized IOU</data>
  <data key="d1">12142856329509817511</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/6/3061</data>
  <data key="d3">Object Detection for UAV Aerial Scenarios Based on Vectorized IOU</data>
  <data key="d4">S Lu, H Lu, J Dong, S Wu</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12142856329509817511&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="11083218512495776976">
  <data key="d0">A deep-learning pipeline to diagnose pediatric intussusception and assess severity during ultrasound scanning: a multicenter retrospective-prospective study</data>
  <data key="d1">11083218512495776976</data>
  <data key="d2">https://www.nature.com/articles/s41746-023-00930-8</data>
  <data key="d3">A deep-learning pipeline to diagnose pediatric intussusception and assess severity during ultrasound scanning: a multicenter retrospective-prospective study</data>
  <data key="d4">Y Pei, G Wang, H Cao, S Jiang, D Wang, H Wang…</data>
  <data key="d5">2023</data>
  <data key="d8">1</data>
</node>
<node id="3110755508632090464">
  <data key="d0">Data-driven model SSD-BSP for multi-target coal-gangue detection</data>
  <data key="d1">3110755508632090464</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0263224123008084</data>
  <data key="d3">Data-driven model SSD-BSP for multi-target coal-gangue detection</data>
  <data key="d4">L Wang, X Wang, B Li</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3110755508632090464&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="4388463373166231389">
  <data key="d0">An eye tracking and brain-computer Interface based human-environment interactive system for amyotrophic lateral sclerosis patients</data>
  <data key="d1">4388463373166231389</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9966509/</data>
  <data key="d3">An eye tracking and brain-computer Interface based human-environment interactive system for amyotrophic lateral sclerosis patients</data>
  <data key="d4">J Wang, S Xu, Y Dai, S Gao</data>
  <data key="d5">2022</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4388463373166231389&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="11986345078302880571">
  <data key="d0">An Active Multi-Object Ultrafast Tracking System with CNN-Based Hybrid Object Detection</data>
  <data key="d1">11986345078302880571</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/8/4150</data>
  <data key="d3">An Active Multi-Object Ultrafast Tracking System with CNN-Based Hybrid Object Detection</data>
  <data key="d4">Q Li, S Hu, K Shimasaki, I Ishii</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11986345078302880571&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="943904167170865655">
  <data key="d0">High-resolution concrete damage image synthesis using conditional generative adversarial network</data>
  <data key="d1">943904167170865655</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0926580522006094</data>
  <data key="d3">High-resolution concrete damage image synthesis using conditional generative adversarial network</data>
  <data key="d4">S Li, X Zhao</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=943904167170865655&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="5907315131746612745">
  <data key="d0">DBF‐YOLO: UAV Small Targets Detection Based on Shallow Feature Fusion</data>
  <data key="d1">5907315131746612745</data>
  <data key="d2">https://onlinelibrary.wiley.com/doi/abs/10.1002/tee.23758</data>
  <data key="d3">DBF‐YOLO: UAV Small Targets Detection Based on Shallow Feature Fusion</data>
  <data key="d4">H Liu, X Duan, H Chen, H Lou…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5907315131746612745&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="2505454209938753585">
  <data key="d0">Efficient-Lightweight YOLO: Improving Small Object Detection in YOLO for Aerial Images</data>
  <data key="d1">2505454209938753585</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/14/6423</data>
  <data key="d3">Efficient-Lightweight YOLO: Improving Small Object Detection in YOLO for Aerial Images</data>
  <data key="d4">M Hu, Z Li, J Yu, X Wan, H Tan, Z Lin</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2505454209938753585&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="1928444155270241570">
  <data key="d0">Object detection based approach for an efficient video summarization with system statistics over cloud</data>
  <data key="d1">1928444155270241570</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9986376/</data>
  <data key="d3">Object detection based approach for an efficient video summarization with system statistics over cloud</data>
  <data key="d4">A Negi, K Kumar, P Saini…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1928444155270241570&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="4550423614333066109">
  <data key="d0">Yolov4: Optimal speed and accuracy of object detection</data>
  <data key="d1">4550423614333066109</data>
  <data key="d2">https://arxiv.org/abs/2004.10934</data>
  <data key="d3">Yolov4: Optimal speed and accuracy of object detection</data>
  <data key="d4">A Bochkovskiy, CY Wang, HYM Liao</data>
  <data key="d5">2020</data>
  <data key="d6">11856</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4550423614333066109&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2143047335646200982">
  <data key="d0">All one needs to know about metaverse: A complete survey on technological singularity, virtual ecosystem, and research agenda</data>
  <data key="d1">2143047335646200982</data>
  <data key="d2">https://arxiv.org/abs/2110.05352</data>
  <data key="d3">All one needs to know about metaverse: A complete survey on technological singularity, virtual ecosystem, and research agenda</data>
  <data key="d4">LH Lee, T Braud, P Zhou, L Wang, D Xu, Z Lin…</data>
  <data key="d5">2021</data>
  <data key="d6">962</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2143047335646200982&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9374811277911686866">
  <data key="d0">Mish: A self regularized non-monotonic activation function</data>
  <data key="d1">9374811277911686866</data>
  <data key="d2">https://arxiv.org/abs/1908.08681</data>
  <data key="d3">Mish: A self regularized non-monotonic activation function</data>
  <data key="d4">D Misra</data>
  <data key="d5">2019</data>
  <data key="d6">1385</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9374811277911686866&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2641915666092458000">
  <data key="d0">Detecting twenty-thousand classes using image-level supervision</data>
  <data key="d1">2641915666092458000</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20077-9_21</data>
  <data key="d3">Detecting twenty-thousand classes using image-level supervision</data>
  <data key="d4">X Zhou, R Girdhar, A Joulin, P Krähenbühl…</data>
  <data key="d5">2022</data>
  <data key="d6">216</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2641915666092458000&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="6847238147022651878">
  <data key="d0">Sensor and sensor fusion technology in autonomous vehicles: A review</data>
  <data key="d1">6847238147022651878</data>
  <data key="d2">https://www.mdpi.com/1424-8220/21/6/2140</data>
  <data key="d3">Sensor and sensor fusion technology in autonomous vehicles: A review</data>
  <data key="d4">DJ Yeong, G Velasco-Hernandez, J Barry, J Walsh</data>
  <data key="d5">2021</data>
  <data key="d6">394</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6847238147022651878&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="15073015961806262305">
  <data key="d0">A Review of Yolo algorithm developments</data>
  <data key="d1">15073015961806262305</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1877050922001363</data>
  <data key="d3">A Review of Yolo algorithm developments</data>
  <data key="d4">P Jiang, D Ergu, F Liu, Y Cai, B Ma</data>
  <data key="d5">2022</data>
  <data key="d6">445</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15073015961806262305&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="4893668343000496575">
  <data key="d0">A forest fire detection system based on ensemble learning</data>
  <data key="d1">4893668343000496575</data>
  <data key="d2">https://www.mdpi.com/1999-4907/12/2/217</data>
  <data key="d3">A forest fire detection system based on ensemble learning</data>
  <data key="d4">R Xu, H Lin, K Lu, L Cao, Y Liu</data>
  <data key="d5">2021</data>
  <data key="d6">359</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4893668343000496575&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2257115641228924434">
  <data key="d0">Robustbench: a standardized adversarial robustness benchmark</data>
  <data key="d1">2257115641228924434</data>
  <data key="d2">https://arxiv.org/abs/2010.09670</data>
  <data key="d3">Robustbench: a standardized adversarial robustness benchmark</data>
  <data key="d4">F Croce, M Andriushchenko, V Sehwag…</data>
  <data key="d5">2020</data>
  <data key="d6">370</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2257115641228924434&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="8384042348747306199">
  <data key="d0">A comparative analysis of object detection metrics with a companion open-source toolkit</data>
  <data key="d1">8384042348747306199</data>
  <data key="d2">https://www.mdpi.com/2079-9292/10/3/279</data>
  <data key="d3">A comparative analysis of object detection metrics with a companion open-source toolkit</data>
  <data key="d4">R Padilla, WL Passos, TLB Dias, SL Netto…</data>
  <data key="d5">2021</data>
  <data key="d6">353</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8384042348747306199&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2188347889974787509">
  <data key="d0">Deep learning in computer vision: A critical review of emerging techniques and application scenarios</data>
  <data key="d1">2188347889974787509</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2666827021000670</data>
  <data key="d3">Deep learning in computer vision: A critical review of emerging techniques and application scenarios</data>
  <data key="d4">J Chai, H Zeng, A Li, EWT Ngai</data>
  <data key="d5">2021</data>
  <data key="d6">255</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2188347889974787509&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4220770694777717094">
  <data key="d0">Using channel pruning-based YOLO v4 deep learning algorithm for the real-time and accurate detection of apple flowers in natural environments</data>
  <data key="d1">4220770694777717094</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169920318986</data>
  <data key="d3">Using channel pruning-based YOLO v4 deep learning algorithm for the real-time and accurate detection of apple flowers in natural environments</data>
  <data key="d4">D Wu, S Lv, M Jiang, H Song</data>
  <data key="d5">2020</data>
  <data key="d6">309</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4220770694777717094&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="8949706078964164796">
  <data key="d0">Pointaugmenting: Cross-modal augmentation for 3d object detection</data>
  <data key="d1">8949706078964164796</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2021/html/Wang_PointAugmenting_Cross-Modal_Augmentation_for_3D_Object_Detection_CVPR_2021_paper.html?utm_campaign=Akira%27s%20Machine%20Learning%20News%20%28ja%29&amp;utm_medium=email&amp;utm_source=Revue%20newsletter</data>
  <data key="d3">Pointaugmenting: Cross-modal augmentation for 3d object detection</data>
  <data key="d4">C Wang, C Ma, M Zhu, X Yang</data>
  <data key="d5">2021</data>
  <data key="d6">197</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8949706078964164796&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="17957175255238019436">
  <data key="d0">Inner monologue: Embodied reasoning through planning with language models</data>
  <data key="d1">17957175255238019436</data>
  <data key="d2">https://arxiv.org/abs/2207.05608</data>
  <data key="d3">Inner monologue: Embodied reasoning through planning with language models</data>
  <data key="d4">W Huang, F Xia, T Xiao, H Chan, J Liang…</data>
  <data key="d5">2022</data>
  <data key="d6">199</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17957175255238019436&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="6960142602186458983">
  <data key="d0">-IoU: A Family of Power Intersection over Union Losses for Bounding Box Regression</data>
  <data key="d1">6960142602186458983</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/a8f15eda80c50adb0e71943adc8015cf-Abstract.html</data>
  <data key="d3">-IoU: A Family of Power Intersection over Union Losses for Bounding Box Regression</data>
  <data key="d4">J He, S Erfani, X Ma, J Bailey…</data>
  <data key="d5">2021</data>
  <data key="d6">148</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6960142602186458983&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14469340457726874491">
  <data key="d0">Face mask wearing detection algorithm based on improved YOLO-v4</data>
  <data key="d1">14469340457726874491</data>
  <data key="d2">https://www.mdpi.com/1424-8220/21/9/3263</data>
  <data key="d3">Face mask wearing detection algorithm based on improved YOLO-v4</data>
  <data key="d4">J Yu, W Zhang</data>
  <data key="d5">2021</data>
  <data key="d6">207</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14469340457726874491&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="1333110857552676288">
  <data key="d0">A small-sized object detection oriented multi-scale feature fusion approach with application to defect detection</data>
  <data key="d1">1333110857552676288</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9720996/</data>
  <data key="d3">A small-sized object detection oriented multi-scale feature fusion approach with application to defect detection</data>
  <data key="d4">N Zeng, P Wu, Z Wang, H Li, W Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">166</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1333110857552676288&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="2870654243860368694">
  <data key="d0">Enhancing geometric factors in model learning and inference for object detection and instance segmentation</data>
  <data key="d1">2870654243860368694</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9523600/</data>
  <data key="d3">Enhancing geometric factors in model learning and inference for object detection and instance segmentation</data>
  <data key="d4">Z Zheng, P Wang, D Ren, W Liu, R Ye…</data>
  <data key="d5">2021</data>
  <data key="d6">367</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2870654243860368694&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14520782850205069410">
  <data key="d0">CNN variants for computer vision: History, architecture, application, challenges and future scope</data>
  <data key="d1">14520782850205069410</data>
  <data key="d2">https://www.mdpi.com/2079-9292/10/20/2470</data>
  <data key="d3">CNN variants for computer vision: History, architecture, application, challenges and future scope</data>
  <data key="d4">D Bhatt, C Patel, H Talsania, J Patel, R Vaghela…</data>
  <data key="d5">2021</data>
  <data key="d6">222</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14520782850205069410&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="8856983795724243302">
  <data key="d0">Defrcn: Decoupled faster r-cnn for few-shot object detection</data>
  <data key="d1">8856983795724243302</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Qiao_DeFRCN_Decoupled_Faster_R-CNN_for_Few-Shot_Object_Detection_ICCV_2021_paper.html</data>
  <data key="d3">Defrcn: Decoupled faster r-cnn for few-shot object detection</data>
  <data key="d4">L Qiao, Y Zhao, Z Li, X Qiu, J Wu…</data>
  <data key="d5">2021</data>
  <data key="d6">138</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8856983795724243302&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="14463958793485183598">
  <data key="d0">Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation</data>
  <data key="d1">14463958793485183598</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Wang_GDR-Net_Geometry-Guided_Direct_Regression_Network_for_Monocular_6D_Object_Pose_CVPR_2021_paper.html</data>
  <data key="d3">Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation</data>
  <data key="d4">G Wang, F Manhardt, F Tombari…</data>
  <data key="d5">2021</data>
  <data key="d6">183</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14463958793485183598&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="11047031223001120005">
  <data key="d0">Gligen: Open-set grounded text-to-image generation</data>
  <data key="d1">11047031223001120005</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Li_GLIGEN_Open-Set_Grounded_Text-to-Image_Generation_CVPR_2023_paper.html</data>
  <data key="d3">Gligen: Open-set grounded text-to-image generation</data>
  <data key="d4">Y Li, H Liu, Q Wu, F Mu, J Yang…</data>
  <data key="d5">2023</data>
  <data key="d6">43</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11047031223001120005&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="10949414717109961618">
  <data key="d0">YOLOv4-5D: An effective and efficient object detector for autonomous driving</data>
  <data key="d1">10949414717109961618</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9374990/</data>
  <data key="d3">YOLOv4-5D: An effective and efficient object detector for autonomous driving</data>
  <data key="d4">Y Cai, T Luan, H Gao, H Wang, L Chen…</data>
  <data key="d5">2021</data>
  <data key="d6">202</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10949414717109961618&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="10684080371797193170">
  <data key="d0">SAR ship detection dataset (SSDD): Official release and comprehensive data analysis</data>
  <data key="d1">10684080371797193170</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/18/3690</data>
  <data key="d3">SAR ship detection dataset (SSDD): Official release and comprehensive data analysis</data>
  <data key="d4">T Zhang, X Zhang, J Li, X Xu, B Wang, X Zhan, Y Xu…</data>
  <data key="d5">2021</data>
  <data key="d6">159</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10684080371797193170&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2381712952938130950">
  <data key="d0">A fast accurate fine-grain object detection model based on YOLOv4 deep neural network</data>
  <data key="d1">2381712952938130950</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00521-021-06651-x</data>
  <data key="d3">A fast accurate fine-grain object detection model based on YOLOv4 deep neural network</data>
  <data key="d4">AM Roy, R Bose, J Bhaduri</data>
  <data key="d5">2022</data>
  <data key="d6">132</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2381712952938130950&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="599070989090902392">
  <data key="d0">Real-time object detection method based on improved YOLOv4-tiny</data>
  <data key="d1">599070989090902392</data>
  <data key="d2">https://arxiv.org/abs/2011.04244</data>
  <data key="d3">Real-time object detection method based on improved YOLOv4-tiny</data>
  <data key="d4">Z Jiang, L Zhao, S Li, Y Jia</data>
  <data key="d5">2020</data>
  <data key="d6">234</data>
  <data key="d7">https://scholar.google.com/scholar?cites=599070989090902392&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="8128999273420313759">
  <data key="d0">State of the art in defect detection based on machine vision</data>
  <data key="d1">8128999273420313759</data>
  <data key="d2">https://link.springer.com/article/10.1007/s40684-021-00343-6</data>
  <data key="d3">State of the art in defect detection based on machine vision</data>
  <data key="d4">Z Ren, F Fang, N Yan, Y Wu</data>
  <data key="d5">2022</data>
  <data key="d6">174</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8128999273420313759&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="6959485123048444799">
  <data key="d0">Comparative analysis of deep learning image detection algorithms</data>
  <data key="d1">6959485123048444799</data>
  <data key="d2">https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00434-w</data>
  <data key="d3">Comparative analysis of deep learning image detection algorithms</data>
  <data key="d4">S Srivastava, AV Divekar…</data>
  <data key="d5">2021</data>
  <data key="d6">168</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6959485123048444799&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="15019162986849498399">
  <data key="d0">Real-time growth stage detection model for high degree of occultation using DenseNet-fused YOLOv4</data>
  <data key="d1">15019162986849498399</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169922000114</data>
  <data key="d3">Real-time growth stage detection model for high degree of occultation using DenseNet-fused YOLOv4</data>
  <data key="d4">AM Roy, J Bhaduri</data>
  <data key="d5">2022</data>
  <data key="d6">90</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15019162986849498399&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="14023415170451607918">
  <data key="d0">Activation functions in deep learning: A comprehensive survey and benchmark</data>
  <data key="d1">14023415170451607918</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0925231222008426</data>
  <data key="d3">Activation functions in deep learning: A comprehensive survey and benchmark</data>
  <data key="d4">SR Dubey, SK Singh, BB Chaudhuri</data>
  <data key="d5">2022</data>
  <data key="d6">177</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14023415170451607918&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2956947656254895097">
  <data key="d0">Medical image segmentation using deep learning: A survey</data>
  <data key="d1">2956947656254895097</data>
  <data key="d2">https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.12419</data>
  <data key="d3">Medical image segmentation using deep learning: A survey</data>
  <data key="d4">R Wang, T Lei, R Cui, B Zhang, H Meng…</data>
  <data key="d5">2022</data>
  <data key="d6">225</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2956947656254895097&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="4134683771649593563">
  <data key="d0">Instant-teaching: An end-to-end semi-supervised object detection framework</data>
  <data key="d1">4134683771649593563</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Instant-Teaching_An_End-to-End_Semi-Supervised_Object_Detection_Framework_CVPR_2021_paper.html</data>
  <data key="d3">Instant-teaching: An end-to-end semi-supervised object detection framework</data>
  <data key="d4">Q Zhou, C Yu, Z Wang, Q Qian…</data>
  <data key="d5">2021</data>
  <data key="d6">123</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4134683771649593563&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9683136153549357939">
  <data key="d0">PP-YOLO: An effective and efficient implementation of object detector</data>
  <data key="d1">9683136153549357939</data>
  <data key="d2">https://arxiv.org/abs/2007.12099</data>
  <data key="d3">PP-YOLO: An effective and efficient implementation of object detector</data>
  <data key="d4">X Long, K Deng, G Wang, Y Zhang, Q Dang…</data>
  <data key="d5">2020</data>
  <data key="d6">226</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9683136153549357939&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9631020688074423843">
  <data key="d0">A real-time detection algorithm for Kiwifruit defects based on YOLOv5</data>
  <data key="d1">9631020688074423843</data>
  <data key="d2">https://www.mdpi.com/2079-9292/10/14/1711</data>
  <data key="d3">A real-time detection algorithm for Kiwifruit defects based on YOLOv5</data>
  <data key="d4">J Yao, J Qi, J Zhang, H Shao, J Yang, X Li</data>
  <data key="d5">2021</data>
  <data key="d6">165</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9631020688074423843&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="12262835944335888935">
  <data key="d0">Deepsocial: Social distancing monitoring and infection risk assessment in covid-19 pandemic</data>
  <data key="d1">12262835944335888935</data>
  <data key="d2">https://www.mdpi.com/2076-3417/10/21/7514</data>
  <data key="d3">Deepsocial: Social distancing monitoring and infection risk assessment in covid-19 pandemic</data>
  <data key="d4">M Rezaei, M Azarmi</data>
  <data key="d5">2020</data>
  <data key="d6">199</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12262835944335888935&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="12301499295205287615">
  <data key="d0">SIoU loss: More powerful learning for bounding box regression</data>
  <data key="d1">12301499295205287615</data>
  <data key="d2">https://arxiv.org/abs/2205.12740</data>
  <data key="d3">SIoU loss: More powerful learning for bounding box regression</data>
  <data key="d4">Z Gevorgyan</data>
  <data key="d5">2022</data>
  <data key="d6">194</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12301499295205287615&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="4438492435191836535">
  <data key="d0">Real-time detection of uneaten feed pellets in underwater images for aquaculture using an improved YOLO-V4 network</data>
  <data key="d1">4438492435191836535</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169921001538</data>
  <data key="d3">Real-time detection of uneaten feed pellets in underwater images for aquaculture using an improved YOLO-V4 network</data>
  <data key="d4">X Hu, Y Liu, Z Zhao, J Liu, X Yang, C Sun…</data>
  <data key="d5">2021</data>
  <data key="d6">131</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4438492435191836535&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="13337861536124971139">
  <data key="d0">Pavement distress detection using convolutional neural networks with images captured via UAV</data>
  <data key="d1">13337861536124971139</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0926580521004428</data>
  <data key="d3">Pavement distress detection using convolutional neural networks with images captured via UAV</data>
  <data key="d4">J Zhu, J Zhong, T Ma, X Huang, W Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">90</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13337861536124971139&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="12863020002569949671">
  <data key="d0">Channel pruned YOLO V5s-based deep learning approach for rapid and accurate apple fruitlet detection before fruit thinning</data>
  <data key="d1">12863020002569949671</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1537511021001999</data>
  <data key="d3">Channel pruned YOLO V5s-based deep learning approach for rapid and accurate apple fruitlet detection before fruit thinning</data>
  <data key="d4">D Wang, D He</data>
  <data key="d5">2021</data>
  <data key="d6">115</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12863020002569949671&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="3656418176994524008">
  <data key="d0">Object detection using YOLO: Challenges, architectural successors, datasets and applications</data>
  <data key="d1">3656418176994524008</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11042-022-13644-y</data>
  <data key="d3">Object detection using YOLO: Challenges, architectural successors, datasets and applications</data>
  <data key="d4">T Diwan, G Anirudh, JV Tembhurne</data>
  <data key="d5">2023</data>
  <data key="d6">135</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3656418176994524008&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2452866517197292093">
  <data key="d0">A comprehensive survey on pretrained foundation models: A history from bert to chatgpt</data>
  <data key="d1">2452866517197292093</data>
  <data key="d2">https://arxiv.org/abs/2302.09419</data>
  <data key="d3">A comprehensive survey on pretrained foundation models: A history from bert to chatgpt</data>
  <data key="d4">C Zhou, Q Li, C Li, J Yu, Y Liu, G Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">118</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2452866517197292093&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="15799285439221317870">
  <data key="d0">LLVIP: A visible-infrared paired dataset for low-light vision</data>
  <data key="d1">15799285439221317870</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021W/RLQ/html/Jia_LLVIP_A_Visible-Infrared_Paired_Dataset_for_Low-Light_Vision_ICCVW_2021_paper.html</data>
  <data key="d3">LLVIP: A visible-infrared paired dataset for low-light vision</data>
  <data key="d4">X Jia, C Zhu, M Li, W Tang…</data>
  <data key="d5">2021</data>
  <data key="d6">117</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15799285439221317870&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9207765007843851263">
  <data key="d0">Point-cloud based 3D object detection and classification methods for self-driving applications: A survey and taxonomy</data>
  <data key="d1">9207765007843851263</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1566253520304097</data>
  <data key="d3">Point-cloud based 3D object detection and classification methods for self-driving applications: A survey and taxonomy</data>
  <data key="d4">D Fernandes, A Silva, R Névoa, C Simões…</data>
  <data key="d5">2021</data>
  <data key="d6">132</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9207765007843851263&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="6026250830209566504">
  <data key="d0">CCTSDB 2021: a more comprehensive traffic sign detection benchmark</data>
  <data key="d1">6026250830209566504</data>
  <data key="d2">https://centaur.reading.ac.uk/106129</data>
  <data key="d3">CCTSDB 2021: a more comprehensive traffic sign detection benchmark</data>
  <data key="d4">J Zhang, X Zou, LD Kuang, J Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">69</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6026250830209566504&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="11882550127852592683">
  <data key="d0">Real-time polyp detection, localization and segmentation in colonoscopy using deep learning</data>
  <data key="d1">11882550127852592683</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9369308/</data>
  <data key="d3">Real-time polyp detection, localization and segmentation in colonoscopy using deep learning</data>
  <data key="d4">D Jha, S Ali, NK Tomar, HD Johansen…</data>
  <data key="d5">2021</data>
  <data key="d6">170</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11882550127852592683&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9014029637884314032">
  <data key="d0">Automatic bunch detection in white grape varieties using YOLOv3, YOLOv4, and YOLOv5 deep learning algorithms</data>
  <data key="d1">9014029637884314032</data>
  <data key="d2">https://www.mdpi.com/2073-4395/12/2/319</data>
  <data key="d3">Automatic bunch detection in white grape varieties using YOLOv3, YOLOv4, and YOLOv5 deep learning algorithms</data>
  <data key="d4">M Sozzi, S Cantalamessa, A Cogato, A Kayad…</data>
  <data key="d5">2022</data>
  <data key="d6">102</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9014029637884314032&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="3909123747085405380">
  <data key="d0">Twin adversarial contrastive learning for underwater image enhancement and beyond</data>
  <data key="d1">3909123747085405380</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9832540/</data>
  <data key="d3">Twin adversarial contrastive learning for underwater image enhancement and beyond</data>
  <data key="d4">R Liu, Z Jiang, S Yang, X Fan</data>
  <data key="d5">2022</data>
  <data key="d6">82</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3909123747085405380&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="15281565077085720279">
  <data key="d0">Image-adaptive YOLO for object detection in adverse weather conditions</data>
  <data key="d1">15281565077085720279</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/20072</data>
  <data key="d3">Image-adaptive YOLO for object detection in adverse weather conditions</data>
  <data key="d4">W Liu, G Ren, R Yu, S Guo, J Zhu…</data>
  <data key="d5">2022</data>
  <data key="d6">115</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15281565077085720279&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="14672720829595281606">
  <data key="d0">ViT-YOLO: Transformer-based YOLO for object detection</data>
  <data key="d1">14672720829595281606</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Zhang_ViT-YOLOTransformer-Based_YOLO_for_Object_Detection_ICCVW_2021_paper.html</data>
  <data key="d3">ViT-YOLO: Transformer-based YOLO for object detection</data>
  <data key="d4">Z Zhang, X Lu, G Cao, Y Yang…</data>
  <data key="d5">2021</data>
  <data key="d6">87</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14672720829595281606&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="7222787612695411352">
  <data key="d0">A review on modern defect detection models using DCNNs–Deep convolutional neural networks</data>
  <data key="d1">7222787612695411352</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2090123221000643</data>
  <data key="d3">A review on modern defect detection models using DCNNs–Deep convolutional neural networks</data>
  <data key="d4">AA Tulbure, AA Tulbure, EH Dulf</data>
  <data key="d5">2022</data>
  <data key="d6">118</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7222787612695411352&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2241015688519496377">
  <data key="d0">Deep learning-based object detection in low-altitude UAV datasets: A survey</data>
  <data key="d1">2241015688519496377</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0262885620301785</data>
  <data key="d3">Deep learning-based object detection in low-altitude UAV datasets: A survey</data>
  <data key="d4">P Mittal, R Singh, A Sharma</data>
  <data key="d5">2020</data>
  <data key="d6">150</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2241015688519496377&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="15226748689642581218">
  <data key="d0">Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion</data>
  <data key="d1">15226748689642581218</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.html</data>
  <data key="d3">Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion</data>
  <data key="d4">Z Zhao, H Bai, J Zhang, Y Zhang, S Xu…</data>
  <data key="d5">2023</data>
  <data key="d6">27</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15226748689642581218&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="16799196534333096171">
  <data key="d0">A vision-based social distancing and critical density detection system for COVID-19</data>
  <data key="d1">16799196534333096171</data>
  <data key="d2">https://www.mdpi.com/1424-8220/21/13/4608</data>
  <data key="d3">A vision-based social distancing and critical density detection system for COVID-19</data>
  <data key="d4">D Yang, E Yurtsever, V Renganathan, KA Redmill…</data>
  <data key="d5">2021</data>
  <data key="d6">174</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16799196534333096171&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9519448529520784463">
  <data key="d0">YOLO5Face: why reinventing a face detector</data>
  <data key="d1">9519448529520784463</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-25072-9_15</data>
  <data key="d3">YOLO5Face: why reinventing a face detector</data>
  <data key="d4">D Qi, W Tan, Q Yao, J Liu</data>
  <data key="d5">2022</data>
  <data key="d6">98</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9519448529520784463&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="8269996013982993565">
  <data key="d0">A detection algorithm for cherry fruits based on the improved YOLO-v4 model</data>
  <data key="d1">8269996013982993565</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00521-021-06029-z</data>
  <data key="d3">A detection algorithm for cherry fruits based on the improved YOLO-v4 model</data>
  <data key="d4">R Gai, N Chen, H Yuan</data>
  <data key="d5">2023</data>
  <data key="d6">105</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8269996013982993565&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="12292417492888691772">
  <data key="d0">Computing systems for autonomous driving: State of the art and challenges</data>
  <data key="d1">12292417492888691772</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9288755/</data>
  <data key="d3">Computing systems for autonomous driving: State of the art and challenges</data>
  <data key="d4">L Liu, S Lu, R Zhong, B Wu, Y Yao…</data>
  <data key="d5">2020</data>
  <data key="d6">167</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12292417492888691772&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="3469053983010224891">
  <data key="d0">Detection of concealed cracks from ground penetrating radar images based on deep learning algorithm</data>
  <data key="d1">3469053983010224891</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0950061820339532</data>
  <data key="d3">Detection of concealed cracks from ground penetrating radar images based on deep learning algorithm</data>
  <data key="d4">S Li, X Gu, X Xu, D Xu, T Zhang, Z Liu…</data>
  <data key="d5">2021</data>
  <data key="d6">133</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3469053983010224891&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="15326684253758327032">
  <data key="d0">VisDrone-DET2021: The vision meets drone object detection challenge results</data>
  <data key="d1">15326684253758327032</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Cao_VisDrone-DET2021_The_Vision_Meets_Drone_Object_Detection_Challenge_Results_ICCVW_2021_paper.html</data>
  <data key="d3">VisDrone-DET2021: The vision meets drone object detection challenge results</data>
  <data key="d4">Y Cao, Z He, L Wang, W Wang…</data>
  <data key="d5">2021</data>
  <data key="d6">76</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15326684253758327032&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="11839204226610748442">
  <data key="d0">A wheat spike detection method in UAV images based on improved YOLOv5</data>
  <data key="d1">11839204226610748442</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/16/3095</data>
  <data key="d3">A wheat spike detection method in UAV images based on improved YOLOv5</data>
  <data key="d4">J Zhao, X Zhang, J Yan, X Qiu, X Yao, Y Tian, Y Zhu…</data>
  <data key="d5">2021</data>
  <data key="d6">102</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11839204226610748442&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="7244644999756957350">
  <data key="d0">Improving YOLOv5 with attention mechanism for detecting boulders from planetary images</data>
  <data key="d1">7244644999756957350</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/18/3776</data>
  <data key="d3">Improving YOLOv5 with attention mechanism for detecting boulders from planetary images</data>
  <data key="d4">L Zhu, X Geng, Z Li, C Liu</data>
  <data key="d5">2021</data>
  <data key="d6">100</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7244644999756957350&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="15430727510373100145">
  <data key="d0">Occupant-density-detection based energy efficient ventilation system: Prevention of infection transmission</data>
  <data key="d1">15430727510373100145</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0378778821001675</data>
  <data key="d3">Occupant-density-detection based energy efficient ventilation system: Prevention of infection transmission</data>
  <data key="d4">J Wang, J Huang, Z Feng, SJ Cao, F Haghighat</data>
  <data key="d5">2021</data>
  <data key="d6">113</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15430727510373100145&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2484140027818923745">
  <data key="d0">A dual weighting label assignment scheme for object detection</data>
  <data key="d1">2484140027818923745</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Li_A_Dual_Weighting_Label_Assignment_Scheme_for_Object_Detection_CVPR_2022_paper.html</data>
  <data key="d3">A dual weighting label assignment scheme for object detection</data>
  <data key="d4">S Li, C He, R Li, L Zhang</data>
  <data key="d5">2022</data>
  <data key="d6">44</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2484140027818923745&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="10402998279197599439">
  <data key="d0">Hallucination improves few-shot object detection</data>
  <data key="d1">10402998279197599439</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Hallucination_Improves_Few-Shot_Object_Detection_CVPR_2021_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">Hallucination improves few-shot object detection</data>
  <data key="d4">W Zhang, YX Wang</data>
  <data key="d5">2021</data>
  <data key="d6">81</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10402998279197599439&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="16564571492344788038">
  <data key="d0">Implementing a real-time, AI-based, people detection and social distancing measuring system for Covid-19</data>
  <data key="d1">16564571492344788038</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11554-021-01070-6</data>
  <data key="d3">Implementing a real-time, AI-based, people detection and social distancing measuring system for Covid-19</data>
  <data key="d4">S Saponara, A Elhanashi, A Gagliardi</data>
  <data key="d5">2021</data>
  <data key="d6">118</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16564571492344788038&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="17634164049223670343">
  <data key="d0">Cross‐scene pavement distress detection by a novel transfer learning framework</data>
  <data key="d1">17634164049223670343</data>
  <data key="d2">https://onlinelibrary.wiley.com/doi/abs/10.1111/mice.12674</data>
  <data key="d3">Cross‐scene pavement distress detection by a novel transfer learning framework</data>
  <data key="d4">Y Li, P Che, C Liu, D Wu, Y Du</data>
  <data key="d5">2021</data>
  <data key="d6">71</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17634164049223670343&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="19561195188667465">
  <data key="d0">WilDect-YOLO: An efficient and robust computer vision-based accurate object localization model for automated endangered wildlife detection</data>
  <data key="d1">19561195188667465</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1574954122003697</data>
  <data key="d3">WilDect-YOLO: An efficient and robust computer vision-based accurate object localization model for automated endangered wildlife detection</data>
  <data key="d4">AM Roy, J Bhaduri, T Kumar, K Raj</data>
  <data key="d5">2023</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=19561195188667465&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="15167698914301610121">
  <data key="d0">A novel spatio-temporal synchronization method of roadside asynchronous MMW radar-camera for sensor fusion</data>
  <data key="d1">15167698914301610121</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9590496/</data>
  <data key="d3">A novel spatio-temporal synchronization method of roadside asynchronous MMW radar-camera for sensor fusion</data>
  <data key="d4">Y Du, B Qin, C Zhao, Y Zhu, J Cao…</data>
  <data key="d5">2021</data>
  <data key="d6">81</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15167698914301610121&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9817510568104192073">
  <data key="d0">Advanced feature extraction and selection approach using deep learning and Aquila optimizer for IoT intrusion detection system</data>
  <data key="d1">9817510568104192073</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/1/140</data>
  <data key="d3">Advanced feature extraction and selection approach using deep learning and Aquila optimizer for IoT intrusion detection system</data>
  <data key="d4">A Fatani, A Dahou, MAA Al-Qaness, S Lu, MA Elaziz</data>
  <data key="d5">2021</data>
  <data key="d6">75</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9817510568104192073&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9665171368816644761">
  <data key="d0">The lottery tickets hypothesis for supervised and self-supervised pre-training in computer vision models</data>
  <data key="d1">9665171368816644761</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Chen_The_Lottery_Tickets_Hypothesis_for_Supervised_and_Self-Supervised_Pre-Training_in_CVPR_2021_paper.html</data>
  <data key="d3">The lottery tickets hypothesis for supervised and self-supervised pre-training in computer vision models</data>
  <data key="d4">T Chen, J Frankle, S Chang, S Liu…</data>
  <data key="d5">2021</data>
  <data key="d6">101</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9665171368816644761&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="15329030602586300416">
  <data key="d0">A deep learning enabled multi-class plant disease detection model based on computer vision</data>
  <data key="d1">15329030602586300416</data>
  <data key="d2">https://www.mdpi.com/2673-2688/2/3/26</data>
  <data key="d3">A deep learning enabled multi-class plant disease detection model based on computer vision</data>
  <data key="d4">AM Roy, J Bhaduri</data>
  <data key="d5">2021</data>
  <data key="d6">83</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15329030602586300416&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="7085275218926344408">
  <data key="d0">VisDrone-DET2019: The vision meets drone object detection in image challenge results</data>
  <data key="d1">7085275218926344408</data>
  <data key="d2">http://openaccess.thecvf.com/content_ICCVW_2019/html/VISDrone/Du_VisDrone-DET2019_The_Vision_Meets_Drone_Object_Detection_in_Image_Challenge_ICCVW_2019_paper.html</data>
  <data key="d3">VisDrone-DET2019: The vision meets drone object detection in image challenge results</data>
  <data key="d4">D Du, P Zhu, L Wen, X Bian, H Lin…</data>
  <data key="d5">2019</data>
  <data key="d6">188</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7085275218926344408&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2700093867622300626">
  <data key="d0">Automatic detection of pothole distress in asphalt pavement using improved convolutional neural networks</data>
  <data key="d1">2700093867622300626</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/16/3892</data>
  <data key="d3">Automatic detection of pothole distress in asphalt pavement using improved convolutional neural networks</data>
  <data key="d4">D Wang, Z Liu, X Gu, W Wu, Y Chen, L Wang</data>
  <data key="d5">2022</data>
  <data key="d6">44</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2700093867622300626&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="12979999094083269078">
  <data key="d0">Deep learning-based detection from the perspective of small or tiny objects: A survey</data>
  <data key="d1">12979999094083269078</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0262885622001007</data>
  <data key="d3">Deep learning-based detection from the perspective of small or tiny objects: A survey</data>
  <data key="d4">K Tong, Y Wu</data>
  <data key="d5">2022</data>
  <data key="d6">49</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12979999094083269078&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="12527627239009594881">
  <data key="d0">Vision-based robotic grasping from object localization, object pose estimation to grasp estimation for parallel grippers: a review</data>
  <data key="d1">12527627239009594881</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10462-020-09888-5</data>
  <data key="d3">Vision-based robotic grasping from object localization, object pose estimation to grasp estimation for parallel grippers: a review</data>
  <data key="d4">G Du, K Wang, S Lian, K Zhao</data>
  <data key="d5">2021</data>
  <data key="d6">259</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12527627239009594881&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="16290848704501469564">
  <data key="d0">PP-YOLOv2: A practical object detector</data>
  <data key="d1">16290848704501469564</data>
  <data key="d2">https://arxiv.org/abs/2104.10419</data>
  <data key="d3">PP-YOLOv2: A practical object detector</data>
  <data key="d4">X Huang, X Wang, W Lv, X Bai, X Long, K Deng…</data>
  <data key="d5">2021</data>
  <data key="d6">99</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16290848704501469564&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9060832344926997444">
  <data key="d0">A survey of computer-aided diagnosis of lung nodules from CT scans using deep learning</data>
  <data key="d1">9060832344926997444</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0010482521006004</data>
  <data key="d3">A survey of computer-aided diagnosis of lung nodules from CT scans using deep learning</data>
  <data key="d4">Y Gu, J Chi, J Liu, L Yang, B Zhang, D Yu…</data>
  <data key="d5">2021</data>
  <data key="d6">74</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9060832344926997444&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="1659700362637596040">
  <data key="d0">Pvt v2: Improved baselines with pyramid vision transformer</data>
  <data key="d1">1659700362637596040</data>
  <data key="d2">https://link.springer.com/article/10.1007/s41095-022-0274-8</data>
  <data key="d3">Pvt v2: Improved baselines with pyramid vision transformer</data>
  <data key="d4">W Wang, E Xie, X Li, DP Fan, K Song, D Liang…</data>
  <data key="d5">2022</data>
  <data key="d6">626</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1659700362637596040&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="8662214348531793431">
  <data key="d0">A review on 2D instance segmentation based on deep neural networks</data>
  <data key="d1">8662214348531793431</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0262885622000300</data>
  <data key="d3">A review on 2D instance segmentation based on deep neural networks</data>
  <data key="d4">W Gu, S Bai, L Kong</data>
  <data key="d5">2022</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8662214348531793431&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="17314410080623672897">
  <data key="d0">Fairmot: On the fairness of detection and re-identification in multiple object tracking</data>
  <data key="d1">17314410080623672897</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11263-021-01513-4</data>
  <data key="d3">Fairmot: On the fairness of detection and re-identification in multiple object tracking</data>
  <data key="d4">Y Zhang, C Wang, X Wang, W Zeng, W Liu</data>
  <data key="d5">2021</data>
  <data key="d6">810</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17314410080623672897&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="10274815087550872131">
  <data key="d0">Transtrack: Multiple object tracking with transformer</data>
  <data key="d1">10274815087550872131</data>
  <data key="d2">https://arxiv.org/abs/2012.15460</data>
  <data key="d3">Transtrack: Multiple object tracking with transformer</data>
  <data key="d4">P Sun, J Cao, Y Jiang, R Zhang, E Xie, Z Yuan…</data>
  <data key="d5">2020</data>
  <data key="d6">361</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10274815087550872131&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="8124073977598762954">
  <data key="d0">Detco: Unsupervised contrastive learning for object detection</data>
  <data key="d1">8124073977598762954</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Xie_DetCo_Unsupervised_Contrastive_Learning_for_Object_Detection_ICCV_2021_paper.html</data>
  <data key="d3">Detco: Unsupervised contrastive learning for object detection</data>
  <data key="d4">E Xie, J Ding, W Wang, X Zhan, H Xu…</data>
  <data key="d5">2021</data>
  <data key="d6">257</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8124073977598762954&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="14069730115218722114">
  <data key="d0">Dynamic detr: End-to-end object detection with dynamic attention</data>
  <data key="d1">14069730115218722114</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021/html/Dai_Dynamic_DETR_End-to-End_Object_Detection_With_Dynamic_Attention_ICCV_2021_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">Dynamic detr: End-to-end object detection with dynamic attention</data>
  <data key="d4">X Dai, Y Chen, J Yang, P Zhang…</data>
  <data key="d5">2021</data>
  <data key="d6">145</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14069730115218722114&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="977587388012773361">
  <data key="d0">Anchor detr: Query design for transformer-based detector</data>
  <data key="d1">977587388012773361</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/20158</data>
  <data key="d3">Anchor detr: Query design for transformer-based detector</data>
  <data key="d4">Y Wang, X Zhang, T Yang, J Sun</data>
  <data key="d5">2022</data>
  <data key="d6">171</data>
  <data key="d7">https://scholar.google.com/scholar?cites=977587388012773361&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="15814018180296500902">
  <data key="d0">Mpvit: Multi-path vision transformer for dense prediction</data>
  <data key="d1">15814018180296500902</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2022/html/Lee_MPViT_Multi-Path_Vision_Transformer_for_Dense_Prediction_CVPR_2022_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">Mpvit: Multi-path vision transformer for dense prediction</data>
  <data key="d4">Y Lee, J Kim, J Willette…</data>
  <data key="d5">2022</data>
  <data key="d6">114</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15814018180296500902&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="17436807480659026891">
  <data key="d0">Instances as queries</data>
  <data key="d1">17436807480659026891</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Fang_Instances_As_Queries_ICCV_2021_paper.html</data>
  <data key="d3">Instances as queries</data>
  <data key="d4">Y Fang, S Yang, X Wang, Y Li, C Fang…</data>
  <data key="d5">2021</data>
  <data key="d6">177</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17436807480659026891&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="7616713930375867128">
  <data key="d0">Global tracking transformers</data>
  <data key="d1">7616713930375867128</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Global_Tracking_Transformers_CVPR_2022_paper.html</data>
  <data key="d3">Global tracking transformers</data>
  <data key="d4">X Zhou, T Yin, V Koltun…</data>
  <data key="d5">2022</data>
  <data key="d6">65</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7616713930375867128&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="17891879498080154736">
  <data key="d0">Efficient training of visual transformers with small datasets</data>
  <data key="d1">17891879498080154736</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/c81e155d85dae5430a8cee6f2242e82c-Abstract.html</data>
  <data key="d3">Efficient training of visual transformers with small datasets</data>
  <data key="d4">Y Liu, E Sangineto, W Bi, N Sebe…</data>
  <data key="d5">2021</data>
  <data key="d6">111</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17891879498080154736&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="8825090207674893811">
  <data key="d0">Crossformer++: A versatile vision transformer hinging on cross-scale attention</data>
  <data key="d1">8825090207674893811</data>
  <data key="d2">https://arxiv.org/abs/2303.06908</data>
  <data key="d3">Crossformer++: A versatile vision transformer hinging on cross-scale attention</data>
  <data key="d4">W Wang, W Chen, Q Qiu, L Chen, B Wu, B Lin…</data>
  <data key="d5">2023</data>
  <data key="d6">175</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8825090207674893811&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="11825390691537168558">
  <data key="d0">Visformer: The vision-friendly transformer</data>
  <data key="d1">11825390691537168558</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Chen_Visformer_The_Vision-Friendly_Transformer_ICCV_2021_paper.html</data>
  <data key="d3">Visformer: The vision-friendly transformer</data>
  <data key="d4">Z Chen, L Xie, J Niu, X Liu, L Wei…</data>
  <data key="d5">2021</data>
  <data key="d6">128</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11825390691537168558&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="4586155361346152499">
  <data key="d0">Shuffle transformer: Rethinking spatial shuffle for vision transformer</data>
  <data key="d1">4586155361346152499</data>
  <data key="d2">https://arxiv.org/abs/2106.03650</data>
  <data key="d3">Shuffle transformer: Rethinking spatial shuffle for vision transformer</data>
  <data key="d4">Z Huang, Y Ben, G Luo, P Cheng, G Yu…</data>
  <data key="d5">2021</data>
  <data key="d6">131</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4586155361346152499&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="9894263145711509588">
  <data key="d0">Wave-vit: Unifying wavelet and transformers for visual representation learning</data>
  <data key="d1">9894263145711509588</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19806-9_19</data>
  <data key="d3">Wave-vit: Unifying wavelet and transformers for visual representation learning</data>
  <data key="d4">T Yao, Y Pan, Y Li, CW Ngo, T Mei</data>
  <data key="d5">2022</data>
  <data key="d6">45</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9894263145711509588&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="1322906163224921925">
  <data key="d0">Cyclemlp: A mlp-like architecture for dense prediction</data>
  <data key="d1">1322906163224921925</data>
  <data key="d2">https://arxiv.org/abs/2107.10224</data>
  <data key="d3">Cyclemlp: A mlp-like architecture for dense prediction</data>
  <data key="d4">S Chen, E Xie, C Ge, R Chen, D Liang…</data>
  <data key="d5">2021</data>
  <data key="d6">171</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1322906163224921925&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="10270081680505767341">
  <data key="d0">Adamixer: A fast-converging query-based object detector</data>
  <data key="d1">10270081680505767341</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Gao_AdaMixer_A_Fast-Converging_Query-Based_Object_Detector_CVPR_2022_paper.html</data>
  <data key="d3">Adamixer: A fast-converging query-based object detector</data>
  <data key="d4">Z Gao, L Wang, B Han, S Guo</data>
  <data key="d5">2022</data>
  <data key="d6">56</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10270081680505767341&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="1534689713476232636">
  <data key="d0">As-mlp: An axial shifted mlp architecture for vision</data>
  <data key="d1">1534689713476232636</data>
  <data key="d2">https://arxiv.org/abs/2107.08391</data>
  <data key="d3">As-mlp: An axial shifted mlp architecture for vision</data>
  <data key="d4">D Lian, Z Yu, X Sun, S Gao</data>
  <data key="d5">2021</data>
  <data key="d6">137</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1534689713476232636&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="1528825781662204302">
  <data key="d0">Oriented reppoints for aerial object detection</data>
  <data key="d1">1528825781662204302</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Li_Oriented_RepPoints_for_Aerial_Object_Detection_CVPR_2022_paper.html</data>
  <data key="d3">Oriented reppoints for aerial object detection</data>
  <data key="d4">W Li, Y Chen, K Hu, J Zhu</data>
  <data key="d5">2022</data>
  <data key="d6">110</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1528825781662204302&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="1852377411269249881">
  <data key="d0">Solq: Segmenting objects by learning queries</data>
  <data key="d1">1852377411269249881</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/b7087c1f4f89e63af8d46f3b20271153-Abstract.html</data>
  <data key="d3">Solq: Segmenting objects by learning queries</data>
  <data key="d4">B Dong, F Zeng, T Wang, X Zhang…</data>
  <data key="d5">2021</data>
  <data key="d6">79</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1852377411269249881&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="2803945007747990107">
  <data key="d0">Sparse instance activation for real-time instance segmentation</data>
  <data key="d1">2803945007747990107</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Sparse_Instance_Activation_for_Real-Time_Instance_Segmentation_CVPR_2022_paper.html</data>
  <data key="d3">Sparse instance activation for real-time instance segmentation</data>
  <data key="d4">T Cheng, X Wang, S Chen, W Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">50</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2803945007747990107&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="15824157200018556836">
  <data key="d0">Language as queries for referring video object segmentation</data>
  <data key="d1">15824157200018556836</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Wu_Language_As_Queries_for_Referring_Video_Object_Segmentation_CVPR_2022_paper.html</data>
  <data key="d3">Language as queries for referring video object segmentation</data>
  <data key="d4">J Wu, Y Jiang, P Sun, Z Yuan…</data>
  <data key="d5">2022</data>
  <data key="d6">60</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15824157200018556836&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="16816930780976029141">
  <data key="d0">Grounding dino: Marrying dino with grounded pre-training for open-set object detection</data>
  <data key="d1">16816930780976029141</data>
  <data key="d2">https://arxiv.org/abs/2303.05499</data>
  <data key="d3">Grounding dino: Marrying dino with grounded pre-training for open-set object detection</data>
  <data key="d4">S Liu, Z Zeng, T Ren, F Li, H Zhang, J Yang…</data>
  <data key="d5">2023</data>
  <data key="d6">91</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16816930780976029141&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="9209014176820981155">
  <data key="d0">Spatially consistent representation learning</data>
  <data key="d1">9209014176820981155</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Roh_Spatially_Consistent_Representation_Learning_CVPR_2021_paper.html</data>
  <data key="d3">Spatially consistent representation learning</data>
  <data key="d4">B Roh, W Shin, I Kim, S Kim</data>
  <data key="d5">2021</data>
  <data key="d6">84</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9209014176820981155&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="5598507831422168925">
  <data key="d0">Dual cross-attention learning for fine-grained visual categorization and object re-identification</data>
  <data key="d1">5598507831422168925</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Dual_Cross-Attention_Learning_for_Fine-Grained_Visual_Categorization_and_Object_Re-Identification_CVPR_2022_paper.html</data>
  <data key="d3">Dual cross-attention learning for fine-grained visual categorization and object re-identification</data>
  <data key="d4">H Zhu, W Ke, D Li, J Liu, L Tian…</data>
  <data key="d5">2022</data>
  <data key="d6">53</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5598507831422168925&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="17182921757850029040">
  <data key="d0">What makes for end-to-end object detection?</data>
  <data key="d1">17182921757850029040</data>
  <data key="d2">https://proceedings.mlr.press/v139/sun21b.html?ref=https://githubhelp.com</data>
  <data key="d3">What makes for end-to-end object detection?</data>
  <data key="d4">P Sun, Y Jiang, E Xie, W Shao, Z Yuan…</data>
  <data key="d5">2021</data>
  <data key="d6">94</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17182921757850029040&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="17304148947425718239">
  <data key="d0">Seqformer: Sequential transformer for video instance segmentation</data>
  <data key="d1">17304148947425718239</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19815-1_32</data>
  <data key="d3">Seqformer: Sequential transformer for video instance segmentation</data>
  <data key="d4">J Wu, Y Jiang, S Bai, W Zhang, X Bai</data>
  <data key="d5">2022</data>
  <data key="d6">48</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17304148947425718239&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="9105314878135623559">
  <data key="d0">Sgtr: End-to-end scene graph generation with transformer</data>
  <data key="d1">9105314878135623559</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Li_SGTR_End-to-End_Scene_Graph_Generation_With_Transformer_CVPR_2022_paper.html</data>
  <data key="d3">Sgtr: End-to-end scene graph generation with transformer</data>
  <data key="d4">R Li, S Zhang, X He</data>
  <data key="d5">2022</data>
  <data key="d6">42</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9105314878135623559&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="15674075284060635644">
  <data key="d0">GraphFPN: Graph feature pyramid network for object detection</data>
  <data key="d1">15674075284060635644</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Zhao_GraphFPN_Graph_Feature_Pyramid_Network_for_Object_Detection_ICCV_2021_paper.html</data>
  <data key="d3">GraphFPN: Graph feature pyramid network for object detection</data>
  <data key="d4">G Zhao, W Ge, Y Yu</data>
  <data key="d5">2021</data>
  <data key="d6">52</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15674075284060635644&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="5425241495538385765">
  <data key="d0">Dual vision transformer</data>
  <data key="d1">5425241495538385765</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10105499/</data>
  <data key="d3">Dual vision transformer</data>
  <data key="d4">T Yao, Y Li, Y Pan, Y Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">34</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5425241495538385765&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="8214441654299017644">
  <data key="d0">An improved swin transformer-based model for remote sensing object detection and instance segmentation</data>
  <data key="d1">8214441654299017644</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/23/4779</data>
  <data key="d3">An improved swin transformer-based model for remote sensing object detection and instance segmentation</data>
  <data key="d4">X Xu, Z Feng, C Cao, M Li, J Wu, Z Wu, Y Shang, S Ye</data>
  <data key="d5">2021</data>
  <data key="d6">62</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8214441654299017644&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="16490222600712608071">
  <data key="d0">RFLA: Gaussian receptive field based label assignment for tiny object detection</data>
  <data key="d1">16490222600712608071</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20077-9_31</data>
  <data key="d3">RFLA: Gaussian receptive field based label assignment for tiny object detection</data>
  <data key="d4">C Xu, J Wang, W Yang, H Yu, L Yu, GS Xia</data>
  <data key="d5">2022</data>
  <data key="d6">31</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16490222600712608071&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="12517757346728435037">
  <data key="d0">Coda: A real-world road corner case dataset for object detection in autonomous driving</data>
  <data key="d1">12517757346728435037</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19839-7_24</data>
  <data key="d3">Coda: A real-world road corner case dataset for object detection in autonomous driving</data>
  <data key="d4">K Li, K Chen, H Wang, L Hong, C Ye, J Han…</data>
  <data key="d5">2022</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12517757346728435037&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="16726572534871887028">
  <data key="d0">Vision-centric bev perception: A survey</data>
  <data key="d1">16726572534871887028</data>
  <data key="d2">https://arxiv.org/abs/2208.02797</data>
  <data key="d3">Vision-centric bev perception: A survey</data>
  <data key="d4">Y Ma, T Wang, X Bai, H Yang, Y Hou, Y Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">40</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16726572534871887028&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="17039945862035123188">
  <data key="d0">Progressive end-to-end object detection in crowded scenes</data>
  <data key="d1">17039945862035123188</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zheng_Progressive_End-to-End_Object_Detection_in_Crowded_Scenes_CVPR_2022_paper.html</data>
  <data key="d3">Progressive end-to-end object detection in crowded scenes</data>
  <data key="d4">A Zheng, Y Zhang, X Zhang, X Qi…</data>
  <data key="d5">2022</data>
  <data key="d6">27</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17039945862035123188&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="16118065418965645421">
  <data key="d0">Yolo-firi: Improved yolov5 for infrared image object detection</data>
  <data key="d1">16118065418965645421</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9576741/</data>
  <data key="d3">Yolo-firi: Improved yolov5 for infrared image object detection</data>
  <data key="d4">S Li, Y Li, Y Li, M Li, X Xu</data>
  <data key="d5">2021</data>
  <data key="d6">85</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16118065418965645421&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="10812290025544152705">
  <data key="d0">More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity</data>
  <data key="d1">10812290025544152705</data>
  <data key="d2">https://arxiv.org/abs/2207.03620</data>
  <data key="d3">More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity</data>
  <data key="d4">S Liu, T Chen, X Chen, X Chen, Q Xiao, B Wu…</data>
  <data key="d5">2022</data>
  <data key="d6">54</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10812290025544152705&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="18413127590718653617">
  <data key="d0">Towards data-efficient detection transformers</data>
  <data key="d1">18413127590718653617</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20077-9_6</data>
  <data key="d3">Towards data-efficient detection transformers</data>
  <data key="d4">W Wang, J Zhang, Y Cao, Y Shen, D Tao</data>
  <data key="d5">2022</data>
  <data key="d6">39</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18413127590718653617&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="6989674085539462525">
  <data key="d0">Swintextspotter: Scene text spotting via better synergy between text detection and text recognition</data>
  <data key="d1">6989674085539462525</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Huang_SwinTextSpotter_Scene_Text_Spotting_via_Better_Synergy_Between_Text_Detection_CVPR_2022_paper.html</data>
  <data key="d3">Swintextspotter: Scene text spotting via better synergy between text detection and text recognition</data>
  <data key="d4">M Huang, Y Liu, Z Peng, C Liu, D Lin…</data>
  <data key="d5">2022</data>
  <data key="d6">42</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6989674085539462525&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="12121457393934602812">
  <data key="d0">Temporally efficient vision transformer for video instance segmentation</data>
  <data key="d1">12121457393934602812</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Yang_Temporally_Efficient_Vision_Transformer_for_Video_Instance_Segmentation_CVPR_2022_paper.html</data>
  <data key="d3">Temporally efficient vision transformer for video instance segmentation</data>
  <data key="d4">S Yang, X Wang, Y Li, Y Fang, J Fang…</data>
  <data key="d5">2022</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12121457393934602812&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="4303557091610097560">
  <data key="d0">Dynamic graph message passing networks</data>
  <data key="d1">4303557091610097560</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Dynamic_Graph_Message_Passing_Networks_CVPR_2020_paper.html</data>
  <data key="d3">Dynamic graph message passing networks</data>
  <data key="d4">L Zhang, D Xu, A Arnab…</data>
  <data key="d5">2020</data>
  <data key="d6">119</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4303557091610097560&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="932093633137744803">
  <data key="d0">Efficient detr: improving end-to-end object detector with dense prior</data>
  <data key="d1">932093633137744803</data>
  <data key="d2">https://arxiv.org/abs/2104.01318</data>
  <data key="d3">Efficient detr: improving end-to-end object detector with dense prior</data>
  <data key="d4">Z Yao, J Ai, B Li, C Zhang</data>
  <data key="d5">2021</data>
  <data key="d6">65</data>
  <data key="d7">https://scholar.google.com/scholar?cites=932093633137744803&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="3247052077202185212">
  <data key="d0">Dense learning based semi-supervised object detection</data>
  <data key="d1">3247052077202185212</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Chen_Dense_Learning_Based_Semi-Supervised_Object_Detection_CVPR_2022_paper.html</data>
  <data key="d3">Dense learning based semi-supervised object detection</data>
  <data key="d4">B Chen, P Li, X Chen, B Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3247052077202185212&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="4349671636320753508">
  <data key="d0">End-to-end video object detection with spatial-temporal transformers</data>
  <data key="d1">4349671636320753508</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3474085.3475285</data>
  <data key="d3">End-to-end video object detection with spatial-temporal transformers</data>
  <data key="d4">L He, Q Zhou, X Li, L Niu, G Cheng, X Li, W Liu…</data>
  <data key="d5">2021</data>
  <data key="d6">65</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4349671636320753508&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="18249743012593625562">
  <data key="d0">TransVOD: end-to-end video object detection with spatial-temporal transformers</data>
  <data key="d1">18249743012593625562</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9960850/</data>
  <data key="d3">TransVOD: end-to-end video object detection with spatial-temporal transformers</data>
  <data key="d4">Q Zhou, X Li, L He, Y Yang, G Cheng…</data>
  <data key="d5">2022</data>
  <data key="d6">54</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18249743012593625562&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="11513198882440237429">
  <data key="d0">Panoptic-partformer: Learning a unified model for panoptic part segmentation</data>
  <data key="d1">11513198882440237429</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19812-0_42</data>
  <data key="d3">Panoptic-partformer: Learning a unified model for panoptic part segmentation</data>
  <data key="d4">X Li, S Xu, Y Yang, G Cheng, Y Tong, D Tao</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11513198882440237429&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="17487592973794344865">
  <data key="d0">Group detr: Fast training convergence with decoupled one-to-many label assignment</data>
  <data key="d1">17487592973794344865</data>
  <data key="d2">https://arxiv.org/abs/2207.13085</data>
  <data key="d3">Group detr: Fast training convergence with decoupled one-to-many label assignment</data>
  <data key="d4">Q Chen, X Chen, G Zeng, J Wang</data>
  <data key="d5">2022</data>
  <data key="d6">44</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17487592973794344865&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="14517707013240558642">
  <data key="d0">Fashionformer: A simple, effective and unified baseline for human fashion segmentation and recognition</data>
  <data key="d1">14517707013240558642</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19836-6_31</data>
  <data key="d3">Fashionformer: A simple, effective and unified baseline for human fashion segmentation and recognition</data>
  <data key="d4">S Xu, X Li, J Wang, G Cheng, Y Tong, D Tao</data>
  <data key="d5">2022</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14517707013240558642&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="17247852947552745742">
  <data key="d0">Efficient video instance segmentation via tracklet query and proposal</data>
  <data key="d1">17247852947552745742</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Wu_Efficient_Video_Instance_Segmentation_via_Tracklet_Query_and_Proposal_CVPR_2022_paper.html</data>
  <data key="d3">Efficient video instance segmentation via tracklet query and proposal</data>
  <data key="d4">J Wu, S Yarram, H Liang, T Lan…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17247852947552745742&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="10634998792902995201">
  <data key="d0">Watch only once: An end-to-end video action detection framework</data>
  <data key="d1">10634998792902995201</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Chen_Watch_Only_Once_An_End-to-End_Video_Action_Detection_Framework_ICCV_2021_paper.html</data>
  <data key="d3">Watch only once: An end-to-end video action detection framework</data>
  <data key="d4">S Chen, P Sun, E Xie, C Ge, J Wu…</data>
  <data key="d5">2021</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10634998792902995201&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="10884589459641707712">
  <data key="d0">Beyond self-attention: External attention using two linear layers for visual tasks</data>
  <data key="d1">10884589459641707712</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9912362/</data>
  <data key="d3">Beyond self-attention: External attention using two linear layers for visual tasks</data>
  <data key="d4">MH Guo, ZN Liu, TJ Mu, SM Hu</data>
  <data key="d5">2022</data>
  <data key="d6">279</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10884589459641707712&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="7760609266921230809">
  <data key="d0">Conquer: Query contrast voxel-detr for 3d object detection</data>
  <data key="d1">7760609266921230809</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhu_ConQueR_Query_Contrast_Voxel-DETR_for_3D_Object_Detection_CVPR_2023_paper.html</data>
  <data key="d3">Conquer: Query contrast voxel-detr for 3d object detection</data>
  <data key="d4">B Zhu, Z Wang, S Shi, H Xu…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7760609266921230809&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="88270295968296053">
  <data key="d0">Ship detection in SAR images based on multi-scale feature extraction and adaptive feature fusion</data>
  <data key="d1">88270295968296053</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/3/755</data>
  <data key="d3">Ship detection in SAR images based on multi-scale feature extraction and adaptive feature fusion</data>
  <data key="d4">K Zhou, M Zhang, H Wang, J Tan</data>
  <data key="d5">2022</data>
  <data key="d6">34</data>
  <data key="d7">https://scholar.google.com/scholar?cites=88270295968296053&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="16950256669529671184">
  <data key="d0">Scan: Cross domain object detection with semantic conditioned adaptation</data>
  <data key="d1">16950256669529671184</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/20031</data>
  <data key="d3">Scan: Cross domain object detection with semantic conditioned adaptation</data>
  <data key="d4">W Li, X Liu, X Yao, Y Yuan</data>
  <data key="d5">2022</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16950256669529671184&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="9968669049352955949">
  <data key="d0">Towards weakly-supervised text spotting using a multi-task transformer</data>
  <data key="d1">9968669049352955949</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Kittenplon_Towards_Weakly-Supervised_Text_Spotting_Using_a_Multi-Task_Transformer_CVPR_2022_paper.html</data>
  <data key="d3">Towards weakly-supervised text spotting using a multi-task transformer</data>
  <data key="d4">Y Kittenplon, I Lavi, S Fogel, Y Bar…</data>
  <data key="d5">2022</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9968669049352955949&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="13477863999935708391">
  <data key="d0">Boosting R-CNN: Reweighting R-CNN samples by RPN's error for underwater object detection</data>
  <data key="d1">13477863999935708391</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0925231223001200</data>
  <data key="d3">Boosting R-CNN: Reweighting R-CNN samples by RPN's error for underwater object detection</data>
  <data key="d4">P Song, P Li, L Dai, T Wang, Z Chen</data>
  <data key="d5">2023</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13477863999935708391&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="17378883393050270914">
  <data key="d0">Scale-aware modulation meet transformer</data>
  <data key="d1">17378883393050270914</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Lin_Scale-Aware_Modulation_Meet_Transformer_ICCV_2023_paper.html</data>
  <data key="d3">Scale-aware modulation meet transformer</data>
  <data key="d4">W Lin, Z Wu, J Chen, J Huang…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17378883393050270914&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="9787590110579187483">
  <data key="d0">Effective adaptation in multi-task co-training for unified autonomous driving</data>
  <data key="d1">9787590110579187483</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/7c319b62e2257b34cb0e1040ced2e007-Abstract-Conference.html</data>
  <data key="d3">Effective adaptation in multi-task co-training for unified autonomous driving</data>
  <data key="d4">X Liang, Y Wu, J Han, H Xu, C Xu…</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9787590110579187483&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="1851007110321536767">
  <data key="d0">Open vocabulary object detection with pseudo bounding-box labels</data>
  <data key="d1">1851007110321536767</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20080-9_16</data>
  <data key="d3">Open vocabulary object detection with pseudo bounding-box labels</data>
  <data key="d4">M Gao, C Xing, JC Niebles, J Li, R Xu, W Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1851007110321536767&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="16309997597335697837">
  <data key="d0">Anchor DETR: Query design for transformer-based object detection</data>
  <data key="d1">16309997597335697837</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/download/20158/version/18455/19917</data>
  <data key="d3">Anchor DETR: Query design for transformer-based object detection</data>
  <data key="d4">Y Wang, X Zhang, T Yang, J Sun</data>
  <data key="d5">2021</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16309997597335697837&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="12695099468525271301">
  <data key="d0">Continual object detection via prototypical task correlation guided gating mechanism</data>
  <data key="d1">12695099468525271301</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Yang_Continual_Object_Detection_via_Prototypical_Task_Correlation_Guided_Gating_Mechanism_CVPR_2022_paper.html</data>
  <data key="d3">Continual object detection via prototypical task correlation guided gating mechanism</data>
  <data key="d4">B Yang, X Deng, H Shi, C Li, G Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12695099468525271301&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="4296423219815701790">
  <data key="d0">Inspro: Propagating instance query and proposal for online video instance segmentation</data>
  <data key="d1">4296423219815701790</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/7ac19fdcdf4f311f3e3ef2e7ef4784d7-Abstract-Conference.html</data>
  <data key="d3">Inspro: Propagating instance query and proposal for online video instance segmentation</data>
  <data key="d4">F He, H Zhang, N Gao, J Jia, Y Shan…</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4296423219815701790&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="3432971989735915692">
  <data key="d0">Beyond fixation: Dynamic window visual transformer</data>
  <data key="d1">3432971989735915692</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Ren_Beyond_Fixation_Dynamic_Window_Visual_Transformer_CVPR_2022_paper.html</data>
  <data key="d3">Beyond fixation: Dynamic window visual transformer</data>
  <data key="d4">P Ren, C Li, G Wang, Y Xiao, Q Du…</data>
  <data key="d5">2022</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3432971989735915692&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="1436259968817456407">
  <data key="d0">Dense Distinct Query for End-to-End Object Detection</data>
  <data key="d1">1436259968817456407</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Dense_Distinct_Query_for_End-to-End_Object_Detection_CVPR_2023_paper.html</data>
  <data key="d3">Dense Distinct Query for End-to-End Object Detection</data>
  <data key="d4">S Zhang, X Wang, J Wang, J Pang…</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1436259968817456407&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="9723387705619955263">
  <data key="d0">Adapt: Efficient multi-agent trajectory prediction with adaptation</data>
  <data key="d1">9723387705619955263</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Aydemir_ADAPT_Efficient_Multi-Agent_Trajectory_Prediction_with_Adaptation_ICCV_2023_paper.html</data>
  <data key="d3">Adapt: Efficient multi-agent trajectory prediction with adaptation</data>
  <data key="d4">G Aydemir, AK Akan, F Güney</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9723387705619955263&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="7127843590064680446">
  <data key="d0">Polyphonicformer: unified query learning for depth-aware video panoptic segmentation</data>
  <data key="d1">7127843590064680446</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19812-0_34</data>
  <data key="d3">Polyphonicformer: unified query learning for depth-aware video panoptic segmentation</data>
  <data key="d4">H Yuan, X Li, Y Yang, G Cheng, J Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7127843590064680446&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="10574929869960191427">
  <data key="d0">Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors</data>
  <data key="d1">10574929869960191427</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Towards_Efficient_Use_of_Multi-Scale_Features_in_Transformer-Based_Object_Detectors_CVPR_2023_paper.html</data>
  <data key="d3">Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors</data>
  <data key="d4">G Zhang, Z Luo, Z Tian, J Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10574929869960191427&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="15062612983329704744">
  <data key="d0">Visual recognition by request</data>
  <data key="d1">15062612983329704744</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023/html/Tang_Visual_Recognition_by_Request_CVPR_2023_paper.html</data>
  <data key="d3">Visual recognition by request</data>
  <data key="d4">C Tang, L Xie, X Zhang, X Hu…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15062612983329704744&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="15276230008077862137">
  <data key="d0">Lctr: On awakening the local continuity of transformer for weakly supervised object localization</data>
  <data key="d1">15276230008077862137</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/19918</data>
  <data key="d3">Lctr: On awakening the local continuity of transformer for weakly supervised object localization</data>
  <data key="d4">Z Chen, C Wang, Y Wang, G Jiang, Y Shen…</data>
  <data key="d5">2022</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15276230008077862137&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="7534423418396554672">
  <data key="d0">An empirical study of adder neural networks for object detection</data>
  <data key="d1">7534423418396554672</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/37693cfc748049e45d87b8c7d8b9aacd-Abstract.html</data>
  <data key="d3">An empirical study of adder neural networks for object detection</data>
  <data key="d4">X Chen, C Xu, M Dong, C Xu…</data>
  <data key="d5">2021</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7534423418396554672&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="2519052790779402959">
  <data key="d0">Dynamic sparse r-cnn</data>
  <data key="d1">2519052790779402959</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Hong_Dynamic_Sparse_R-CNN_CVPR_2022_paper.html</data>
  <data key="d3">Dynamic sparse r-cnn</data>
  <data key="d4">Q Hong, F Liu, D Li, J Liu, L Tian…</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2519052790779402959&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="4642654167987856293">
  <data key="d0">Ship detection based on deep learning using SAR imagery: a systematic literature review</data>
  <data key="d1">4642654167987856293</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00500-022-07522-w</data>
  <data key="d3">Ship detection based on deep learning using SAR imagery: a systematic literature review</data>
  <data key="d4">M Yasir, W Jianhua, X Mingming, S Hui, Z Zhe…</data>
  <data key="d5">2023</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4642654167987856293&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="12417942115314361068">
  <data key="d0">Unsupervised continual learning for gradually varying domains</data>
  <data key="d1">12417942115314361068</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022W/CLVision/html/Taufique_Unsupervised_Continual_Learning_for_Gradually_Varying_Domains_CVPRW_2022_paper.html</data>
  <data key="d3">Unsupervised continual learning for gradually varying domains</data>
  <data key="d4">AMN Taufique, CS Jahan…</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12417942115314361068&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="3862789668091273745">
  <data key="d0">Structured sparse r-cnn for direct scene graph generation</data>
  <data key="d1">3862789668091273745</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Teng_Structured_Sparse_R-CNN_for_Direct_Scene_Graph_Generation_CVPR_2022_paper.html</data>
  <data key="d3">Structured sparse r-cnn for direct scene graph generation</data>
  <data key="d4">Y Teng, L Wang</data>
  <data key="d5">2022</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3862789668091273745&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="13055852169785823920">
  <data key="d0">Distance-IoU loss: Faster and better learning for bounding box regression</data>
  <data key="d1">13055852169785823920</data>
  <data key="d2">https://aaai.org/ojs/index.php/AAAI/article/view/6999</data>
  <data key="d3">Distance-IoU loss: Faster and better learning for bounding box regression</data>
  <data key="d4">Z Zheng, P Wang, W Liu, J Li, R Ye, D Ren</data>
  <data key="d5">2020</data>
  <data key="d6">2222</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13055852169785823920&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="10191050231224943760">
  <data key="d0">SE-SSD: Self-ensembling single-stage object detector from point cloud</data>
  <data key="d1">10191050231224943760</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Zheng_SE-SSD_Self-Ensembling_Single-Stage_Object_Detector_From_Point_Cloud_CVPR_2021_paper.html</data>
  <data key="d3">SE-SSD: Self-ensembling single-stage object detector from point cloud</data>
  <data key="d4">W Zheng, W Tang, L Jiang…</data>
  <data key="d5">2021</data>
  <data key="d6">261</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10191050231224943760&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="12829565963573040312">
  <data key="d0">R3det: Refined single-stage detector with feature refinement for rotating object</data>
  <data key="d1">12829565963573040312</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/16426</data>
  <data key="d3">R3det: Refined single-stage detector with feature refinement for rotating object</data>
  <data key="d4">X Yang, J Yan, Z Feng, T He</data>
  <data key="d5">2021</data>
  <data key="d6">574</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12829565963573040312&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="17430652832439326106">
  <data key="d0">A survey and performance evaluation of deep learning methods for small object detection</data>
  <data key="d1">17430652832439326106</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417421000439</data>
  <data key="d3">A survey and performance evaluation of deep learning methods for small object detection</data>
  <data key="d4">Y Liu, P Sun, N Wergeles, Y Shang</data>
  <data key="d5">2021</data>
  <data key="d6">262</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17430652832439326106&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="7988129384525076421">
  <data key="d0">Focal and efficient IOU loss for accurate bounding box regression</data>
  <data key="d1">7988129384525076421</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0925231222009018</data>
  <data key="d3">Focal and efficient IOU loss for accurate bounding box regression</data>
  <data key="d4">YF Zhang, W Ren, Z Zhang, Z Jia, L Wang, T Tan</data>
  <data key="d5">2022</data>
  <data key="d6">359</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7988129384525076421&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="7487171131395966182">
  <data key="d0">Actionformer: Localizing moments of actions with transformers</data>
  <data key="d1">7487171131395966182</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19772-7_29</data>
  <data key="d3">Actionformer: Localizing moments of actions with transformers</data>
  <data key="d4">CL Zhang, J Wu, Y Li</data>
  <data key="d5">2022</data>
  <data key="d6">130</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7487171131395966182&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="9458084216549029781">
  <data key="d0">Rethinking rotated object detection with gaussian wasserstein distance loss</data>
  <data key="d1">9458084216549029781</data>
  <data key="d2">https://proceedings.mlr.press/v139/yang21l</data>
  <data key="d3">Rethinking rotated object detection with gaussian wasserstein distance loss</data>
  <data key="d4">X Yang, J Yan, Q Ming, W Wang…</data>
  <data key="d5">2021</data>
  <data key="d6">214</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9458084216549029781&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="15795399494869889077">
  <data key="d0">Learning high-precision bounding box for rotated object detection via kullback-leibler divergence</data>
  <data key="d1">15795399494869889077</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/98f13708210194c475687be6106a3b84-Abstract.html</data>
  <data key="d3">Learning high-precision bounding box for rotated object detection via kullback-leibler divergence</data>
  <data key="d4">X Yang, X Yang, J Yang, Q Ming…</data>
  <data key="d5">2021</data>
  <data key="d6">170</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15795399494869889077&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="8762454778937977659">
  <data key="d0">Imbalance problems in object detection: A review</data>
  <data key="d1">8762454778937977659</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9042296/</data>
  <data key="d3">Imbalance problems in object detection: A review</data>
  <data key="d4">K Oksuz, BC Cam, S Kalkan…</data>
  <data key="d5">2020</data>
  <data key="d6">373</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8762454778937977659&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="9575584799952236610">
  <data key="d0">Rethinking the competition between detection and reid in multiobject tracking</data>
  <data key="d1">9575584799952236610</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9756236/</data>
  <data key="d3">Rethinking the competition between detection and reid in multiobject tracking</data>
  <data key="d4">C Liang, Z Zhang, X Zhou, B Li, S Zhu…</data>
  <data key="d5">2022</data>
  <data key="d6">175</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9575584799952236610&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="13728347746290657939">
  <data key="d0">Automatic recognition of pavement cracks from combined GPR B-scan and C-scan images using multiscale feature fusion deep neural networks</data>
  <data key="d1">13728347746290657939</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0926580522005684</data>
  <data key="d3">Automatic recognition of pavement cracks from combined GPR B-scan and C-scan images using multiscale feature fusion deep neural networks</data>
  <data key="d4">Z Liu, X Gu, J Chen, D Wang, Y Chen, L Wang</data>
  <data key="d5">2023</data>
  <data key="d6">52</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13728347746290657939&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="13222542974410927057">
  <data key="d0">Artificial intelligence-assisted colorimetric lateral flow immunoassay for sensitive and quantitative detection of COVID-19 neutralizing antibody</data>
  <data key="d1">13222542974410927057</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0956566322004894</data>
  <data key="d3">Artificial intelligence-assisted colorimetric lateral flow immunoassay for sensitive and quantitative detection of COVID-19 neutralizing antibody</data>
  <data key="d4">H Tong, C Cao, M You, S Han, Z Liu, Y Xiao…</data>
  <data key="d5">2022</data>
  <data key="d6">43</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13222542974410927057&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="1654562328017701106">
  <data key="d0">Deep learning based online metallic surface defect detection method for wire and arc additive manufacturing</data>
  <data key="d1">1654562328017701106</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0736584522001521</data>
  <data key="d3">Deep learning based online metallic surface defect detection method for wire and arc additive manufacturing</data>
  <data key="d4">W Li, H Zhang, G Wang, G Xiong, M Zhao, G Li…</data>
  <data key="d5">2023</data>
  <data key="d6">36</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1654562328017701106&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="13553623797919213094">
  <data key="d0">Real-time defects detection for apple sorting using NIR cameras with pruning-based YOLOV4 network</data>
  <data key="d1">13553623797919213094</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169922000321</data>
  <data key="d3">Real-time defects detection for apple sorting using NIR cameras with pruning-based YOLOV4 network</data>
  <data key="d4">S Fan, X Liang, W Huang, VJ Zhang, Q Pang…</data>
  <data key="d5">2022</data>
  <data key="d6">52</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13553623797919213094&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="13816665854585781255">
  <data key="d0">Real-time vehicle detection based on improved yolo v5</data>
  <data key="d1">13816665854585781255</data>
  <data key="d2">https://www.mdpi.com/2071-1050/14/19/12274</data>
  <data key="d3">Real-time vehicle detection based on improved yolo v5</data>
  <data key="d4">Y Zhang, Z Guo, J Wu, Y Tian, H Tang, X Guo</data>
  <data key="d5">2022</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13816665854585781255&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="5767715645618653984">
  <data key="d0">DCC-CenterNet: A rapid detection method for steel surface defects</data>
  <data key="d1">5767715645618653984</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0263224121011210</data>
  <data key="d3">DCC-CenterNet: A rapid detection method for steel surface defects</data>
  <data key="d4">R Tian, M Jia</data>
  <data key="d5">2022</data>
  <data key="d6">58</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5767715645618653984&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="15818286087987627641">
  <data key="d0">Object detection from UAV thermal infrared images and videos using YOLO models</data>
  <data key="d1">15818286087987627641</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1569843222001145</data>
  <data key="d3">Object detection from UAV thermal infrared images and videos using YOLO models</data>
  <data key="d4">C Jiang, H Ren, X Ye, J Zhu, H Zeng, Y Nan…</data>
  <data key="d5">2022</data>
  <data key="d6">40</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15818286087987627641&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="16403067771390791423">
  <data key="d0">Improved YOLOv5 network for real-time multi-scale traffic sign detection</data>
  <data key="d1">16403067771390791423</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00521-022-08077-5</data>
  <data key="d3">Improved YOLOv5 network for real-time multi-scale traffic sign detection</data>
  <data key="d4">J Wang, Y Chen, Z Dong, M Gao</data>
  <data key="d5">2023</data>
  <data key="d6">71</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16403067771390791423&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="5845461691817459977">
  <data key="d0">A new spatial-oriented object detection framework for remote sensing images</data>
  <data key="d1">5845461691817459977</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9611255/</data>
  <data key="d3">A new spatial-oriented object detection framework for remote sensing images</data>
  <data key="d4">D Yu, S Ji</data>
  <data key="d5">2021</data>
  <data key="d6">60</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5845461691817459977&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="14373500618604811887">
  <data key="d0">Improved YOLOv4 marine target detection combined with CBAM</data>
  <data key="d1">14373500618604811887</data>
  <data key="d2">https://www.mdpi.com/2073-8994/13/4/623</data>
  <data key="d3">Improved YOLOv4 marine target detection combined with CBAM</data>
  <data key="d4">H Fu, G Song, Y Wang</data>
  <data key="d5">2021</data>
  <data key="d6">87</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14373500618604811887&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="14509614284638697910">
  <data key="d0">Real-time railroad track components inspection based on the improved YOLOv4 framework</data>
  <data key="d1">14509614284638697910</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0926580521000479</data>
  <data key="d3">Real-time railroad track components inspection based on the improved YOLOv4 framework</data>
  <data key="d4">F Guo, Y Qian, Y Shi</data>
  <data key="d5">2021</data>
  <data key="d6">71</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14509614284638697910&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="2822888602570219347">
  <data key="d0">Interior attention-aware network for infrared small target detection</data>
  <data key="d1">2822888602570219347</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9745054/</data>
  <data key="d3">Interior attention-aware network for infrared small target detection</data>
  <data key="d4">K Wang, S Du, C Liu, Z Cao</data>
  <data key="d5">2022</data>
  <data key="d6">58</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2822888602570219347&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="2124912804095600023">
  <data key="d0">A novel nonlocal-aware pyramid and multiscale multitask refinement detector for object detection in remote sensing images</data>
  <data key="d1">2124912804095600023</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9364888/</data>
  <data key="d3">A novel nonlocal-aware pyramid and multiscale multitask refinement detector for object detection in remote sensing images</data>
  <data key="d4">Z Huang, W Li, XG Xia, X Wu, Z Cai…</data>
  <data key="d5">2021</data>
  <data key="d6">75</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2124912804095600023&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="16401671466632432676">
  <data key="d0">BiFA-YOLO: A novel YOLO-based method for arbitrary-oriented ship detection in high-resolution SAR images</data>
  <data key="d1">16401671466632432676</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/21/4209</data>
  <data key="d3">BiFA-YOLO: A novel YOLO-based method for arbitrary-oriented ship detection in high-resolution SAR images</data>
  <data key="d4">Z Sun, X Leng, Y Lei, B Xiong, K Ji, G Kuang</data>
  <data key="d5">2021</data>
  <data key="d6">63</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16401671466632432676&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="18286028574106509245">
  <data key="d0">VerSe: a vertebrae labelling and segmentation benchmark for multi-detector CT images</data>
  <data key="d1">18286028574106509245</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841521002127</data>
  <data key="d3">VerSe: a vertebrae labelling and segmentation benchmark for multi-detector CT images</data>
  <data key="d4">A Sekuboyina, ME Husseini, A Bayat, M Löffler…</data>
  <data key="d5">2021</data>
  <data key="d6">123</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18286028574106509245&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="7167680945390018074">
  <data key="d0">Objectbox: From centers to boxes for anchor-free object detection</data>
  <data key="d1">7167680945390018074</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20080-9_23</data>
  <data key="d3">Objectbox: From centers to boxes for anchor-free object detection</data>
  <data key="d4">M Zand, A Etemad, M Greenspan</data>
  <data key="d5">2022</data>
  <data key="d6">24</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7167680945390018074&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="4019899772206366950">
  <data key="d0">Pillarnet: Real-time and high-performance pillar-based 3d object detection</data>
  <data key="d1">4019899772206366950</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20080-9_3</data>
  <data key="d3">Pillarnet: Real-time and high-performance pillar-based 3d object detection</data>
  <data key="d4">G Shi, R Li, C Ma</data>
  <data key="d5">2022</data>
  <data key="d6">35</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4019899772206366950&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="3815706068371757718">
  <data key="d0">An improved Yolov5 real-time detection method for small objects captured by UAV</data>
  <data key="d1">3815706068371757718</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00500-021-06407-8</data>
  <data key="d3">An improved Yolov5 real-time detection method for small objects captured by UAV</data>
  <data key="d4">W Zhan, C Sun, M Wang, J She, Y Zhang, Z Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">63</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3815706068371757718&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="3273783590798016013">
  <data key="d0">Multi-scale ship detection from SAR and optical imagery via a more accurate YOLOv3</data>
  <data key="d1">3273783590798016013</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9448440/</data>
  <data key="d3">Multi-scale ship detection from SAR and optical imagery via a more accurate YOLOv3</data>
  <data key="d4">Z Hong, T Yang, X Tong, Y Zhang…</data>
  <data key="d5">2021</data>
  <data key="d6">59</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3273783590798016013&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="249972322094479786">
  <data key="d0">Sample and computation redistribution for efficient face detection</data>
  <data key="d1">249972322094479786</data>
  <data key="d2">https://arxiv.org/abs/2105.04714</data>
  <data key="d3">Sample and computation redistribution for efficient face detection</data>
  <data key="d4">J Guo, J Deng, A Lattas, S Zafeiriou</data>
  <data key="d5">2021</data>
  <data key="d6">57</data>
  <data key="d7">https://scholar.google.com/scholar?cites=249972322094479786&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="7697764778571254929">
  <data key="d0">Learning to track objects from unlabeled videos</data>
  <data key="d1">7697764778571254929</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Zheng_Learning_To_Track_Objects_From_Unlabeled_Videos_ICCV_2021_paper.html</data>
  <data key="d3">Learning to track objects from unlabeled videos</data>
  <data key="d4">J Zheng, C Ma, H Peng, X Yang</data>
  <data key="d5">2021</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7697764778571254929&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="16919329449044452538">
  <data key="d0">A lightweight detector based on attention mechanism for aluminum strip surface defect detection</data>
  <data key="d1">16919329449044452538</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0166361521001925</data>
  <data key="d3">A lightweight detector based on attention mechanism for aluminum strip surface defect detection</data>
  <data key="d4">MA Zhuxi, Y Li, M Huang, Q Huang, J Cheng…</data>
  <data key="d5">2022</data>
  <data key="d6">41</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16919329449044452538&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="6421431120514564979">
  <data key="d0">Tinaface: Strong but simple baseline for face detection</data>
  <data key="d1">6421431120514564979</data>
  <data key="d2">https://arxiv.org/abs/2011.13183</data>
  <data key="d3">Tinaface: Strong but simple baseline for face detection</data>
  <data key="d4">Y Zhu, H Cai, S Zhang, C Wang, Y Xiong</data>
  <data key="d5">2020</data>
  <data key="d6">71</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6421431120514564979&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="13529499341139137951">
  <data key="d0">Dbcface: Towards pure convolutional neural network face detection</data>
  <data key="d1">13529499341139137951</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9438673/</data>
  <data key="d3">Dbcface: Towards pure convolutional neural network face detection</data>
  <data key="d4">X Li, S Lai, X Qian</data>
  <data key="d5">2021</data>
  <data key="d6">55</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13529499341139137951&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="15464155576948087293">
  <data key="d0">Optimized YOLOv3 algorithm and its application in traffic flow detections</data>
  <data key="d1">15464155576948087293</data>
  <data key="d2">https://www.mdpi.com/2076-3417/10/9/3079</data>
  <data key="d3">Optimized YOLOv3 algorithm and its application in traffic flow detections</data>
  <data key="d4">YQ Huang, JC Zheng, SD Sun, CF Yang, J Liu</data>
  <data key="d5">2020</data>
  <data key="d6">95</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15464155576948087293&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="14962706247913538159">
  <data key="d0">Multi-scale ship detection algorithm based on a lightweight neural network for spaceborne SAR images</data>
  <data key="d1">14962706247913538159</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/5/1149</data>
  <data key="d3">Multi-scale ship detection algorithm based on a lightweight neural network for spaceborne SAR images</data>
  <data key="d4">S Liu, W Kong, X Chen, M Xu, M Yasir, L Zhao, J Li</data>
  <data key="d5">2022</data>
  <data key="d6">36</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14962706247913538159&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="7855712559170638610">
  <data key="d0">DefectTR: End-to-end defect detection for sewage networks using a transformer</data>
  <data key="d1">7855712559170638610</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0950061822002744</data>
  <data key="d3">DefectTR: End-to-end defect detection for sewage networks using a transformer</data>
  <data key="d4">LM Dang, H Wang, Y Li, TN Nguyen, H Moon</data>
  <data key="d5">2022</data>
  <data key="d6">27</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7855712559170638610&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="302515988675284467">
  <data key="d0">Refining yolov4 for vehicle detection</data>
  <data key="d1">302515988675284467</data>
  <data key="d2">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3628439</data>
  <data key="d3">Refining yolov4 for vehicle detection</data>
  <data key="d4">P Mahto, P Garg, P Seth, J Panda</data>
  <data key="d5">2020</data>
  <data key="d6">79</data>
  <data key="d7">https://scholar.google.com/scholar?cites=302515988675284467&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="17326028169687643062">
  <data key="d0">Insulator faults detection in aerial images from high-voltage transmission lines based on deep learning model</data>
  <data key="d1">17326028169687643062</data>
  <data key="d2">https://www.mdpi.com/2076-3417/11/10/4647</data>
  <data key="d3">Insulator faults detection in aerial images from high-voltage transmission lines based on deep learning model</data>
  <data key="d4">C Liu, Y Wu, J Liu, Z Sun, H Xu</data>
  <data key="d5">2021</data>
  <data key="d6">50</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17326028169687643062&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="4612145705765974530">
  <data key="d0">The KFIoU loss for rotated object detection</data>
  <data key="d1">4612145705765974530</data>
  <data key="d2">https://arxiv.org/abs/2201.12558</data>
  <data key="d3">The KFIoU loss for rotated object detection</data>
  <data key="d4">X Yang, Y Zhou, G Zhang, J Yang, W Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">56</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4612145705765974530&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="6077963581280858299">
  <data key="d0">Deep learning for SAR ship detection: Past, present and future</data>
  <data key="d1">6077963581280858299</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/11/2712</data>
  <data key="d3">Deep learning for SAR ship detection: Past, present and future</data>
  <data key="d4">J Li, C Xu, H Su, L Gao, T Wang</data>
  <data key="d5">2022</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6077963581280858299&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="6155236788302466355">
  <data key="d0">A deep learning-based hybrid framework for object detection and recognition in autonomous driving</data>
  <data key="d1">6155236788302466355</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9238023/</data>
  <data key="d3">A deep learning-based hybrid framework for object detection and recognition in autonomous driving</data>
  <data key="d4">Y Li, H Wang, LM Dang, TN Nguyen, D Han…</data>
  <data key="d5">2020</data>
  <data key="d6">71</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6155236788302466355&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="4790015723049709885">
  <data key="d0">Deep learning for computational cytology: A survey</data>
  <data key="d1">4790015723049709885</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S136184152200319X</data>
  <data key="d3">Deep learning for computational cytology: A survey</data>
  <data key="d4">H Jiang, Y Zhou, Y Lin, RCK Chan, J Liu, H Chen</data>
  <data key="d5">2022</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4790015723049709885&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="4059997502371762707">
  <data key="d0">Drone vs. bird detection: Deep learning algorithms and results from a grand challenge</data>
  <data key="d1">4059997502371762707</data>
  <data key="d2">https://www.mdpi.com/1424-8220/21/8/2824</data>
  <data key="d3">Drone vs. bird detection: Deep learning algorithms and results from a grand challenge</data>
  <data key="d4">A Coluccia, A Fascista, A Schumann, L Sommer…</data>
  <data key="d5">2021</data>
  <data key="d6">59</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4059997502371762707&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="5682759315963442168">
  <data key="d0">A small attentional YOLO model for landslide detection from satellite remote sensing images</data>
  <data key="d1">5682759315963442168</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10346-021-01694-6</data>
  <data key="d3">A small attentional YOLO model for landslide detection from satellite remote sensing images</data>
  <data key="d4">L Cheng, J Li, P Duan, M Wang</data>
  <data key="d5">2021</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5682759315963442168&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="17354835527964479444">
  <data key="d0">Vehicle detection from aerial images using deep learning: A comparative study</data>
  <data key="d1">17354835527964479444</data>
  <data key="d2">https://www.mdpi.com/2079-9292/10/7/820</data>
  <data key="d3">Vehicle detection from aerial images using deep learning: A comparative study</data>
  <data key="d4">A Ammar, A Koubaa, M Ahmed, A Saad, B Benjdira</data>
  <data key="d5">2021</data>
  <data key="d6">48</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17354835527964479444&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="13938085636022527711">
  <data key="d0">Apple detection in complex scene using the improved YOLOv4 model</data>
  <data key="d1">13938085636022527711</data>
  <data key="d2">https://www.mdpi.com/2073-4395/11/3/476</data>
  <data key="d3">Apple detection in complex scene using the improved YOLOv4 model</data>
  <data key="d4">L Wu, J Ma, Y Zhao, H Liu</data>
  <data key="d5">2021</data>
  <data key="d6">50</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13938085636022527711&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="4094576052109847144">
  <data key="d0">YOLO-ACN: Focusing on small target and occluded object detection</data>
  <data key="d1">4094576052109847144</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9303478/</data>
  <data key="d3">YOLO-ACN: Focusing on small target and occluded object detection</data>
  <data key="d4">Y Li, S Li, H Du, L Chen, D Zhang, Y Li</data>
  <data key="d5">2020</data>
  <data key="d6">57</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4094576052109847144&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="10745444013023171384">
  <data key="d0">Localization distillation for dense object detection</data>
  <data key="d1">10745444013023171384</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zheng_Localization_Distillation_for_Dense_Object_Detection_CVPR_2022_paper.html</data>
  <data key="d3">Localization distillation for dense object detection</data>
  <data key="d4">Z Zheng, R Ye, P Wang, D Ren…</data>
  <data key="d5">2022</data>
  <data key="d6">48</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10745444013023171384&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="6927002374951805343">
  <data key="d0">Rethinking IoU-based optimization for single-stage 3D object detection</data>
  <data key="d1">6927002374951805343</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20077-9_32</data>
  <data key="d3">Rethinking IoU-based optimization for single-stage 3D object detection</data>
  <data key="d4">H Sheng, S Cai, N Zhao, B Deng, J Huang…</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6927002374951805343&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="5562955281835677624">
  <data key="d0">Vision-language pre-training: Basics, recent advances, and future trends</data>
  <data key="d1">5562955281835677624</data>
  <data key="d2">https://www.nowpublishers.com/article/Details/CGV-105</data>
  <data key="d3">Vision-language pre-training: Basics, recent advances, and future trends</data>
  <data key="d4">Z Gan, L Li, C Li, L Wang, Z Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">56</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5562955281835677624&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="8448388555539304854">
  <data key="d0">Simple open-vocabulary object detection</data>
  <data key="d1">8448388555539304854</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20080-9_42</data>
  <data key="d3">Simple open-vocabulary object detection</data>
  <data key="d4">M Minderer, A Gritsenko, A Stone, M Neumann…</data>
  <data key="d5">2022</data>
  <data key="d6">120</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8448388555539304854&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="4160517527641475312">
  <data key="d0">Glipv2: Unifying localization and vision-language understanding</data>
  <data key="d1">4160517527641475312</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/ea370419760b421ce12e3082eb2ae1a8-Abstract-Conference.html</data>
  <data key="d3">Glipv2: Unifying localization and vision-language understanding</data>
  <data key="d4">H Zhang, P Zhang, X Hu, YC Chen…</data>
  <data key="d5">2022</data>
  <data key="d6">93</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4160517527641475312&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="2958442793928445860">
  <data key="d0">Large ai models in health informatics: Applications, challenges, and the future</data>
  <data key="d1">2958442793928445860</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10261199/</data>
  <data key="d3">Large ai models in health informatics: Applications, challenges, and the future</data>
  <data key="d4">J Qiu, L Li, J Sun, J Peng, P Shi…</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2958442793928445860&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="10156380014983934707">
  <data key="d0">Bridging the gap between object and image-level representations for open-vocabulary detection</data>
  <data key="d1">10156380014983934707</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/dabf612543b97ea9c8f46d058d33cf74-Abstract-Conference.html</data>
  <data key="d3">Bridging the gap between object and image-level representations for open-vocabulary detection</data>
  <data key="d4">H Bangalath, M Maaz, MU Khattak…</data>
  <data key="d5">2022</data>
  <data key="d6">49</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10156380014983934707&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="1828958259233079068">
  <data key="d0">Unified-io: A unified model for vision, language, and multi-modal tasks</data>
  <data key="d1">1828958259233079068</data>
  <data key="d2">https://arxiv.org/abs/2206.08916</data>
  <data key="d3">Unified-io: A unified model for vision, language, and multi-modal tasks</data>
  <data key="d4">J Lu, C Clark, R Zellers, R Mottaghi…</data>
  <data key="d5">2022</data>
  <data key="d6">133</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1828958259233079068&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="9556534593478071448">
  <data key="d0">Generalized decoding for pixel, image, and language</data>
  <data key="d1">9556534593478071448</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.html</data>
  <data key="d3">Generalized decoding for pixel, image, and language</data>
  <data key="d4">X Zou, ZY Dou, J Yang, Z Gan, L Li…</data>
  <data key="d5">2023</data>
  <data key="d6">39</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9556534593478071448&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="2013709511789422934">
  <data key="d0">Detclip: Dictionary-enriched visual-concept paralleled pre-training for open-world detection</data>
  <data key="d1">2013709511789422934</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/3ba960559212691be13fa81d9e5e0047-Abstract-Conference.html</data>
  <data key="d3">Detclip: Dictionary-enriched visual-concept paralleled pre-training for open-world detection</data>
  <data key="d4">L Yao, J Han, Y Wen, X Liang, D Xu…</data>
  <data key="d5">2022</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2013709511789422934&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="213109028691722316">
  <data key="d0">Test-time prompt tuning for zero-shot generalization in vision-language models</data>
  <data key="d1">213109028691722316</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/5bf2b802e24106064dc547ae9283bb0c-Abstract-Conference.html</data>
  <data key="d3">Test-time prompt tuning for zero-shot generalization in vision-language models</data>
  <data key="d4">M Shu, W Nie, DA Huang, Z Yu…</data>
  <data key="d5">2022</data>
  <data key="d6">53</data>
  <data key="d7">https://scholar.google.com/scholar?cites=213109028691722316&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="9083483030705185424">
  <data key="d0">Visual instruction tuning</data>
  <data key="d1">9083483030705185424</data>
  <data key="d2">https://arxiv.org/abs/2304.08485</data>
  <data key="d3">Visual instruction tuning</data>
  <data key="d4">H Liu, C Li, Q Wu, YJ Lee</data>
  <data key="d5">2023</data>
  <data key="d6">178</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9083483030705185424&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="8163168166663635565">
  <data key="d0">Open-vocabulary panoptic segmentation with text-to-image diffusion models</data>
  <data key="d1">8163168166663635565</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Xu_Open-Vocabulary_Panoptic_Segmentation_With_Text-to-Image_Diffusion_Models_CVPR_2023_paper.html</data>
  <data key="d3">Open-vocabulary panoptic segmentation with text-to-image diffusion models</data>
  <data key="d4">J Xu, S Liu, A Vahdat, W Byeon…</data>
  <data key="d5">2023</data>
  <data key="d6">43</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8163168166663635565&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="477874232529254013">
  <data key="d0">ULIP: Learning a unified representation of language, images, and point clouds for 3D understanding</data>
  <data key="d1">477874232529254013</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Gao_ULIP_Learning_a_Unified_Representation_of_Language_Images_and_Point_CVPR_2023_paper.html</data>
  <data key="d3">ULIP: Learning a unified representation of language, images, and point clouds for 3D understanding</data>
  <data key="d4">L Xue, M Gao, C Xing, R Martín-Martín…</data>
  <data key="d5">2023</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=477874232529254013&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="14173651844217137684">
  <data key="d0">Elevater: A benchmark and toolkit for evaluating language-augmented visual models</data>
  <data key="d1">14173651844217137684</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/3c4688b6a76f25f2311daa0d75a58f1a-Abstract-Datasets_and_Benchmarks.html</data>
  <data key="d3">Elevater: A benchmark and toolkit for evaluating language-augmented visual models</data>
  <data key="d4">C Li, H Liu, L Li, P Zhang, J Aneja…</data>
  <data key="d5">2022</data>
  <data key="d6">49</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14173651844217137684&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="7539527092820284785">
  <data key="d0">Coarse-to-fine vision-language pre-training with fusion in the backbone</data>
  <data key="d1">7539527092820284785</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/d4b6ccf3acd6ccbc1093e093df345ba2-Abstract-Conference.html</data>
  <data key="d3">Coarse-to-fine vision-language pre-training with fusion in the backbone</data>
  <data key="d4">ZY Dou, A Kamath, Z Gan, P Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">45</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7539527092820284785&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="10493115215078116970">
  <data key="d0">Segclip: Patch aggregation with learnable centers for open-vocabulary semantic segmentation</data>
  <data key="d1">10493115215078116970</data>
  <data key="d2">https://proceedings.mlr.press/v202/luo23a.html</data>
  <data key="d3">Segclip: Patch aggregation with learnable centers for open-vocabulary semantic segmentation</data>
  <data key="d4">H Luo, J Bao, Y Wu, X He, T Li</data>
  <data key="d5">2023</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10493115215078116970&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="2135826005582191986">
  <data key="d0">Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks</data>
  <data key="d1">2135826005582191986</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Li_Uni-Perceiver_v2_A_Generalist_Model_for_Large-Scale_Vision_and_Vision-Language_CVPR_2023_paper.html</data>
  <data key="d3">Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks</data>
  <data key="d4">H Li, J Zhu, X Jiang, X Zhu, H Li…</data>
  <data key="d5">2023</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2135826005582191986&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="238317474783907025">
  <data key="d0">Fine-grained semantically aligned vision-language pre-training</data>
  <data key="d1">238317474783907025</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/2fb4be70fc9668e9ec2c71b34fb127d4-Abstract-Conference.html</data>
  <data key="d3">Fine-grained semantically aligned vision-language pre-training</data>
  <data key="d4">J Li, X He, L Wei, L Qian, L Zhu, L Xie…</data>
  <data key="d5">2022</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=238317474783907025&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="5712399572413041920">
  <data key="d0">K-lite: Learning transferable visual models with external knowledge</data>
  <data key="d1">5712399572413041920</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/63fef0802863f47775c3563e18cbba17-Abstract-Conference.html</data>
  <data key="d3">K-lite: Learning transferable visual models with external knowledge</data>
  <data key="d4">S Shen, C Li, X Hu, Y Xie, J Yang…</data>
  <data key="d5">2022</data>
  <data key="d6">40</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5712399572413041920&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="4463177057029404685">
  <data key="d0">A simple framework for open-vocabulary segmentation and detection</data>
  <data key="d1">4463177057029404685</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Zhang_A_Simple_Framework_for_Open-Vocabulary_Segmentation_and_Detection_ICCV_2023_paper.html</data>
  <data key="d3">A simple framework for open-vocabulary segmentation and detection</data>
  <data key="d4">H Zhang, F Li, X Zou, S Liu, C Li…</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4463177057029404685&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="527701684105539342">
  <data key="d0">Zero-shot temporal action detection via vision-language prompting</data>
  <data key="d1">527701684105539342</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20062-5_39</data>
  <data key="d3">Zero-shot temporal action detection via vision-language prompting</data>
  <data key="d4">S Nag, X Zhu, YZ Song, T Xiang</data>
  <data key="d5">2022</data>
  <data key="d6">24</data>
  <data key="d7">https://scholar.google.com/scholar?cites=527701684105539342&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="11009706863402152282">
  <data key="d0">Training-free structured diffusion guidance for compositional text-to-image synthesis</data>
  <data key="d1">11009706863402152282</data>
  <data key="d2">https://arxiv.org/abs/2212.05032</data>
  <data key="d3">Training-free structured diffusion guidance for compositional text-to-image synthesis</data>
  <data key="d4">W Feng, X He, TJ Fu, V Jampani, A Akula…</data>
  <data key="d5">2022</data>
  <data key="d6">67</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11009706863402152282&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="427793634810618701">
  <data key="d0">Multi-modal knowledge graph construction and application: A survey</data>
  <data key="d1">427793634810618701</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9961954/</data>
  <data key="d3">Multi-modal knowledge graph construction and application: A survey</data>
  <data key="d4">X Zhu, Z Li, X Wang, X Jiang, P Sun…</data>
  <data key="d5">2022</data>
  <data key="d6">62</data>
  <data key="d7">https://scholar.google.com/scholar?cites=427793634810618701&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="3710797496089968926">
  <data key="d0">Vid2seq: Large-scale pretraining of a visual language model for dense video captioning</data>
  <data key="d1">3710797496089968926</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Yang_Vid2Seq_Large-Scale_Pretraining_of_a_Visual_Language_Model_for_Dense_CVPR_2023_paper.html</data>
  <data key="d3">Vid2seq: Large-scale pretraining of a visual language model for dense video captioning</data>
  <data key="d4">A Yang, A Nagrani, PH Seo, A Miech…</data>
  <data key="d5">2023</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3710797496089968926&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="16719721271879905477">
  <data key="d0">Exploring clip for assessing the look and feel of images</data>
  <data key="d1">16719721271879905477</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/25353</data>
  <data key="d3">Exploring clip for assessing the look and feel of images</data>
  <data key="d4">J Wang, KCK Chan, CC Loy</data>
  <data key="d5">2023</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16719721271879905477&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="4650814090908712272">
  <data key="d0">Vipergpt: Visual inference via python execution for reasoning</data>
  <data key="d1">4650814090908712272</data>
  <data key="d2">https://arxiv.org/abs/2303.08128</data>
  <data key="d3">Vipergpt: Visual inference via python execution for reasoning</data>
  <data key="d4">D Surís, S Menon, C Vondrick</data>
  <data key="d5">2023</data>
  <data key="d6">49</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4650814090908712272&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="4803373243514933274">
  <data key="d0">Unitab: Unifying text and box outputs for grounded vision-language modeling</data>
  <data key="d1">4803373243514933274</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20059-5_30</data>
  <data key="d3">Unitab: Unifying text and box outputs for grounded vision-language modeling</data>
  <data key="d4">Z Yang, Z Gan, J Wang, X Hu, F Ahmed, Z Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">36</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4803373243514933274&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="13480193018115242663">
  <data key="d0">The unreasonable effectiveness of CLIP features for image captioning: an experimental analysis</data>
  <data key="d1">13480193018115242663</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2022W/MULA/html/Barraco_The_Unreasonable_Effectiveness_of_CLIP_Features_for_Image_Captioning_An_CVPRW_2022_paper.html</data>
  <data key="d3">The unreasonable effectiveness of CLIP features for image captioning: an experimental analysis</data>
  <data key="d4">M Barraco, M Cornia, S Cascianelli…</data>
  <data key="d5">2022</data>
  <data key="d6">32</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13480193018115242663&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="8802764110576830272">
  <data key="d0">X-detr: A versatile architecture for instance-wise vision-language tasks</data>
  <data key="d1">8802764110576830272</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20059-5_17</data>
  <data key="d3">X-detr: A versatile architecture for instance-wise vision-language tasks</data>
  <data key="d4">Z Cai, G Kwon, A Ravichandran, E Bas, Z Tu…</data>
  <data key="d5">2022</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8802764110576830272&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="15826539500910476875">
  <data key="d0">Revive: Regional visual representation matters in knowledge-based visual question answering</data>
  <data key="d1">15826539500910476875</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/44956951349095f74492a5471128a7e0-Abstract-Conference.html</data>
  <data key="d3">Revive: Regional visual representation matters in knowledge-based visual question answering</data>
  <data key="d4">Y Lin, Y Xie, D Chen, Y Xu, C Zhu…</data>
  <data key="d5">2022</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15826539500910476875&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="15237439848602268466">
  <data key="d0">Rlip: Relational language-image pre-training for human-object interaction detection</data>
  <data key="d1">15237439848602268466</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/f37347375d8b54e3203e5d24aeb6c58c-Abstract-Conference.html</data>
  <data key="d3">Rlip: Relational language-image pre-training for human-object interaction detection</data>
  <data key="d4">H Yuan, J Jiang, S Albanie, T Feng…</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15237439848602268466&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="11416070587823639118">
  <data key="d0">Multimodality helps unimodality: Cross-modal few-shot learning with multimodal models</data>
  <data key="d1">11416070587823639118</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Lin_Multimodality_Helps_Unimodality_Cross-Modal_Few-Shot_Learning_With_Multimodal_Models_CVPR_2023_paper.html</data>
  <data key="d3">Multimodality helps unimodality: Cross-modal few-shot learning with multimodal models</data>
  <data key="d4">Z Lin, S Yu, Z Kuang, D Pathak…</data>
  <data key="d5">2023</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11416070587823639118&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="1637260800010177981">
  <data key="d0">Learning open-vocabulary semantic segmentation models from natural language supervision</data>
  <data key="d1">1637260800010177981</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Xu_Learning_Open-Vocabulary_Semantic_Segmentation_Models_From_Natural_Language_Supervision_CVPR_2023_paper.html</data>
  <data key="d3">Learning open-vocabulary semantic segmentation models from natural language supervision</data>
  <data key="d4">J Xu, J Hou, Y Zhang, R Feng…</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1637260800010177981&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="10981280129508603338">
  <data key="d0">Open-vocabulary panoptic segmentation with maskclip</data>
  <data key="d1">10981280129508603338</data>
  <data key="d2">https://arxiv.org/abs/2208.08984</data>
  <data key="d3">Open-vocabulary panoptic segmentation with maskclip</data>
  <data key="d4">Z Ding, J Wang, Z Tu</data>
  <data key="d5">2022</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10981280129508603338&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="4328967688191824247">
  <data key="d0">Detecting everything in the open world: Towards universal object detection</data>
  <data key="d1">4328967688191824247</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Wang_Detecting_Everything_in_the_Open_World_Towards_Universal_Object_Detection_CVPR_2023_paper.html</data>
  <data key="d3">Detecting everything in the open world: Towards universal object detection</data>
  <data key="d4">Z Wang, Y Li, X Chen, SN Lim…</data>
  <data key="d5">2023</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4328967688191824247&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="7638029543193686772">
  <data key="d0">Aligning bag of regions for open-vocabulary object detection</data>
  <data key="d1">7638029543193686772</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Wu_Aligning_Bag_of_Regions_for_Open-Vocabulary_Object_Detection_CVPR_2023_paper.html</data>
  <data key="d3">Aligning bag of regions for open-vocabulary object detection</data>
  <data key="d4">S Wu, W Zhang, S Jin, W Liu…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7638029543193686772&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="13377052506571091555">
  <data key="d0">Partslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models</data>
  <data key="d1">13377052506571091555</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Liu_PartSLIP_Low-Shot_Part_Segmentation_for_3D_Point_Clouds_via_Pretrained_CVPR_2023_paper.html</data>
  <data key="d3">Partslip: Low-shot part segmentation for 3d point clouds via pretrained image-language models</data>
  <data key="d4">M Liu, Y Zhu, H Cai, S Han, Z Ling…</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13377052506571091555&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="1437976452746021768">
  <data key="d0">Scenecomposer: Any-level semantic image synthesis</data>
  <data key="d1">1437976452746021768</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zeng_SceneComposer_Any-Level_Semantic_Image_Synthesis_CVPR_2023_paper.html</data>
  <data key="d3">Scenecomposer: Any-level semantic image synthesis</data>
  <data key="d4">Y Zeng, Z Lin, J Zhang, Q Liu…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1437976452746021768&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="6353566274637563919">
  <data key="d0">Tarvis: A unified approach for target-based video segmentation</data>
  <data key="d1">6353566274637563919</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Athar_TarViS_A_Unified_Approach_for_Target-Based_Video_Segmentation_CVPR_2023_paper.html</data>
  <data key="d3">Tarvis: A unified approach for target-based video segmentation</data>
  <data key="d4">A Athar, A Hermans, J Luiten…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6353566274637563919&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="10393073040573897009">
  <data key="d0">Coupalign: Coupling word-pixel with sentence-mask alignments for referring image segmentation</data>
  <data key="d1">10393073040573897009</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/5e773d319e310f1e4d695159484143b8-Abstract-Conference.html</data>
  <data key="d3">Coupalign: Coupling word-pixel with sentence-mask alignments for referring image segmentation</data>
  <data key="d4">Z Zhang, Y Zhu, J Liu, X Liang…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10393073040573897009&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="8912529817908967060">
  <data key="d0">Capdet: Unifying dense captioning and open-world detection pretraining</data>
  <data key="d1">8912529817908967060</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Long_CapDet_Unifying_Dense_Captioning_and_Open-World_Detection_Pretraining_CVPR_2023_paper.html</data>
  <data key="d3">Capdet: Unifying dense captioning and open-world detection pretraining</data>
  <data key="d4">Y Long, Y Wen, J Han, H Xu, P Ren…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8912529817908967060&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="6499513093496013030">
  <data key="d0">Mixgen: A new multi-modal data augmentation</data>
  <data key="d1">6499513093496013030</data>
  <data key="d2">https://openaccess.thecvf.com/content/WACV2023W/Pretrain/html/Hao_MixGen_A_New_Multi-Modal_Data_Augmentation_WACVW_2023_paper.html</data>
  <data key="d3">Mixgen: A new multi-modal data augmentation</data>
  <data key="d4">X Hao, Y Zhu, S Appalaraju, A Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6499513093496013030&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="12164426444376333064">
  <data key="d0">Detclipv2: Scalable open-vocabulary object detection pre-training via word-region alignment</data>
  <data key="d1">12164426444376333064</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Yao_DetCLIPv2_Scalable_Open-Vocabulary_Object_Detection_Pre-Training_via_Word-Region_Alignment_CVPR_2023_paper.html</data>
  <data key="d3">Detclipv2: Scalable open-vocabulary object detection pre-training via word-region alignment</data>
  <data key="d4">L Yao, J Han, X Liang, D Xu…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12164426444376333064&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="12516751945568333636">
  <data key="d0">Task residual for tuning vision-language models</data>
  <data key="d1">12516751945568333636</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Yu_Task_Residual_for_Tuning_Vision-Language_Models_CVPR_2023_paper.html</data>
  <data key="d3">Task residual for tuning vision-language models</data>
  <data key="d4">T Yu, Z Lu, X Jin, Z Chen…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12516751945568333636&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="4093024291231214911">
  <data key="d0">Policy Adaptation From Foundation Model Feedback</data>
  <data key="d1">4093024291231214911</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023/html/Ge_Policy_Adaptation_From_Foundation_Model_Feedback_CVPR_2023_paper.html</data>
  <data key="d3">Policy Adaptation From Foundation Model Feedback</data>
  <data key="d4">Y Ge, A Macaluso, LE Li, P Luo…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4093024291231214911&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="8402450993508627791">
  <data key="d0">Large-scale multi-modal pre-trained models: A comprehensive survey</data>
  <data key="d1">8402450993508627791</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11633-022-1410-8</data>
  <data key="d3">Large-scale multi-modal pre-trained models: A comprehensive survey</data>
  <data key="d4">X Wang, G Chen, G Qian, P Gao, XY Wei…</data>
  <data key="d5">2023</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8402450993508627791&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="17253288657998487561">
  <data key="d0">EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding</data>
  <data key="d1">17253288657998487561</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Wu_EDA_Explicit_Text-Decoupling_and_Dense_Alignment_for_3D_Visual_Grounding_CVPR_2023_paper.html</data>
  <data key="d3">EDA: Explicit Text-Decoupling and Dense Alignment for 3D Visual Grounding</data>
  <data key="d4">Y Wu, X Cheng, R Zhang, Z Cheng…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17253288657998487561&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="17311003549850461359">
  <data key="d0">Blind image quality assessment via vision-language correspondence: A multitask learning perspective</data>
  <data key="d1">17311003549850461359</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Blind_Image_Quality_Assessment_via_Vision-Language_Correspondence_A_Multitask_Learning_CVPR_2023_paper.html</data>
  <data key="d3">Blind image quality assessment via vision-language correspondence: A multitask learning perspective</data>
  <data key="d4">W Zhang, G Zhai, Y Wei, X Yang…</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17311003549850461359&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="12473981189594605407">
  <data key="d0">A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning</data>
  <data key="d1">12473981189594605407</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023/html/Kamath_A_New_Path_Scaling_Vision-and-Language_Navigation_With_Synthetic_Instructions_and_CVPR_2023_paper.html</data>
  <data key="d3">A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning</data>
  <data key="d4">A Kamath, P Anderson, S Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12473981189594605407&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="5975344897755588707">
  <data key="d0">Bottom up top down detection transformers for language grounding in images and point clouds</data>
  <data key="d1">5975344897755588707</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20059-5_24</data>
  <data key="d3">Bottom up top down detection transformers for language grounding in images and point clouds</data>
  <data key="d4">A Jain, N Gkanatsios, I Mediratta…</data>
  <data key="d5">2022</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5975344897755588707&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="11304746998376087883">
  <data key="d0">Distilling Large Vision-Language Model with Out-of-Distribution Generalizability</data>
  <data key="d1">11304746998376087883</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Li_Distilling_Large_Vision-Language_Model_with_Out-of-Distribution_Generalizability_ICCV_2023_paper.html</data>
  <data key="d3">Distilling Large Vision-Language Model with Out-of-Distribution Generalizability</data>
  <data key="d4">X Li, Y Fang, M Liu, Z Ling, Z Tu…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11304746998376087883&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="347260239260204132">
  <data key="d0">Detection hub: Unifying object detection datasets via query adaptation on language embedding</data>
  <data key="d1">347260239260204132</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023/html/Meng_Detection_Hub_Unifying_Object_Detection_Datasets_via_Query_Adaptation_on_CVPR_2023_paper.html</data>
  <data key="d3">Detection hub: Unifying object detection datasets via query adaptation on language embedding</data>
  <data key="d4">L Meng, X Dai, Y Chen, P Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=347260239260204132&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="11224588046887428191">
  <data key="d0">Internchat: Solving vision-centric tasks by interacting with chatbots beyond language</data>
  <data key="d1">11224588046887428191</data>
  <data key="d2">https://arxiv.org/abs/2305.05662</data>
  <data key="d3">Internchat: Solving vision-centric tasks by interacting with chatbots beyond language</data>
  <data key="d4">Z Liu, Y He, W Wang, W Wang, Y Wang, S Chen…</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11224588046887428191&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="6014107160773218396">
  <data key="d0">Review of large vision models and visual prompt engineering</data>
  <data key="d1">6014107160773218396</data>
  <data key="d2">https://arxiv.org/abs/2307.00855</data>
  <data key="d3">Review of large vision models and visual prompt engineering</data>
  <data key="d4">J Wang, Z Liu, L Zhao, Z Wu, C Ma, S Yu, H Dai…</data>
  <data key="d5">2023</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6014107160773218396&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="15281327127593550121">
  <data key="d0">Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining</data>
  <data key="d1">15281327127593550121</data>
  <data key="d2">https://arxiv.org/abs/2302.02318</data>
  <data key="d3">Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining</data>
  <data key="d4">Z Qi, R Dong, G Fan, Z Ge, X Zhang, K Ma…</data>
  <data key="d5">2023</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15281327127593550121&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="2811646611673890603">
  <data key="d0">Learning object-language alignments for open-vocabulary object detection</data>
  <data key="d1">2811646611673890603</data>
  <data key="d2">https://arxiv.org/abs/2211.14843</data>
  <data key="d3">Learning object-language alignments for open-vocabulary object detection</data>
  <data key="d4">C Lin, P Sun, Y Jiang, P Luo, L Qu, G Haffari…</data>
  <data key="d5">2022</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2811646611673890603&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="12439397764083500705">
  <data key="d0">Write and paint: Generative vision-language models are unified modal learners</data>
  <data key="d1">12439397764083500705</data>
  <data key="d2">https://openreview.net/forum?id=HgQR0mXQ1_a</data>
  <data key="d3">Write and paint: Generative vision-language models are unified modal learners</data>
  <data key="d4">S Diao, W Zhou, X Zhang, J Wang</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12439397764083500705&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="5897644011303594318">
  <data key="d0">xGQA: Cross-lingual visual question answering</data>
  <data key="d1">5897644011303594318</data>
  <data key="d2">https://arxiv.org/abs/2109.06082</data>
  <data key="d3">xGQA: Cross-lingual visual question answering</data>
  <data key="d4">J Pfeiffer, G Geigle, A Kamath, JMO Steitz…</data>
  <data key="d5">2021</data>
  <data key="d6">24</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5897644011303594318&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="8565593079230529362">
  <data key="d0">Dq-detr: Dual query detection transformer for phrase extraction and grounding</data>
  <data key="d1">8565593079230529362</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/25261</data>
  <data key="d3">Dq-detr: Dual query detection transformer for phrase extraction and grounding</data>
  <data key="d4">S Liu, S Huang, F Li, H Zhang, Y Liang, H Su…</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8565593079230529362&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="3496963086485613469">
  <data key="d0">Transferable decoding with visual entities for zero-shot image captioning</data>
  <data key="d1">3496963086485613469</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Fei_Transferable_Decoding_with_Visual_Entities_for_Zero-Shot_Image_Captioning_ICCV_2023_paper.html</data>
  <data key="d3">Transferable decoding with visual entities for zero-shot image captioning</data>
  <data key="d4">J Fei, T Wang, J Zhang, Z He…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3496963086485613469&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="7990405874739376119">
  <data key="d0">Accelerating vision-language pretraining with free language modeling</data>
  <data key="d1">7990405874739376119</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Wang_Accelerating_Vision-Language_Pretraining_With_Free_Language_Modeling_CVPR_2023_paper.html</data>
  <data key="d3">Accelerating vision-language pretraining with free language modeling</data>
  <data key="d4">T Wang, Y Ge, F Zheng, R Cheng…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7990405874739376119&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="14743366429652248033">
  <data key="d0">CLIP the Gap: A Single Domain Generalization Approach for Object Detection</data>
  <data key="d1">14743366429652248033</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Vidit_CLIP_the_Gap_A_Single_Domain_Generalization_Approach_for_Object_CVPR_2023_paper.html</data>
  <data key="d3">CLIP the Gap: A Single Domain Generalization Approach for Object Detection</data>
  <data key="d4">V Vidit, M Engilberge…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14743366429652248033&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="10883123266865516659">
  <data key="d0">Medical image understanding with pretrained vision language models: A comprehensive study</data>
  <data key="d1">10883123266865516659</data>
  <data key="d2">https://arxiv.org/abs/2209.15517</data>
  <data key="d3">Medical image understanding with pretrained vision language models: A comprehensive study</data>
  <data key="d4">Z Qin, H Yi, Q Lao, K Li</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10883123266865516659&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="3817102158084191521">
  <data key="d0">Zero-shot Generative Model Adaptation via Image-specific Prompt Learning</data>
  <data key="d1">3817102158084191521</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Guo_Zero-Shot_Generative_Model_Adaptation_via_Image-Specific_Prompt_Learning_CVPR_2023_paper.html</data>
  <data key="d3">Zero-shot Generative Model Adaptation via Image-specific Prompt Learning</data>
  <data key="d4">J Guo, C Wang, Y Wu, E Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3817102158084191521&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="17782492311519284315">
  <data key="d0">Regionplc: Regional point-language contrastive learning for open-world 3d scene understanding</data>
  <data key="d1">17782492311519284315</data>
  <data key="d2">https://arxiv.org/abs/2304.00962</data>
  <data key="d3">Regionplc: Regional point-language contrastive learning for open-world 3d scene understanding</data>
  <data key="d4">J Yang, R Ding, Z Wang, X Qi</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17782492311519284315&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="10132989367820823432">
  <data key="d0">Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations</data>
  <data key="d1">10132989367820823432</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Yang_Improving_Visual_Grounding_by_Encouraging_Consistent_Gradient-Based_Explanations_CVPR_2023_paper.html</data>
  <data key="d3">Improving Visual Grounding by Encouraging Consistent Gradient-based Explanations</data>
  <data key="d4">Z Yang, K Kafle, F Dernoncourt…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10132989367820823432&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="13594362999251312694">
  <data key="d0">Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving</data>
  <data key="d1">13594362999251312694</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Najibi_Unsupervised_3D_Perception_with_2D_Vision-Language_Distillation_for_Autonomous_Driving_ICCV_2023_paper.html</data>
  <data key="d3">Unsupervised 3D Perception with 2D Vision-Language Distillation for Autonomous Driving</data>
  <data key="d4">M Najibi, J Ji, Y Zhou, CR Qi, X Yan…</data>
  <data key="d5">2023</data>
  <data key="d8">10</data>
</node>
<node id="5126083329506829425">
  <data key="d0">Comclip: Training-free compositional image and text matching</data>
  <data key="d1">5126083329506829425</data>
  <data key="d2">https://arxiv.org/abs/2211.13854</data>
  <data key="d3">Comclip: Training-free compositional image and text matching</data>
  <data key="d4">K Jiang, X He, R Xu, XE Wang</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5126083329506829425&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="841216030252728204">
  <data key="d0">V3det: Vast vocabulary visual detection dataset</data>
  <data key="d1">841216030252728204</data>
  <data key="d2">https://arxiv.org/abs/2304.03752</data>
  <data key="d3">V3det: Vast vocabulary visual detection dataset</data>
  <data key="d4">J Wang, P Zhang, T Chu, Y Cao, Y Zhou, T Wu…</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=841216030252728204&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="8BD3FPNAIHoJ">
  <data key="d0">Segment Every Reference Object in Spatial and Temporal Spaces</data>
  <data key="d1">8BD3FPNAIHoJ</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Wu_Segment_Every_Reference_Object_in_Spatial_and_Temporal_Spaces_ICCV_2023_paper.html</data>
  <data key="d3">Segment Every Reference Object in Spatial and Temporal Spaces</data>
  <data key="d4">J Wu, Y Jiang, B Yan, H Lu…</data>
  <data key="d5">2023</data>
  <data key="d8">10</data>
</node>
<node id="12075572838672919294">
  <data key="d0">You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model</data>
  <data key="d1">12075572838672919294</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Tang_You_Need_Multiple_Exiting_Dynamic_Early_Exiting_for_Accelerating_Unified_CVPR_2023_paper.html</data>
  <data key="d3">You Need Multiple Exiting: Dynamic Early Exiting for Accelerating Unified Vision Language Model</data>
  <data key="d4">S Tang, Y Wang, Z Kong, T Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12075572838672919294&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="11630631256493667109">
  <data key="d0">Punifiedner: a prompting-based unified ner system for diverse datasets</data>
  <data key="d1">11630631256493667109</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/26564</data>
  <data key="d3">Punifiedner: a prompting-based unified ner system for diverse datasets</data>
  <data key="d4">J Lu, R Zhao, B Mac Namee, F Tan</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11630631256493667109&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="8636762778479383554">
  <data key="d0">A data-scalable transformer for medical image segmentation: architecture, model efficiency, and benchmark</data>
  <data key="d1">8636762778479383554</data>
  <data key="d2">https://arxiv.org/abs/2203.00131</data>
  <data key="d3">A data-scalable transformer for medical image segmentation: architecture, model efficiency, and benchmark</data>
  <data key="d4">Y Gao, M Zhou, D Liu, Z Yan, S Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8636762778479383554&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="8814192339048828786">
  <data key="d0">Are Multimodal Models Robust to Image and Text Perturbations?</data>
  <data key="d1">8814192339048828786</data>
  <data key="d2">https://arxiv.org/abs/2212.08044</data>
  <data key="d3">Are Multimodal Models Robust to Image and Text Perturbations?</data>
  <data key="d4">J Qiu, Y Zhu, X Shi, F Wenzel, Z Tang, D Zhao…</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8814192339048828786&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="b5DIgD2ZqeQJ">
  <data key="d0">SLAN: Self-Locator Aided Network for Vision-Language Understanding</data>
  <data key="d1">b5DIgD2ZqeQJ</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Zhai_SLAN_Self-Locator_Aided_Network_for_Vision-Language_Understanding_ICCV_2023_paper.html</data>
  <data key="d3">SLAN: Self-Locator Aided Network for Vision-Language Understanding</data>
  <data key="d4">JT Zhai, Q Zhang, T Wu, XY Chen…</data>
  <data key="d5">2023</data>
  <data key="d8">10</data>
</node>
<node id="1273811038957334386">
  <data key="d0">Mvitv2: Improved multiscale vision transformers for classification and detection</data>
  <data key="d1">1273811038957334386</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.html</data>
  <data key="d3">Mvitv2: Improved multiscale vision transformers for classification and detection</data>
  <data key="d4">Y Li, CY Wu, H Fan, K Mangalam…</data>
  <data key="d5">2022</data>
  <data key="d6">310</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1273811038957334386&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="12617218192144474322">
  <data key="d0">Masked feature prediction for self-supervised visual pre-training</data>
  <data key="d1">12617218192144474322</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Wei_Masked_Feature_Prediction_for_Self-Supervised_Visual_Pre-Training_CVPR_2022_paper.html</data>
  <data key="d3">Masked feature prediction for self-supervised visual pre-training</data>
  <data key="d4">C Wei, H Fan, S Xie, CY Wu, A Yuille…</data>
  <data key="d5">2022</data>
  <data key="d6">357</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12617218192144474322&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="5215096183189163093">
  <data key="d0">Masked autoencoders as spatiotemporal learners</data>
  <data key="d1">5215096183189163093</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/e97d1081481a4017df96b51be31001d3-Abstract-Conference.html</data>
  <data key="d3">Masked autoencoders as spatiotemporal learners</data>
  <data key="d4">C Feichtenhofer, Y Li, K He</data>
  <data key="d5">2022</data>
  <data key="d6">193</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5215096183189163093&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="17757348919061164318">
  <data key="d0">Volo: Vision outlooker for visual recognition</data>
  <data key="d1">17757348919061164318</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9888055/</data>
  <data key="d3">Volo: Vision outlooker for visual recognition</data>
  <data key="d4">L Yuan, Q Hou, Z Jiang, J Feng…</data>
  <data key="d5">2022</data>
  <data key="d6">192</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17757348919061164318&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="12692106295877813680">
  <data key="d0">Efficientformer: Vision transformers at mobilenet speed</data>
  <data key="d1">12692106295877813680</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/5452ad8ee6ea6e7dc41db1cbd31ba0b8-Abstract-Conference.html</data>
  <data key="d3">Efficientformer: Vision transformers at mobilenet speed</data>
  <data key="d4">Y Li, G Yuan, Y Wen, J Hu…</data>
  <data key="d5">2022</data>
  <data key="d6">90</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12692106295877813680&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="1144066736404687657">
  <data key="d0">Expanding language-image pretrained models for general video recognition</data>
  <data key="d1">1144066736404687657</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19772-7_1</data>
  <data key="d3">Expanding language-image pretrained models for general video recognition</data>
  <data key="d4">B Ni, H Peng, M Chen, S Zhang, G Meng, J Fu…</data>
  <data key="d5">2022</data>
  <data key="d6">93</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1144066736404687657&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="12972864106896201781">
  <data key="d0">Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition</data>
  <data key="d1">12972864106896201781</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Wu_MeMViT_Memory-Augmented_Multiscale_Vision_Transformer_for_Efficient_Long-Term_Video_Recognition_CVPR_2022_paper.html</data>
  <data key="d3">Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition</data>
  <data key="d4">CY Wu, Y Li, K Mangalam, H Fan…</data>
  <data key="d5">2022</data>
  <data key="d6">86</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12972864106896201781&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="15930910090552432609">
  <data key="d0">St-adapter: Parameter-efficient image-to-video transfer learning</data>
  <data key="d1">15930910090552432609</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/a92e9165b22d4456fc6d87236e04c266-Abstract-Conference.html</data>
  <data key="d3">St-adapter: Parameter-efficient image-to-video transfer learning</data>
  <data key="d4">J Pan, Z Lin, X Zhu, J Shao, H Li</data>
  <data key="d5">2022</data>
  <data key="d6">53</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15930910090552432609&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="16057670792750577500">
  <data key="d0">Frozen clip models are efficient video learners</data>
  <data key="d1">16057670792750577500</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19833-5_23</data>
  <data key="d3">Frozen clip models are efficient video learners</data>
  <data key="d4">Z Lin, S Geng, R Zhang, P Gao, G de Melo…</data>
  <data key="d5">2022</data>
  <data key="d6">63</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16057670792750577500&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="3876751087281584278">
  <data key="d0">Rethinking vision transformers for mobilenet size and speed</data>
  <data key="d1">3876751087281584278</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Li_Rethinking_Vision_Transformers_for_MobileNet_Size_and_Speed_ICCV_2023_paper.html</data>
  <data key="d3">Rethinking vision transformers for mobilenet size and speed</data>
  <data key="d4">Y Li, J Hu, Y Wen, G Evangelidis…</data>
  <data key="d5">2023</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3876751087281584278&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="467066746241725706">
  <data key="d0">Benchmarking detection transfer learning with vision transformers</data>
  <data key="d1">467066746241725706</data>
  <data key="d2">https://arxiv.org/abs/2111.11429</data>
  <data key="d3">Benchmarking detection transfer learning with vision transformers</data>
  <data key="d4">Y Li, S Xie, X Chen, P Dollar, K He…</data>
  <data key="d5">2021</data>
  <data key="d6">94</data>
  <data key="d7">https://scholar.google.com/scholar?cites=467066746241725706&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="5506925343943935516">
  <data key="d0">Videomae v2: Scaling video masked autoencoders with dual masking</data>
  <data key="d1">5506925343943935516</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Wang_VideoMAE_V2_Scaling_Video_Masked_Autoencoders_With_Dual_Masking_CVPR_2023_paper.html</data>
  <data key="d3">Videomae v2: Scaling video masked autoencoders with dual masking</data>
  <data key="d4">L Wang, B Huang, Z Zhao, Z Tong…</data>
  <data key="d5">2023</data>
  <data key="d6">31</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5506925343943935516&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="2000389979125404276">
  <data key="d0">CoBEVT: Cooperative bird's eye view semantic segmentation with sparse transformers</data>
  <data key="d1">2000389979125404276</data>
  <data key="d2">https://arxiv.org/abs/2207.02202</data>
  <data key="d3">CoBEVT: Cooperative bird's eye view semantic segmentation with sparse transformers</data>
  <data key="d4">R Xu, Z Tu, H Xiang, W Shao, B Zhou, J Ma</data>
  <data key="d5">2022</data>
  <data key="d6">66</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2000389979125404276&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="17534197269359092183">
  <data key="d0">Masked autoencoders that listen</data>
  <data key="d1">17534197269359092183</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/b89d5e209990b19e33b418e14f323998-Abstract-Conference.html</data>
  <data key="d3">Masked autoencoders that listen</data>
  <data key="d4">PY Huang, H Xu, J Li, A Baevski…</data>
  <data key="d5">2022</data>
  <data key="d6">45</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17534197269359092183&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="5763129354275940103">
  <data key="d0">Hitea: Hierarchical temporal-aware video-language pre-training</data>
  <data key="d1">5763129354275940103</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Ye_HiTeA_Hierarchical_Temporal-Aware_Video-Language_Pre-training_ICCV_2023_paper.html</data>
  <data key="d3">Hitea: Hierarchical temporal-aware video-language pre-training</data>
  <data key="d4">Q Ye, G Xu, M Yan, H Xu, Q Qian…</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5763129354275940103&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="13233494379811120690">
  <data key="d0">Masked autoencoders that listen</data>
  <data key="d1">13233494379811120690</data>
  <data key="d2">https://arxiv.org/abs/2207.06405</data>
  <data key="d3">Masked autoencoders that listen</data>
  <data key="d4">PY Huang, H Xu, J Li, A Baevski, M Auli…</data>
  <data key="d5">2022</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13233494379811120690&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="9867831262985002468">
  <data key="d0">Omnimae: Single model masked pretraining on images and videos</data>
  <data key="d1">9867831262985002468</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Girdhar_OmniMAE_Single_Model_Masked_Pretraining_on_Images_and_Videos_CVPR_2023_paper.html</data>
  <data key="d3">Omnimae: Single model masked pretraining on images and videos</data>
  <data key="d4">R Girdhar, A El-Nouby, M Singh…</data>
  <data key="d5">2023</data>
  <data key="d6">27</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9867831262985002468&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="8988041508983958224">
  <data key="d0">Squeezeformer: An efficient transformer for automatic speech recognition</data>
  <data key="d1">8988041508983958224</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/3ccf6da39eeb8fefc8bbb1b0124adbd1-Abstract-Conference.html</data>
  <data key="d3">Squeezeformer: An efficient transformer for automatic speech recognition</data>
  <data key="d4">S Kim, A Gholami, A Shaw, N Lee…</data>
  <data key="d5">2022</data>
  <data key="d6">27</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8988041508983958224&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="13961698310318632817">
  <data key="d0">Rethinking video vits: Sparse video tubes for joint image and video learning</data>
  <data key="d1">13961698310318632817</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Piergiovanni_Rethinking_Video_ViTs_Sparse_Video_Tubes_for_Joint_Image_and_CVPR_2023_paper.html</data>
  <data key="d3">Rethinking video vits: Sparse video tubes for joint image and video learning</data>
  <data key="d4">AJ Piergiovanni, W Kuo…</data>
  <data key="d5">2023</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13961698310318632817&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="17264780080323807782">
  <data key="d0">Metaformer baselines for vision</data>
  <data key="d1">17264780080323807782</data>
  <data key="d2">https://arxiv.org/abs/2210.13452</data>
  <data key="d3">Metaformer baselines for vision</data>
  <data key="d4">W Yu, C Si, P Zhou, M Luo, Y Zhou, J Feng…</data>
  <data key="d5">2022</data>
  <data key="d6">27</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17264780080323807782&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="7521998026481226724">
  <data key="d0">Ted-spad: Temporal distinctiveness for self-supervised privacy-preservation for video anomaly detection</data>
  <data key="d1">7521998026481226724</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Fioresi_TeD-SPAD_Temporal_Distinctiveness_for_Self-Supervised_Privacy-Preservation_for_Video_Anomaly_Detection_ICCV_2023_paper.html</data>
  <data key="d3">Ted-spad: Temporal distinctiveness for self-supervised privacy-preservation for video anomaly detection</data>
  <data key="d4">J Fioresi, IR Dave, M Shah</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7521998026481226724&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="14292753405609492066">
  <data key="d0">Stargazer: A transformer-based driver action detection system for intelligent transportation</data>
  <data key="d1">14292753405609492066</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Liang_Stargazer_A_Transformer-Based_Driver_Action_Detection_System_for_Intelligent_Transportation_CVPRW_2022_paper.html</data>
  <data key="d3">Stargazer: A transformer-based driver action detection system for intelligent transportation</data>
  <data key="d4">J Liang, H Zhu, E Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14292753405609492066&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="15701650176909360920">
  <data key="d0">Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning</data>
  <data key="d1">15701650176909360920</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Wang_Masked_Video_Distillation_Rethinking_Masked_Feature_Modeling_for_Self-Supervised_Video_CVPR_2023_paper.html</data>
  <data key="d3">Masked video distillation: Rethinking masked feature modeling for self-supervised video representation learning</data>
  <data key="d4">R Wang, D Chen, Z Wu, Y Chen…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15701650176909360920&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="7083069404926056970">
  <data key="d0">Re2tal: Rewiring pretrained video backbones for reversible temporal action localization</data>
  <data key="d1">7083069404926056970</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Re2TAL_Rewiring_Pretrained_Video_Backbones_for_Reversible_Temporal_Action_Localization_CVPR_2023_paper.html</data>
  <data key="d3">Re2tal: Rewiring pretrained video backbones for reversible temporal action localization</data>
  <data key="d4">C Zhao, S Liu, K Mangalam…</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7083069404926056970&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="16345516296958117809">
  <data key="d0">Conv2former: A simple transformer-style convnet for visual recognition</data>
  <data key="d1">16345516296958117809</data>
  <data key="d2">https://arxiv.org/abs/2211.11943</data>
  <data key="d3">Conv2former: A simple transformer-style convnet for visual recognition</data>
  <data key="d4">Q Hou, CZ Lu, MM Cheng, J Feng</data>
  <data key="d5">2022</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16345516296958117809&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="18352250779745802826">
  <data key="d0">MsSVT: Mixed-scale sparse voxel transformer for 3d object detection on point clouds</data>
  <data key="d1">18352250779745802826</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/4bad7c27534efca029ca0d366c47c0e3-Abstract-Conference.html</data>
  <data key="d3">MsSVT: Mixed-scale sparse voxel transformer for 3d object detection on point clouds</data>
  <data key="d4">S Dong, L Ding, H Wang, T Xu, X Xu…</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18352250779745802826&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="12521341369829058727">
  <data key="d0">Mobilevitv3: Mobile-friendly vision transformer with simple and effective fusion of local, global and input features</data>
  <data key="d1">12521341369829058727</data>
  <data key="d2">https://arxiv.org/abs/2209.15159</data>
  <data key="d3">Mobilevitv3: Mobile-friendly vision transformer with simple and effective fusion of local, global and input features</data>
  <data key="d4">SN Wadekar, A Chaurasia</data>
  <data key="d5">2022</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12521341369829058727&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="18278928779930263666">
  <data key="d0">Multi-dataset Training of Transformers for Robust Action Recognition</data>
  <data key="d1">18278928779930263666</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/5d2e24df9cfaad3189833b819c40b392-Abstract-Conference.html</data>
  <data key="d3">Multi-dataset Training of Transformers for Robust Action Recognition</data>
  <data key="d4">J Liang, E Zhang, J Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18278928779930263666&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="11435180467185856094">
  <data key="d0">Attention-based distributed deep learning model for air quality forecasting</data>
  <data key="d1">11435180467185856094</data>
  <data key="d2">https://www.mdpi.com/2071-1050/14/6/3269</data>
  <data key="d3">Attention-based distributed deep learning model for air quality forecasting</data>
  <data key="d4">AG Mengara Mengara, E Park, J Jang, Y Yoo</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11435180467185856094&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="297116116872847031">
  <data key="d0">Moat: Alternating mobile convolution and attention brings strong vision models</data>
  <data key="d1">297116116872847031</data>
  <data key="d2">https://arxiv.org/abs/2210.01820</data>
  <data key="d3">Moat: Alternating mobile convolution and attention brings strong vision models</data>
  <data key="d4">C Yang, S Qiao, Q Yu, X Yuan, Y Zhu, A Yuille…</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=297116116872847031&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="10401914588824198986">
  <data key="d0">Transformer-based visual segmentation: A survey</data>
  <data key="d1">10401914588824198986</data>
  <data key="d2">https://arxiv.org/abs/2304.09854</data>
  <data key="d3">Transformer-based visual segmentation: A survey</data>
  <data key="d4">X Li, H Ding, W Zhang, H Yuan, J Pang…</data>
  <data key="d5">2023</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10401914588824198986&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="1636570585907882285">
  <data key="d0">Marlin: Masked autoencoder for facial video representation learning</data>
  <data key="d1">1636570585907882285</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Cai_MARLIN_Masked_Autoencoder_for_Facial_Video_Representation_LearnINg_CVPR_2023_paper.html</data>
  <data key="d3">Marlin: Masked autoencoder for facial video representation learning</data>
  <data key="d4">Z Cai, S Ghosh, K Stefanov, A Dhall…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1636570585907882285&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="12251423014012987213">
  <data key="d0">Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations</data>
  <data key="d1">12251423014012987213</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Yu_Learning_Procedure-Aware_Video_Representation_From_Instructional_Videos_and_Their_Narrations_CVPR_2023_paper.html</data>
  <data key="d3">Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations</data>
  <data key="d4">Y Zhong, L Yu, Y Bai, S Li, X Yan…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12251423014012987213&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="16223923155940056199">
  <data key="d0">HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining</data>
  <data key="d1">16223923155940056199</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Tang_HumanBench_Towards_General_Human-Centric_Perception_With_Projector_Assisted_Pretraining_CVPR_2023_paper.html</data>
  <data key="d3">HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining</data>
  <data key="d4">S Tang, C Chen, Q Xie, M Chen…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16223923155940056199&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="1475939297738772704">
  <data key="d0">Prune spatio-temporal tokens by semantic-aware temporal accumulation</data>
  <data key="d1">1475939297738772704</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Ding_Prune_Spatio-temporal_Tokens_by_Semantic-aware_Temporal_Accumulation_ICCV_2023_paper.html</data>
  <data key="d3">Prune spatio-temporal tokens by semantic-aware temporal accumulation</data>
  <data key="d4">S Ding, P Zhao, X Zhang, R Qian…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1475939297738772704&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="163853940069191532">
  <data key="d0">Transformers meet visual learning understanding: A comprehensive review</data>
  <data key="d1">163853940069191532</data>
  <data key="d2">https://arxiv.org/abs/2203.12944</data>
  <data key="d3">Transformers meet visual learning understanding: A comprehensive review</data>
  <data key="d4">Y Yang, L Jiao, X Liu, F Liu, S Yang, Z Feng…</data>
  <data key="d5">2022</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=163853940069191532&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="17353513126312288026">
  <data key="d0">Unmasked teacher: Towards training-efficient video foundation models</data>
  <data key="d1">17353513126312288026</data>
  <data key="d2">https://arxiv.org/abs/2303.16058</data>
  <data key="d3">Unmasked teacher: Towards training-efficient video foundation models</data>
  <data key="d4">K Li, Y Wang, Y Li, Y Wang, Y He, L Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17353513126312288026&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="4553933449540878013">
  <data key="d0">Revisiting temporal modeling for clip-based image-to-video knowledge transferring</data>
  <data key="d1">4553933449540878013</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Liu_Revisiting_Temporal_Modeling_for_CLIP-Based_Image-to-Video_Knowledge_Transferring_CVPR_2023_paper.html</data>
  <data key="d3">Revisiting temporal modeling for clip-based image-to-video knowledge transferring</data>
  <data key="d4">R Liu, J Huang, G Li, J Feng…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4553933449540878013&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="7548052650525629965">
  <data key="d0">Rethinking attention mechanism in time series classification</data>
  <data key="d1">7548052650525629965</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0020025523000968</data>
  <data key="d3">Rethinking attention mechanism in time series classification</data>
  <data key="d4">B Zhao, H Xing, X Wang, F Song, Z Xiao</data>
  <data key="d5">2023</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7548052650525629965&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="13374404244463301147">
  <data key="d0">Mar: Masked autoencoders for efficient action recognition</data>
  <data key="d1">13374404244463301147</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10089159/</data>
  <data key="d3">Mar: Masked autoencoders for efficient action recognition</data>
  <data key="d4">Z Qing, S Zhang, Z Huang, X Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13374404244463301147&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="11917368591479581577">
  <data key="d0">Vision Transformer with Super Token Sampling</data>
  <data key="d1">11917368591479581577</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Huang_Vision_Transformer_With_Super_Token_Sampling_CVPR_2023_paper.html</data>
  <data key="d3">Vision Transformer with Super Token Sampling</data>
  <data key="d4">H Huang, X Zhou, J Cao, R He…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11917368591479581577&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="18135230557293693450">
  <data key="d0">HierVL: Learning Hierarchical Video-Language Embeddings</data>
  <data key="d1">18135230557293693450</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Ashutosh_HierVL_Learning_Hierarchical_Video-Language_Embeddings_CVPR_2023_paper.html</data>
  <data key="d3">HierVL: Learning Hierarchical Video-Language Embeddings</data>
  <data key="d4">K Ashutosh, R Girdhar, L Torresani…</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18135230557293693450&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="2687678320604906488">
  <data key="d0">D3Former: Debiased Dual Distilled Transformer for Incremental Learning</data>
  <data key="d1">2687678320604906488</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/CLVision/html/Mohamed_D3Former_Debiased_Dual_Distilled_Transformer_for_Incremental_Learning_CVPRW_2023_paper.html</data>
  <data key="d3">D3Former: Debiased Dual Distilled Transformer for Incremental Learning</data>
  <data key="d4">A Mohamed, R Grandhe, KJ Joseph…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2687678320604906488&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="7457602010468600062">
  <data key="d0">Inceptionnext: When inception meets convnext</data>
  <data key="d1">7457602010468600062</data>
  <data key="d2">https://arxiv.org/abs/2303.16900</data>
  <data key="d3">Inceptionnext: When inception meets convnext</data>
  <data key="d4">W Yu, P Zhou, S Yan, X Wang</data>
  <data key="d5">2023</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7457602010468600062&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="14160811618017343674">
  <data key="d0">Dual-domain attention for image deblurring</data>
  <data key="d1">14160811618017343674</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/25122</data>
  <data key="d3">Dual-domain attention for image deblurring</data>
  <data key="d4">Y Cui, Y Tao, W Ren, A Knoll</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14160811618017343674&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="7586367217993538099">
  <data key="d0">Design and Application of a UAV Autonomous Inspection System for High-Voltage Power Transmission Lines</data>
  <data key="d1">7586367217993538099</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/3/865</data>
  <data key="d3">Design and Application of a UAV Autonomous Inspection System for High-Voltage Power Transmission Lines</data>
  <data key="d4">Z Li, Y Zhang, H Wu, S Suzuki, A Namiki, W Wang</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7586367217993538099&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="k2InbV7B6EkJ">
  <data key="d0">Ego-Only: Egocentric Action Detection without Exocentric Transferring</data>
  <data key="d1">k2InbV7B6EkJ</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Ego-Only_Egocentric_Action_Detection_without_Exocentric_Transferring_ICCV_2023_paper.html</data>
  <data key="d3">Ego-Only: Egocentric Action Detection without Exocentric Transferring</data>
  <data key="d4">H Wang, MK Singh, L Torresani</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="8672277335438437194">
  <data key="d0">Adapool: Exponential adaptive pooling for information-retaining downsampling</data>
  <data key="d1">8672277335438437194</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9982650/</data>
  <data key="d3">Adapool: Exponential adaptive pooling for information-retaining downsampling</data>
  <data key="d4">A Stergiou, R Poppe</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8672277335438437194&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="8783919417057800696">
  <data key="d0">Bringing image scene structure to video via frame-clip consistency of object tokens</data>
  <data key="d1">8783919417057800696</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/abc1943857a42935ceacff03c524bb44-Abstract-Conference.html</data>
  <data key="d3">Bringing image scene structure to video via frame-clip consistency of object tokens</data>
  <data key="d4">E Ben Avraham, R Herzig…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8783919417057800696&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="17020241982343048724">
  <data key="d0">Promptonomyvit: Multi-task prompt learning improves video transformers using synthetic scene data</data>
  <data key="d1">17020241982343048724</data>
  <data key="d2">https://arxiv.org/abs/2212.04821</data>
  <data key="d3">Promptonomyvit: Multi-task prompt learning improves video transformers using synthetic scene data</data>
  <data key="d4">R Herzig, O Abramovich, E Ben-Avraham…</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17020241982343048724&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="6135323271371562232">
  <data key="d0">Relational Space-Time Query in Long-Form Videos</data>
  <data key="d1">6135323271371562232</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023/html/Yang_Relational_Space-Time_Query_in_Long-Form_Videos_CVPR_2023_paper.html</data>
  <data key="d3">Relational Space-Time Query in Long-Form Videos</data>
  <data key="d4">X Yang, FJ Chu, M Feiszli, R Goyal…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6135323271371562232&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="9897568004629027598">
  <data key="d0">On the Benefits of 3D Pose and Tracking for Human Action Recognition</data>
  <data key="d1">9897568004629027598</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Rajasegaran_On_the_Benefits_of_3D_Pose_and_Tracking_for_Human_CVPR_2023_paper.html</data>
  <data key="d3">On the Benefits of 3D Pose and Tracking for Human Action Recognition</data>
  <data key="d4">J Rajasegaran, G Pavlakos…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9897568004629027598&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="iPZOpP141k4J">
  <data key="d0">UniFormerV2: Unlocking the Potential of Image ViTs for Video Understanding</data>
  <data key="d1">iPZOpP141k4J</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Li_UniFormerV2_Unlocking_the_Potential_of_Image_ViTs_for_Video_Understanding_ICCV_2023_paper.html</data>
  <data key="d3">UniFormerV2: Unlocking the Potential of Image ViTs for Video Understanding</data>
  <data key="d4">K Li, Y Wang, Y He, Y Li, Y Wang…</data>
  <data key="d5">2023</data>
  <data key="d8">2</data>
</node>
<node id="5346646808921915555">
  <data key="d0">TransCNN: Hybrid CNN and transformer mechanism for surveillance anomaly detection</data>
  <data key="d1">5346646808921915555</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0952197623003573</data>
  <data key="d3">TransCNN: Hybrid CNN and transformer mechanism for surveillance anomaly detection</data>
  <data key="d4">W Ullah, T Hussain, FUM Ullah, MY Lee…</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5346646808921915555&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="14765226393767823271">
  <data key="d0">Latency Matters: Real-Time Action Forecasting Transformer</data>
  <data key="d1">14765226393767823271</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Girase_Latency_Matters_Real-Time_Action_Forecasting_Transformer_CVPR_2023_paper.html</data>
  <data key="d3">Latency Matters: Real-Time Action Forecasting Transformer</data>
  <data key="d4">H Girase, N Agarwal, C Choi…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14765226393767823271&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="5678901956168261131">
  <data key="d0">MAViL: Masked Audio-Video Learners</data>
  <data key="d1">5678901956168261131</data>
  <data key="d2">https://arxiv.org/abs/2212.08071</data>
  <data key="d3">MAViL: Masked Audio-Video Learners</data>
  <data key="d4">PY Huang, V Sharma, H Xu, C Ryali, H Fan, Y Li…</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5678901956168261131&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="1431633311194488622">
  <data key="d0">A comprehensive survey on graph neural networks</data>
  <data key="d1">1431633311194488622</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9046288/</data>
  <data key="d3">A comprehensive survey on graph neural networks</data>
  <data key="d4">Z Wu, S Pan, F Chen, G Long, C Zhang…</data>
  <data key="d5">2020</data>
  <data key="d6">6447</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1431633311194488622&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="17457133269069328267">
  <data key="d0">Diffusion models: A comprehensive survey of methods and applications</data>
  <data key="d1">17457133269069328267</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3626235</data>
  <data key="d3">Diffusion models: A comprehensive survey of methods and applications</data>
  <data key="d4">L Yang, Z Zhang, Y Song, S Hong, R Xu, Y Zhao…</data>
  <data key="d5">2022</data>
  <data key="d6">257</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17457133269069328267&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="5640775539800892593">
  <data key="d0">A comprehensive survey on graph anomaly detection with deep learning</data>
  <data key="d1">5640775539800892593</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9565320/</data>
  <data key="d3">A comprehensive survey on graph anomaly detection with deep learning</data>
  <data key="d4">X Ma, J Wu, S Xue, J Yang, C Zhou…</data>
  <data key="d5">2021</data>
  <data key="d6">486</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5640775539800892593&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="17228116273163901194">
  <data key="d0">Deep learning--based text classification: a comprehensive review</data>
  <data key="d1">17228116273163901194</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3439726</data>
  <data key="d3">Deep learning--based text classification: a comprehensive review</data>
  <data key="d4">S Minaee, N Kalchbrenner, E Cambria…</data>
  <data key="d5">2021</data>
  <data key="d6">1136</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17228116273163901194&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="5656297883023258429">
  <data key="d0">How attentive are graph attention networks?</data>
  <data key="d1">5656297883023258429</data>
  <data key="d2">https://arxiv.org/abs/2105.14491</data>
  <data key="d3">How attentive are graph attention networks?</data>
  <data key="d4">S Brody, U Alon, E Yahav</data>
  <data key="d5">2021</data>
  <data key="d6">471</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5656297883023258429&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="2381021153585051804">
  <data key="d0">Graph neural networks: A review of methods and applications</data>
  <data key="d1">2381021153585051804</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2666651021000012</data>
  <data key="d3">Graph neural networks: A review of methods and applications</data>
  <data key="d4">J Zhou, G Cui, S Hu, Z Zhang, C Yang, Z Liu, L Wang…</data>
  <data key="d5">2020</data>
  <data key="d6">4126</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2381021153585051804&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="11131425815493661687">
  <data key="d0">Contrastive multi-view representation learning on graphs</data>
  <data key="d1">11131425815493661687</data>
  <data key="d2">http://proceedings.mlr.press/v119/hassani20a.html</data>
  <data key="d3">Contrastive multi-view representation learning on graphs</data>
  <data key="d4">K Hassani, AH Khasahmadi</data>
  <data key="d5">2020</data>
  <data key="d6">850</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11131425815493661687&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="16559789848821467555">
  <data key="d0">Graph neural networks in recommender systems: a survey</data>
  <data key="d1">16559789848821467555</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3535101</data>
  <data key="d3">Graph neural networks in recommender systems: a survey</data>
  <data key="d4">S Wu, F Sun, W Zhang, X Xie, B Cui</data>
  <data key="d5">2022</data>
  <data key="d6">516</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16559789848821467555&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="13748354740225969894">
  <data key="d0">Neural ordinary differential equations</data>
  <data key="d1">13748354740225969894</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2018/hash/69386f6bb1dfed68692a24c8686939b9-Abstract.html</data>
  <data key="d3">Neural ordinary differential equations</data>
  <data key="d4">RTQ Chen, Y Rubanova…</data>
  <data key="d5">2018</data>
  <data key="d6">3698</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13748354740225969894&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="17348071344751182786">
  <data key="d0">Simplifying graph convolutional networks</data>
  <data key="d1">17348071344751182786</data>
  <data key="d2">https://proceedings.mlr.press/v97/wu19e.html</data>
  <data key="d3">Simplifying graph convolutional networks</data>
  <data key="d4">F Wu, A Souza, T Zhang, C Fifty, T Yu…</data>
  <data key="d5">2019</data>
  <data key="d6">2379</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17348071344751182786&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="2846334625875347669">
  <data key="d0">Deep learning for generic object detection: A survey</data>
  <data key="d1">2846334625875347669</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11263-019-01247-4</data>
  <data key="d3">Deep learning for generic object detection: A survey</data>
  <data key="d4">L Liu, W Ouyang, X Wang, P Fieguth, J Chen…</data>
  <data key="d5">2020</data>
  <data key="d6">2510</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2846334625875347669&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="7012076773417880547">
  <data key="d0">Graph neural network for traffic forecasting: A survey</data>
  <data key="d1">7012076773417880547</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417422011654</data>
  <data key="d3">Graph neural network for traffic forecasting: A survey</data>
  <data key="d4">W Jiang, J Luo</data>
  <data key="d5">2022</data>
  <data key="d6">408</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7012076773417880547&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="14188108076981434930">
  <data key="d0">Disentangling and unifying graph convolutions for skeleton-based action recognition</data>
  <data key="d1">14188108076981434930</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Liu_Disentangling_and_Unifying_Graph_Convolutions_for_Skeleton-Based_Action_Recognition_CVPR_2020_paper.html</data>
  <data key="d3">Disentangling and unifying graph convolutions for skeleton-based action recognition</data>
  <data key="d4">Z Liu, H Zhang, Z Chen, Z Wang…</data>
  <data key="d5">2020</data>
  <data key="d6">691</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14188108076981434930&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="10560769186027812935">
  <data key="d0">Gman: A graph multi-attention network for traffic prediction</data>
  <data key="d1">10560769186027812935</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/5477</data>
  <data key="d3">Gman: A graph multi-attention network for traffic prediction</data>
  <data key="d4">C Zheng, X Fan, C Wang, J Qi</data>
  <data key="d5">2020</data>
  <data key="d6">806</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10560769186027812935&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="17598489487193491440">
  <data key="d0">Deep graph library: A graph-centric, highly-performant package for graph neural networks</data>
  <data key="d1">17598489487193491440</data>
  <data key="d2">https://arxiv.org/abs/1909.01315</data>
  <data key="d3">Deep graph library: A graph-centric, highly-performant package for graph neural networks</data>
  <data key="d4">M Wang, D Zheng, Z Ye, Q Gan, M Li, X Song…</data>
  <data key="d5">2019</data>
  <data key="d6">817</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17598489487193491440&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="824590046745083591">
  <data key="d0">Graph convolutional networks: a comprehensive review</data>
  <data key="d1">824590046745083591</data>
  <data key="d2">https://computationalsocialnetworks.springeropen.com/articles/10.1186/s40649-019-0069-y?ref=https://githubhelp.com</data>
  <data key="d3">Graph convolutional networks: a comprehensive review</data>
  <data key="d4">S Zhang, H Tong, J Xu…</data>
  <data key="d5">2019</data>
  <data key="d6">714</data>
  <data key="d7">https://scholar.google.com/scholar?cites=824590046745083591&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="3987152815255001944">
  <data key="d0">Point-gnn: Graph neural network for 3d object detection in a point cloud</data>
  <data key="d1">3987152815255001944</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Shi_Point-GNN_Graph_Neural_Network_for_3D_Object_Detection_in_a_CVPR_2020_paper.html</data>
  <data key="d3">Point-gnn: Graph neural network for 3d object detection in a point cloud</data>
  <data key="d4">W Shi, R Rajkumar</data>
  <data key="d5">2020</data>
  <data key="d6">596</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3987152815255001944&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="1405916394303361407">
  <data key="d0">Deepgcns: Can gcns go as deep as cnns?</data>
  <data key="d1">1405916394303361407</data>
  <data key="d2">http://openaccess.thecvf.com/content_ICCV_2019/html/Li_DeepGCNs_Can_GCNs_Go_As_Deep_As_CNNs_ICCV_2019_paper.html</data>
  <data key="d3">Deepgcns: Can gcns go as deep as cnns?</data>
  <data key="d4">G Li, M Muller, A Thabet…</data>
  <data key="d5">2019</data>
  <data key="d6">1087</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1405916394303361407&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="3929066136257329737">
  <data key="d0">Deep learning on graphs: A survey</data>
  <data key="d1">3929066136257329737</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9039675/</data>
  <data key="d3">Deep learning on graphs: A survey</data>
  <data key="d4">Z Zhang, P Cui, W Zhu</data>
  <data key="d5">2020</data>
  <data key="d6">1191</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3929066136257329737&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="6969950659727941449">
  <data key="d0">Beyond low-frequency information in graph convolutional networks</data>
  <data key="d1">6969950659727941449</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/16514</data>
  <data key="d3">Beyond low-frequency information in graph convolutional networks</data>
  <data key="d4">D Bo, X Wang, C Shi, H Shen</data>
  <data key="d5">2021</data>
  <data key="d6">303</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6969950659727941449&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="4712708126956095125">
  <data key="d0">Knowledge graphs</data>
  <data key="d1">4712708126956095125</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3447772</data>
  <data key="d3">Knowledge graphs</data>
  <data key="d4">A Hogan, E Blomqvist, M Cochez, C d'Amato…</data>
  <data key="d5">2021</data>
  <data key="d6">963</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4712708126956095125&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="5884209795367025285">
  <data key="d0">On the bottleneck of graph neural networks and its practical implications</data>
  <data key="d1">5884209795367025285</data>
  <data key="d2">https://arxiv.org/abs/2006.05205</data>
  <data key="d3">On the bottleneck of graph neural networks and its practical implications</data>
  <data key="d4">U Alon, E Yahav</data>
  <data key="d5">2020</data>
  <data key="d6">412</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5884209795367025285&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="16286153053570566579">
  <data key="d0">Towards multi-modal causability with graph neural networks enabling information fusion for explainable AI</data>
  <data key="d1">16286153053570566579</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1566253521000142</data>
  <data key="d3">Towards multi-modal causability with graph neural networks enabling information fusion for explainable AI</data>
  <data key="d4">A Holzinger, B Malle, A Saranti, B Pfeifer</data>
  <data key="d5">2021</data>
  <data key="d6">248</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16286153053570566579&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="8218634905233683023">
  <data key="d0">An introductory review of deep learning for prediction models with big data</data>
  <data key="d1">8218634905233683023</data>
  <data key="d2">https://www.frontiersin.org/articles/10.3389/frai.2020.00004/full</data>
  <data key="d3">An introductory review of deep learning for prediction models with big data</data>
  <data key="d4">F Emmert-Streib, Z Yang, H Feng, S Tripathi…</data>
  <data key="d5">2020</data>
  <data key="d6">441</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8218634905233683023&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="8173578619526272020">
  <data key="d0">Self-supervised hypergraph convolutional networks for session-based recommendation</data>
  <data key="d1">8173578619526272020</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/16578</data>
  <data key="d3">Self-supervised hypergraph convolutional networks for session-based recommendation</data>
  <data key="d4">X Xia, H Yin, J Yu, Q Wang, L Cui…</data>
  <data key="d5">2021</data>
  <data key="d6">287</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8173578619526272020&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="15628822607882875170">
  <data key="d0">Am-gcn: Adaptive multi-channel graph convolutional networks</data>
  <data key="d1">15628822607882875170</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3394486.3403177</data>
  <data key="d3">Am-gcn: Adaptive multi-channel graph convolutional networks</data>
  <data key="d4">X Wang, M Zhu, D Bo, P Cui, C Shi, J Pei</data>
  <data key="d5">2020</data>
  <data key="d6">326</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15628822607882875170&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="13519623104352135728">
  <data key="d0">Graph structure learning for robust graph neural networks</data>
  <data key="d1">13519623104352135728</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3394486.3403049</data>
  <data key="d3">Graph structure learning for robust graph neural networks</data>
  <data key="d4">W Jin, Y Ma, X Liu, X Tang, S Wang…</data>
  <data key="d5">2020</data>
  <data key="d6">398</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13519623104352135728&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="11740996352054171303">
  <data key="d0">Graph self-supervised learning: A survey</data>
  <data key="d1">11740996352054171303</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9770382/</data>
  <data key="d3">Graph self-supervised learning: A survey</data>
  <data key="d4">Y Liu, M Jin, S Pan, C Zhou, Y Zheng…</data>
  <data key="d5">2022</data>
  <data key="d6">256</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11740996352054171303&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="13319126561466992426">
  <data key="d0">Deep closest point: Learning representations for point cloud registration</data>
  <data key="d1">13319126561466992426</data>
  <data key="d2">http://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Deep_Closest_Point_Learning_Representations_for_Point_Cloud_Registration_ICCV_2019_paper.html</data>
  <data key="d3">Deep closest point: Learning representations for point cloud registration</data>
  <data key="d4">Y Wang, JM Solomon</data>
  <data key="d5">2019</data>
  <data key="d6">702</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13319126561466992426&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="18175359140763832369">
  <data key="d0">Graph representation learning in biomedicine and healthcare</data>
  <data key="d1">18175359140763832369</data>
  <data key="d2">https://www.nature.com/articles/s41551-022-00942-x</data>
  <data key="d3">Graph representation learning in biomedicine and healthcare</data>
  <data key="d4">MM Li, K Huang, M Zitnik</data>
  <data key="d5">2022</data>
  <data key="d6">62</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18175359140763832369&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="8609729441168460418">
  <data key="d0">Spectral temporal graph neural network for multivariate time-series forecasting</data>
  <data key="d1">8609729441168460418</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2020/hash/cdf6581cb7aca4b7e19ef136c6e601a5-Abstract.html</data>
  <data key="d3">Spectral temporal graph neural network for multivariate time-series forecasting</data>
  <data key="d4">D Cao, Y Wang, J Duan, C Zhang…</data>
  <data key="d5">2020</data>
  <data key="d6">262</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8609729441168460418&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="12338916099901568166">
  <data key="d0">Hierarchically structured bioinspired nanocomposites</data>
  <data key="d1">12338916099901568166</data>
  <data key="d2">https://www.nature.com/articles/s41563-022-01384-1</data>
  <data key="d3">Hierarchically structured bioinspired nanocomposites</data>
  <data key="d4">D Nepal, S Kang, KM Adstedt, K Kanhaiya…</data>
  <data key="d5">2023</data>
  <data key="d6">61</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12338916099901568166&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="10361934383634103378">
  <data key="d0">Traffic flow prediction via spatial temporal graph neural network</data>
  <data key="d1">10361934383634103378</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3366423.3380186</data>
  <data key="d3">Traffic flow prediction via spatial temporal graph neural network</data>
  <data key="d4">X Wang, Y Ma, Y Wang, W Jin, X Wang, J Tang…</data>
  <data key="d5">2020</data>
  <data key="d6">337</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10361934383634103378&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="10034248469511743317">
  <data key="d0">Semi-supervised city-wide parking availability prediction via hierarchical recurrent graph neural network</data>
  <data key="d1">10034248469511743317</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9241427/</data>
  <data key="d3">Semi-supervised city-wide parking availability prediction via hierarchical recurrent graph neural network</data>
  <data key="d4">W Zhang, H Liu, Y Liu, J Zhou, T Xu…</data>
  <data key="d5">2020</data>
  <data key="d6">249</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10034248469511743317&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="13094368588973025667">
  <data key="d0">Are graph augmentations necessary? simple graph contrastive learning for recommendation</data>
  <data key="d1">13094368588973025667</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3477495.3531937</data>
  <data key="d3">Are graph augmentations necessary? simple graph contrastive learning for recommendation</data>
  <data key="d4">J Yu, H Yin, X Xia, T Chen, L Cui…</data>
  <data key="d5">2022</data>
  <data key="d6">174</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13094368588973025667&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="14507474565925582559">
  <data key="d0">Software vulnerability detection using deep neural networks: a survey</data>
  <data key="d1">14507474565925582559</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9108283/</data>
  <data key="d3">Software vulnerability detection using deep neural networks: a survey</data>
  <data key="d4">G Lin, S Wen, QL Han, J Zhang…</data>
  <data key="d5">2020</data>
  <data key="d6">285</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14507474565925582559&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="8608635600889702315">
  <data key="d0">Self-supervised multi-channel hypergraph convolutional network for social recommendation</data>
  <data key="d1">8608635600889702315</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3442381.3449844</data>
  <data key="d3">Self-supervised multi-channel hypergraph convolutional network for social recommendation</data>
  <data key="d4">J Yu, H Yin, J Li, Q Wang, NQV Hung…</data>
  <data key="d5">2021</data>
  <data key="d6">230</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8608635600889702315&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="3480504377185349195">
  <data key="d0">Combustion machine learning: Principles, progress and prospects</data>
  <data key="d1">3480504377185349195</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0360128522000193</data>
  <data key="d3">Combustion machine learning: Principles, progress and prospects</data>
  <data key="d4">M Ihme, WT Chung, AA Mishra</data>
  <data key="d5">2022</data>
  <data key="d6">74</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3480504377185349195&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="8264388784225550416">
  <data key="d0">Graph learning: A survey</data>
  <data key="d1">8264388784225550416</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9416834/</data>
  <data key="d3">Graph learning: A survey</data>
  <data key="d4">F Xia, K Sun, S Yu, A Aziz, L Wan…</data>
  <data key="d5">2021</data>
  <data key="d6">193</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8264388784225550416&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="16185498123764366373">
  <data key="d0">A survey of community detection approaches: From statistical modeling to deep learning</data>
  <data key="d1">16185498123764366373</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9511798/</data>
  <data key="d3">A survey of community detection approaches: From statistical modeling to deep learning</data>
  <data key="d4">D Jin, Z Yu, P Jiao, S Pan, D He, J Wu…</data>
  <data key="d5">2021</data>
  <data key="d6">195</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16185498123764366373&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="3692815723362239451">
  <data key="d0">A compact review of molecular property prediction with graph neural networks</data>
  <data key="d1">3692815723362239451</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1740674920300305</data>
  <data key="d3">A compact review of molecular property prediction with graph neural networks</data>
  <data key="d4">O Wieder, S Kohlbacher, M Kuenemann…</data>
  <data key="d5">2020</data>
  <data key="d6">223</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3692815723362239451&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="17960766448265380456">
  <data key="d0">How powerful are spectral graph neural networks</data>
  <data key="d1">17960766448265380456</data>
  <data key="d2">https://proceedings.mlr.press/v162/wang22am.html</data>
  <data key="d3">How powerful are spectral graph neural networks</data>
  <data key="d4">X Wang, M Zhang</data>
  <data key="d5">2022</data>
  <data key="d6">57</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17960766448265380456&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="3243408560707200262">
  <data key="d0">Review on deep learning applications in frequency analysis and control of modern power system</data>
  <data key="d1">3243408560707200262</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0142061521009686</data>
  <data key="d3">Review on deep learning applications in frequency analysis and control of modern power system</data>
  <data key="d4">Y Zhang, X Shi, H Zhang, Y Cao, V Terzija</data>
  <data key="d5">2022</data>
  <data key="d6">141</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3243408560707200262&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="11612055028743993917">
  <data key="d0">A tutorial on ultrareliable and low-latency communications in 6G: Integrating domain knowledge into deep learning</data>
  <data key="d1">11612055028743993917</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9369424/</data>
  <data key="d3">A tutorial on ultrareliable and low-latency communications in 6G: Integrating domain knowledge into deep learning</data>
  <data key="d4">C She, C Sun, Z Gu, Y Li, C Yang…</data>
  <data key="d5">2021</data>
  <data key="d6">225</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11612055028743993917&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="942093327978373135">
  <data key="d0">Specter: Document-level representation learning using citation-informed transformers</data>
  <data key="d1">942093327978373135</data>
  <data key="d2">https://arxiv.org/abs/2004.07180</data>
  <data key="d3">Specter: Document-level representation learning using citation-informed transformers</data>
  <data key="d4">A Cohan, S Feldman, I Beltagy, D Downey…</data>
  <data key="d5">2020</data>
  <data key="d6">311</data>
  <data key="d7">https://scholar.google.com/scholar?cites=942093327978373135&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="6584430158722490128">
  <data key="d0">Self-supervised heterogeneous graph neural network with co-contrastive learning</data>
  <data key="d1">6584430158722490128</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3447548.3467415</data>
  <data key="d3">Self-supervised heterogeneous graph neural network with co-contrastive learning</data>
  <data key="d4">X Wang, N Liu, H Han, C Shi</data>
  <data key="d5">2021</data>
  <data key="d6">178</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6584430158722490128&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="204598726516650366">
  <data key="d0">Learning knowledge graph embedding with heterogeneous relation attention networks</data>
  <data key="d1">204598726516650366</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9359364/</data>
  <data key="d3">Learning knowledge graph embedding with heterogeneous relation attention networks</data>
  <data key="d4">Z Li, H Liu, Z Zhang, T Liu…</data>
  <data key="d5">2021</data>
  <data key="d6">187</data>
  <data key="d7">https://scholar.google.com/scholar?cites=204598726516650366&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="17047112888909822506">
  <data key="d0">Linking points with labels in 3D: A review of point cloud semantic segmentation</data>
  <data key="d1">17047112888909822506</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9028090/</data>
  <data key="d3">Linking points with labels in 3D: A review of point cloud semantic segmentation</data>
  <data key="d4">Y Xie, J Tian, XX Zhu</data>
  <data key="d5">2020</data>
  <data key="d6">325</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17047112888909822506&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="7072607443445254562">
  <data key="d0">Utilizing graph machine learning within drug discovery and development</data>
  <data key="d1">7072607443445254562</data>
  <data key="d2">https://academic.oup.com/bib/article-abstract/22/6/bbab159/6278145</data>
  <data key="d3">Utilizing graph machine learning within drug discovery and development</data>
  <data key="d4">T Gaudelet, B Day, AR Jamasb, J Soman…</data>
  <data key="d5">2021</data>
  <data key="d6">171</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7072607443445254562&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="7755255229432407884">
  <data key="d0">Enhancing graph neural network-based fraud detectors against camouflaged fraudsters</data>
  <data key="d1">7755255229432407884</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3340531.3411903</data>
  <data key="d3">Enhancing graph neural network-based fraud detectors against camouflaged fraudsters</data>
  <data key="d4">Y Dou, Z Liu, L Sun, Y Deng, H Peng…</data>
  <data key="d5">2020</data>
  <data key="d6">237</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7755255229432407884&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="6916383400010189934">
  <data key="d0">An overview of deep semi-supervised learning</data>
  <data key="d1">6916383400010189934</data>
  <data key="d2">https://arxiv.org/abs/2006.05278</data>
  <data key="d3">An overview of deep semi-supervised learning</data>
  <data key="d4">Y Ouali, C Hudelot, M Tami</data>
  <data key="d5">2020</data>
  <data key="d6">254</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6916383400010189934&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="13318009799245280479">
  <data key="d0">Multipole graph neural operator for parametric partial differential equations</data>
  <data key="d1">13318009799245280479</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2020/hash/4b21cf96d4cf612f239a6c322b10c8fe-Abstract.html</data>
  <data key="d3">Multipole graph neural operator for parametric partial differential equations</data>
  <data key="d4">Z Li, N Kovachki, K Azizzadenesheli…</data>
  <data key="d5">2020</data>
  <data key="d6">214</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13318009799245280479&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="3632941331539070968">
  <data key="d0">Deep learning in human activity recognition with wearable sensors: A review on advances</data>
  <data key="d1">3632941331539070968</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/4/1476</data>
  <data key="d3">Deep learning in human activity recognition with wearable sensors: A review on advances</data>
  <data key="d4">S Zhang, Y Li, S Zhang, F Shahabi, S Xia, Y Deng…</data>
  <data key="d5">2022</data>
  <data key="d6">137</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3632941331539070968&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="16803457256664781582">
  <data key="d0">EEG-based emotion recognition using regularized graph neural networks</data>
  <data key="d1">16803457256664781582</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9091308/</data>
  <data key="d3">EEG-based emotion recognition using regularized graph neural networks</data>
  <data key="d4">P Zhong, D Wang, C Miao</data>
  <data key="d5">2020</data>
  <data key="d6">304</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16803457256664781582&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="18317541865909512163">
  <data key="d0">Adversarial attacks on graph neural networks: Perturbations and their patterns</data>
  <data key="d1">18317541865909512163</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3394520</data>
  <data key="d3">Adversarial attacks on graph neural networks: Perturbations and their patterns</data>
  <data key="d4">D Zügner, O Borchert, A Akbarnejad…</data>
  <data key="d5">2020</data>
  <data key="d6">476</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18317541865909512163&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="18185187817423073352">
  <data key="d0">Integration strategies of multi-omics data for machine learning analysis</data>
  <data key="d1">18185187817423073352</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2001037021002683</data>
  <data key="d3">Integration strategies of multi-omics data for machine learning analysis</data>
  <data key="d4">M Picard, MP Scott-Boyer, A Bodein, O Périn…</data>
  <data key="d5">2021</data>
  <data key="d6">167</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18185187817423073352&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="4523751644674550881">
  <data key="d0">Graph neural networks in particle physics</data>
  <data key="d1">4523751644674550881</data>
  <data key="d2">https://iopscience.iop.org/article/10.1088/2632-2153/abbf9a/meta</data>
  <data key="d3">Graph neural networks in particle physics</data>
  <data key="d4">J Shlomi, P Battaglia, JR Vlimant</data>
  <data key="d5">2020</data>
  <data key="d6">223</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4523751644674550881&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="16804172820179337006">
  <data key="d0">Deep learning for spatio-temporal data mining: A survey</data>
  <data key="d1">16804172820179337006</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9204396/</data>
  <data key="d3">Deep learning for spatio-temporal data mining: A survey</data>
  <data key="d4">S Wang, J Cao, SY Philip</data>
  <data key="d5">2020</data>
  <data key="d6">424</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16804172820179337006&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="1939691424133781861">
  <data key="d0">Scaling graph neural networks with approximate pagerank</data>
  <data key="d1">1939691424133781861</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3394486.3403296</data>
  <data key="d3">Scaling graph neural networks with approximate pagerank</data>
  <data key="d4">A Bojchevski, J Gasteiger, B Perozzi, A Kapoor…</data>
  <data key="d5">2020</data>
  <data key="d6">205</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1939691424133781861&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="6018075345772831348">
  <data key="d0">Hierarchical adversarial attacks against graph-neural-network-based IoT network intrusion detection system</data>
  <data key="d1">6018075345772831348</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9626144/</data>
  <data key="d3">Hierarchical adversarial attacks against graph-neural-network-based IoT network intrusion detection system</data>
  <data key="d4">X Zhou, W Liang, W Li, K Yan, S Shimizu…</data>
  <data key="d5">2021</data>
  <data key="d6">141</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6018075345772831348&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="17854740042612276767">
  <data key="d0">Learning dynamics and heterogeneity of spatial-temporal graph data for traffic forecasting</data>
  <data key="d1">17854740042612276767</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9346058/</data>
  <data key="d3">Learning dynamics and heterogeneity of spatial-temporal graph data for traffic forecasting</data>
  <data key="d4">S Guo, Y Lin, H Wan, X Li…</data>
  <data key="d5">2021</data>
  <data key="d6">180</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17854740042612276767&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="1656421726766747841">
  <data key="d0">Atomistic line graph neural network for improved materials property predictions</data>
  <data key="d1">1656421726766747841</data>
  <data key="d2">https://www.nature.com/articles/s41524-021-00650-1</data>
  <data key="d3">Atomistic line graph neural network for improved materials property predictions</data>
  <data key="d4">K Choudhary, B DeCost</data>
  <data key="d5">2021</data>
  <data key="d6">119</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1656421726766747841&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="9732428124783939302">
  <data key="d0">Hygcn: A gcn accelerator with hybrid architecture</data>
  <data key="d1">9732428124783939302</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9065592/</data>
  <data key="d3">Hygcn: A gcn accelerator with hybrid architecture</data>
  <data key="d4">M Yan, L Deng, X Hu, L Liang, Y Feng…</data>
  <data key="d5">2020</data>
  <data key="d6">248</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9732428124783939302&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="7651463217590835983">
  <data key="d0">A survey of graph neural networks for recommender systems: Challenges, methods, and directions</data>
  <data key="d1">7651463217590835983</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3568022</data>
  <data key="d3">A survey of graph neural networks for recommender systems: Challenges, methods, and directions</data>
  <data key="d4">C Gao, Y Zheng, N Li, Y Li, Y Qin, J Piao…</data>
  <data key="d5">2023</data>
  <data key="d6">102</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7651463217590835983&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="7594913044183235646">
  <data key="d0">How to find your friendly neighborhood: Graph attention design with self-supervision</data>
  <data key="d1">7594913044183235646</data>
  <data key="d2">https://arxiv.org/abs/2204.04879</data>
  <data key="d3">How to find your friendly neighborhood: Graph attention design with self-supervision</data>
  <data key="d4">D Kim, A Oh</data>
  <data key="d5">2022</data>
  <data key="d6">189</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7594913044183235646&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="11598438983746935209">
  <data key="d0">Benchmarking graph neural networks for materials chemistry</data>
  <data key="d1">11598438983746935209</data>
  <data key="d2">https://www.nature.com/articles/s41524-021-00554-0</data>
  <data key="d3">Benchmarking graph neural networks for materials chemistry</data>
  <data key="d4">V Fung, J Zhang, E Juarez, BG Sumpter</data>
  <data key="d5">2021</data>
  <data key="d6">120</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11598438983746935209&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="8359089573172587095">
  <data key="d0">When does self-supervision help graph convolutional networks?</data>
  <data key="d1">8359089573172587095</data>
  <data key="d2">http://proceedings.mlr.press/v119/you20a.html</data>
  <data key="d3">When does self-supervision help graph convolutional networks?</data>
  <data key="d4">Y You, T Chen, Z Wang, Y Shen</data>
  <data key="d5">2020</data>
  <data key="d6">183</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8359089573172587095&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="531500407384902218">
  <data key="d0">Adaptive graph convolutional recurrent network for traffic forecasting</data>
  <data key="d1">531500407384902218</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2020/file/ce1aad92b939420fc17005e5461e6f48-Paper.pdf</data>
  <data key="d3">Adaptive graph convolutional recurrent network for traffic forecasting</data>
  <data key="d4">L Bai, L Yao, C Li, X Wang…</data>
  <data key="d5">2020</data>
  <data key="d6">581</data>
  <data key="d7">https://scholar.google.com/scholar?cites=531500407384902218&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="15397526244877086732">
  <data key="d0">Can graph neural networks count substructures?</data>
  <data key="d1">15397526244877086732</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2020/hash/75877cb75154206c4e65e76b88a12712-Abstract.html</data>
  <data key="d3">Can graph neural networks count substructures?</data>
  <data key="d4">Z Chen, L Chen, S Villar…</data>
  <data key="d5">2020</data>
  <data key="d6">223</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15397526244877086732&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="18161333932150601045">
  <data key="d0">A gentle introduction to deep learning for graphs</data>
  <data key="d1">18161333932150601045</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0893608020302197</data>
  <data key="d3">A gentle introduction to deep learning for graphs</data>
  <data key="d4">D Bacciu, F Errica, A Micheli, M Podda</data>
  <data key="d5">2020</data>
  <data key="d6">225</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18161333932150601045&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="12908963143152569355">
  <data key="d0">A survey of knowledge-enhanced text generation</data>
  <data key="d1">12908963143152569355</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3512467</data>
  <data key="d3">A survey of knowledge-enhanced text generation</data>
  <data key="d4">W Yu, C Zhu, Z Li, Z Hu, Q Wang, H Ji…</data>
  <data key="d5">2022</data>
  <data key="d6">155</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12908963143152569355&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="4499233656145369438">
  <data key="d0">Anomaly detection on attributed networks via contrastive self-supervised learning</data>
  <data key="d1">4499233656145369438</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9395172/</data>
  <data key="d3">Anomaly detection on attributed networks via contrastive self-supervised learning</data>
  <data key="d4">Y Liu, Z Li, S Pan, C Gong, C Zhou…</data>
  <data key="d5">2021</data>
  <data key="d6">140</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4499233656145369438&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="8291535141920945881">
  <data key="d0">Heterogeneous graph structure learning for graph neural networks</data>
  <data key="d1">8291535141920945881</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/16600</data>
  <data key="d3">Heterogeneous graph structure learning for graph neural networks</data>
  <data key="d4">J Zhao, X Wang, C Shi, B Hu, G Song…</data>
  <data key="d5">2021</data>
  <data key="d6">136</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8291535141920945881&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="16222652166929144204">
  <data key="d0">Computing graph neural networks: A survey from algorithms to accelerators</data>
  <data key="d1">16222652166929144204</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3477141</data>
  <data key="d3">Computing graph neural networks: A survey from algorithms to accelerators</data>
  <data key="d4">S Abadal, A Jain, R Guirado, J López-Alonso…</data>
  <data key="d5">2021</data>
  <data key="d6">149</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16222652166929144204&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="11491875380349289404">
  <data key="d0">Examining covid-19 forecasting using spatio-temporal graph neural networks</data>
  <data key="d1">11491875380349289404</data>
  <data key="d2">https://arxiv.org/abs/2007.03113</data>
  <data key="d3">Examining covid-19 forecasting using spatio-temporal graph neural networks</data>
  <data key="d4">A Kapoor, X Ben, L Liu, B Perozzi, M Barnes…</data>
  <data key="d5">2020</data>
  <data key="d6">199</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11491875380349289404&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="3720958980403020816">
  <data key="d0">Graph neural networks for materials science and chemistry</data>
  <data key="d1">3720958980403020816</data>
  <data key="d2">https://www.nature.com/articles/s43246-022-00315-6</data>
  <data key="d3">Graph neural networks for materials science and chemistry</data>
  <data key="d4">P Reiser, M Neubert, A Eberhard, L Torresi…</data>
  <data key="d5">2022</data>
  <data key="d6">74</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3720958980403020816&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="9486832350006291839">
  <data key="d0">A survey on trajectory-prediction methods for autonomous driving</data>
  <data key="d1">9486832350006291839</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9756903/</data>
  <data key="d3">A survey on trajectory-prediction methods for autonomous driving</data>
  <data key="d4">Y Huang, J Du, Z Yang, Z Zhou…</data>
  <data key="d5">2022</data>
  <data key="d6">99</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9486832350006291839&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="1051374793971189386">
  <data key="d0">Text level graph neural network for text classification</data>
  <data key="d1">1051374793971189386</data>
  <data key="d2">https://arxiv.org/abs/1910.02356</data>
  <data key="d3">Text level graph neural network for text classification</data>
  <data key="d4">L Huang, D Ma, S Li, X Zhang, H Wang</data>
  <data key="d5">2019</data>
  <data key="d6">248</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1051374793971189386&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="3658250324739476352">
  <data key="d0">Node similarity preserving graph convolutional networks</data>
  <data key="d1">3658250324739476352</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3437963.3441735</data>
  <data key="d3">Node similarity preserving graph convolutional networks</data>
  <data key="d4">W Jin, T Derr, Y Wang, Y Ma, Z Liu, J Tang</data>
  <data key="d5">2021</data>
  <data key="d6">158</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3658250324739476352&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="16313647976714156830">
  <data key="d0">G-mixup: Graph data augmentation for graph classification</data>
  <data key="d1">16313647976714156830</data>
  <data key="d2">https://proceedings.mlr.press/v162/han22c.html</data>
  <data key="d3">G-mixup: Graph data augmentation for graph classification</data>
  <data key="d4">X Han, Z Jiang, N Liu, X Hu</data>
  <data key="d5">2022</data>
  <data key="d6">74</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16313647976714156830&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="2604085176257305633">
  <data key="d0">Attention, please! A survey of neural attention models in deep learning</data>
  <data key="d1">2604085176257305633</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10462-022-10148-x</data>
  <data key="d3">Attention, please! A survey of neural attention models in deep learning</data>
  <data key="d4">A de Santana Correia, EL Colombini</data>
  <data key="d5">2022</data>
  <data key="d6">114</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2604085176257305633&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="17770512424346032016">
  <data key="d0">Heterogeneous graph neural network via attribute completion</data>
  <data key="d1">17770512424346032016</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3442381.3449914</data>
  <data key="d3">Heterogeneous graph neural network via attribute completion</data>
  <data key="d4">D Jin, C Huo, C Liang, L Yang</data>
  <data key="d5">2021</data>
  <data key="d6">114</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17770512424346032016&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="11601132339555571470">
  <data key="d0">Graph learning based recommender systems: A review</data>
  <data key="d1">11601132339555571470</data>
  <data key="d2">https://arxiv.org/abs/2105.06339</data>
  <data key="d3">Graph learning based recommender systems: A review</data>
  <data key="d4">S Wang, L Hu, Y Wang, X He, QZ Sheng…</data>
  <data key="d5">2021</data>
  <data key="d6">124</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11601132339555571470&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="1211835406473191863">
  <data key="d0">Multi-range attentive bicomponent graph convolutional network for traffic forecasting</data>
  <data key="d1">1211835406473191863</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/5758</data>
  <data key="d3">Multi-range attentive bicomponent graph convolutional network for traffic forecasting</data>
  <data key="d4">W Chen, L Chen, Y Xie, W Cao, Y Gao…</data>
  <data key="d5">2020</data>
  <data key="d6">224</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1211835406473191863&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="4347083033764848435">
  <data key="d0">The emerging graph neural networks for intelligent fault diagnostics and prognostics: A guideline and a benchmark study</data>
  <data key="d1">4347083033764848435</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0888327021009791</data>
  <data key="d3">The emerging graph neural networks for intelligent fault diagnostics and prognostics: A guideline and a benchmark study</data>
  <data key="d4">T Li, Z Zhou, S Li, C Sun, R Yan, X Chen</data>
  <data key="d5">2022</data>
  <data key="d6">92</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4347083033764848435&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="9538084449875791919">
  <data key="d0">Fcos: Fully convolutional one-stage object detection</data>
  <data key="d1">9538084449875791919</data>
  <data key="d2">http://openaccess.thecvf.com/content_ICCV_2019/html/Tian_FCOS_Fully_Convolutional_One-Stage_Object_Detection_ICCV_2019_paper.html</data>
  <data key="d3">Fcos: Fully convolutional one-stage object detection</data>
  <data key="d4">Z Tian, C Shen, H Chen, T He</data>
  <data key="d5">2019</data>
  <data key="d6">4413</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9538084449875791919&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11408603882397693774">
  <data key="d0">Recent advances in deep learning for object detection</data>
  <data key="d1">11408603882397693774</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0925231220301430</data>
  <data key="d3">Recent advances in deep learning for object detection</data>
  <data key="d4">X Wu, D Sahoo, SCH Hoi</data>
  <data key="d5">2020</data>
  <data key="d6">688</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11408603882397693774&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="12500443856801763727">
  <data key="d0">Scaling up your kernels to 31x31: Revisiting large kernel design in cnns</data>
  <data key="d1">12500443856801763727</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.html</data>
  <data key="d3">Scaling up your kernels to 31x31: Revisiting large kernel design in cnns</data>
  <data key="d4">X Ding, X Zhang, J Han, G Ding</data>
  <data key="d5">2022</data>
  <data key="d6">357</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12500443856801763727&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="16138254679061222132">
  <data key="d0">Efficientdet: Scalable and efficient object detection</data>
  <data key="d1">16138254679061222132</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Tan_EfficientDet_Scalable_and_Efficient_Object_Detection_CVPR_2020_paper.html</data>
  <data key="d3">Efficientdet: Scalable and efficient object detection</data>
  <data key="d4">M Tan, R Pang, QV Le</data>
  <data key="d5">2020</data>
  <data key="d6">4585</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16138254679061222132&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="15505711795824202735">
  <data key="d0">Deep high-resolution representation learning for visual recognition</data>
  <data key="d1">15505711795824202735</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9052469/</data>
  <data key="d3">Deep high-resolution representation learning for visual recognition</data>
  <data key="d4">J Wang, K Sun, T Cheng, B Jiang…</data>
  <data key="d5">2020</data>
  <data key="d6">2478</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15505711795824202735&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11324600873272743514">
  <data key="d0">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</data>
  <data key="d1">11324600873272743514</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Bridging_the_Gap_Between_Anchor-Based_and_Anchor-Free_Detection_via_Adaptive_CVPR_2020_paper.html</data>
  <data key="d3">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</data>
  <data key="d4">S Zhang, C Chi, Y Yao, Z Lei…</data>
  <data key="d5">2020</data>
  <data key="d6">1230</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11324600873272743514&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="1564404986299438483">
  <data key="d0">Tracking objects as points</data>
  <data key="d1">1564404986299438483</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-58548-8_28</data>
  <data key="d3">Tracking objects as points</data>
  <data key="d4">X Zhou, V Koltun, P Krähenbühl</data>
  <data key="d5">2020</data>
  <data key="d6">837</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1564404986299438483&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4116193485073279318">
  <data key="d0">MMDetection: Open mmlab detection toolbox and benchmark</data>
  <data key="d1">4116193485073279318</data>
  <data key="d2">https://arxiv.org/abs/1906.07155</data>
  <data key="d3">MMDetection: Open mmlab detection toolbox and benchmark</data>
  <data key="d4">K Chen, J Wang, J Pang, Y Cao, Y Xiong, X Li…</data>
  <data key="d5">2019</data>
  <data key="d6">2251</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4116193485073279318&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14550673639887731046">
  <data key="d0">End-to-end semi-supervised object detection with soft teacher</data>
  <data key="d1">14550673639887731046</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Xu_End-to-End_Semi-Supervised_Object_Detection_With_Soft_Teacher_ICCV_2021_paper.html</data>
  <data key="d3">End-to-end semi-supervised object detection with soft teacher</data>
  <data key="d4">M Xu, Z Zhang, H Hu, J Wang, L Wang…</data>
  <data key="d5">2021</data>
  <data key="d6">310</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14550673639887731046&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="5146522853377591327">
  <data key="d0">Oriented R-CNN for object detection</data>
  <data key="d1">5146522853377591327</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Xie_Oriented_R-CNN_for_Object_Detection_ICCV_2021_paper.html</data>
  <data key="d3">Oriented R-CNN for object detection</data>
  <data key="d4">X Xie, G Cheng, J Wang, X Yao…</data>
  <data key="d5">2021</data>
  <data key="d6">316</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5146522853377591327&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="18039062144864581963">
  <data key="d0">Bevfusion: Multi-task multi-sensor fusion with unified bird's-eye view representation</data>
  <data key="d1">18039062144864581963</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10160968/</data>
  <data key="d3">Bevfusion: Multi-task multi-sensor fusion with unified bird's-eye view representation</data>
  <data key="d4">Z Liu, H Tang, A Amini, X Yang, H Mao…</data>
  <data key="d5">2023</data>
  <data key="d6">222</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18039062144864581963&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="5669364687087032958">
  <data key="d0">A survey of deep learning-based object detection</data>
  <data key="d1">5669364687087032958</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/8825470/</data>
  <data key="d3">A survey of deep learning-based object detection</data>
  <data key="d4">L Jiao, F Zhang, F Liu, S Yang, L Li, Z Feng…</data>
  <data key="d5">2019</data>
  <data key="d6">1065</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5669364687087032958&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="9040151999224305592">
  <data key="d0">Siamese box adaptive network for visual tracking</data>
  <data key="d1">9040151999224305592</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Siamese_Box_Adaptive_Network_for_Visual_Tracking_CVPR_2020_paper.html</data>
  <data key="d3">Siamese box adaptive network for visual tracking</data>
  <data key="d4">Z Chen, B Zhong, G Li, S Zhang…</data>
  <data key="d5">2020</data>
  <data key="d6">642</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9040151999224305592&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="3720837586692381467">
  <data key="d0">3dssd: Point-based 3d single stage object detector</data>
  <data key="d1">3720837586692381467</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Yang_3DSSD_Point-Based_3D_Single_Stage_Object_Detector_CVPR_2020_paper.html</data>
  <data key="d3">3dssd: Point-based 3d single stage object detector</data>
  <data key="d4">Z Yang, Y Sun, S Liu, J Jia</data>
  <data key="d5">2020</data>
  <data key="d6">692</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3720837586692381467&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="16050906463468676059">
  <data key="d0">Fcos3d: Fully convolutional one-stage monocular 3d object detection</data>
  <data key="d1">16050906463468676059</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021W/3DODI/html/Wang_FCOS3D_Fully_Convolutional_One-Stage_Monocular_3D_Object_Detection_ICCVW_2021_paper.html</data>
  <data key="d3">Fcos3d: Fully convolutional one-stage monocular 3d object detection</data>
  <data key="d4">T Wang, X Zhu, J Pang, D Lin</data>
  <data key="d5">2021</data>
  <data key="d6">299</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16050906463468676059&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="18267268624032061291">
  <data key="d0">Varifocalnet: An iou-aware dense object detector</data>
  <data key="d1">18267268624032061291</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Zhang_VarifocalNet_An_IoU-Aware_Dense_Object_Detector_CVPR_2021_paper.html</data>
  <data key="d3">Varifocalnet: An iou-aware dense object detector</data>
  <data key="d4">H Zhang, Y Wang, F Dayoub…</data>
  <data key="d5">2021</data>
  <data key="d6">432</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18267268624032061291&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4993232610053036190">
  <data key="d0">Solov2: Dynamic and fast instance segmentation</data>
  <data key="d1">4993232610053036190</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2020/hash/cd3afef9b8b89558cd56638c3631868a-Abstract.html</data>
  <data key="d3">Solov2: Dynamic and fast instance segmentation</data>
  <data key="d4">X Wang, R Zhang, T Kong, L Li…</data>
  <data key="d5">2020</data>
  <data key="d6">596</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4993232610053036190&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="3466946125865762121">
  <data key="d0">Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution</data>
  <data key="d1">3466946125865762121</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Qiao_DetectoRS_Detecting_Objects_With_Recursive_Feature_Pyramid_and_Switchable_Atrous_CVPR_2021_paper.html</data>
  <data key="d3">Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution</data>
  <data key="d4">S Qiao, LC Chen, A Yuille</data>
  <data key="d5">2021</data>
  <data key="d6">612</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3466946125865762121&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="16305632232773240100">
  <data key="d0">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</data>
  <data key="d1">16305632232773240100</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2020/hash/f0bda020d2470f2e74990a07a607ebd9-Abstract.html</data>
  <data key="d3">Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection</data>
  <data key="d4">X Li, W Wang, L Wu, S Chen, X Hu…</data>
  <data key="d5">2020</data>
  <data key="d6">519</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16305632232773240100&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="10517065804384853053">
  <data key="d0">Ocean: Object-aware anchor-free tracking</data>
  <data key="d1">10517065804384853053</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-58589-1_46</data>
  <data key="d3">Ocean: Object-aware anchor-free tracking</data>
  <data key="d4">Z Zhang, H Peng, J Fu, B Li, W Hu</data>
  <data key="d5">2020</data>
  <data key="d6">505</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10517065804384853053&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14858856697824684994">
  <data key="d0">Ota: Optimal transport assignment for object detection</data>
  <data key="d1">14858856697824684994</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Ge_OTA_Optimal_Transport_Assignment_for_Object_Detection_CVPR_2021_paper.html</data>
  <data key="d3">Ota: Optimal transport assignment for object detection</data>
  <data key="d4">Z Ge, S Liu, Z Li, O Yoshie…</data>
  <data key="d5">2021</data>
  <data key="d6">289</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14858856697824684994&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="2731814227384441305">
  <data key="d0">Hinet: Half instance normalization network for image restoration</data>
  <data key="d1">2731814227384441305</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Chen_HINet_Half_Instance_Normalization_Network_for_Image_Restoration_CVPRW_2021_paper.html</data>
  <data key="d3">Hinet: Half instance normalization network for image restoration</data>
  <data key="d4">L Chen, X Lu, J Zhang, X Chu…</data>
  <data key="d5">2021</data>
  <data key="d6">257</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2731814227384441305&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="10020660180667111529">
  <data key="d0">Regionclip: Region-based language-image pretraining</data>
  <data key="d1">10020660180667111529</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.html</data>
  <data key="d3">Regionclip: Region-based language-image pretraining</data>
  <data key="d4">Y Zhong, J Yang, P Zhang, C Li…</data>
  <data key="d5">2022</data>
  <data key="d6">165</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10020660180667111529&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="3426844125187450693">
  <data key="d0">Solo: Segmenting objects by locations</data>
  <data key="d1">3426844125187450693</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-58523-5_38</data>
  <data key="d3">Solo: Segmenting objects by locations</data>
  <data key="d4">X Wang, T Kong, C Shen, Y Jiang, L Li</data>
  <data key="d5">2020</data>
  <data key="d6">580</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3426844125187450693&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4790520819340140445">
  <data key="d0">Siamfc++: Towards robust and accurate visual tracking with target estimation guidelines</data>
  <data key="d1">4790520819340140445</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/6944</data>
  <data key="d3">Siamfc++: Towards robust and accurate visual tracking with target estimation guidelines</data>
  <data key="d4">Y Xu, Z Wang, Z Li, Y Yuan, G Yu</data>
  <data key="d5">2020</data>
  <data key="d6">610</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4790520819340140445&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="12328462444653843486">
  <data key="d0">SiamCAR: Siamese fully convolutional classification and regression for visual tracking</data>
  <data key="d1">12328462444653843486</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Guo_SiamCAR_Siamese_Fully_Convolutional_Classification_and_Regression_for_Visual_Tracking_CVPR_2020_paper.html</data>
  <data key="d3">SiamCAR: Siamese fully convolutional classification and regression for visual tracking</data>
  <data key="d4">D Guo, J Wang, Y Cui, Z Wang…</data>
  <data key="d5">2020</data>
  <data key="d6">564</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12328462444653843486&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="12541147144211358106">
  <data key="d0">Conditional convolutions for instance segmentation</data>
  <data key="d1">12541147144211358106</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-58452-8_17</data>
  <data key="d3">Conditional convolutions for instance segmentation</data>
  <data key="d4">Z Tian, C Shen, H Chen</data>
  <data key="d5">2020</data>
  <data key="d6">490</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12541147144211358106&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11565832587997700911">
  <data key="d0">Asymmetric loss for multi-label classification</data>
  <data key="d1">11565832587997700911</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Ridnik_Asymmetric_Loss_for_Multi-Label_Classification_ICCV_2021_paper.html</data>
  <data key="d3">Asymmetric loss for multi-label classification</data>
  <data key="d4">T Ridnik, E Ben-Baruch, N Zamir…</data>
  <data key="d5">2021</data>
  <data key="d6">206</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11565832587997700911&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="13904438580236939165">
  <data key="d0">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</data>
  <data key="d1">13904438580236939165</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Xie_Propagate_Yourself_Exploring_Pixel-Level_Consistency_for_Unsupervised_Visual_Representation_Learning_CVPR_2021_paper.html</data>
  <data key="d3">Propagate yourself: Exploring pixel-level consistency for unsupervised visual representation learning</data>
  <data key="d4">Z Xie, Y Lin, Z Zhang, Y Cao…</data>
  <data key="d5">2021</data>
  <data key="d6">331</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13904438580236939165&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="15407313785451367942">
  <data key="d0">Polarmask: Single shot instance segmentation with polar representation</data>
  <data key="d1">15407313785451367942</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Xie_PolarMask_Single_Shot_Instance_Segmentation_With_Polar_Representation_CVPR_2020_paper.html</data>
  <data key="d3">Polarmask: Single shot instance segmentation with polar representation</data>
  <data key="d4">E Xie, P Sun, X Song, W Wang, X Liu…</data>
  <data key="d5">2020</data>
  <data key="d6">523</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15407313785451367942&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="6294085712262467065">
  <data key="d0">Tood: Task-aligned one-stage object detection</data>
  <data key="d1">6294085712262467065</data>
  <data key="d2">https://www.computer.org/csdl/proceedings-article/iccv/2021/281200d490/1BmEvqSJIEE</data>
  <data key="d3">Tood: Task-aligned one-stage object detection</data>
  <data key="d4">C Feng, Y Zhong, Y Gao, MR Scott…</data>
  <data key="d5">2021</data>
  <data key="d6">278</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6294085712262467065&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="15376920793982085069">
  <data key="d0">Is pseudo-lidar needed for monocular 3d object detection?</data>
  <data key="d1">15376920793982085069</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021/html/Park_Is_Pseudo-Lidar_Needed_for_Monocular_3D_Object_Detection_ICCV_2021_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">Is pseudo-lidar needed for monocular 3d object detection?</data>
  <data key="d4">D Park, R Ambrus, V Guizilini, J Li…</data>
  <data key="d5">2021</data>
  <data key="d6">190</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15376920793982085069&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="3308996211432317749">
  <data key="d0">Fsce: Few-shot object detection via contrastive proposal encoding</data>
  <data key="d1">3308996211432317749</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Sun_FSCE_Few-Shot_Object_Detection_via_Contrastive_Proposal_Encoding_CVPR_2021_paper.html</data>
  <data key="d3">Fsce: Few-shot object detection via contrastive proposal encoding</data>
  <data key="d4">B Sun, B Li, S Cai, Y Yuan…</data>
  <data key="d5">2021</data>
  <data key="d6">230</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3308996211432317749&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="9190599054932808738">
  <data key="d0">Centermask: Real-time anchor-free instance segmentation</data>
  <data key="d1">9190599054932808738</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Lee_CenterMask_Real-Time_Anchor-Free_Instance_Segmentation_CVPR_2020_paper.html</data>
  <data key="d3">Centermask: Real-time anchor-free instance segmentation</data>
  <data key="d4">Y Lee, J Park</data>
  <data key="d5">2020</data>
  <data key="d6">481</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9190599054932808738&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="6999319186345681083">
  <data key="d0">Blendmask: Top-down meets bottom-up for instance segmentation</data>
  <data key="d1">6999319186345681083</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Chen_BlendMask_Top-Down_Meets_Bottom-Up_for_Instance_Segmentation_CVPR_2020_paper.html</data>
  <data key="d3">Blendmask: Top-down meets bottom-up for instance segmentation</data>
  <data key="d4">H Chen, K Sun, Z Tian, C Shen…</data>
  <data key="d5">2020</data>
  <data key="d6">455</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6999319186345681083&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4793705234619070656">
  <data key="d0">Foveabox: Beyound anchor-based object detection</data>
  <data key="d1">4793705234619070656</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9123553/</data>
  <data key="d3">Foveabox: Beyound anchor-based object detection</data>
  <data key="d4">T Kong, F Sun, H Liu, Y Jiang, L Li…</data>
  <data key="d5">2020</data>
  <data key="d6">792</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4793705234619070656&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11352324708624125387">
  <data key="d0">Rethinking transformer-based set prediction for object detection</data>
  <data key="d1">11352324708624125387</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Sun_Rethinking_Transformer-Based_Set_Prediction_for_Object_Detection_ICCV_2021_paper.html</data>
  <data key="d3">Rethinking transformer-based set prediction for object detection</data>
  <data key="d4">Z Sun, S Cao, Y Yang…</data>
  <data key="d5">2021</data>
  <data key="d6">252</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11352324708624125387&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="760283713428624225">
  <data key="d0">The seventh visual object tracking VOT2019 challenge results</data>
  <data key="d1">760283713428624225</data>
  <data key="d2">http://openaccess.thecvf.com/content_ICCVW_2019/html/VOT/Kristan_The_Seventh_Visual_Object_Tracking_VOT2019_Challenge_Results_ICCVW_2019_paper.html</data>
  <data key="d3">The seventh visual object tracking VOT2019 challenge results</data>
  <data key="d4">M Kristan, J Matas, A Leonardis…</data>
  <data key="d5">2019</data>
  <data key="d6">457</data>
  <data key="d7">https://scholar.google.com/scholar?cites=760283713428624225&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="3077479873636042159">
  <data key="d0">Learning spatial fusion for single-shot object detection</data>
  <data key="d1">3077479873636042159</data>
  <data key="d2">https://arxiv.org/abs/1911.09516</data>
  <data key="d3">Learning spatial fusion for single-shot object detection</data>
  <data key="d4">S Liu, D Huang, Y Wang</data>
  <data key="d5">2019</data>
  <data key="d6">465</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3077479873636042159&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="15278380461775262117">
  <data key="d0">Align deep features for oriented object detection</data>
  <data key="d1">15278380461775262117</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9377550/</data>
  <data key="d3">Align deep features for oriented object detection</data>
  <data key="d4">J Han, J Ding, J Li, GS Xia</data>
  <data key="d5">2021</data>
  <data key="d6">360</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15278380461775262117&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="10187246864084975926">
  <data key="d0">Transforming model prediction for tracking</data>
  <data key="d1">10187246864084975926</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Mayer_Transforming_Model_Prediction_for_Tracking_CVPR_2022_paper.html</data>
  <data key="d3">Transforming model prediction for tracking</data>
  <data key="d4">C Mayer, M Danelljan, G Bhat, M Paul…</data>
  <data key="d5">2022</data>
  <data key="d6">111</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10187246864084975926&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="12308813102675740697">
  <data key="d0">BERTERS: Multimodal Representation Learning for Expert Recommendation System with Transformer</data>
  <data key="d1">12308813102675740697</data>
  <data key="d2">https://www.academia.edu/download/80829631/2007.07229v1.pdf</data>
  <data key="d3">BERTERS: Multimodal Representation Learning for Expert Recommendation System with Transformer</data>
  <data key="d4">N Nikzad–Khasmakhia, M Balafara…</data>
  <data key="d5">2020</data>
  <data key="d6">329</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12308813102675740697&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="17049037070556303909">
  <data key="d0">Dynamic R-CNN: Towards high quality object detection via dynamic training</data>
  <data key="d1">17049037070556303909</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-58555-6_16</data>
  <data key="d3">Dynamic R-CNN: Towards high quality object detection via dynamic training</data>
  <data key="d4">H Zhang, H Chang, B Ma, N Wang, X Chen</data>
  <data key="d5">2020</data>
  <data key="d6">352</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17049037070556303909&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14304438718574157071">
  <data key="d0">Fast convergence of detr with spatially modulated co-attention</data>
  <data key="d1">14304438718574157071</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Gao_Fast_Convergence_of_DETR_With_Spatially_Modulated_Co-Attention_ICCV_2021_paper.html</data>
  <data key="d3">Fast convergence of detr with spatially modulated co-attention</data>
  <data key="d4">P Gao, M Zheng, X Wang, J Dai…</data>
  <data key="d5">2021</data>
  <data key="d6">206</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14304438718574157071&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11305281026744071790">
  <data key="d0">Bevdet: High-performance multi-camera 3d object detection in bird-eye-view</data>
  <data key="d1">11305281026744071790</data>
  <data key="d2">https://arxiv.org/abs/2112.11790</data>
  <data key="d3">Bevdet: High-performance multi-camera 3d object detection in bird-eye-view</data>
  <data key="d4">J Huang, G Huang, Z Zhu, Y Ye, D Du</data>
  <data key="d5">2021</data>
  <data key="d6">209</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11305281026744071790&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="2210620764419829906">
  <data key="d0">Probabilistic anchor assignment with iou prediction for object detection</data>
  <data key="d1">2210620764419829906</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-58595-2_22</data>
  <data key="d3">Probabilistic anchor assignment with iou prediction for object detection</data>
  <data key="d4">K Kim, HS Lee</data>
  <data key="d5">2020</data>
  <data key="d6">307</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2210620764419829906&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="1321793621715878501">
  <data key="d0">Probabilistic and geometric depth: Detecting objects in perspective</data>
  <data key="d1">1321793621715878501</data>
  <data key="d2">https://proceedings.mlr.press/v164/wang22i.html</data>
  <data key="d3">Probabilistic and geometric depth: Detecting objects in perspective</data>
  <data key="d4">T Wang, ZHU Xinge, J Pang…</data>
  <data key="d5">2022</data>
  <data key="d6">157</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1321793621715878501&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="15248432581458880980">
  <data key="d0">Stmtrack: Template-free visual tracking with space-time memory networks</data>
  <data key="d1">15248432581458880980</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Fu_STMTrack_Template-Free_Visual_Tracking_With_Space-Time_Memory_Networks_CVPR_2021_paper.html</data>
  <data key="d3">Stmtrack: Template-free visual tracking with space-time memory networks</data>
  <data key="d4">Z Fu, Q Liu, Z Fu, Y Wang</data>
  <data key="d5">2021</data>
  <data key="d6">167</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15248432581458880980&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="15847441662530625185">
  <data key="d0">Structured knowledge distillation for semantic segmentation</data>
  <data key="d1">15847441662530625185</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.html</data>
  <data key="d3">Structured knowledge distillation for semantic segmentation</data>
  <data key="d4">Y Liu, K Chen, C Liu, Z Qin, Z Luo…</data>
  <data key="d5">2019</data>
  <data key="d6">601</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15847441662530625185&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="890350355816522800">
  <data key="d0">Augfpn: Improving multi-scale feature learning for object detection</data>
  <data key="d1">890350355816522800</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Guo_AugFPN_Improving_Multi-Scale_Feature_Learning_for_Object_Detection_CVPR_2020_paper.html</data>
  <data key="d3">Augfpn: Improving multi-scale feature learning for object detection</data>
  <data key="d4">C Guo, B Fan, Q Zhang, S Xiang…</data>
  <data key="d5">2020</data>
  <data key="d6">339</data>
  <data key="d7">https://scholar.google.com/scholar?cites=890350355816522800&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="7794016071296671172">
  <data key="d0">Revisiting the sibling head in object detector</data>
  <data key="d1">7794016071296671172</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Song_Revisiting_the_Sibling_Head_in_Object_Detector_CVPR_2020_paper</data>
  <data key="d3">Revisiting the sibling head in object detector</data>
  <data key="d4">G Song, Y Liu, X Wang</data>
  <data key="d5">2020</data>
  <data key="d6">326</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7794016071296671172&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14672664302977559137">
  <data key="d0">Pf-net: Point fractal network for 3d point cloud completion</data>
  <data key="d1">14672664302977559137</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Huang_PF-Net_Point_Fractal_Network_for_3D_Point_Cloud_Completion_CVPR_2020_paper.html</data>
  <data key="d3">Pf-net: Point fractal network for 3d point cloud completion</data>
  <data key="d4">Z Huang, Y Yu, J Xu, F Ni, X Le</data>
  <data key="d5">2020</data>
  <data key="d6">316</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14672664302977559137&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="13259842884566962187">
  <data key="d0">Enforcing geometric constraints of virtual normal for depth prediction</data>
  <data key="d1">13259842884566962187</data>
  <data key="d2">http://openaccess.thecvf.com/content_ICCV_2019/html/Yin_Enforcing_Geometric_Constraints_of_Virtual_Normal_for_Depth_Prediction_ICCV_2019_paper.html</data>
  <data key="d3">Enforcing geometric constraints of virtual normal for depth prediction</data>
  <data key="d4">W Yin, Y Liu, C Shen, Y Yan</data>
  <data key="d5">2019</data>
  <data key="d6">384</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13259842884566962187&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="12458152692195612427">
  <data key="d0">Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection</data>
  <data key="d1">12458152692195612427</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Li_Generalized_Focal_Loss_V2_Learning_Reliable_Localization_Quality_Estimation_for_CVPR_2021_paper.html</data>
  <data key="d3">Generalized focal loss v2: Learning reliable localization quality estimation for dense object detection</data>
  <data key="d4">X Li, W Wang, X Hu, J Li, J Tang…</data>
  <data key="d5">2021</data>
  <data key="d6">190</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12458152692195612427&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="18416642050963457459">
  <data key="d0">FCOS: A simple and strong anchor-free object detector</data>
  <data key="d1">18416642050963457459</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9229517/</data>
  <data key="d3">FCOS: A simple and strong anchor-free object detector</data>
  <data key="d4">Z Tian, C Shen, H Chen, T He</data>
  <data key="d5">2020</data>
  <data key="d6">272</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18416642050963457459&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="3028370674147212605">
  <data key="d0">Relationtrack: Relation-aware multiple object tracking with decoupled representation</data>
  <data key="d1">3028370674147212605</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9709649/</data>
  <data key="d3">Relationtrack: Relation-aware multiple object tracking with decoupled representation</data>
  <data key="d4">E Yu, Z Li, S Han, H Wang</data>
  <data key="d5">2022</data>
  <data key="d6">80</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3028370674147212605&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="11978445553624214380">
  <data key="d0">ISNet: Towards improving separability for remote sensing image change detection</data>
  <data key="d1">11978445553624214380</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9772654/</data>
  <data key="d3">ISNet: Towards improving separability for remote sensing image change detection</data>
  <data key="d4">G Cheng, G Wang, J Han</data>
  <data key="d5">2022</data>
  <data key="d6">39</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11978445553624214380&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="949946091219062353">
  <data key="d0">Rethinking efficient lane detection via curve modeling</data>
  <data key="d1">949946091219062353</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Feng_Rethinking_Efficient_Lane_Detection_via_Curve_Modeling_CVPR_2022_paper.html</data>
  <data key="d3">Rethinking efficient lane detection via curve modeling</data>
  <data key="d4">Z Feng, S Guo, X Tan, K Xu…</data>
  <data key="d5">2022</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=949946091219062353&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="9132276678803270854">
  <data key="d0">Head: Hetero-assists distillation for heterogeneous object detectors</data>
  <data key="d1">9132276678803270854</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20077-9_19</data>
  <data key="d3">Head: Hetero-assists distillation for heterogeneous object detectors</data>
  <data key="d4">L Wang, X Li, Y Liao, Z Jiang, J Wu, F Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9132276678803270854&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="6230352926792784452">
  <data key="d0">SegContrast: 3D point cloud feature representation learning through self-supervised segment discrimination</data>
  <data key="d1">6230352926792784452</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9681336/</data>
  <data key="d3">SegContrast: 3D point cloud feature representation learning through self-supervised segment discrimination</data>
  <data key="d4">L Nunes, R Marcuzzi, X Chen, J Behley…</data>
  <data key="d5">2022</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6230352926792784452&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="12916943626500149813">
  <data key="d0">Ship detection in SAR images based on feature enhancement Swin transformer and adjacent feature fusion</data>
  <data key="d1">12916943626500149813</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/13/3186</data>
  <data key="d3">Ship detection in SAR images based on feature enhancement Swin transformer and adjacent feature fusion</data>
  <data key="d4">K Li, M Zhang, M Xu, R Tang, L Wang, H Wang</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12916943626500149813&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="282502688385514417">
  <data key="d0">Instance-aware distillation for efficient object detection in remote sensing images</data>
  <data key="d1">282502688385514417</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10024393/</data>
  <data key="d3">Instance-aware distillation for efficient object detection in remote sensing images</data>
  <data key="d4">C Li, G Cheng, G Wang, P Zhou…</data>
  <data key="d5">2023</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=282502688385514417&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="13126613410294485366">
  <data key="d0">PAG-YOLO: A portable attention-guided YOLO network for small ship detection</data>
  <data key="d1">13126613410294485366</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/16/3059</data>
  <data key="d3">PAG-YOLO: A portable attention-guided YOLO network for small ship detection</data>
  <data key="d4">J Hu, X Zhi, T Shi, W Zhang, Y Cui, S Zhao</data>
  <data key="d5">2021</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13126613410294485366&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="7011964847765802199">
  <data key="d0">Fovea: Foveated image magnification for autonomous navigation</data>
  <data key="d1">7011964847765802199</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Thavamani_FOVEA_Foveated_Image_Magnification_for_Autonomous_Navigation_ICCV_2021_paper.html</data>
  <data key="d3">Fovea: Foveated image magnification for autonomous navigation</data>
  <data key="d4">C Thavamani, M Li, N Cebron…</data>
  <data key="d5">2021</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7011964847765802199&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="8654903568547047842">
  <data key="d0">PillarNeXt: Rethinking network designs for 3D object detection in LiDAR point clouds</data>
  <data key="d1">8654903568547047842</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Li_PillarNeXt_Rethinking_Network_Designs_for_3D_Object_Detection_in_LiDAR_CVPR_2023_paper.html</data>
  <data key="d3">PillarNeXt: Rethinking network designs for 3D object detection in LiDAR point clouds</data>
  <data key="d4">J Li, C Luo, X Yang</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8654903568547047842&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="5278530239378148719">
  <data key="d0">Efficient decoder-free object detection with transformers</data>
  <data key="d1">5278530239378148719</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20080-9_5</data>
  <data key="d3">Efficient decoder-free object detection with transformers</data>
  <data key="d4">P Chen, M Zhang, Y Shen, K Sheng, Y Gao…</data>
  <data key="d5">2022</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5278530239378148719&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="2885916562392086198">
  <data key="d0">Half-UNet: A simplified U-Net architecture for medical image segmentation</data>
  <data key="d1">2885916562392086198</data>
  <data key="d2">https://www.frontiersin.org/articles/10.3389/fninf.2022.911679/full</data>
  <data key="d3">Half-UNet: A simplified U-Net architecture for medical image segmentation</data>
  <data key="d4">H Lu, Y She, J Tie, S Xu</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2885916562392086198&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="283347957351040289">
  <data key="d0">Pai3d: Painting adaptive instance-prior for 3d object detection</data>
  <data key="d1">283347957351040289</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-25072-9_32</data>
  <data key="d3">Pai3d: Painting adaptive instance-prior for 3d object detection</data>
  <data key="d4">H Liu, Z Xu, D Wang, B Zhang, G Wang, B Dong…</data>
  <data key="d5">2022</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=283347957351040289&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="2620378263331322045">
  <data key="d0">A lightweight network based on one-level feature for ship detection in SAR images</data>
  <data key="d1">2620378263331322045</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/14/3321</data>
  <data key="d3">A lightweight network based on one-level feature for ship detection in SAR images</data>
  <data key="d4">W Yu, Z Wang, J Li, Y Luo, Z Yu</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2620378263331322045&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="8136487588159115675">
  <data key="d0">A fast and accurate method of power line intelligent inspection based on edge computing</data>
  <data key="d1">8136487588159115675</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9722917/</data>
  <data key="d3">A fast and accurate method of power line intelligent inspection based on edge computing</data>
  <data key="d4">M Liu, Z Li, Y Li, Y Liu</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8136487588159115675&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="3137631071934908410">
  <data key="d0">Detecting wheat heads from UAV low-altitude remote sensing images using Deep Learning based on transformer</data>
  <data key="d1">3137631071934908410</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/20/5141</data>
  <data key="d3">Detecting wheat heads from UAV low-altitude remote sensing images using Deep Learning based on transformer</data>
  <data key="d4">J Zhu, G Yang, X Feng, X Li, H Fang, J Zhang, X Bai…</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3137631071934908410&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="1587380561678705672">
  <data key="d0">Domain generalization in deep learning based mass detection in mammography: A large-scale multi-center study</data>
  <data key="d1">1587380561678705672</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0933365722001415</data>
  <data key="d3">Domain generalization in deep learning based mass detection in mammography: A large-scale multi-center study</data>
  <data key="d4">L Garrucho, K Kushibar, S Jouide, O Diaz…</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1587380561678705672&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="18269904853006408245">
  <data key="d0">MS-IAF: Multi-Scale information augmentation framework for aircraft detection</data>
  <data key="d1">18269904853006408245</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/15/3696</data>
  <data key="d3">MS-IAF: Multi-Scale information augmentation framework for aircraft detection</data>
  <data key="d4">Y Zhao, J Li, W Li, P Shan, X Wang, L Li, Q Fu</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18269904853006408245&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="8884306980263647674">
  <data key="d0">Improving object detection by label assignment distillation</data>
  <data key="d1">8884306980263647674</data>
  <data key="d2">https://openaccess.thecvf.com/content/WACV2022/html/Nguyen_Improving_Object_Detection_by_Label_Assignment_Distillation_WACV_2022_paper.html</data>
  <data key="d3">Improving object detection by label assignment distillation</data>
  <data key="d4">CH Nguyen, TC Nguyen, TN Tang…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8884306980263647674&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="11272776230560973281">
  <data key="d0">Rethinking general underwater object detection: Datasets, challenges, and solutions</data>
  <data key="d1">11272776230560973281</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0925231222013169</data>
  <data key="d3">Rethinking general underwater object detection: Datasets, challenges, and solutions</data>
  <data key="d4">C Fu, R Liu, X Fan, P Chen, H Fu, W Yuan, M Zhu…</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11272776230560973281&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="1187656217922278561">
  <data key="d0">A review on anchor assignment and sampling heuristics in deep learning-based object detection</data>
  <data key="d1">1187656217922278561</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S092523122200861X</data>
  <data key="d3">A review on anchor assignment and sampling heuristics in deep learning-based object detection</data>
  <data key="d4">XT Vo, KH Jo</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1187656217922278561&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="17222739228813142360">
  <data key="d0">An efficient and lightweight CNN model with soft quantification for ship detection in SAR images</data>
  <data key="d1">17222739228813142360</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9805750/</data>
  <data key="d3">An efficient and lightweight CNN model with soft quantification for ship detection in SAR images</data>
  <data key="d4">X Yang, J Zhang, C Chen…</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17222739228813142360&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="9386562677703754376">
  <data key="d0">Object detection based on fusion of sparse point cloud and image information</data>
  <data key="d1">9386562677703754376</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9507517/</data>
  <data key="d3">Object detection based on fusion of sparse point cloud and image information</data>
  <data key="d4">X Xu, L Zhang, J Yang, C Cao, Z Tan…</data>
  <data key="d5">2021</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9386562677703754376&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="18231280624475386436">
  <data key="d0">FEA-swin: Foreground enhancement attention swin transformer network for accurate UAV-based dense object detection</data>
  <data key="d1">18231280624475386436</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/18/6993</data>
  <data key="d3">FEA-swin: Foreground enhancement attention swin transformer network for accurate UAV-based dense object detection</data>
  <data key="d4">W Xu, C Zhang, Q Wang, P Dai</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18231280624475386436&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="18294362283448718927">
  <data key="d0">AgriPest-YOLO: A rapid light-trap agricultural pest detection method based on deep learning</data>
  <data key="d1">18294362283448718927</data>
  <data key="d2">https://www.frontiersin.org/articles/10.3389/fpls.2022.1079384/full</data>
  <data key="d3">AgriPest-YOLO: A rapid light-trap agricultural pest detection method based on deep learning</data>
  <data key="d4">W Zhang, H Huang, Y Sun, X Wu</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18294362283448718927&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="9346886979579970159">
  <data key="d0">Automatic Check-Out via Prototype-Based Classifier Learning from Single-Product Exemplars</data>
  <data key="d1">9346886979579970159</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19806-9_16</data>
  <data key="d3">Automatic Check-Out via Prototype-Based Classifier Learning from Single-Product Exemplars</data>
  <data key="d4">H Chen, XS Wei, F Zhang, Y Shen, H Xu…</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9346886979579970159&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="1810771332585372148">
  <data key="d0">Mask or non-mask? robust face mask detector via triplet-consistency representation learning</data>
  <data key="d1">1810771332585372148</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3472623</data>
  <data key="d3">Mask or non-mask? robust face mask detector via triplet-consistency representation learning</data>
  <data key="d4">CW Yang, TH Phung, HH Shuai…</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1810771332585372148&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="1147425233131687163">
  <data key="d0">Cross attention redistribution with contrastive learning for few shot object detection</data>
  <data key="d1">1147425233131687163</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0141938222000129</data>
  <data key="d3">Cross attention redistribution with contrastive learning for few shot object detection</data>
  <data key="d4">J Quan, B Ge, L Chen</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1147425233131687163&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="4888428022255023575">
  <data key="d0">A guide to image and video based small object detection using deep learning: Case study of maritime surveillance</data>
  <data key="d1">4888428022255023575</data>
  <data key="d2">https://arxiv.org/abs/2207.12926</data>
  <data key="d3">A guide to image and video based small object detection using deep learning: Case study of maritime surveillance</data>
  <data key="d4">AM Rekavandi, L Xu, F Boussaid…</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4888428022255023575&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="18156777187430112022">
  <data key="d0">Trident‐YOLO: Improving the precision and speed of mobile device object detection</data>
  <data key="d1">18156777187430112022</data>
  <data key="d2">https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.12340</data>
  <data key="d3">Trident‐YOLO: Improving the precision and speed of mobile device object detection</data>
  <data key="d4">G Wang, H Ding, B Li, R Nie, Y Zhao</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18156777187430112022&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="4470676289942389642">
  <data key="d0">YOLO-SD: Small Ship Detection in SAR Images by Multi-Scale Convolution and Feature Transformer Module</data>
  <data key="d1">4470676289942389642</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/20/5268</data>
  <data key="d3">YOLO-SD: Small Ship Detection in SAR Images by Multi-Scale Convolution and Feature Transformer Module</data>
  <data key="d4">S Wang, S Gao, L Zhou, R Liu, H Zhang, J Liu, Y Jia…</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4470676289942389642&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="6294936790423161188">
  <data key="d0">Mae-det: Revisiting maximum entropy principle in zero-shot nas for efficient object detection</data>
  <data key="d1">6294936790423161188</data>
  <data key="d2">https://arxiv.org/abs/2111.13336</data>
  <data key="d3">Mae-det: Revisiting maximum entropy principle in zero-shot nas for efficient object detection</data>
  <data key="d4">Z Sun, M Lin, X Sun, Z Tan, H Li, R Jin</data>
  <data key="d5">2021</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6294936790423161188&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="10175988650775937306">
  <data key="d0">BoxMask: Revisiting Bounding Box Supervision for Video Object Detection</data>
  <data key="d1">10175988650775937306</data>
  <data key="d2">https://openaccess.thecvf.com/content/WACV2023/html/Hashmi_BoxMask_Revisiting_Bounding_Box_Supervision_for_Video_Object_Detection_WACV_2023_paper.html</data>
  <data key="d3">BoxMask: Revisiting Bounding Box Supervision for Video Object Detection</data>
  <data key="d4">KA Hashmi, A Pagani, D Stricker…</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10175988650775937306&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="12727040761225033710">
  <data key="d0">REAP: A Large-Scale Realistic Adversarial Patch Benchmark</data>
  <data key="d1">12727040761225033710</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Hingun_REAP_A_Large-Scale_Realistic_Adversarial_Patch_Benchmark_ICCV_2023_paper.html</data>
  <data key="d3">REAP: A Large-Scale Realistic Adversarial Patch Benchmark</data>
  <data key="d4">N Hingun, C Sitawarin, J Li…</data>
  <data key="d5">2023</data>
  <data key="d8">11</data>
</node>
<node id="13204595328459365591">
  <data key="d0">MFANet: multi-scale feature fusion network with attention mechanism</data>
  <data key="d1">13204595328459365591</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00371-022-02503-4</data>
  <data key="d3">MFANet: multi-scale feature fusion network with attention mechanism</data>
  <data key="d4">G Wang, X Gan, Q Cao, Q Zhai</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13204595328459365591&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="17237009222277721047">
  <data key="d0">Design, integration, and evaluation of a robotic peach packaging system based on deep learning</data>
  <data key="d1">17237009222277721047</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923004015</data>
  <data key="d3">Design, integration, and evaluation of a robotic peach packaging system based on deep learning</data>
  <data key="d4">Q Wang, D Wu, Z Sun, M Zhou, D Cui, L Xie…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17237009222277721047&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="17855267959511452037">
  <data key="d0">Remote sensing object detection based on receptive field expansion block</data>
  <data key="d1">17855267959511452037</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9537586/</data>
  <data key="d3">Remote sensing object detection based on receptive field expansion block</data>
  <data key="d4">X Dong, R Fu, Y Gao, Y Qin, Y Ye…</data>
  <data key="d5">2021</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17855267959511452037&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="3563115954885327440">
  <data key="d0">Uncertainty-aware accurate insulator fault detection based on an improved YOLOX model</data>
  <data key="d1">3563115954885327440</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2352484722019205</data>
  <data key="d3">Uncertainty-aware accurate insulator fault detection based on an improved YOLOX model</data>
  <data key="d4">Z Dai</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3563115954885327440&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="7854236750952897970">
  <data key="d0">Reasonable object detection guided by knowledge of global context and category relationship</data>
  <data key="d1">7854236750952897970</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417422014221</data>
  <data key="d3">Reasonable object detection guided by knowledge of global context and category relationship</data>
  <data key="d4">H Ji, K Ye, Q Wan, L Shen</data>
  <data key="d5">2022</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7854236750952897970&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="7711517380639147333">
  <data key="d0">CCST: crowd counting with swin transformer</data>
  <data key="d1">7711517380639147333</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00371-022-02485-3</data>
  <data key="d3">CCST: crowd counting with swin transformer</data>
  <data key="d4">B Li, Y Zhang, H Xu, B Yin</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7711517380639147333&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="1089623869034577539">
  <data key="d0">A Sample Balance-Based Regression Module for Object Detection in Construction Sites</data>
  <data key="d1">1089623869034577539</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/13/6752</data>
  <data key="d3">A Sample Balance-Based Regression Module for Object Detection in Construction Sites</data>
  <data key="d4">X Wang, H Wang, C Zhang, Q He, L Huo</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1089623869034577539&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="8366499969990031131">
  <data key="d0">M2YOLOF: Based on effective receptive fields and multiple-in-single-out encoder for object detection</data>
  <data key="d1">8366499969990031131</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417422019467</data>
  <data key="d3">M2YOLOF: Based on effective receptive fields and multiple-in-single-out encoder for object detection</data>
  <data key="d4">Q Wang, Y Qian, Y Hu, C Wang, X Ye…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8366499969990031131&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="3451836672949095488">
  <data key="d0">Manipal-UAV person detection dataset: A step towards benchmarking dataset and algorithms for small object detection</data>
  <data key="d1">3451836672949095488</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0924271622003008</data>
  <data key="d3">Manipal-UAV person detection dataset: A step towards benchmarking dataset and algorithms for small object detection</data>
  <data key="d4">KR Akshatha, AK Karunakar, S Shenoy…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3451836672949095488&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="Lq8Umk34rDIJ">
  <data key="d0">PODA: Prompt-driven Zero-shot Domain Adaptation</data>
  <data key="d1">Lq8Umk34rDIJ</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Fahes_PODA_Prompt-driven_Zero-shot_Domain_Adaptation_ICCV_2023_paper.html</data>
  <data key="d3">PODA: Prompt-driven Zero-shot Domain Adaptation</data>
  <data key="d4">M Fahes, TH Vu, A Bursuc, P Pérez…</data>
  <data key="d5">2023</data>
  <data key="d8">11</data>
</node>
<node id="17178883444476643109">
  <data key="d0">Joint Attention-Guided feature fusion network for saliency detection of surface defects</data>
  <data key="d1">17178883444476643109</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9933904/</data>
  <data key="d3">Joint Attention-Guided feature fusion network for saliency detection of surface defects</data>
  <data key="d4">X Jiang, F Yan, Y Lu, K Wang, S Guo…</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17178883444476643109&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="15088715437227844190">
  <data key="d0">Finding nonrigid tiny person with densely cropped and local attention object detector networks in low-altitude aerial images</data>
  <data key="d1">15088715437227844190</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9779100/</data>
  <data key="d3">Finding nonrigid tiny person with densely cropped and local attention object detector networks in low-altitude aerial images</data>
  <data key="d4">X Zhang, Y Feng, S Zhang, N Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15088715437227844190&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="2642922896219719634">
  <data key="d0">Optimal correction cost for object detection evaluation</data>
  <data key="d1">2642922896219719634</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Otani_Optimal_Correction_Cost_for_Object_Detection_Evaluation_CVPR_2022_paper.html</data>
  <data key="d3">Optimal correction cost for object detection evaluation</data>
  <data key="d4">M Otani, R Togashi, Y Nakashima…</data>
  <data key="d5">2022</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2642922896219719634&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="4505986135620301670">
  <data key="d0">Traffic incident detection based on a global trajectory spatiotemporal map</data>
  <data key="d1">4505986135620301670</data>
  <data key="d2">https://link.springer.com/article/10.1007/s40747-021-00602-8</data>
  <data key="d3">Traffic incident detection based on a global trajectory spatiotemporal map</data>
  <data key="d4">H Liang, H Song, X Yun, S Sun, Y Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4505986135620301670&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="4657133348792854073">
  <data key="d0">Sdtp: Semantic-aware decoupled transformer pyramid for dense image prediction</data>
  <data key="d1">4657133348792854073</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9740668/</data>
  <data key="d3">Sdtp: Semantic-aware decoupled transformer pyramid for dense image prediction</data>
  <data key="d4">Z Li, Y Liu, B Li, B Feng, K Wu…</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4657133348792854073&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="4748746639276173174">
  <data key="d0">Pavement Surface Defect Detection Using Mask Region-Based Convolutional Neural Networks and Transfer Learning</data>
  <data key="d1">4748746639276173174</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/15/7364</data>
  <data key="d3">Pavement Surface Defect Detection Using Mask Region-Based Convolutional Neural Networks and Transfer Learning</data>
  <data key="d4">Y He, Z Jin, J Zhang, S Teng, G Chen, X Sun, F Cui</data>
  <data key="d5">2022</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4748746639276173174&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="1365022424711941058">
  <data key="d0">PVDet: Towards pedestrian and vehicle detection on gigapixel-level images</data>
  <data key="d1">1365022424711941058</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0952197622006959</data>
  <data key="d3">PVDet: Towards pedestrian and vehicle detection on gigapixel-level images</data>
  <data key="d4">W Mo, W Zhang, H Wei, R Cao, Y Ke, Y Luo</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1365022424711941058&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="4381569612155749359">
  <data key="d0">Advances in medical image analysis with vision transformers: A comprehensive review</data>
  <data key="d1">4381569612155749359</data>
  <data key="d2">https://arxiv.org/abs/2301.03505</data>
  <data key="d3">Advances in medical image analysis with vision transformers: A comprehensive review</data>
  <data key="d4">R Azad, A Kazerouni, M Heidari, EK Aghdam…</data>
  <data key="d5">2023</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4381569612155749359&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="9304804814105874282">
  <data key="d0">TE-YOLOF: Tiny and efficient YOLOF for blood cell detection</data>
  <data key="d1">9304804814105874282</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1746809421010132</data>
  <data key="d3">TE-YOLOF: Tiny and efficient YOLOF for blood cell detection</data>
  <data key="d4">F Xu, X Li, H Yang, Y Wang, W Xiang</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9304804814105874282&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="745746885882128143">
  <data key="d0">EYOLOX: An Efficient One-Stage Object Detection Network Based on YOLOX</data>
  <data key="d1">745746885882128143</data>
  <data key="d2">https://www.mdpi.com/2076-3417/13/3/1506</data>
  <data key="d3">EYOLOX: An Efficient One-Stage Object Detection Network Based on YOLOX</data>
  <data key="d4">R Tang, H Sun, D Liu, H Xu, M Qi, J Kong</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=745746885882128143&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="3394653354596589290">
  <data key="d0">Deep Learning for Logo Detection: A Survey</data>
  <data key="d1">3394653354596589290</data>
  <data key="d2">https://arxiv.org/abs/2210.04399</data>
  <data key="d3">Deep Learning for Logo Detection: A Survey</data>
  <data key="d4">S Hou, J Li, W Min, Q Hou, Y Zhao, Y Zheng…</data>
  <data key="d5">2022</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3394653354596589290&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="2190081206030913404">
  <data key="d0">An anchor-free defect detector for complex background based on pixelwise adaptive multiscale feature fusion</data>
  <data key="d1">2190081206030913404</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9992183/</data>
  <data key="d3">An anchor-free defect detector for complex background based on pixelwise adaptive multiscale feature fusion</data>
  <data key="d4">H Lu, M Fang, Y Qiu, W Xu</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2190081206030913404&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="9343841596799945450">
  <data key="d0">PersonGONE: Image inpainting for automated checkout solution</data>
  <data key="d1">9343841596799945450</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022W/AICity/html/Bartl_PersonGONE_Image_Inpainting_for_Automated_Checkout_Solution_CVPRW_2022_paper.html</data>
  <data key="d3">PersonGONE: Image inpainting for automated checkout solution</data>
  <data key="d4">V Bartl, J Špaňhel, A Herout</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9343841596799945450&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="8353648664327356722">
  <data key="d0">Surround-view Fisheye BEV-Perception for Valet Parking: Dataset, Baseline and Distortion-insensitive Multi-task Framework</data>
  <data key="d1">8353648664327356722</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9941140/</data>
  <data key="d3">Surround-view Fisheye BEV-Perception for Valet Parking: Dataset, Baseline and Distortion-insensitive Multi-task Framework</data>
  <data key="d4">Z Wu, Y Gan, X Li, Y Wu, X Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8353648664327356722&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="4120546132280571779">
  <data key="d0">GLE-Net: A global and local ensemble network for aerial object detection</data>
  <data key="d1">4120546132280571779</data>
  <data key="d2">https://link.springer.com/article/10.1007/s44196-021-00056-3</data>
  <data key="d3">GLE-Net: A global and local ensemble network for aerial object detection</data>
  <data key="d4">J Liao, Y Liu, Y Piao, J Su, G Cai, Y Wu</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4120546132280571779&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="13843016997180263542">
  <data key="d0">SE-COTR: A novel fruit segmentation model for green apples application in complex orchard</data>
  <data key="d1">13843016997180263542</data>
  <data key="d2">https://spj.science.org/doi/abs/10.34133/plantphenomics.0005</data>
  <data key="d3">SE-COTR: A novel fruit segmentation model for green apples application in complex orchard</data>
  <data key="d4">Z Wang, Z Zhang, Y Lu, R Luo, Y Niu, X Yang…</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13843016997180263542&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="18113388340145752965">
  <data key="d0">Aircraft Target Detection in Low Signal-to-Noise Ratio Visible Remote Sensing Images</data>
  <data key="d1">18113388340145752965</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/8/1971</data>
  <data key="d3">Aircraft Target Detection in Low Signal-to-Noise Ratio Visible Remote Sensing Images</data>
  <data key="d4">R Niu, X Zhi, S Jiang, J Gong, W Zhang, L Yu</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18113388340145752965&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="10442483506407696721">
  <data key="d0">A Dynamic Weights-Based Wavelet Attention Neural Network for Defect Detection</data>
  <data key="d1">10442483506407696721</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10179489/</data>
  <data key="d3">A Dynamic Weights-Based Wavelet Attention Neural Network for Defect Detection</data>
  <data key="d4">J Liu, H Zhao, Z Chen, Q Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10442483506407696721&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="728770006208310182">
  <data key="d0">A review on deep learning in UAV remote sensing</data>
  <data key="d1">728770006208310182</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S030324342100163X</data>
  <data key="d3">A review on deep learning in UAV remote sensing</data>
  <data key="d4">LP Osco, JM Junior, APM Ramos…</data>
  <data key="d5">2021</data>
  <data key="d6">203</data>
  <data key="d7">https://scholar.google.com/scholar?cites=728770006208310182&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="2381336199697874290">
  <data key="d0">Recent advances on loss functions in deep learning for computer vision</data>
  <data key="d1">2381336199697874290</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0925231222005239</data>
  <data key="d3">Recent advances on loss functions in deep learning for computer vision</data>
  <data key="d4">Y Tian, D Su, S Lauria, X Liu</data>
  <data key="d5">2022</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2381336199697874290&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="8767327019590446489">
  <data key="d0">Learning salient boundary feature for anchor-free temporal action localization</data>
  <data key="d1">8767327019590446489</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Lin_Learning_Salient_Boundary_Feature_for_Anchor-free_Temporal_Action_Localization_CVPR_2021_paper.html</data>
  <data key="d3">Learning salient boundary feature for anchor-free temporal action localization</data>
  <data key="d4">C Lin, C Xu, D Luo, Y Wang, Y Tai…</data>
  <data key="d5">2021</data>
  <data key="d6">154</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8767327019590446489&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="5174677530138095658">
  <data key="d0">End-to-end object detection with fully convolutional network</data>
  <data key="d1">5174677530138095658</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2021/html/Wang_End-to-End_Object_Detection_With_Fully_Convolutional_Network_CVPR_2021_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">End-to-end object detection with fully convolutional network</data>
  <data key="d4">J Wang, L Song, Z Li, H Sun, J Sun…</data>
  <data key="d5">2021</data>
  <data key="d6">172</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5174677530138095658&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="10604255508526386376">
  <data key="d0">Dynamic anchor learning for arbitrary-oriented object detection</data>
  <data key="d1">10604255508526386376</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/16336</data>
  <data key="d3">Dynamic anchor learning for arbitrary-oriented object detection</data>
  <data key="d4">Q Ming, Z Zhou, L Miao, H Zhang, L Li</data>
  <data key="d5">2021</data>
  <data key="d6">192</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10604255508526386376&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="6196888269192586051">
  <data key="d0">Generalized few-shot object detection without forgetting</data>
  <data key="d1">6196888269192586051</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2021/html/Fan_Generalized_Few-Shot_Object_Detection_Without_Forgetting_CVPR_2021_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">Generalized few-shot object detection without forgetting</data>
  <data key="d4">Z Fan, Y Ma, Z Li, J Sun</data>
  <data key="d5">2021</data>
  <data key="d6">117</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6196888269192586051&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="2965311170207987165">
  <data key="d0">Anchor-free oriented proposal generator for object detection</data>
  <data key="d1">2965311170207987165</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9795321/</data>
  <data key="d3">Anchor-free oriented proposal generator for object detection</data>
  <data key="d4">G Cheng, J Wang, K Li, X Xie, C Lang…</data>
  <data key="d5">2022</data>
  <data key="d6">109</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2965311170207987165&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11140668418037577536">
  <data key="d0">Mmrotate: A rotated object detection benchmark using pytorch</data>
  <data key="d1">11140668418037577536</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3503161.3548541</data>
  <data key="d3">Mmrotate: A rotated object detection benchmark using pytorch</data>
  <data key="d4">Y Zhou, X Yang, G Zhang, J Wang, Y Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">86</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11140668418037577536&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="17675636447045263550">
  <data key="d0">Dense teacher: Dense pseudo-labels for semi-supervised object detection</data>
  <data key="d1">17675636447045263550</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20077-9_3</data>
  <data key="d3">Dense teacher: Dense pseudo-labels for semi-supervised object detection</data>
  <data key="d4">H Zhou, Z Ge, S Liu, W Mao, Z Li, H Yu…</data>
  <data key="d5">2022</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17675636447045263550&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="1878991916435892092">
  <data key="d0">Liga-stereo: Learning lidar geometry aware representations for stereo-based 3d detector</data>
  <data key="d1">1878991916435892092</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Guo_LIGA-Stereo_Learning_LiDAR_Geometry_Aware_Representations_for_Stereo-Based_3D_Detector_ICCV_2021_paper.html</data>
  <data key="d3">Liga-stereo: Learning lidar geometry aware representations for stereo-based 3d detector</data>
  <data key="d4">X Guo, S Shi, X Wang, H Li</data>
  <data key="d5">2021</data>
  <data key="d6">71</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1878991916435892092&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4988547052049874893">
  <data key="d0">Tresnet: High performance gpu-dedicated architecture</data>
  <data key="d1">4988547052049874893</data>
  <data key="d2">http://openaccess.thecvf.com/content/WACV2021/html/Ridnik_TResNet_High_Performance_GPU-Dedicated_Architecture_WACV_2021_paper.html</data>
  <data key="d3">Tresnet: High performance gpu-dedicated architecture</data>
  <data key="d4">T Ridnik, H Lawen, A Noy…</data>
  <data key="d5">2021</data>
  <data key="d6">187</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4988547052049874893&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4350254585552726003">
  <data key="d0">Autoassign: Differentiable label assignment for dense object detection</data>
  <data key="d1">4350254585552726003</data>
  <data key="d2">https://arxiv.org/abs/2007.03496</data>
  <data key="d3">Autoassign: Differentiable label assignment for dense object detection</data>
  <data key="d4">B Zhu, J Wang, Z Jiang, F Zong, S Liu, Z Li…</data>
  <data key="d5">2020</data>
  <data key="d6">181</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4350254585552726003&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="124174586124414315">
  <data key="d0">LS-SSDD-v1. 0: A deep learning dataset dedicated to small ship detection from large-scale Sentinel-1 SAR images</data>
  <data key="d1">124174586124414315</data>
  <data key="d2">https://www.mdpi.com/2072-4292/12/18/2997</data>
  <data key="d3">LS-SSDD-v1. 0: A deep learning dataset dedicated to small ship detection from large-scale Sentinel-1 SAR images</data>
  <data key="d4">T Zhang, X Zhang, X Ke, X Zhan, J Shi, S Wei, D Pan…</data>
  <data key="d5">2020</data>
  <data key="d6">144</data>
  <data key="d7">https://scholar.google.com/scholar?cites=124174586124414315&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="6486278392250669413">
  <data key="d0">Monocular 3d object detection with depth from motion</data>
  <data key="d1">6486278392250669413</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20077-9_23</data>
  <data key="d3">Monocular 3d object detection with depth from motion</data>
  <data key="d4">T Wang, J Pang, D Lin</data>
  <data key="d5">2022</data>
  <data key="d6">33</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6486278392250669413&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11137887749868459671">
  <data key="d0">Beyond bounding-box: Convex-hull feature adaptation for oriented and densely packed object detection</data>
  <data key="d1">11137887749868459671</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Guo_Beyond_Bounding-Box_Convex-Hull_Feature_Adaptation_for_Oriented_and_Densely_Packed_CVPR_2021_paper.html</data>
  <data key="d3">Beyond bounding-box: Convex-hull feature adaptation for oriented and densely packed object detection</data>
  <data key="d4">Z Guo, C Liu, X Zhang, J Jiao, X Ji…</data>
  <data key="d5">2021</data>
  <data key="d6">82</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11137887749868459671&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="2275159817454841223">
  <data key="d0">Object discovery and representation networks</data>
  <data key="d1">2275159817454841223</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19812-0_8</data>
  <data key="d3">Object discovery and representation networks</data>
  <data key="d4">OJ Hénaff, S Koppula, E Shelhamer, D Zoran…</data>
  <data key="d5">2022</data>
  <data key="d6">39</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2275159817454841223&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="7436704720048829343">
  <data key="d0">Group fisher pruning for practical network compression</data>
  <data key="d1">7436704720048829343</data>
  <data key="d2">http://proceedings.mlr.press/v139/liu21ab.html?ref=https://githubhelp.com</data>
  <data key="d3">Group fisher pruning for practical network compression</data>
  <data key="d4">L Liu, S Zhang, Z Kuang, A Zhou…</data>
  <data key="d5">2021</data>
  <data key="d6">81</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7436704720048829343&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="10062072141692223467">
  <data key="d0">QueryDet: Cascaded sparse query for accelerating high-resolution small object detection</data>
  <data key="d1">10062072141692223467</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2022/html/Yang_QueryDet_Cascaded_Sparse_Query_for_Accelerating_High-Resolution_Small_Object_Detection_CVPR_2022_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">QueryDet: Cascaded sparse query for accelerating high-resolution small object detection</data>
  <data key="d4">C Yang, Z Huang, N Wang</data>
  <data key="d5">2022</data>
  <data key="d6">105</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10062072141692223467&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14766840776929327982">
  <data key="d0">Imvoxelnet: Image to voxels projection for monocular and multi-view general-purpose 3d object detection</data>
  <data key="d1">14766840776929327982</data>
  <data key="d2">http://openaccess.thecvf.com/content/WACV2022/html/Rukhovich_ImVoxelNet_Image_to_Voxels_Projection_for_Monocular_and_Multi-View_General-Purpose_WACV_2022_paper.html</data>
  <data key="d3">Imvoxelnet: Image to voxels projection for monocular and multi-view general-purpose 3d object detection</data>
  <data key="d4">D Rukhovich, A Vorontsova…</data>
  <data key="d5">2022</data>
  <data key="d6">95</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14766840776929327982&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="10951300284096917451">
  <data key="d0">An empirical study of remote sensing pretraining</data>
  <data key="d1">10951300284096917451</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9782149/</data>
  <data key="d3">An empirical study of remote sensing pretraining</data>
  <data key="d4">D Wang, J Zhang, B Du, GS Xia…</data>
  <data key="d5">2022</data>
  <data key="d6">65</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10951300284096917451&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="15129909426072124916">
  <data key="d0">Unbiased teacher v2: Semi-supervised object detection for anchor-free and anchor-based detectors</data>
  <data key="d1">15129909426072124916</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Liu_Unbiased_Teacher_v2_Semi-Supervised_Object_Detection_for_Anchor-Free_and_Anchor-Based_CVPR_2022_paper.html</data>
  <data key="d3">Unbiased teacher v2: Semi-supervised object detection for anchor-free and anchor-based detectors</data>
  <data key="d4">YC Liu, CY Ma, Z Kira</data>
  <data key="d5">2022</data>
  <data key="d6">47</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15129909426072124916&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="12867622177717661888">
  <data key="d0">Balance learning for ship detection from synthetic aperture radar remote sensing imagery</data>
  <data key="d1">12867622177717661888</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0924271621002781</data>
  <data key="d3">Balance learning for ship detection from synthetic aperture radar remote sensing imagery</data>
  <data key="d4">T Zhang, X Zhang, C Liu, J Shi, S Wei, I Ahmad…</data>
  <data key="d5">2021</data>
  <data key="d6">61</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12867622177717661888&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="625028681591269424">
  <data key="d0">Video self-stitching graph network for temporal action localization</data>
  <data key="d1">625028681591269424</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Zhao_Video_Self-Stitching_Graph_Network_for_Temporal_Action_Localization_ICCV_2021_paper.html</data>
  <data key="d3">Video self-stitching graph network for temporal action localization</data>
  <data key="d4">C Zhao, AK Thabet, B Ghanem</data>
  <data key="d5">2021</data>
  <data key="d6">90</data>
  <data key="d7">https://scholar.google.com/scholar?cites=625028681591269424&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="9394900835405963044">
  <data key="d0">Shape-adaptive selection and measurement for oriented object detection</data>
  <data key="d1">9394900835405963044</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/19975</data>
  <data key="d3">Shape-adaptive selection and measurement for oriented object detection</data>
  <data key="d4">L Hou, K Lu, J Xue, Y Li</data>
  <data key="d5">2022</data>
  <data key="d6">55</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9394900835405963044&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4089155253019708361">
  <data key="d0">Unsupervised learning of accurate siamese tracking</data>
  <data key="d1">4089155253019708361</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Shen_Unsupervised_Learning_of_Accurate_Siamese_Tracking_CVPR_2022_paper.html</data>
  <data key="d3">Unsupervised learning of accurate siamese tracking</data>
  <data key="d4">Q Shen, L Qiao, J Guo, P Li, X Li, B Li…</data>
  <data key="d5">2022</data>
  <data key="d6">34</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4089155253019708361&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="17297670040926232009">
  <data key="d0">Label matching semi-supervised object detection</data>
  <data key="d1">17297670040926232009</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Chen_Label_Matching_Semi-Supervised_Object_Detection_CVPR_2022_paper.html</data>
  <data key="d3">Label matching semi-supervised object detection</data>
  <data key="d4">B Chen, W Chen, S Yang, Y Xuan…</data>
  <data key="d5">2022</data>
  <data key="d6">34</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17297670040926232009&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14843700105251392523">
  <data key="d0">Reppoints v2: Verification meets regression for object detection</data>
  <data key="d1">14843700105251392523</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2020/hash/3ce3bd7d63a2c9c81983cc8e9bd02ae5-Abstract.html</data>
  <data key="d3">Reppoints v2: Verification meets regression for object detection</data>
  <data key="d4">Y Chen, Z Zhang, Y Cao, L Wang…</data>
  <data key="d5">2020</data>
  <data key="d6">101</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14843700105251392523&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="8225197904370189459">
  <data key="d0">Point-set anchors for object detection, instance segmentation and pose estimation</data>
  <data key="d1">8225197904370189459</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-58607-2_31</data>
  <data key="d3">Point-set anchors for object detection, instance segmentation and pose estimation</data>
  <data key="d4">F Wei, X Sun, H Li, J Wang, S Lin</data>
  <data key="d5">2020</data>
  <data key="d6">106</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8225197904370189459&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4527666642194648577">
  <data key="d0">Fcaf3d: Fully convolutional anchor-free 3d object detection</data>
  <data key="d1">4527666642194648577</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20080-9_28</data>
  <data key="d3">Fcaf3d: Fully convolutional anchor-free 3d object detection</data>
  <data key="d4">D Rukhovich, A Vorontsova, A Konushin</data>
  <data key="d5">2022</data>
  <data key="d6">52</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4527666642194648577&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11128139934147031880">
  <data key="d0">FPCB surface defect detection: A decoupled two-stage object detection framework</data>
  <data key="d1">11128139934147031880</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9465824/</data>
  <data key="d3">FPCB surface defect detection: A decoupled two-stage object detection framework</data>
  <data key="d4">J Luo, Z Yang, S Li, Y Wu</data>
  <data key="d5">2021</data>
  <data key="d6">63</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11128139934147031880&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="3406496252840731715">
  <data key="d0">SiamCorners: Siamese corner networks for visual tracking</data>
  <data key="d1">3406496252840731715</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9410380/</data>
  <data key="d3">SiamCorners: Siamese corner networks for visual tracking</data>
  <data key="d4">K Yang, Z He, W Pei, Z Zhou, X Li…</data>
  <data key="d5">2021</data>
  <data key="d6">68</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3406496252840731715&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="13568718297544497544">
  <data key="d0">Corner proposal network for anchor-free, two-stage object detection</data>
  <data key="d1">13568718297544497544</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-58580-8_24</data>
  <data key="d3">Corner proposal network for anchor-free, two-stage object detection</data>
  <data key="d4">K Duan, L Xie, H Qi, S Bai, Q Huang, Q Tian</data>
  <data key="d5">2020</data>
  <data key="d6">93</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13568718297544497544&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="12711419494084177503">
  <data key="d0">Open-vocabulary one-stage detection with hierarchical visual-language knowledge distillation</data>
  <data key="d1">12711419494084177503</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Ma_Open-Vocabulary_One-Stage_Detection_With_Hierarchical_Visual-Language_Knowledge_Distillation_CVPR_2022_paper.html</data>
  <data key="d3">Open-vocabulary one-stage detection with hierarchical visual-language knowledge distillation</data>
  <data key="d4">Z Ma, G Luo, J Gao, L Li, Y Chen…</data>
  <data key="d5">2022</data>
  <data key="d6">36</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12711419494084177503&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="9396440723658947289">
  <data key="d0">A survey of convolutional neural networks: analysis, applications, and prospects</data>
  <data key="d1">9396440723658947289</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9451544/</data>
  <data key="d3">A survey of convolutional neural networks: analysis, applications, and prospects</data>
  <data key="d4">Z Li, F Liu, W Yang, S Peng…</data>
  <data key="d5">2021</data>
  <data key="d6">1166</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9396440723658947289&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="17659240868782185606">
  <data key="d0">Federated vehicular transformers and their federations: Privacy-preserving computing and cooperation for autonomous driving</data>
  <data key="d1">17659240868782185606</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9857660/</data>
  <data key="d3">Federated vehicular transformers and their federations: Privacy-preserving computing and cooperation for autonomous driving</data>
  <data key="d4">Y Tian, J Wang, Y Wang, C Zhao…</data>
  <data key="d5">2022</data>
  <data key="d6">24</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17659240868782185606&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="1330128288162812959">
  <data key="d0">Application and progress of chemometrics in voltammetric biosensing</data>
  <data key="d1">1330128288162812959</data>
  <data key="d2">https://www.mdpi.com/2079-6374/12/7/494</data>
  <data key="d3">Application and progress of chemometrics in voltammetric biosensing</data>
  <data key="d4">J Liu, Y Xu, S Liu, S Yu, Z Yu, SS Low</data>
  <data key="d5">2022</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1330128288162812959&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="12461780382342897766">
  <data key="d0">Cyber security in smart cities: a review of deep learning-based applications and case studies</data>
  <data key="d1">12461780382342897766</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2210670720308714</data>
  <data key="d3">Cyber security in smart cities: a review of deep learning-based applications and case studies</data>
  <data key="d4">D Chen, P Wawrzynski, Z Lv</data>
  <data key="d5">2021</data>
  <data key="d6">167</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12461780382342897766&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="2894457065609999138">
  <data key="d0">Uni-perceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks</data>
  <data key="d1">2894457065609999138</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Uni-Perceiver_Pre-Training_Unified_Architecture_for_Generic_Perception_for_Zero-Shot_and_CVPR_2022_paper.html</data>
  <data key="d3">Uni-perceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks</data>
  <data key="d4">X Zhu, J Zhu, H Li, X Wu, H Li…</data>
  <data key="d5">2022</data>
  <data key="d6">61</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2894457065609999138&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="3367852488199289115">
  <data key="d0">Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence</data>
  <data key="d1">3367852488199289115</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1566253523001148</data>
  <data key="d3">Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence</data>
  <data key="d4">S Ali, T Abuhmed, S El-Sappagh, K Muhammad…</data>
  <data key="d5">2023</data>
  <data key="d6">24</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3367852488199289115&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="10959765252077951834">
  <data key="d0">Hardware and software optimizations for accelerating deep neural networks: Survey of current trends, challenges, and the road ahead</data>
  <data key="d1">10959765252077951834</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9269334/</data>
  <data key="d3">Hardware and software optimizations for accelerating deep neural networks: Survey of current trends, challenges, and the road ahead</data>
  <data key="d4">M Capra, B Bussolino, A Marchisio, G Masera…</data>
  <data key="d5">2020</data>
  <data key="d6">116</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10959765252077951834&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="6330853643103469907">
  <data key="d0">Chaotic harris hawks optimization with quasi-reflection-based learning: An application to enhance cnn design</data>
  <data key="d1">6330853643103469907</data>
  <data key="d2">https://www.mdpi.com/1424-8220/21/19/6654</data>
  <data key="d3">Chaotic harris hawks optimization with quasi-reflection-based learning: An application to enhance cnn design</data>
  <data key="d4">J Basha, N Bacanin, N Vukobrat, M Zivkovic…</data>
  <data key="d5">2021</data>
  <data key="d6">55</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6330853643103469907&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="11264330757232304805">
  <data key="d0">A novel deep convolution multi-adversarial domain adaptation model for rolling bearing fault diagnosis</data>
  <data key="d1">11264330757232304805</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0263224122000549</data>
  <data key="d3">A novel deep convolution multi-adversarial domain adaptation model for rolling bearing fault diagnosis</data>
  <data key="d4">L Wan, Y Li, K Chen, K Gong, C Li</data>
  <data key="d5">2022</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11264330757232304805&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="104652044397290954">
  <data key="d0">Deep CNN-based damage classification of milled rice grains using a high-magnification image dataset</data>
  <data key="d1">104652044397290954</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169922001284</data>
  <data key="d3">Deep CNN-based damage classification of milled rice grains using a high-magnification image dataset</data>
  <data key="d4">K Moses, A Miglani, PK Kankar</data>
  <data key="d5">2022</data>
  <data key="d6">34</data>
  <data key="d7">https://scholar.google.com/scholar?cites=104652044397290954&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="8405812116415915225">
  <data key="d0">Uni-perceiver-moe: Learning sparse generalist models with conditional moes</data>
  <data key="d1">8405812116415915225</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/11fc8c98b46d4cbdfe8157267228f7d7-Abstract-Conference.html</data>
  <data key="d3">Uni-perceiver-moe: Learning sparse generalist models with conditional moes</data>
  <data key="d4">J Zhu, X Zhu, W Wang, X Wang, H Li…</data>
  <data key="d5">2022</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8405812116415915225&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="11817815354857298574">
  <data key="d0">Multi-level feature fusion for multimodal human activity recognition in Internet of Healthcare Things</data>
  <data key="d1">11817815354857298574</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1566253523000246</data>
  <data key="d3">Multi-level feature fusion for multimodal human activity recognition in Internet of Healthcare Things</data>
  <data key="d4">MM Islam, S Nooruddin, F Karray, G Muhammad</data>
  <data key="d5">2023</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11817815354857298574&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="16381610666186844878">
  <data key="d0">A hybrid DNN–LSTM model for detecting phishing URLs</data>
  <data key="d1">16381610666186844878</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00521-021-06401-z</data>
  <data key="d3">A hybrid DNN–LSTM model for detecting phishing URLs</data>
  <data key="d4">A Ozcan, C Catal, E Donmez, B Senturk</data>
  <data key="d5">2021</data>
  <data key="d6">47</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16381610666186844878&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="3245855105634480603">
  <data key="d0">A computer-aided diagnostic framework for coronavirus diagnosis using texture-based radiomics images</data>
  <data key="d1">3245855105634480603</data>
  <data key="d2">https://journals.sagepub.com/doi/abs/10.1177/20552076221092543</data>
  <data key="d3">A computer-aided diagnostic framework for coronavirus diagnosis using texture-based radiomics images</data>
  <data key="d4">O Attallah</data>
  <data key="d5">2022</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3245855105634480603&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="3529628295378086556">
  <data key="d0">Deep CNN and deep GAN in computational visual perception-driven image analysis</data>
  <data key="d1">3529628295378086556</data>
  <data key="d2">https://www.hindawi.com/journals/complexity/2021/5541134/</data>
  <data key="d3">Deep CNN and deep GAN in computational visual perception-driven image analysis</data>
  <data key="d4">R Nandhini Abirami, PM Durai Raj Vincent…</data>
  <data key="d5">2021</data>
  <data key="d6">48</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3529628295378086556&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="7737148347777017891">
  <data key="d0">Reveal of vision transformers robustness against adversarial attacks</data>
  <data key="d1">7737148347777017891</data>
  <data key="d2">https://arxiv.org/abs/2106.03734</data>
  <data key="d3">Reveal of vision transformers robustness against adversarial attacks</data>
  <data key="d4">A Aldahdooh, W Hamidouche, O Deforges</data>
  <data key="d5">2021</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7737148347777017891&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="7585820726866106888">
  <data key="d0">Challenging the security of logic locking schemes in the era of deep learning: A neuroevolutionary approach</data>
  <data key="d1">7585820726866106888</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3431389</data>
  <data key="d3">Challenging the security of logic locking schemes in the era of deep learning: A neuroevolutionary approach</data>
  <data key="d4">D Sisejkovic, F Merchant, LM Reimann…</data>
  <data key="d5">2021</data>
  <data key="d6">53</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7585820726866106888&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="16752346668949596807">
  <data key="d0">Application of reinforcement learning and deep learning in multiple-input and multiple-output (MIMO) systems</data>
  <data key="d1">16752346668949596807</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/1/309</data>
  <data key="d3">Application of reinforcement learning and deep learning in multiple-input and multiple-output (MIMO) systems</data>
  <data key="d4">M Naeem, G De Pietro, A Coronato</data>
  <data key="d5">2021</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16752346668949596807&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="9766170352567939917">
  <data key="d0">Survey of deep learning paradigms for speech processing</data>
  <data key="d1">9766170352567939917</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11277-022-09640-y</data>
  <data key="d3">Survey of deep learning paradigms for speech processing</data>
  <data key="d4">KB Bhangale, M Kothandaraman</data>
  <data key="d5">2022</data>
  <data key="d6">36</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9766170352567939917&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="13409251986329658691">
  <data key="d0">EDNC: Ensemble deep neural network for COVID-19 recognition</data>
  <data key="d1">13409251986329658691</data>
  <data key="d2">https://www.mdpi.com/2379-139X/8/2/71</data>
  <data key="d3">EDNC: Ensemble deep neural network for COVID-19 recognition</data>
  <data key="d4">L Yang, SH Wang, YD Zhang</data>
  <data key="d5">2022</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13409251986329658691&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="473530282656916811">
  <data key="d0">A CSI-based human activity recognition using deep learning</data>
  <data key="d1">473530282656916811</data>
  <data key="d2">https://www.mdpi.com/1424-8220/21/21/7225</data>
  <data key="d3">A CSI-based human activity recognition using deep learning</data>
  <data key="d4">PF Moshiri, R Shahbazian, M Nabati, SA Ghorashi</data>
  <data key="d5">2021</data>
  <data key="d6">35</data>
  <data key="d7">https://scholar.google.com/scholar?cites=473530282656916811&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="1190843868981231273">
  <data key="d0">Deep learning approaches and interventions for futuristic engineering in agriculture</data>
  <data key="d1">1190843868981231273</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00521-022-07744-x</data>
  <data key="d3">Deep learning approaches and interventions for futuristic engineering in agriculture</data>
  <data key="d4">SK Chakraborty, NS Chandel, D Jat, MK Tiwari…</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1190843868981231273&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="11786070565691042647">
  <data key="d0">A novel information processing method based on an ensemble of Auto-Encoders for unsupervised fault detection</data>
  <data key="d1">11786070565691042647</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0166361522001403</data>
  <data key="d3">A novel information processing method based on an ensemble of Auto-Encoders for unsupervised fault detection</data>
  <data key="d4">S Plakias, YS Boutalis</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11786070565691042647&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="9568123547245923264">
  <data key="d0">A novel approach for APT attack detection based on combined deep learning model</data>
  <data key="d1">9568123547245923264</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00521-021-05952-5</data>
  <data key="d3">A novel approach for APT attack detection based on combined deep learning model</data>
  <data key="d4">C Do Xuan, MH Dao</data>
  <data key="d5">2021</data>
  <data key="d6">39</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9568123547245923264&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="9275353594998479018">
  <data key="d0">Large‐scale structural health monitoring using composite recurrent neural networks and grid environments</data>
  <data key="d1">9275353594998479018</data>
  <data key="d2">https://onlinelibrary.wiley.com/doi/abs/10.1111/mice.12845</data>
  <data key="d3">Large‐scale structural health monitoring using composite recurrent neural networks and grid environments</data>
  <data key="d4">KA Eltouny, X Liang</data>
  <data key="d5">2023</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9275353594998479018&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="674676834371939814">
  <data key="d0">Promoting smart tourism personalised services via a combination of deep learning techniques</data>
  <data key="d1">674676834371939814</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417421013154</data>
  <data key="d3">Promoting smart tourism personalised services via a combination of deep learning techniques</data>
  <data key="d4">A Kontogianni, E Alepis, C Patsakis</data>
  <data key="d5">2022</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=674676834371939814&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="16147182476848743143">
  <data key="d0">AI meets UAVs: A survey on AI empowered UAV perception systems for precision agriculture</data>
  <data key="d1">16147182476848743143</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0925231222013996</data>
  <data key="d3">AI meets UAVs: A survey on AI empowered UAV perception systems for precision agriculture</data>
  <data key="d4">J Su, X Zhu, S Li, WH Chen</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16147182476848743143&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="6604331675399620890">
  <data key="d0">A comprehensive survey of machine learning applied to radar signal processing</data>
  <data key="d1">6604331675399620890</data>
  <data key="d2">https://arxiv.org/abs/2009.13702</data>
  <data key="d3">A comprehensive survey of machine learning applied to radar signal processing</data>
  <data key="d4">P Lang, X Fu, M Martorella, J Dong, R Qin…</data>
  <data key="d5">2020</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6604331675399620890&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="17835922521314202374">
  <data key="d0">Convolutional neural networks: A survey</data>
  <data key="d1">17835922521314202374</data>
  <data key="d2">https://www.mdpi.com/2073-431X/12/8/151</data>
  <data key="d3">Convolutional neural networks: A survey</data>
  <data key="d4">M Krichen</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17835922521314202374&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="8491094163353345221">
  <data key="d0">A review of driver fatigue detection and its advances on the use of RGB-D camera and deep learning</data>
  <data key="d1">8491094163353345221</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0952197622003967</data>
  <data key="d3">A review of driver fatigue detection and its advances on the use of RGB-D camera and deep learning</data>
  <data key="d4">F Liu, D Chen, J Zhou, F Xu</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8491094163353345221&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="2776188056280590893">
  <data key="d0">Deep ensemble learning for human activity recognition using wearable sensors via filter activation</data>
  <data key="d1">2776188056280590893</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3551486</data>
  <data key="d3">Deep ensemble learning for human activity recognition using wearable sensors via filter activation</data>
  <data key="d4">W Huang, L Zhang, S Wang, H Wu…</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2776188056280590893&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="12148457517294144158">
  <data key="d0">DSMNN-Net: A deep siamese morphological neural network model for burned area mapping using multispectral sentinel-2 and hyperspectral PRISMA …</data>
  <data key="d1">12148457517294144158</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/24/5138</data>
  <data key="d3">DSMNN-Net: A deep siamese morphological neural network model for burned area mapping using multispectral sentinel-2 and hyperspectral PRISMA …</data>
  <data key="d4">ST Seydi, M Hasanlou, J Chanussot</data>
  <data key="d5">2021</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12148457517294144158&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="162944317775498564">
  <data key="d0">Encoding contextual information by interlacing transformer and convolution for remote sensing imagery semantic segmentation</data>
  <data key="d1">162944317775498564</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/16/4065</data>
  <data key="d3">Encoding contextual information by interlacing transformer and convolution for remote sensing imagery semantic segmentation</data>
  <data key="d4">X Li, F Xu, R Xia, T Li, Z Chen, X Wang, Z Xu, X Lyu</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=162944317775498564&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="7022412739527890738">
  <data key="d0">Review of machine learning based fault detection for centrifugal pump induction motors</data>
  <data key="d1">7022412739527890738</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9812600/</data>
  <data key="d3">Review of machine learning based fault detection for centrifugal pump induction motors</data>
  <data key="d4">CE Sunal, V Dyo, V Velisavljevic</data>
  <data key="d5">2022</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7022412739527890738&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="16729230970251043916">
  <data key="d0">An intelligent environment for preventing medication errors in home treatment</data>
  <data key="d1">16729230970251043916</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417421017218</data>
  <data key="d3">An intelligent environment for preventing medication errors in home treatment</data>
  <data key="d4">M Ciampi, A Coronato, M Naeem, S Silvestri</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16729230970251043916&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="7643365245113467622">
  <data key="d0">IEViT: An enhanced vision transformer architecture for chest X-ray image classification</data>
  <data key="d1">7643365245113467622</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0169260722005223</data>
  <data key="d3">IEViT: An enhanced vision transformer architecture for chest X-ray image classification</data>
  <data key="d4">GI Okolo, S Katsigiannis, N Ramzan</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7643365245113467622&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="11258068837507688644">
  <data key="d0">Intrusion detection systems: A state-of-the-art taxonomy and survey</data>
  <data key="d1">11258068837507688644</data>
  <data key="d2">https://link.springer.com/article/10.1007/s13369-022-07412-1</data>
  <data key="d3">Intrusion detection systems: A state-of-the-art taxonomy and survey</data>
  <data key="d4">M Alkasassbeh, S Al-Haj Baddar</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11258068837507688644&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="17820547851845778893">
  <data key="d0">Homo–heterogenous transformer learning framework for RS scene classification</data>
  <data key="d1">17820547851845778893</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9726930/</data>
  <data key="d3">Homo–heterogenous transformer learning framework for RS scene classification</data>
  <data key="d4">J Ma, M Li, X Tang, X Zhang, F Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17820547851845778893&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="13749289119625037185">
  <data key="d0">QTTNet: Quantized tensor train neural networks for 3D object and video recognition</data>
  <data key="d1">13749289119625037185</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0893608021002306</data>
  <data key="d3">QTTNet: Quantized tensor train neural networks for 3D object and video recognition</data>
  <data key="d4">D Lee, D Wang, Y Yang, L Deng, G Zhao, G Li</data>
  <data key="d5">2021</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13749289119625037185&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="7991192741706114520">
  <data key="d0">Hybrid-COVID: a novel hybrid 2D/3D CNN based on cross-domain adaptation approach for COVID-19 screening from chest X-ray images</data>
  <data key="d1">7991192741706114520</data>
  <data key="d2">https://link.springer.com/article/10.1007/s13246-020-00957-1</data>
  <data key="d3">Hybrid-COVID: a novel hybrid 2D/3D CNN based on cross-domain adaptation approach for COVID-19 screening from chest X-ray images</data>
  <data key="d4">K Bayoudh, F Hamdaoui, A Mtibaa</data>
  <data key="d5">2020</data>
  <data key="d6">33</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7991192741706114520&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="7647425146852033954">
  <data key="d0">Dig information of nanogenerators by machine learning</data>
  <data key="d1">7647425146852033954</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2211285523004937</data>
  <data key="d3">Dig information of nanogenerators by machine learning</data>
  <data key="d4">J Zhang, Y Yu, L Zhang, J Chen, X Wang, X Wang</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7647425146852033954&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="11663125968410326892">
  <data key="d0">CNN-CNN: Dual Convolutional Neural Network Approach for Feature Selection and Attack Detection on Internet of Things Networks</data>
  <data key="d1">11663125968410326892</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/14/6507</data>
  <data key="d3">CNN-CNN: Dual Convolutional Neural Network Approach for Feature Selection and Attack Detection on Internet of Things Networks</data>
  <data key="d4">BA Alabsi, M Anbar, SDA Rihan</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11663125968410326892&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="166903673990182366">
  <data key="d0">Combining deep learning and fluorescence imaging to automatically identify fecal contamination on meat carcasses</data>
  <data key="d1">166903673990182366</data>
  <data key="d2">https://www.nature.com/articles/s41598-022-06379-1</data>
  <data key="d3">Combining deep learning and fluorescence imaging to automatically identify fecal contamination on meat carcasses</data>
  <data key="d4">HT Gorji, SM Shahabi, A Sharma, LQ Tande…</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=166903673990182366&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="9882802714230086643">
  <data key="d0">An AI-empowered infrastructure for risk prevention during medical examination</data>
  <data key="d1">9882802714230086643</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S095741742300550X</data>
  <data key="d3">An AI-empowered infrastructure for risk prevention during medical examination</data>
  <data key="d4">SIH Shah, M Naeem, G Paragliola, A Coronato…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9882802714230086643&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="15888192171340754642">
  <data key="d0">A novel vibration-based prognostic scheme for gear health management in surface wear progression of the intelligent manufacturing system</data>
  <data key="d1">15888192171340754642</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0043164823000807</data>
  <data key="d3">A novel vibration-based prognostic scheme for gear health management in surface wear progression of the intelligent manufacturing system</data>
  <data key="d4">K Feng, JC Ji, Q Ni, Y Li, W Mao, L Liu</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15888192171340754642&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="12874330746450003457">
  <data key="d0">Semi-supervised vision transformer with adaptive token sampling for breast cancer classification</data>
  <data key="d1">12874330746450003457</data>
  <data key="d2">https://www.frontiersin.org/articles/10.3389/fphar.2022.929755/full</data>
  <data key="d3">Semi-supervised vision transformer with adaptive token sampling for breast cancer classification</data>
  <data key="d4">W Wang, R Jiang, N Cui, Q Li, F Yuan…</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12874330746450003457&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="11163389046080764508">
  <data key="d0">Computational chromatography: A machine learning strategy for demixing individual chemical components in complex mixtures</data>
  <data key="d1">11163389046080764508</data>
  <data key="d2">https://www.pnas.org/doi/abs/10.1073/pnas.2211406119</data>
  <data key="d3">Computational chromatography: A machine learning strategy for demixing individual chemical components in complex mixtures</data>
  <data key="d4">MM Bajomo, Y Ju, J Zhou…</data>
  <data key="d5">2022</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11163389046080764508&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="11069243657064968619">
  <data key="d0">Relation-attention networks for remote sensing scene classification</data>
  <data key="d1">11069243657064968619</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9652121/</data>
  <data key="d3">Relation-attention networks for remote sensing scene classification</data>
  <data key="d4">X Wang, L Duan, C Ning, H Zhou</data>
  <data key="d5">2021</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11069243657064968619&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="8285293036267904321">
  <data key="d0">Deep mutual attention network for acoustic scene classification</data>
  <data key="d1">8285293036267904321</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1051200422000677</data>
  <data key="d3">Deep mutual attention network for acoustic scene classification</data>
  <data key="d4">W Xie, Q He, Z Yu, Y Li</data>
  <data key="d5">2022</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8285293036267904321&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="10942600018958755822">
  <data key="d0">Recommender systems based on graph embedding techniques: A review</data>
  <data key="d1">10942600018958755822</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9772660/</data>
  <data key="d3">Recommender systems based on graph embedding techniques: A review</data>
  <data key="d4">Y Deng</data>
  <data key="d5">2022</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10942600018958755822&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="14522233362639662098">
  <data key="d0">An evolutionary explainable deep learning approach for Alzheimer's MRI classification</data>
  <data key="d1">14522233362639662098</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417423002105</data>
  <data key="d3">An evolutionary explainable deep learning approach for Alzheimer's MRI classification</data>
  <data key="d4">S Shojaei, MS Abadeh, Z Momeni</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14522233362639662098&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="5432221077116828342">
  <data key="d0">Dualformer: Local-global stratified transformer for efficient video recognition</data>
  <data key="d1">5432221077116828342</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19830-4_33</data>
  <data key="d3">Dualformer: Local-global stratified transformer for efficient video recognition</data>
  <data key="d4">Y Liang, P Zhou, R Zimmermann, S Yan</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5432221077116828342&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="12109569770609306860">
  <data key="d0">A bayesian framework for integrated deep metric learning and tracking of vulnerable road users using automotive radars</data>
  <data key="d1">12109569770609306860</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9423952/</data>
  <data key="d3">A bayesian framework for integrated deep metric learning and tracking of vulnerable road users using automotive radars</data>
  <data key="d4">A Dubey, A Santra, J Fuchs, M Lübke, R Weigel…</data>
  <data key="d5">2021</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12109569770609306860&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="9800786600952641254">
  <data key="d0">LMFFNet: a well-balanced lightweight network for fast and accurate semantic segmentation</data>
  <data key="d1">9800786600952641254</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9783460/</data>
  <data key="d3">LMFFNet: a well-balanced lightweight network for fast and accurate semantic segmentation</data>
  <data key="d4">M Shi, J Shen, Q Yi, J Weng, Z Huang…</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9800786600952641254&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="3851853212057137221">
  <data key="d0">Automatic segmentation of optic disc in retinal fundus images using semi-supervised deep learning</data>
  <data key="d1">3851853212057137221</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11042-020-09778-6</data>
  <data key="d3">Automatic segmentation of optic disc in retinal fundus images using semi-supervised deep learning</data>
  <data key="d4">S Bengani</data>
  <data key="d5">2021</data>
  <data key="d6">27</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3851853212057137221&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="5116550664858304460">
  <data key="d0">Enhancing ensemble diversity based on multiscale dilated convolution in image classification</data>
  <data key="d1">5116550664858304460</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0020025522004960</data>
  <data key="d3">Enhancing ensemble diversity based on multiscale dilated convolution in image classification</data>
  <data key="d4">GR You, YR Shiue, CT Su, QL Huang</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5116550664858304460&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="6524394779160024104">
  <data key="d0">Unsupervised deep multitask anomaly detection with robust alarm strategy for online evaluation of bearing early fault occurrence</data>
  <data key="d1">6524394779160024104</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9862997/</data>
  <data key="d3">Unsupervised deep multitask anomaly detection with robust alarm strategy for online evaluation of bearing early fault occurrence</data>
  <data key="d4">W Mao, H Shi, G Wang, X Liang</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6524394779160024104&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="9586487638855151494">
  <data key="d0">A Deep Learning Approach to Organic Pollutants Classification Using Voltammetry</data>
  <data key="d1">9586487638855151494</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/20/8032</data>
  <data key="d3">A Deep Learning Approach to Organic Pollutants Classification Using Voltammetry</data>
  <data key="d4">M Molinara, R Cancelliere, A Di Tinno, L Ferrigno…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9586487638855151494&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="409384955469315628">
  <data key="d0">Artificial intelligence methodologies for data management</data>
  <data key="d1">409384955469315628</data>
  <data key="d2">https://www.mdpi.com/2073-8994/13/11/2040</data>
  <data key="d3">Artificial intelligence methodologies for data management</data>
  <data key="d4">J Serey, L Quezada, M Alfaro, G Fuertes, M Vargas…</data>
  <data key="d5">2021</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=409384955469315628&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="9288583655021028647">
  <data key="d0">Learning in RKHM: a C*-Algebraic Twist for Kernel Machines</data>
  <data key="d1">9288583655021028647</data>
  <data key="d2">https://proceedings.mlr.press/v206/hashimoto23a.html</data>
  <data key="d3">Learning in RKHM: a C*-Algebraic Twist for Kernel Machines</data>
  <data key="d4">Y Hashimoto, M Ikeda, H Kadri</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9288583655021028647&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="15152142901961614455">
  <data key="d0">Continuous estimation of power system inertia using convolutional neural networks</data>
  <data key="d1">15152142901961614455</data>
  <data key="d2">https://www.nature.com/articles/s41467-023-40192-2</data>
  <data key="d3">Continuous estimation of power system inertia using convolutional neural networks</data>
  <data key="d4">D Linaro, F Bizzarri, D Del Giudice, C Pisani…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15152142901961614455&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="17659066342463746102">
  <data key="d0">Engineering semantic communication: A survey</data>
  <data key="d1">17659066342463746102</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10038657/</data>
  <data key="d3">Engineering semantic communication: A survey</data>
  <data key="d4">D Wheeler, B Natarajan</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17659066342463746102&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="3079241190308487697">
  <data key="d0">Applying a deep residual network coupling with transfer learning for recyclable waste sorting</data>
  <data key="d1">3079241190308487697</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11356-022-22167-w</data>
  <data key="d3">Applying a deep residual network coupling with transfer learning for recyclable waste sorting</data>
  <data key="d4">K Lin, Y Zhao, X Gao, M Zhang, C Zhao, L Peng…</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3079241190308487697&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="5544945261975848615">
  <data key="d0">A deep learning-based intelligent receiver for improving the reliability of the MIMO wireless communication system</data>
  <data key="d1">5544945261975848615</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9733027/</data>
  <data key="d3">A deep learning-based intelligent receiver for improving the reliability of the MIMO wireless communication system</data>
  <data key="d4">B Wang, K Xu, S Zheng, H Zhou…</data>
  <data key="d5">2022</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5544945261975848615&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="8932281142246040268">
  <data key="d0">A deep convolutional neural network-based multi-class image classification for automatic wafer map failure recognition in semiconductor manufacturing</data>
  <data key="d1">8932281142246040268</data>
  <data key="d2">https://www.mdpi.com/2076-3417/11/20/9769</data>
  <data key="d3">A deep convolutional neural network-based multi-class image classification for automatic wafer map failure recognition in semiconductor manufacturing</data>
  <data key="d4">H Zheng, SWA Sherazi, SH Son, JY Lee</data>
  <data key="d5">2021</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8932281142246040268&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="4804878180815758179">
  <data key="d0">DANTD: A deep abnormal network traffic detection model for security of industrial internet of things using high-order features</data>
  <data key="d1">4804878180815758179</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10061601/</data>
  <data key="d3">DANTD: A deep abnormal network traffic detection model for security of industrial internet of things using high-order features</data>
  <data key="d4">G Shi, X Shen, F Xiao, Y He</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4804878180815758179&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="12494571714440080437">
  <data key="d0">Combination of political optimizer, particle swarm optimizer, and convolutional neural network for brain tumor detection</data>
  <data key="d1">12494571714440080437</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1746809422008886</data>
  <data key="d3">Combination of political optimizer, particle swarm optimizer, and convolutional neural network for brain tumor detection</data>
  <data key="d4">AH Bashkandi, K Sadoughi, F Aflaki…</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12494571714440080437&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="12934231027519550777">
  <data key="d0">PPNNP: A privacy-preserving neural network prediction with separated data providers using multi-client inner-product encryption</data>
  <data key="d1">12934231027519550777</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0920548922000460</data>
  <data key="d3">PPNNP: A privacy-preserving neural network prediction with separated data providers using multi-client inner-product encryption</data>
  <data key="d4">M Zhang, S Huang, G Shen, Y Wang</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12934231027519550777&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="4286407019373546306">
  <data key="d0">Automatic generation of artificial images of leukocytes and leukemic cells using generative adversarial networks (syntheticcellgan)</data>
  <data key="d1">4286407019373546306</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0169260722006952</data>
  <data key="d3">Automatic generation of artificial images of leukocytes and leukemic cells using generative adversarial networks (syntheticcellgan)</data>
  <data key="d4">K Barrera, A Merino, A Molina, J Rodellar</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4286407019373546306&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="12934860564933811755">
  <data key="d0">Convolutional neural networks with transfer learning for recognition of COVID-19: a comparative study of different approaches</data>
  <data key="d1">12934860564933811755</data>
  <data key="d2">https://www.mdpi.com/2673-2688/1/4/34</data>
  <data key="d3">Convolutional neural networks with transfer learning for recognition of COVID-19: a comparative study of different approaches</data>
  <data key="d4">T Garg, M Garg, OP Mahela, AR Garg</data>
  <data key="d5">2020</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12934860564933811755&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="6471952400524995967">
  <data key="d0">Deep learning for visual speech analysis: A survey</data>
  <data key="d1">6471952400524995967</data>
  <data key="d2">https://arxiv.org/abs/2205.10839</data>
  <data key="d3">Deep learning for visual speech analysis: A survey</data>
  <data key="d4">C Sheng, G Kuang, L Bai, C Hou, Y Guo, X Xu…</data>
  <data key="d5">2022</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6471952400524995967&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="2936922659962418453">
  <data key="d0">Graph convolutional networks in language and vision: A survey</data>
  <data key="d1">2936922659962418453</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0950705122006220</data>
  <data key="d3">Graph convolutional networks in language and vision: A survey</data>
  <data key="d4">H Ren, W Lu, Y Xiao, X Chang, X Wang, Z Dong…</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2936922659962418453&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="5343150201557663140">
  <data key="d0">Theoretical understanding of convolutional neural network: concepts, architectures, applications, future directions</data>
  <data key="d1">5343150201557663140</data>
  <data key="d2">https://www.mdpi.com/2079-3197/11/3/52</data>
  <data key="d3">Theoretical understanding of convolutional neural network: concepts, architectures, applications, future directions</data>
  <data key="d4">MM Taye</data>
  <data key="d5">2023</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5343150201557663140&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="288293461476145835">
  <data key="d0">BreathMentor: Acoustic-based Diaphragmatic Breathing Monitor System</data>
  <data key="d1">288293461476145835</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3534595</data>
  <data key="d3">BreathMentor: Acoustic-based Diaphragmatic Breathing Monitor System</data>
  <data key="d4">Y Gong, Q Zhang, BHP NG, W Li</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=288293461476145835&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="1946486029826048685">
  <data key="d0">Black-box dataset ownership verification via backdoor watermarking</data>
  <data key="d1">1946486029826048685</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10097580/</data>
  <data key="d3">Black-box dataset ownership verification via backdoor watermarking</data>
  <data key="d4">Y Li, M Zhu, X Yang, Y Jiang, T Wei…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1946486029826048685&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="13147547687598130899">
  <data key="d0">Designing optimal convolutional neural network architecture using differential evolution algorithm</data>
  <data key="d1">13147547687598130899</data>
  <data key="d2">https://www.cell.com/patterns/pdf/S2666-3899(22)00178-7.pdf</data>
  <data key="d3">Designing optimal convolutional neural network architecture using differential evolution algorithm</data>
  <data key="d4">A Ghosh, ND Jana, S Mallik, Z Zhao</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13147547687598130899&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="1188818162646880118">
  <data key="d0">Enhanced Arabic Sentiment Analysis Using a Novel Stacking Ensemble of Hybrid and Deep Learning Models</data>
  <data key="d1">1188818162646880118</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/18/8967</data>
  <data key="d3">Enhanced Arabic Sentiment Analysis Using a Novel Stacking Ensemble of Hybrid and Deep Learning Models</data>
  <data key="d4">H Saleh, S Mostafa, LA Gabralla, A O. Aseeri…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1188818162646880118&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="8069973205387436662">
  <data key="d0">Real-time deep anomaly detection framework for multivariate time-series data in industrial iot</data>
  <data key="d1">8069973205387436662</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9915308/</data>
  <data key="d3">Real-time deep anomaly detection framework for multivariate time-series data in industrial iot</data>
  <data key="d4">H Nizam, S Zafar, Z Lv, F Wang, X Hu</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8069973205387436662&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="7181641912111691081">
  <data key="d0">Machine Learning for Millimeter Wave and Terahertz Beam Management: A Survey and Open Challenges</data>
  <data key="d1">7181641912111691081</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10036372/</data>
  <data key="d3">Machine Learning for Millimeter Wave and Terahertz Beam Management: A Survey and Open Challenges</data>
  <data key="d4">MQ Khan, A Gaber, P Schulz, G Fettweis</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7181641912111691081&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="18387253888735733879">
  <data key="d0">A mathematical framework for improved weight initialization of neural networks using Lagrange multipliers</data>
  <data key="d1">18387253888735733879</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0893608023003921</data>
  <data key="d3">A mathematical framework for improved weight initialization of neural networks using Lagrange multipliers</data>
  <data key="d4">I de Pater, M Mitici</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18387253888735733879&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="14318958312133554657">
  <data key="d0">A multimodal hyper-fusion transformer for remote sensing image classification</data>
  <data key="d1">14318958312133554657</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1566253523000866</data>
  <data key="d3">A multimodal hyper-fusion transformer for remote sensing image classification</data>
  <data key="d4">M Ma, W Ma, L Jiao, X Liu, L Li, Z Feng, S Yang</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14318958312133554657&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="6617971465675626792">
  <data key="d0">A continuous convolutional trainable filter for modelling unstructured data</data>
  <data key="d1">6617971465675626792</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00466-023-02291-1</data>
  <data key="d3">A continuous convolutional trainable filter for modelling unstructured data</data>
  <data key="d4">D Coscia, L Meneghetti, N Demo, G Stabile…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6617971465675626792&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="16908822867886701327">
  <data key="d0">An intelligent anti-jamming scheme for cognitive radio based on deep reinforcement learning</data>
  <data key="d1">16908822867886701327</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9249051/</data>
  <data key="d3">An intelligent anti-jamming scheme for cognitive radio based on deep reinforcement learning</data>
  <data key="d4">J Xu, H Lou, W Zhang, G Sang</data>
  <data key="d5">2020</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16908822867886701327&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="3307690701392927370">
  <data key="d0">Weedgan: a novel generative adversarial network for cotton weed identification</data>
  <data key="d1">3307690701392927370</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00371-022-02742-5</data>
  <data key="d3">Weedgan: a novel generative adversarial network for cotton weed identification</data>
  <data key="d4">V Sharma, AK Tripathi, H Mittal, A Parmar, A Soni…</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3307690701392927370&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="11526591086958196796">
  <data key="d0">CNN-Pred: Prediction of single-stranded and double-stranded DNA-binding protein using convolutional neural networks</data>
  <data key="d1">11526591086958196796</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0378111922008654</data>
  <data key="d3">CNN-Pred: Prediction of single-stranded and double-stranded DNA-binding protein using convolutional neural networks</data>
  <data key="d4">F Manavi, A Sharma, R Sharma, T Tsunoda…</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11526591086958196796&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="12934682784061910585">
  <data key="d0">A BERT encoding with Recurrent Neural Network and Long-Short Term Memory for breast cancer image classification</data>
  <data key="d1">12934682784061910585</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2772662223000176</data>
  <data key="d3">A BERT encoding with Recurrent Neural Network and Long-Short Term Memory for breast cancer image classification</data>
  <data key="d4">S Chaudhury, K Sau</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12934682784061910585&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="7854799250999961474">
  <data key="d0">The Geometry of Feature Space in Deep Learning Models: A Holistic Perspective and Comprehensive Review</data>
  <data key="d1">7854799250999961474</data>
  <data key="d2">https://www.mdpi.com/2227-7390/11/10/2375</data>
  <data key="d3">The Geometry of Feature Space in Deep Learning Models: A Holistic Perspective and Comprehensive Review</data>
  <data key="d4">M Lee</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7854799250999961474&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="5383924793672854664">
  <data key="d0">Polarimetric Imaging via Deep Learning: A Review</data>
  <data key="d1">5383924793672854664</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/6/1540</data>
  <data key="d3">Polarimetric Imaging via Deep Learning: A Review</data>
  <data key="d4">X Li, L Yan, P Qi, L Zhang, F Goudail, T Liu, J Zhai…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5383924793672854664&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="10139905476651083212">
  <data key="d0">Lightweight deep CNN-based models for early detection of COVID-19 patients from chest X-ray images</data>
  <data key="d1">10139905476651083212</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417423004013</data>
  <data key="d3">Lightweight deep CNN-based models for early detection of COVID-19 patients from chest X-ray images</data>
  <data key="d4">HI Hussein, AO Mohammed, MM Hassan…</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10139905476651083212&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="7149880764356096625">
  <data key="d0">Self-supervised bi-channel transformer networks for computer-aided diagnosis</data>
  <data key="d1">7149880764356096625</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9721019/</data>
  <data key="d3">Self-supervised bi-channel transformer networks for computer-aided diagnosis</data>
  <data key="d4">R Gong, X Han, J Wang, S Ying…</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7149880764356096625&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="17663852253493236753">
  <data key="d0">Machine learning algorithms to detect subclinical keratoconus: systematic review</data>
  <data key="d1">17663852253493236753</data>
  <data key="d2">https://medinform.jmir.org/2021/12/e27363</data>
  <data key="d3">Machine learning algorithms to detect subclinical keratoconus: systematic review</data>
  <data key="d4">H Maile, JPO Li, D Gore, M Leucci…</data>
  <data key="d5">2021</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17663852253493236753&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="12644740604491229422">
  <data key="d0">Generalized intersection over union: A metric and a loss for bounding box regression</data>
  <data key="d1">12644740604491229422</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2019/html/Rezatofighi_Generalized_Intersection_Over_Union_A_Metric_and_a_Loss_for_CVPR_2019_paper.html</data>
  <data key="d3">Generalized intersection over union: A metric and a loss for bounding box regression</data>
  <data key="d4">H Rezatofighi, N Tsoi, JY Gwak…</data>
  <data key="d5">2019</data>
  <data key="d6">3587</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12644740604491229422&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="13495361512235022596">
  <data key="d0">CSPNet: A new backbone that can enhance learning capability of CNN</data>
  <data key="d1">13495361512235022596</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPRW_2020/html/w28/Wang_CSPNet_A_New_Backbone_That_Can_Enhance_Learning_Capability_of_CVPRW_2020_paper.html</data>
  <data key="d3">CSPNet: A new backbone that can enhance learning capability of CNN</data>
  <data key="d4">CY Wang, HYM Liao, YH Wu…</data>
  <data key="d5">2020</data>
  <data key="d6">2882</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13495361512235022596&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="15866259573388647937">
  <data key="d0">Motr: End-to-end multiple-object tracking with transformer</data>
  <data key="d1">15866259573388647937</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19812-0_38</data>
  <data key="d3">Motr: End-to-end multiple-object tracking with transformer</data>
  <data key="d4">F Zeng, B Dong, Y Zhang, T Wang, X Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">238</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15866259573388647937&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="5167657309745527366">
  <data key="d0">Swintrack: A simple and strong baseline for transformer tracking</data>
  <data key="d1">5167657309745527366</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/6a5c23219f401f3efd322579002dbb80-Abstract-Conference.html</data>
  <data key="d3">Swintrack: A simple and strong baseline for transformer tracking</data>
  <data key="d4">L Lin, H Fan, Z Zhang, Y Xu…</data>
  <data key="d5">2022</data>
  <data key="d6">123</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5167657309745527366&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4582080155258437560">
  <data key="d0">Multimodal virtual point 3d detection</data>
  <data key="d1">4582080155258437560</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/895daa408f494ad58006c47a30f51c1f-Abstract.html</data>
  <data key="d3">Multimodal virtual point 3d detection</data>
  <data key="d4">T Yin, X Zhou, P Krähenbühl</data>
  <data key="d5">2021</data>
  <data key="d6">110</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4582080155258437560&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="17229720160752682638">
  <data key="d0">Mask dino: Towards a unified transformer-based framework for object detection and segmentation</data>
  <data key="d1">17229720160752682638</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Li_Mask_DINO_Towards_a_Unified_Transformer-Based_Framework_for_Object_Detection_CVPR_2023_paper.html</data>
  <data key="d3">Mask dino: Towards a unified transformer-based framework for object detection and segmentation</data>
  <data key="d4">F Li, H Zhang, H Xu, S Liu, L Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">102</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17229720160752682638&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11676336857650567467">
  <data key="d0">Transvg: End-to-end visual grounding with transformers</data>
  <data key="d1">11676336857650567467</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Deng_TransVG_End-to-End_Visual_Grounding_With_Transformers_ICCV_2021_paper.html</data>
  <data key="d3">Transvg: End-to-end visual grounding with transformers</data>
  <data key="d4">J Deng, Z Yang, T Chen, W Zhou…</data>
  <data key="d5">2021</data>
  <data key="d6">167</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11676336857650567467&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="2733828785339010244">
  <data key="d0">Objects are different: Flexible monocular 3d object detection</data>
  <data key="d1">2733828785339010244</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Zhang_Objects_Are_Different_Flexible_Monocular_3D_Object_Detection_CVPR_2021_paper.html</data>
  <data key="d3">Objects are different: Flexible monocular 3d object detection</data>
  <data key="d4">Y Zhang, J Lu, J Zhou</data>
  <data key="d5">2021</data>
  <data key="d6">152</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2733828785339010244&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="6724748843400977919">
  <data key="d0">Aiatrack: Attention in attention for transformer visual tracking</data>
  <data key="d1">6724748843400977919</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20047-2_9</data>
  <data key="d3">Aiatrack: Attention in attention for transformer visual tracking</data>
  <data key="d4">S Gao, C Zhou, C Ma, X Wang, J Yuan</data>
  <data key="d5">2022</data>
  <data key="d6">70</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6724748843400977919&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="16852040746567843557">
  <data key="d0">A comprehensive survey of loss functions in machine learning</data>
  <data key="d1">16852040746567843557</data>
  <data key="d2">https://link.springer.com/article/10.1007/s40745-020-00253-5</data>
  <data key="d3">A comprehensive survey of loss functions in machine learning</data>
  <data key="d4">Q Wang, Y Ma, K Zhao, Y Tian</data>
  <data key="d5">2020</data>
  <data key="d6">305</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16852040746567843557&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="9897780157833606863">
  <data key="d0">Tubetk: Adopting tubes to track multi-object in a one-step training model</data>
  <data key="d1">9897780157833606863</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Pang_TubeTK_Adopting_Tubes_to_Track_Multi-Object_in_a_One-Step_Training_CVPR_2020_paper.html</data>
  <data key="d3">Tubetk: Adopting tubes to track multi-object in a one-step training model</data>
  <data key="d4">B Pang, Y Li, Y Zhang, M Li…</data>
  <data key="d5">2020</data>
  <data key="d6">241</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9897780157833606863&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="1516895187053369438">
  <data key="d0">Joint feature learning and relation modeling for tracking: A one-stream framework</data>
  <data key="d1">1516895187053369438</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20047-2_20</data>
  <data key="d3">Joint feature learning and relation modeling for tracking: A one-stream framework</data>
  <data key="d4">B Ye, H Chang, B Ma, S Shan, X Chen</data>
  <data key="d5">2022</data>
  <data key="d6">101</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1516895187053369438&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="5255251082771415776">
  <data key="d0">End-to-end human object interaction detection with hoi transformer</data>
  <data key="d1">5255251082771415776</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Zou_End-to-End_Human_Object_Interaction_Detection_With_HOI_Transformer_CVPR_2021_paper.html</data>
  <data key="d3">End-to-end human object interaction detection with hoi transformer</data>
  <data key="d4">C Zou, B Wang, Y Hu, J Liu, Q Wu…</data>
  <data key="d5">2021</data>
  <data key="d6">148</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5255251082771415776&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="8435222954649090273">
  <data key="d0">An improved YOLOv5 model based on visual attention mechanism: Application to recognition of tomato virus disease</data>
  <data key="d1">8435222954649090273</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169922000977</data>
  <data key="d3">An improved YOLOv5 model based on visual attention mechanism: Application to recognition of tomato virus disease</data>
  <data key="d4">J Qi, X Liu, K Liu, F Xu, H Guo, X Tian, M Li…</data>
  <data key="d5">2022</data>
  <data key="d6">105</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8435222954649090273&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="2447420766194254891">
  <data key="d0">Delving into localization errors for monocular 3d object detection</data>
  <data key="d1">2447420766194254891</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Ma_Delving_Into_Localization_Errors_for_Monocular_3D_Object_Detection_CVPR_2021_paper.html</data>
  <data key="d3">Delving into localization errors for monocular 3d object detection</data>
  <data key="d4">X Ma, Y Zhang, D Xu, D Zhou, S Yi…</data>
  <data key="d5">2021</data>
  <data key="d6">126</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2447420766194254891&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="12087685277533101192">
  <data key="d0">Iou loss for 2d/3d object detection</data>
  <data key="d1">12087685277533101192</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/8886046/</data>
  <data key="d3">Iou loss for 2d/3d object detection</data>
  <data key="d4">D Zhou, J Fang, X Song, C Guan, J Yin…</data>
  <data key="d5">2019</data>
  <data key="d6">311</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12087685277533101192&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="1340805047015841402">
  <data key="d0">An enhanced CNN-enabled learning method for promoting ship detection in maritime surveillance system</data>
  <data key="d1">1340805047015841402</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0029801821008404</data>
  <data key="d3">An enhanced CNN-enabled learning method for promoting ship detection in maritime surveillance system</data>
  <data key="d4">RW Liu, W Yuan, X Chen, Y Lu</data>
  <data key="d5">2021</data>
  <data key="d6">124</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1340805047015841402&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="8119995839638175849">
  <data key="d0">Multi-grained vision language pre-training: Aligning texts with visual concepts</data>
  <data key="d1">8119995839638175849</data>
  <data key="d2">https://arxiv.org/abs/2111.08276</data>
  <data key="d3">Multi-grained vision language pre-training: Aligning texts with visual concepts</data>
  <data key="d4">Y Zeng, X Zhang, H Li</data>
  <data key="d5">2021</data>
  <data key="d6">127</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8119995839638175849&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="15843316366415866499">
  <data key="d0">Sg-net: Spatial granularity network for one-stage video instance segmentation</data>
  <data key="d1">15843316366415866499</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Liu_SG-Net_Spatial_Granularity_Network_for_One-Stage_Video_Instance_Segmentation_CVPR_2021_paper.html</data>
  <data key="d3">Sg-net: Spatial granularity network for one-stage video instance segmentation</data>
  <data key="d4">D Liu, Y Cui, W Tan, Y Chen</data>
  <data key="d5">2021</data>
  <data key="d6">124</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15843316366415866499&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="9376665948461610119">
  <data key="d0">End-to-end dense video captioning with parallel decoding</data>
  <data key="d1">9376665948461610119</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Wang_End-to-End_Dense_Video_Captioning_With_Parallel_Decoding_ICCV_2021_paper.html</data>
  <data key="d3">End-to-end dense video captioning with parallel decoding</data>
  <data key="d4">T Wang, R Zhang, Z Lu, F Zheng…</data>
  <data key="d5">2021</data>
  <data key="d6">88</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9376665948461610119&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="8315666909540964380">
  <data key="d0">Toward transformer-based object detection</data>
  <data key="d1">8315666909540964380</data>
  <data key="d2">https://arxiv.org/abs/2012.09958</data>
  <data key="d3">Toward transformer-based object detection</data>
  <data key="d4">J Beal, E Kim, E Tzeng, DH Park, A Zhai…</data>
  <data key="d5">2020</data>
  <data key="d6">163</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8315666909540964380&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="10407924981028323116">
  <data key="d0">Contournet: Taking a further step toward accurate arbitrary-shaped scene text detection</data>
  <data key="d1">10407924981028323116</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Wang_ContourNet_Taking_a_Further_Step_Toward_Accurate_Arbitrary-Shaped_Scene_Text_CVPR_2020_paper.html</data>
  <data key="d3">Contournet: Taking a further step toward accurate arbitrary-shaped scene text detection</data>
  <data key="d4">Y Wang, H Xie, ZJ Zha, M Xing…</data>
  <data key="d5">2020</data>
  <data key="d6">189</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10407924981028323116&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14661055790907170296">
  <data key="d0">Face mask recognition system with YOLOV5 based on image recognition</data>
  <data key="d1">14661055790907170296</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9345042/</data>
  <data key="d3">Face mask recognition system with YOLOV5 based on image recognition</data>
  <data key="d4">G Yang, W Feng, J Jin, Q Lei, X Li…</data>
  <data key="d5">2020</data>
  <data key="d6">179</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14661055790907170296&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11874667315223245954">
  <data key="d0">Alpha-refine: Boosting tracking performance by precise bounding box estimation</data>
  <data key="d1">11874667315223245954</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Yan_Alpha-Refine_Boosting_Tracking_Performance_by_Precise_Bounding_Box_Estimation_CVPR_2021_paper.html</data>
  <data key="d3">Alpha-refine: Boosting tracking performance by precise bounding box estimation</data>
  <data key="d4">B Yan, X Zhang, D Wang, H Lu…</data>
  <data key="d5">2021</data>
  <data key="d6">153</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11874667315223245954&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="12353159982052869953">
  <data key="d0">Piou loss: Towards accurate oriented object detection in complex environments</data>
  <data key="d1">12353159982052869953</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-58558-7_12</data>
  <data key="d3">Piou loss: Towards accurate oriented object detection in complex environments</data>
  <data key="d4">Z Chen, K Chen, W Lin, J See, H Yu, Y Ke…</data>
  <data key="d5">2020</data>
  <data key="d6">172</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12353159982052869953&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="2040091967364618062">
  <data key="d0">Qpic: Query-based pairwise human-object interaction detection with image-wide contextual information</data>
  <data key="d1">2040091967364618062</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Tamura_QPIC_Query-Based_Pairwise_Human-Object_Interaction_Detection_With_Image-Wide_Contextual_Information_CVPR_2021_paper.html</data>
  <data key="d3">Qpic: Query-based pairwise human-object interaction detection with image-wide contextual information</data>
  <data key="d4">M Tamura, H Ohashi…</data>
  <data key="d5">2021</data>
  <data key="d6">118</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2040091967364618062&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="10434862692373421904">
  <data key="d0">Tubedetr: Spatio-temporal video grounding with transformers</data>
  <data key="d1">10434862692373421904</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Yang_TubeDETR_Spatio-Temporal_Video_Grounding_With_Transformers_CVPR_2022_paper.html</data>
  <data key="d3">Tubedetr: Spatio-temporal video grounding with transformers</data>
  <data key="d4">A Yang, A Miech, J Sivic, I Laptev…</data>
  <data key="d5">2022</data>
  <data key="d6">45</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10434862692373421904&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="8448943115025253905">
  <data key="d0">Dense contrastive learning for self-supervised visual pre-training</data>
  <data key="d1">8448943115025253905</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Wang_Dense_Contrastive_Learning_for_Self-Supervised_Visual_Pre-Training_CVPR_2021_paper.html</data>
  <data key="d3">Dense contrastive learning for self-supervised visual pre-training</data>
  <data key="d4">X Wang, R Zhang, C Shen…</data>
  <data key="d5">2021</data>
  <data key="d6">513</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8448943115025253905&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="6668235945473015803">
  <data key="d0">ibot: Image bert pre-training with online tokenizer</data>
  <data key="d1">6668235945473015803</data>
  <data key="d2">https://arxiv.org/abs/2111.07832</data>
  <data key="d3">ibot: Image bert pre-training with online tokenizer</data>
  <data key="d4">J Zhou, C Wei, H Wang, W Shen, C Xie, A Yuille…</data>
  <data key="d5">2021</data>
  <data key="d6">400</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6668235945473015803&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="12719486318898360519">
  <data key="d0">Denseclip: Language-guided dense prediction with context-aware prompting</data>
  <data key="d1">12719486318898360519</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Rao_DenseCLIP_Language-Guided_Dense_Prediction_With_Context-Aware_Prompting_CVPR_2022_paper.html</data>
  <data key="d3">Denseclip: Language-guided dense prediction with context-aware prompting</data>
  <data key="d4">Y Rao, W Zhao, G Chen, Y Tang…</data>
  <data key="d5">2022</data>
  <data key="d6">220</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12719486318898360519&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="16431656661146854759">
  <data key="d0">Exploring cross-image pixel contrast for semantic segmentation</data>
  <data key="d1">16431656661146854759</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Wang_Exploring_Cross-Image_Pixel_Contrast_for_Semantic_Segmentation_ICCV_2021_paper.html</data>
  <data key="d3">Exploring cross-image pixel contrast for semantic segmentation</data>
  <data key="d4">W Wang, T Zhou, F Yu, J Dai…</data>
  <data key="d5">2021</data>
  <data key="d6">339</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16431656661146854759&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="3484260075890953852">
  <data key="d0">Context autoencoder for self-supervised representation learning</data>
  <data key="d1">3484260075890953852</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11263-023-01852-4</data>
  <data key="d3">Context autoencoder for self-supervised representation learning</data>
  <data key="d4">X Chen, M Ding, X Wang, Y Xin, S Mo, Y Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">175</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3484260075890953852&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="12396058489435840533">
  <data key="d0">Cris: Clip-driven referring image segmentation</data>
  <data key="d1">12396058489435840533</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Wang_CRIS_CLIP-Driven_Referring_Image_Segmentation_CVPR_2022_paper.html</data>
  <data key="d3">Cris: Clip-driven referring image segmentation</data>
  <data key="d4">Z Wang, Y Lu, Q Li, X Tao, Y Guo…</data>
  <data key="d5">2022</data>
  <data key="d6">121</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12396058489435840533&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="15469437604545198809">
  <data key="d0">Efficient self-supervised vision transformers for representation learning</data>
  <data key="d1">15469437604545198809</data>
  <data key="d2">https://arxiv.org/abs/2106.09785</data>
  <data key="d3">Efficient self-supervised vision transformers for representation learning</data>
  <data key="d4">C Li, J Yang, P Zhang, M Gao, B Xiao, X Dai…</data>
  <data key="d5">2021</data>
  <data key="d6">151</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15469437604545198809&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="3898607331439114201">
  <data key="d0">Semi-supervised semantic segmentation with pixel-level contrastive learning from a class-wise memory bank</data>
  <data key="d1">3898607331439114201</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Alonso_Semi-Supervised_Semantic_Segmentation_With_Pixel-Level_Contrastive_Learning_From_a_Class-Wise_ICCV_2021_paper.html</data>
  <data key="d3">Semi-supervised semantic segmentation with pixel-level contrastive learning from a class-wise memory bank</data>
  <data key="d4">I Alonso, A Sabater, D Ferstl…</data>
  <data key="d5">2021</data>
  <data key="d6">151</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3898607331439114201&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="11133634648290997125">
  <data key="d0">Vicregl: Self-supervised learning of local visual features</data>
  <data key="d1">11133634648290997125</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/39cee562b91611c16ac0b100f0bc1ea1-Abstract-Conference.html</data>
  <data key="d3">Vicregl: Self-supervised learning of local visual features</data>
  <data key="d4">A Bardes, J Ponce, Y LeCun</data>
  <data key="d5">2022</data>
  <data key="d6">41</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11133634648290997125&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="4695735279498312558">
  <data key="d0">Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization</data>
  <data key="d1">4695735279498312558</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Melas-Kyriazi_Deep_Spectral_Methods_A_Surprisingly_Strong_Baseline_for_Unsupervised_Semantic_CVPR_2022_paper.html</data>
  <data key="d3">Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization</data>
  <data key="d4">L Melas-Kyriazi, C Rupprecht…</data>
  <data key="d5">2022</data>
  <data key="d6">70</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4695735279498312558&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="12107148927806619132">
  <data key="d0">Ts2vec: Towards universal representation of time series</data>
  <data key="d1">12107148927806619132</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/20881</data>
  <data key="d3">Ts2vec: Towards universal representation of time series</data>
  <data key="d4">Z Yue, Y Wang, J Duan, T Yang, C Huang…</data>
  <data key="d5">2022</data>
  <data key="d6">161</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12107148927806619132&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="10949691838250411133">
  <data key="d0">Pixel contrastive-consistent semi-supervised semantic segmentation</data>
  <data key="d1">10949691838250411133</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Zhong_Pixel_Contrastive-Consistent_Semi-Supervised_Semantic_Segmentation_ICCV_2021_paper.html</data>
  <data key="d3">Pixel contrastive-consistent semi-supervised semantic segmentation</data>
  <data key="d4">Y Zhong, B Yuan, H Wu, Z Yuan…</data>
  <data key="d5">2021</data>
  <data key="d6">107</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10949691838250411133&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="2183335709889759560">
  <data key="d0">Contrastive learning rivals masked image modeling in fine-tuning via feature distillation</data>
  <data key="d1">2183335709889759560</data>
  <data key="d2">https://arxiv.org/abs/2205.14141</data>
  <data key="d3">Contrastive learning rivals masked image modeling in fine-tuning via feature distillation</data>
  <data key="d4">Y Wei, H Hu, Z Xie, Z Zhang, Y Cao, J Bao…</data>
  <data key="d5">2022</data>
  <data key="d6">84</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2183335709889759560&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="5084328956172259959">
  <data key="d0">Regional semantic contrast and aggregation for weakly supervised semantic segmentation</data>
  <data key="d1">5084328956172259959</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Regional_Semantic_Contrast_and_Aggregation_for_Weakly_Supervised_Semantic_Segmentation_CVPR_2022_paper.html</data>
  <data key="d3">Regional semantic contrast and aggregation for weakly supervised semantic segmentation</data>
  <data key="d4">T Zhou, M Zhang, F Zhao, J Li</data>
  <data key="d5">2022</data>
  <data key="d6">57</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5084328956172259959&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="8779019959052555626">
  <data key="d0">Mst: Masked self-supervised transformer for visual representation</data>
  <data key="d1">8779019959052555626</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/6dbbe6abe5f14af882ff977fc3f35501-Abstract.html</data>
  <data key="d3">Mst: Masked self-supervised transformer for visual representation</data>
  <data key="d4">Z Li, Z Chen, F Yang, W Li, Y Zhu…</data>
  <data key="d5">2021</data>
  <data key="d6">98</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8779019959052555626&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="12960288265914815983">
  <data key="d0">Contrastive learning for label efficient semantic segmentation</data>
  <data key="d1">12960288265914815983</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Zhao_Contrastive_Learning_for_Label_Efficient_Semantic_Segmentation_ICCV_2021_paper.html</data>
  <data key="d3">Contrastive learning for label efficient semantic segmentation</data>
  <data key="d4">X Zhao, R Vemulapalli, PA Mansfield…</data>
  <data key="d5">2021</data>
  <data key="d6">126</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12960288265914815983&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="10341392145675802868">
  <data key="d0">Revealing the dark secrets of masked image modeling</data>
  <data key="d1">10341392145675802868</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Xie_Revealing_the_Dark_Secrets_of_Masked_Image_Modeling_CVPR_2023_paper.html</data>
  <data key="d3">Revealing the dark secrets of masked image modeling</data>
  <data key="d4">Z Xie, Z Geng, J Hu, Z Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">47</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10341392145675802868&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="9757750069113028831">
  <data key="d0">Aligning pretraining for detection via object-level contrastive learning</data>
  <data key="d1">9757750069113028831</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/bf5cd8b2509011b9502a72296edc14a0-Abstract.html</data>
  <data key="d3">Aligning pretraining for detection via object-level contrastive learning</data>
  <data key="d4">F Wei, Y Gao, Z Wu, H Hu, S Lin</data>
  <data key="d5">2021</data>
  <data key="d6">90</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9757750069113028831&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="33927402341244667">
  <data key="d0">Region-aware contrastive learning for semantic segmentation</data>
  <data key="d1">33927402341244667</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Hu_Region-Aware_Contrastive_Learning_for_Semantic_Segmentation_ICCV_2021_paper.html</data>
  <data key="d3">Region-aware contrastive learning for semantic segmentation</data>
  <data key="d4">H Hu, J Cui, L Wang</data>
  <data key="d5">2021</data>
  <data key="d6">79</data>
  <data key="d7">https://scholar.google.com/scholar?cites=33927402341244667&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="14524693012131206782">
  <data key="d0">Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation</data>
  <data key="d1">14524693012131206782</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10018569/</data>
  <data key="d3">Sepico: Semantic-guided pixel contrast for domain adaptive semantic segmentation</data>
  <data key="d4">B Xie, S Li, M Li, CH Liu, G Huang…</data>
  <data key="d5">2023</data>
  <data key="d6">54</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14524693012131206782&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="532394135980020430">
  <data key="d0">Cut and learn for unsupervised object detection and instance segmentation</data>
  <data key="d1">532394135980020430</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Wang_Cut_and_Learn_for_Unsupervised_Object_Detection_and_Instance_Segmentation_CVPR_2023_paper.html</data>
  <data key="d3">Cut and learn for unsupervised object detection and instance segmentation</data>
  <data key="d4">X Wang, R Girdhar, SX Yu…</data>
  <data key="d5">2023</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=532394135980020430&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="17284983713036766691">
  <data key="d0">Crafting better contrastive views for siamese representation learning</data>
  <data key="d1">17284983713036766691</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Peng_Crafting_Better_Contrastive_Views_for_Siamese_Representation_Learning_CVPR_2022_paper.html</data>
  <data key="d3">Crafting better contrastive views for siamese representation learning</data>
  <data key="d4">X Peng, K Wang, Z Zhu, M Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">71</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17284983713036766691&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="633087071652505085">
  <data key="d0">Spot-the-difference self-supervised pre-training for anomaly detection and segmentation</data>
  <data key="d1">633087071652505085</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20056-4_23</data>
  <data key="d3">Spot-the-difference self-supervised pre-training for anomaly detection and segmentation</data>
  <data key="d4">Y Zou, J Jeong, L Pemula, D Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">39</data>
  <data key="d7">https://scholar.google.com/scholar?cites=633087071652505085&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="14101725080609270309">
  <data key="d0">A survey on label-efficient deep image segmentation: Bridging the gap between weak supervision and dense prediction</data>
  <data key="d1">14101725080609270309</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10048555/</data>
  <data key="d3">A survey on label-efficient deep image segmentation: Bridging the gap between weak supervision and dense prediction</data>
  <data key="d4">W Shen, Z Peng, X Wang, H Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14101725080609270309&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="17624445298553964188">
  <data key="d0">Region similarity representation learning</data>
  <data key="d1">17624445298553964188</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Xiao_Region_Similarity_Representation_Learning_ICCV_2021_paper.html</data>
  <data key="d3">Region similarity representation learning</data>
  <data key="d4">T Xiao, CJ Reed, X Wang…</data>
  <data key="d5">2021</data>
  <data key="d6">95</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17624445298553964188&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="10023428544428348799">
  <data key="d0">Self-supervised learning of object parts for semantic segmentation</data>
  <data key="d1">10023428544428348799</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Ziegler_Self-Supervised_Learning_of_Object_Parts_for_Semantic_Segmentation_CVPR_2022_paper.html</data>
  <data key="d3">Self-supervised learning of object parts for semantic segmentation</data>
  <data key="d4">A Ziegler, YM Asano</data>
  <data key="d5">2022</data>
  <data key="d6">43</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10023428544428348799&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="3726545131723476858">
  <data key="d0">Freesolo: Learning to segment objects without annotations</data>
  <data key="d1">3726545131723476858</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Wang_FreeSOLO_Learning_To_Segment_Objects_Without_Annotations_CVPR_2022_paper.html</data>
  <data key="d3">Freesolo: Learning to segment objects without annotations</data>
  <data key="d4">X Wang, Z Yu, S De Mello, J Kautz…</data>
  <data key="d5">2022</data>
  <data key="d6">47</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3726545131723476858&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="12927720534552603420">
  <data key="d0">Locvtp: Video-text pre-training for temporal localization</data>
  <data key="d1">12927720534552603420</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19809-0_3</data>
  <data key="d3">Locvtp: Video-text pre-training for temporal localization</data>
  <data key="d4">M Cao, T Yang, J Weng, C Zhang, J Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12927720534552603420&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="10551610679278163665">
  <data key="d0">Detreg: Unsupervised pretraining with region priors for object detection</data>
  <data key="d1">10551610679278163665</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Bar_DETReg_Unsupervised_Pretraining_With_Region_Priors_for_Object_Detection_CVPR_2022_paper.html</data>
  <data key="d3">Detreg: Unsupervised pretraining with region priors for object detection</data>
  <data key="d4">A Bar, X Wang, V Kantorov, CJ Reed…</data>
  <data key="d5">2022</data>
  <data key="d6">72</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10551610679278163665&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="8481021928568633762">
  <data key="d0">Point-level region contrast for object detection pre-training</data>
  <data key="d1">8481021928568633762</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Bai_Point-Level_Region_Contrast_for_Object_Detection_Pre-Training_CVPR_2022_paper.html</data>
  <data key="d3">Point-level region contrast for object detection pre-training</data>
  <data key="d4">Y Bai, X Chen, A Kirillov, A Yuille…</data>
  <data key="d5">2022</data>
  <data key="d6">37</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8481021928568633762&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="7983433851687883883">
  <data key="d0">Image-to-lidar self-supervised distillation for autonomous driving data</data>
  <data key="d1">7983433851687883883</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Sautier_Image-to-Lidar_Self-Supervised_Distillation_for_Autonomous_Driving_Data_CVPR_2022_paper.html</data>
  <data key="d3">Image-to-lidar self-supervised distillation for autonomous driving data</data>
  <data key="d4">C Sautier, G Puy, S Gidaris, A Boulch…</data>
  <data key="d5">2022</data>
  <data key="d6">42</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7983433851687883883&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="13592875691592926608">
  <data key="d0">Denoising pretraining for semantic segmentation</data>
  <data key="d1">13592875691592926608</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2022W/L3D-IVU/html/Brempong_Denoising_Pretraining_for_Semantic_Segmentation_CVPRW_2022_paper.html</data>
  <data key="d3">Denoising pretraining for semantic segmentation</data>
  <data key="d4">EA Brempong, S Kornblith, T Chen…</data>
  <data key="d5">2022</data>
  <data key="d6">37</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13592875691592926608&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="9426593234701255657">
  <data key="d0">Weakly supervised semantic segmentation by pixel-to-prototype contrast</data>
  <data key="d1">9426593234701255657</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Du_Weakly_Supervised_Semantic_Segmentation_by_Pixel-to-Prototype_Contrast_CVPR_2022_paper.html</data>
  <data key="d3">Weakly supervised semantic segmentation by pixel-to-prototype contrast</data>
  <data key="d4">Y Du, Z Fu, Q Liu, Y Wang</data>
  <data key="d5">2022</data>
  <data key="d6">52</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9426593234701255657&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="1924986113269211163">
  <data key="d0">Self-supervised learning for scene classification in remote sensing: Current state of the art and perspectives</data>
  <data key="d1">1924986113269211163</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/16/3995</data>
  <data key="d3">Self-supervised learning for scene classification in remote sensing: Current state of the art and perspectives</data>
  <data key="d4">P Berg, MT Pham, N Courty</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1924986113269211163&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="17074925293404018328">
  <data key="d0">C3-semiseg: Contrastive semi-supervised segmentation via cross-set learning and dynamic class-balancing</data>
  <data key="d1">17074925293404018328</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Zhou_C3-SemiSeg_Contrastive_Semi-Supervised_Segmentation_via_Cross-Set_Learning_and_Dynamic_Class-Balancing_ICCV_2021_paper.html</data>
  <data key="d3">C3-semiseg: Contrastive semi-supervised segmentation via cross-set learning and dynamic class-balancing</data>
  <data key="d4">Y Zhou, H Xu, W Zhang, B Gao…</data>
  <data key="d5">2021</data>
  <data key="d6">53</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17074925293404018328&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="5868001138380150922">
  <data key="d0">Improving contrastive learning by visualizing feature transformation</data>
  <data key="d1">5868001138380150922</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Zhu_Improving_Contrastive_Learning_by_Visualizing_Feature_Transformation_ICCV_2021_paper.html</data>
  <data key="d3">Improving contrastive learning by visualizing feature transformation</data>
  <data key="d4">R Zhu, B Zhao, J Liu, Z Sun…</data>
  <data key="d5">2021</data>
  <data key="d6">57</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5868001138380150922&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="5575721172969217810">
  <data key="d0">Green hierarchical vision transformer for masked image modeling</data>
  <data key="d1">5575721172969217810</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/7e487c72fce6e45879a78ee0872d991d-Abstract-Conference.html</data>
  <data key="d3">Green hierarchical vision transformer for masked image modeling</data>
  <data key="d4">L Huang, S You, M Zheng, F Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">31</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5575721172969217810&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="11947642466448713378">
  <data key="d0">Unsupervised object-level representation learning from scene images</data>
  <data key="d1">11947642466448713378</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/f1b6f2857fb6d44dd73c7041e0aa0f19-Abstract.html</data>
  <data key="d3">Unsupervised object-level representation learning from scene images</data>
  <data key="d4">J Xie, X Zhan, Z Liu, YS Ong…</data>
  <data key="d5">2021</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11947642466448713378&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="6750819842213893633">
  <data key="d0">Hyperbolic contrastive learning for visual representations beyond objects</data>
  <data key="d1">6750819842213893633</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Ge_Hyperbolic_Contrastive_Learning_for_Visual_Representations_Beyond_Objects_CVPR_2023_paper.html</data>
  <data key="d3">Hyperbolic contrastive learning for visual representations beyond objects</data>
  <data key="d4">S Ge, S Mishra, S Kornblith, CL Li…</data>
  <data key="d5">2023</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6750819842213893633&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="11904217105191322390">
  <data key="d0">Dual temperature helps contrastive learning without many negative samples: Towards understanding and simplifying moco</data>
  <data key="d1">11904217105191322390</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Dual_Temperature_Helps_Contrastive_Learning_Without_Many_Negative_Samples_Towards_CVPR_2022_paper.html</data>
  <data key="d3">Dual temperature helps contrastive learning without many negative samples: Towards understanding and simplifying moco</data>
  <data key="d4">C Zhang, K Zhang, TX Pham, A Niu…</data>
  <data key="d5">2022</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11904217105191322390&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="1775977373440976785">
  <data key="d0">Leveraging real talking faces via self-supervision for robust forgery detection</data>
  <data key="d1">1775977373440976785</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Haliassos_Leveraging_Real_Talking_Faces_via_Self-Supervision_for_Robust_Forgery_Detection_CVPR_2022_paper.html</data>
  <data key="d3">Leveraging real talking faces via self-supervision for robust forgery detection</data>
  <data key="d4">A Haliassos, R Mira, S Petridis…</data>
  <data key="d5">2022</data>
  <data key="d6">37</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1775977373440976785&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="1318173233917005478">
  <data key="d0">Cross-patch dense contrastive learning for semi-supervised segmentation of cellular nuclei in histopathologic images</data>
  <data key="d1">1318173233917005478</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Wu_Cross-Patch_Dense_Contrastive_Learning_for_Semi-Supervised_Segmentation_of_Cellular_Nuclei_CVPR_2022_paper.html</data>
  <data key="d3">Cross-patch dense contrastive learning for semi-supervised segmentation of cellular nuclei in histopathologic images</data>
  <data key="d4">H Wu, Z Wang, Y Song, L Yang…</data>
  <data key="d5">2022</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1318173233917005478&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="14529545999567875627">
  <data key="d0">BigDatasetGAN: Synthesizing ImageNet with pixel-wise annotations</data>
  <data key="d1">14529545999567875627</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Li_BigDatasetGAN_Synthesizing_ImageNet_With_Pixel-Wise_Annotations_CVPR_2022_paper.html</data>
  <data key="d3">BigDatasetGAN: Synthesizing ImageNet with pixel-wise annotations</data>
  <data key="d4">D Li, H Ling, SW Kim, K Kreis…</data>
  <data key="d5">2022</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14529545999567875627&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="1661264717430846222">
  <data key="d0">Self-supervised visual representations learning by contrastive mask prediction</data>
  <data key="d1">1661264717430846222</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Zhao_Self-Supervised_Visual_Representations_Learning_by_Contrastive_Mask_Prediction_ICCV_2021_paper.html</data>
  <data key="d3">Self-supervised visual representations learning by contrastive mask prediction</data>
  <data key="d4">Y Zhao, G Wang, C Luo, W Zeng…</data>
  <data key="d5">2021</data>
  <data key="d6">37</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1661264717430846222&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="15793792687497316784">
  <data key="d0">Patch-level representation learning for self-supervised vision transformers</data>
  <data key="d1">15793792687497316784</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Yun_Patch-Level_Representation_Learning_for_Self-Supervised_Vision_Transformers_CVPR_2022_paper.html</data>
  <data key="d3">Patch-level representation learning for self-supervised vision transformers</data>
  <data key="d4">S Yun, H Lee, J Kim, J Shin</data>
  <data key="d5">2022</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15793792687497316784&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="8981892223562201199">
  <data key="d0">Milan: Masked image pretraining on language assisted representation</data>
  <data key="d1">8981892223562201199</data>
  <data key="d2">https://arxiv.org/abs/2208.06049</data>
  <data key="d3">Milan: Masked image pretraining on language assisted representation</data>
  <data key="d4">Z Hou, F Sun, YK Chen, Y Xie, SY Kung</data>
  <data key="d5">2022</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8981892223562201199&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="10833379900462253180">
  <data key="d0">Looking beyond single images for contrastive semantic segmentation learning</data>
  <data key="d1">10833379900462253180</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/1a68e5f4ade56ed1d4bf273e55510750-Abstract.html</data>
  <data key="d3">Looking beyond single images for contrastive semantic segmentation learning</data>
  <data key="d4">F Zhang, P Torr, R Ranftl…</data>
  <data key="d5">2021</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10833379900462253180&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="1047156362824031830">
  <data key="d0">Dtg-ssod: Dense teacher guidance for semi-supervised object detection</data>
  <data key="d1">1047156362824031830</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/3a02b6df276223b68c69ca572cb3c4a8-Abstract-Conference.html</data>
  <data key="d3">Dtg-ssod: Dense teacher guidance for semi-supervised object detection</data>
  <data key="d4">G Li, X Li, Y Wang, W Yichao…</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1047156362824031830&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="735436327696041641">
  <data key="d0">Revisiting contrastive methods for unsupervised learning of visual representations</data>
  <data key="d1">735436327696041641</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/8757150decbd89b0f5442ca3db4d0e0e-Abstract.html</data>
  <data key="d3">Revisiting contrastive methods for unsupervised learning of visual representations</data>
  <data key="d4">W Van Gansbeke, S Vandenhende…</data>
  <data key="d5">2021</data>
  <data key="d6">43</data>
  <data key="d7">https://scholar.google.com/scholar?cites=735436327696041641&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="9214065931586807664">
  <data key="d0">Multisiam: Self-supervised multi-instance siamese representation learning for autonomous driving</data>
  <data key="d1">9214065931586807664</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Chen_MultiSiam_Self-Supervised_Multi-Instance_Siamese_Representation_Learning_for_Autonomous_Driving_ICCV_2021_paper.html</data>
  <data key="d3">Multisiam: Self-supervised multi-instance siamese representation learning for autonomous driving</data>
  <data key="d4">K Chen, L Hong, H Xu, Z Li…</data>
  <data key="d5">2021</data>
  <data key="d6">36</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9214065931586807664&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="16212063836582850356">
  <data key="d0">Univip: A unified framework for self-supervised visual pre-training</data>
  <data key="d1">16212063836582850356</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Li_UniVIP_A_Unified_Framework_for_Self-Supervised_Visual_Pre-Training_CVPR_2022_paper.html</data>
  <data key="d3">Univip: A unified framework for self-supervised visual pre-training</data>
  <data key="d4">Z Li, Y Zhu, F Yang, W Li, C Zhao…</data>
  <data key="d5">2022</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16212063836582850356&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="14964804060777605755">
  <data key="d0">Unsupervised pre-training for temporal action localization tasks</data>
  <data key="d1">14964804060777605755</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Unsupervised_Pre-Training_for_Temporal_Action_Localization_Tasks_CVPR_2022_paper.html</data>
  <data key="d3">Unsupervised pre-training for temporal action localization tasks</data>
  <data key="d4">C Zhang, T Yang, J Weng, M Cao…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14964804060777605755&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="10173276721588368719">
  <data key="d0">DiRA: Discriminative, restorative, and adversarial learning for self-supervised medical image analysis</data>
  <data key="d1">10173276721588368719</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Haghighi_DiRA_Discriminative_Restorative_and_Adversarial_Learning_for_Self-Supervised_Medical_Image_CVPR_2022_paper.html</data>
  <data key="d3">DiRA: Discriminative, restorative, and adversarial learning for self-supervised medical image analysis</data>
  <data key="d4">F Haghighi, MRH Taher…</data>
  <data key="d5">2022</data>
  <data key="d6">35</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10173276721588368719&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="16722403537302150812">
  <data key="d0">Multi-granularity cross-modal alignment for generalized medical visual representation learning</data>
  <data key="d1">16722403537302150812</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/d925bda407ada0df3190df323a212661-Abstract-Conference.html</data>
  <data key="d3">Multi-granularity cross-modal alignment for generalized medical visual representation learning</data>
  <data key="d4">F Wang, Y Zhou, S Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16722403537302150812&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="8555368968556268636">
  <data key="d0">Not just selection, but exploration: Online class-incremental continual learning via dual view consistency</data>
  <data key="d1">8555368968556268636</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Gu_Not_Just_Selection_but_Exploration_Online_Class-Incremental_Continual_Learning_via_CVPR_2022_paper.html</data>
  <data key="d3">Not just selection, but exploration: Online class-incremental continual learning via dual view consistency</data>
  <data key="d4">Y Gu, X Yang, K Wei, C Deng</data>
  <data key="d5">2022</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8555368968556268636&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="33793130511872188">
  <data key="d0">Decoupled adversarial contrastive learning for self-supervised adversarial robustness</data>
  <data key="d1">33793130511872188</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20056-4_42</data>
  <data key="d3">Decoupled adversarial contrastive learning for self-supervised adversarial robustness</data>
  <data key="d4">C Zhang, K Zhang, C Zhang, A Niu, J Feng…</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=33793130511872188&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="15449354411100450148">
  <data key="d0">A closer look at invariances in self-supervised pre-training for 3d vision</data>
  <data key="d1">15449354411100450148</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20056-4_38</data>
  <data key="d3">A closer look at invariances in self-supervised pre-training for 3d vision</data>
  <data key="d4">L Li, M Heizmann</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15449354411100450148&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="9049923034415496270">
  <data key="d0">Joint learning of localized representations from medical images and reports</data>
  <data key="d1">9049923034415496270</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19809-0_39</data>
  <data key="d3">Joint learning of localized representations from medical images and reports</data>
  <data key="d4">P Müller, G Kaissis, C Zou, D Rueckert</data>
  <data key="d5">2022</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9049923034415496270&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="432444698543425795">
  <data key="d0">Towards discriminative representation: Multi-view trajectory contrastive learning for online multi-object tracking</data>
  <data key="d1">432444698543425795</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Yu_Towards_Discriminative_Representation_Multi-View_Trajectory_Contrastive_Learning_for_Online_Multi-Object_CVPR_2022_paper.html</data>
  <data key="d3">Towards discriminative representation: Multi-view trajectory contrastive learning for online multi-object tracking</data>
  <data key="d4">E Yu, Z Li, S Han</data>
  <data key="d5">2022</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=432444698543425795&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="9021031822777383648">
  <data key="d0">A cookbook of self-supervised learning</data>
  <data key="d1">9021031822777383648</data>
  <data key="d2">https://arxiv.org/abs/2304.12210</data>
  <data key="d3">A cookbook of self-supervised learning</data>
  <data key="d4">R Balestriero, M Ibrahim, V Sobal, A Morcos…</data>
  <data key="d5">2023</data>
  <data key="d6">44</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9021031822777383648&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="8430936739774837199">
  <data key="d0">Unsupervised hierarchical semantic segmentation with multiview cosegmentation and clustering transformers</data>
  <data key="d1">8430936739774837199</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Ke_Unsupervised_Hierarchical_Semantic_Segmentation_With_Multiview_Cosegmentation_and_Clustering_Transformers_CVPR_2022_paper.html</data>
  <data key="d3">Unsupervised hierarchical semantic segmentation with multiview cosegmentation and clustering transformers</data>
  <data key="d4">TW Ke, JJ Hwang, Y Guo, X Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8430936739774837199&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="5638477121687230939">
  <data key="d0">Exploring set similarity for dense self-supervised representation learning</data>
  <data key="d1">5638477121687230939</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Wang_Exploring_Set_Similarity_for_Dense_Self-Supervised_Representation_Learning_CVPR_2022_paper.html</data>
  <data key="d3">Exploring set similarity for dense self-supervised representation learning</data>
  <data key="d4">Z Wang, Q Li, G Zhang, P Wan…</data>
  <data key="d5">2022</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5638477121687230939&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="6099615041197158188">
  <data key="d0">Use all the labels: A hierarchical multi-label contrastive learning framework</data>
  <data key="d1">6099615041197158188</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Use_All_the_Labels_A_Hierarchical_Multi-Label_Contrastive_Learning_Framework_CVPR_2022_paper.html</data>
  <data key="d3">Use all the labels: A hierarchical multi-label contrastive learning framework</data>
  <data key="d4">S Zhang, R Xu, C Xiong…</data>
  <data key="d5">2022</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6099615041197158188&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="8773982247708585699">
  <data key="d0">Learning self-supervised low-rank network for single-stage weakly and semi-supervised semantic segmentation</data>
  <data key="d1">8773982247708585699</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11263-022-01590-z</data>
  <data key="d3">Learning self-supervised low-rank network for single-stage weakly and semi-supervised semantic segmentation</data>
  <data key="d4">J Pan, P Zhu, K Zhang, B Cao, Y Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8773982247708585699&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="6776914221737060740">
  <data key="d0">Learning where to learn in cross-view self-supervised learning</data>
  <data key="d1">6776914221737060740</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Huang_Learning_Where_To_Learn_in_Cross-View_Self-Supervised_Learning_CVPR_2022_paper.html</data>
  <data key="d3">Learning where to learn in cross-view self-supervised learning</data>
  <data key="d4">L Huang, S You, M Zheng, F Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6776914221737060740&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="9515469045960583745">
  <data key="d0">Semantic-aware fine-grained correspondence</data>
  <data key="d1">9515469045960583745</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19821-2_6</data>
  <data key="d3">Semantic-aware fine-grained correspondence</data>
  <data key="d4">Y Hu, R Wang, K Zhang, Y Gao</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9515469045960583745&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="18400465047273061750">
  <data key="d0">Graftnet: Towards domain generalized stereo matching with a broad-spectrum and task-oriented feature</data>
  <data key="d1">18400465047273061750</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Liu_GraftNet_Towards_Domain_Generalized_Stereo_Matching_With_a_Broad-Spectrum_and_CVPR_2022_paper.html</data>
  <data key="d3">Graftnet: Towards domain generalized stereo matching with a broad-spectrum and task-oriented feature</data>
  <data key="d4">B Liu, H Yu, G Qi</data>
  <data key="d5">2022</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18400465047273061750&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="16150052612237999167">
  <data key="d0">Vibus: Data-efficient 3d scene parsing with viewpoint bottleneck and uncertainty-spectrum modeling</data>
  <data key="d1">16150052612237999167</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0924271622002817</data>
  <data key="d3">Vibus: Data-efficient 3d scene parsing with viewpoint bottleneck and uncertainty-spectrum modeling</data>
  <data key="d4">B Tian, L Luo, H Zhao, G Zhou</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16150052612237999167&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="2476318727654891437">
  <data key="d0">Residual pattern learning for pixel-wise out-of-distribution detection in semantic segmentation</data>
  <data key="d1">2476318727654891437</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Liu_Residual_Pattern_Learning_for_Pixel-Wise_Out-of-Distribution_Detection_in_Semantic_Segmentation_ICCV_2023_paper.html</data>
  <data key="d3">Residual pattern learning for pixel-wise out-of-distribution detection in semantic segmentation</data>
  <data key="d4">Y Liu, C Ding, Y Tian, G Pang…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2476318727654891437&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="9361339446003315812">
  <data key="d0">Unleashing the power of contrastive self-supervised visual models via contrast-regularized fine-tuning</data>
  <data key="d1">9361339446003315812</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/fa14d4fe2f19414de3ebd9f63d5c0169-Abstract.html</data>
  <data key="d3">Unleashing the power of contrastive self-supervised visual models via contrast-regularized fine-tuning</data>
  <data key="d4">Y Zhang, B Hooi, D Hu, J Liang…</data>
  <data key="d5">2021</data>
  <data key="d6">37</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9361339446003315812&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="2507058243298497028">
  <data key="d0">Dense semantic contrast for self-supervised visual representation learning</data>
  <data key="d1">2507058243298497028</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3474085.3475551</data>
  <data key="d3">Dense semantic contrast for self-supervised visual representation learning</data>
  <data key="d4">X Li, Y Zhou, Y Zhang, A Zhang, W Wang…</data>
  <data key="d5">2021</data>
  <data key="d6">33</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2507058243298497028&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="8173455362624893467">
  <data key="d0">Move: Unsupervised movable object segmentation and detection</data>
  <data key="d1">8173455362624893467</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/d7eb232f196124894f2e65b9010a5c57-Abstract-Conference.html</data>
  <data key="d3">Move: Unsupervised movable object segmentation and detection</data>
  <data key="d4">A Bielski, P Favaro</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8173455362624893467&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="5902003836142023006">
  <data key="d0">Revisiting domain generalized stereo matching networks from a feature consistency perspective</data>
  <data key="d1">5902003836142023006</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Revisiting_Domain_Generalized_Stereo_Matching_Networks_From_a_Feature_Consistency_CVPR_2022_paper.html</data>
  <data key="d3">Revisiting domain generalized stereo matching networks from a feature consistency perspective</data>
  <data key="d4">J Zhang, X Wang, X Bai, C Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5902003836142023006&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="4005108257678361504">
  <data key="d0">Rethinking the augmentation module in contrastive learning: Learning hierarchical augmentation invariance with expanded views</data>
  <data key="d1">4005108257678361504</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Rethinking_the_Augmentation_Module_in_Contrastive_Learning_Learning_Hierarchical_Augmentation_CVPR_2022_paper.html</data>
  <data key="d3">Rethinking the augmentation module in contrastive learning: Learning hierarchical augmentation invariance with expanded views</data>
  <data key="d4">J Zhang, K Ma</data>
  <data key="d5">2022</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4005108257678361504&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="7085743912913543637">
  <data key="d0">Contrastmask: Contrastive learning to segment every thing</data>
  <data key="d1">7085743912913543637</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Wang_ContrastMask_Contrastive_Learning_To_Segment_Every_Thing_CVPR_2022_paper.html</data>
  <data key="d3">Contrastmask: Contrastive learning to segment every thing</data>
  <data key="d4">X Wang, K Zhao, R Zhang, S Ding…</data>
  <data key="d5">2022</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7085743912913543637&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="8011144875870384891">
  <data key="d0">Large-scale unsupervised semantic segmentation</data>
  <data key="d1">8011144875870384891</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9933726/</data>
  <data key="d3">Large-scale unsupervised semantic segmentation</data>
  <data key="d4">S Gao, ZY Li, MH Yang, MM Cheng…</data>
  <data key="d5">2022</data>
  <data key="d6">33</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8011144875870384891&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="2931986225761676831">
  <data key="d0">Local contrastive loss with pseudo-label based self-training for semi-supervised medical image segmentation</data>
  <data key="d1">2931986225761676831</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841523000531</data>
  <data key="d3">Local contrastive loss with pseudo-label based self-training for semi-supervised medical image segmentation</data>
  <data key="d4">K Chaitanya, E Erdil, N Karani, E Konukoglu</data>
  <data key="d5">2023</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2931986225761676831&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="11156672461051367506">
  <data key="d0">Supervised masked knowledge distillation for few-shot transformers</data>
  <data key="d1">11156672461051367506</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Lin_Supervised_Masked_Knowledge_Distillation_for_Few-Shot_Transformers_CVPR_2023_paper.html</data>
  <data key="d3">Supervised masked knowledge distillation for few-shot transformers</data>
  <data key="d4">H Lin, G Han, J Ma, S Huang, X Lin…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11156672461051367506&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="6354529551574600680">
  <data key="d0">Investigating the role of negatives in contrastive representation learning</data>
  <data key="d1">6354529551574600680</data>
  <data key="d2">https://arxiv.org/abs/2106.09943</data>
  <data key="d3">Investigating the role of negatives in contrastive representation learning</data>
  <data key="d4">JT Ash, S Goel, A Krishnamurthy, D Misra</data>
  <data key="d5">2021</data>
  <data key="d6">33</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6354529551574600680&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="12655218227669740570">
  <data key="d0">SAM: Self-supervised learning of pixel-wise anatomical embeddings in radiological images</data>
  <data key="d1">12655218227669740570</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9760421/</data>
  <data key="d3">SAM: Self-supervised learning of pixel-wise anatomical embeddings in radiological images</data>
  <data key="d4">K Yan, J Cai, D Jin, S Miao, D Guo…</data>
  <data key="d5">2022</data>
  <data key="d6">49</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12655218227669740570&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="248902349160897339">
  <data key="d0">Self-Supervised Learning by Estimating Twin Class Distribution</data>
  <data key="d1">248902349160897339</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10102765/</data>
  <data key="d3">Self-Supervised Learning by Estimating Twin Class Distribution</data>
  <data key="d4">F Wang, T Kong, R Zhang, H Liu…</data>
  <data key="d5">2023</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=248902349160897339&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="535744563039386055">
  <data key="d0">A unifying review of deep and shallow anomaly detection</data>
  <data key="d1">535744563039386055</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9347460/</data>
  <data key="d3">A unifying review of deep and shallow anomaly detection</data>
  <data key="d4">L Ruff, JR Kauffmann, RA Vandermeulen…</data>
  <data key="d5">2021</data>
  <data key="d6">608</data>
  <data key="d7">https://scholar.google.com/scholar?cites=535744563039386055&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="12768464336489196273">
  <data key="d0">Explaining deep neural networks and beyond: A review of methods and applications</data>
  <data key="d1">12768464336489196273</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9369420/</data>
  <data key="d3">Explaining deep neural networks and beyond: A review of methods and applications</data>
  <data key="d4">W Samek, G Montavon, S Lapuschkin…</data>
  <data key="d5">2021</data>
  <data key="d6">611</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12768464336489196273&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="16299082709654826538">
  <data key="d0">Cutpaste: Self-supervised learning for anomaly detection and localization</data>
  <data key="d1">16299082709654826538</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Li_CutPaste_Self-Supervised_Learning_for_Anomaly_Detection_and_Localization_CVPR_2021_paper.html</data>
  <data key="d3">Cutpaste: Self-supervised learning for anomaly detection and localization</data>
  <data key="d4">CL Li, K Sohn, J Yoon, T Pfister</data>
  <data key="d5">2021</data>
  <data key="d6">439</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16299082709654826538&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="18092310435120949940">
  <data key="d0">Generalized out-of-distribution detection: A survey</data>
  <data key="d1">18092310435120949940</data>
  <data key="d2">https://arxiv.org/abs/2110.11334</data>
  <data key="d3">Generalized out-of-distribution detection: A survey</data>
  <data key="d4">J Yang, K Zhou, Y Li, Z Liu</data>
  <data key="d5">2021</data>
  <data key="d6">368</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18092310435120949940&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="17544397226639680198">
  <data key="d0">Combining machine learning and computational chemistry for predictive insights into chemical systems</data>
  <data key="d1">17544397226639680198</data>
  <data key="d2">https://pubs.acs.org/doi/abs/10.1021/acs.chemrev.1c00107</data>
  <data key="d3">Combining machine learning and computational chemistry for predictive insights into chemical systems</data>
  <data key="d4">JA Keith, V Vassilev-Galindo, B Cheng…</data>
  <data key="d5">2021</data>
  <data key="d6">278</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17544397226639680198&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="411578991690355580">
  <data key="d0">Machine learning for electrocatalyst and photocatalyst design and discovery</data>
  <data key="d1">411578991690355580</data>
  <data key="d2">https://pubs.acs.org/doi/abs/10.1021/acs.chemrev.2c00061</data>
  <data key="d3">Machine learning for electrocatalyst and photocatalyst design and discovery</data>
  <data key="d4">H Mai, TC Le, D Chen, DA Winkler…</data>
  <data key="d5">2022</data>
  <data key="d6">78</data>
  <data key="d7">https://scholar.google.com/scholar?cites=411578991690355580&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="4407607921916219597">
  <data key="d0">Adbench: Anomaly detection benchmark</data>
  <data key="d1">4407607921916219597</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/cf93972b116ca5268827d575f2cc226b-Abstract-Datasets_and_Benchmarks.html</data>
  <data key="d3">Adbench: Anomaly detection benchmark</data>
  <data key="d4">S Han, X Hu, H Huang, M Jiang…</data>
  <data key="d5">2022</data>
  <data key="d6">102</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4407607921916219597&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="5644429611266863213">
  <data key="d0">Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows</data>
  <data key="d1">5644429611266863213</data>
  <data key="d2">https://openaccess.thecvf.com/content/WACV2022/html/Gudovskiy_CFLOW-AD_Real-Time_Unsupervised_Anomaly_Detection_With_Localization_via_Conditional_Normalizing_WACV_2022_paper.html</data>
  <data key="d3">Cflow-ad: Real-time unsupervised anomaly detection with localization via conditional normalizing flows</data>
  <data key="d4">D Gudovskiy, S Ishizaka…</data>
  <data key="d5">2022</data>
  <data key="d6">176</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5644429611266863213&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="9152627594047007367">
  <data key="d0">Information fusion as an integrative cross-cutting enabler to achieve robust, explainable, and trustworthy medical artificial intelligence</data>
  <data key="d1">9152627594047007367</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1566253521002050</data>
  <data key="d3">Information fusion as an integrative cross-cutting enabler to achieve robust, explainable, and trustworthy medical artificial intelligence</data>
  <data key="d4">A Holzinger, M Dehmer, F Emmert-Streib, R Cucchiara…</data>
  <data key="d5">2022</data>
  <data key="d6">111</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9152627594047007367&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="6458276904017990971">
  <data key="d0">Learning and evaluating representations for deep one-class classification</data>
  <data key="d1">6458276904017990971</data>
  <data key="d2">https://arxiv.org/abs/2011.02578</data>
  <data key="d3">Learning and evaluating representations for deep one-class classification</data>
  <data key="d4">K Sohn, CL Li, J Yoon, M Jin, T Pfister</data>
  <data key="d5">2020</data>
  <data key="d6">167</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6458276904017990971&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="4245256336048193979">
  <data key="d0">Pixmix: Dreamlike pictures comprehensively improve safety measures</data>
  <data key="d1">4245256336048193979</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Hendrycks_PixMix_Dreamlike_Pictures_Comprehensively_Improve_Safety_Measures_CVPR_2022_paper.html</data>
  <data key="d3">Pixmix: Dreamlike pictures comprehensively improve safety measures</data>
  <data key="d4">D Hendrycks, A Zou, M Mazeika…</data>
  <data key="d5">2022</data>
  <data key="d6">59</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4245256336048193979&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="1382712243609022780">
  <data key="d0">Explainable deep one-class classification</data>
  <data key="d1">1382712243609022780</data>
  <data key="d2">https://arxiv.org/abs/2007.01760</data>
  <data key="d3">Explainable deep one-class classification</data>
  <data key="d4">P Liznerski, L Ruff, RA Vandermeulen…</data>
  <data key="d5">2020</data>
  <data key="d6">166</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1382712243609022780&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="11441760293318611832">
  <data key="d0">A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges</data>
  <data key="d1">11441760293318611832</data>
  <data key="d2">https://arxiv.org/abs/2110.14051</data>
  <data key="d3">A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges</data>
  <data key="d4">M Salehi, H Mirzaei, D Hendrycks, Y Li…</data>
  <data key="d5">2021</data>
  <data key="d6">100</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11441760293318611832&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="1292087033558963213">
  <data key="d0">Neural transformation learning for deep anomaly detection beyond images</data>
  <data key="d1">1292087033558963213</data>
  <data key="d2">https://proceedings.mlr.press/v139/qiu21a.html</data>
  <data key="d3">Neural transformation learning for deep anomaly detection beyond images</data>
  <data key="d4">C Qiu, T Pfrommer, M Kloft, S Mandt…</data>
  <data key="d5">2021</data>
  <data key="d6">66</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1292087033558963213&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="10712076956509058252">
  <data key="d0">Graph neural networks for anomaly detection in industrial internet of things</data>
  <data key="d1">10712076956509058252</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9471816/</data>
  <data key="d3">Graph neural networks for anomaly detection in industrial internet of things</data>
  <data key="d4">Y Wu, HN Dai, H Tang</data>
  <data key="d5">2021</data>
  <data key="d6">85</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10712076956509058252&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="16040050996847565363">
  <data key="d0">SHIFT: a synthetic driving dataset for continuous multi-task domain adaptation</data>
  <data key="d1">16040050996847565363</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Sun_SHIFT_A_Synthetic_Driving_Dataset_for_Continuous_Multi-Task_Domain_Adaptation_CVPR_2022_paper.html</data>
  <data key="d3">SHIFT: a synthetic driving dataset for continuous multi-task domain adaptation</data>
  <data key="d4">T Sun, M Segu, J Postels, Y Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">39</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16040050996847565363&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="2307421743935657923">
  <data key="d0">Unsupervised deep anomaly detection for multi-sensor time-series signals</data>
  <data key="d1">2307421743935657923</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9507359/</data>
  <data key="d3">Unsupervised deep anomaly detection for multi-sensor time-series signals</data>
  <data key="d4">Y Zhang, Y Chen, J Wang, Z Pan</data>
  <data key="d5">2021</data>
  <data key="d6">82</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2307421743935657923&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="678143692114235571">
  <data key="d0">Does your dermatology classifier know what it doesn't know? detecting the long-tail of unseen conditions</data>
  <data key="d1">678143692114235571</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841521003194</data>
  <data key="d3">Does your dermatology classifier know what it doesn't know? detecting the long-tail of unseen conditions</data>
  <data key="d4">AG Roy, J Ren, S Azizi, A Loh, V Natarajan…</data>
  <data key="d5">2022</data>
  <data key="d6">80</data>
  <data key="d7">https://scholar.google.com/scholar?cites=678143692114235571&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="2021435255736482892">
  <data key="d0">Autoencoders for unsupervised anomaly detection in high energy physics</data>
  <data key="d1">2021435255736482892</data>
  <data key="d2">https://link.springer.com/article/10.1007/JHEP06(2021)161</data>
  <data key="d3">Autoencoders for unsupervised anomaly detection in high energy physics</data>
  <data key="d4">T Finke, M Krämer, A Morandini, A Mück…</data>
  <data key="d5">2021</data>
  <data key="d6">67</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2021435255736482892&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="10864892511232457463">
  <data key="d0">Demystifying mlops and presenting a recipe for the selection of open-source tools</data>
  <data key="d1">10864892511232457463</data>
  <data key="d2">https://www.mdpi.com/2076-3417/11/19/8861</data>
  <data key="d3">Demystifying mlops and presenting a recipe for the selection of open-source tools</data>
  <data key="d4">P Ruf, M Madan, C Reich, D Ould-Abdeslam</data>
  <data key="d5">2021</data>
  <data key="d6">59</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10864892511232457463&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="9579459676985496387">
  <data key="d0">Deep learning for unsupervised anomaly localization in industrial images: A survey</data>
  <data key="d1">9579459676985496387</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9849507/</data>
  <data key="d3">Deep learning for unsupervised anomaly localization in industrial images: A survey</data>
  <data key="d4">X Tao, X Gong, X Zhang, S Yan…</data>
  <data key="d5">2022</data>
  <data key="d6">45</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9579459676985496387&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="8190770042431966238">
  <data key="d0">Toward explainable artificial intelligence for regression models: A methodological perspective</data>
  <data key="d1">8190770042431966238</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9810062/</data>
  <data key="d3">Toward explainable artificial intelligence for regression models: A methodological perspective</data>
  <data key="d4">S Letzgus, P Wagner, J Lederer…</data>
  <data key="d5">2022</data>
  <data key="d6">46</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8190770042431966238&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="5768724617166869747">
  <data key="d0">Anomaly detection-inspired few-shot medical image segmentation through self-supervision with supervoxels</data>
  <data key="d1">5768724617166869747</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841522000378</data>
  <data key="d3">Anomaly detection-inspired few-shot medical image segmentation through self-supervision with supervoxels</data>
  <data key="d4">S Hansen, S Gautam, R Jenssen…</data>
  <data key="d5">2022</data>
  <data key="d6">36</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5768724617166869747&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="6162716714810201955">
  <data key="d0">Task-relevant failure detection for trajectory predictors in autonomous vehicles</data>
  <data key="d1">6162716714810201955</data>
  <data key="d2">https://proceedings.mlr.press/v205/farid23a.html</data>
  <data key="d3">Task-relevant failure detection for trajectory predictors in autonomous vehicles</data>
  <data key="d4">A Farid, S Veer, B Ivanovic, K Leung…</data>
  <data key="d5">2023</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6162716714810201955&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="13078428699608452477">
  <data key="d0">Out-of-scope intent detection with self-supervision and discriminative training</data>
  <data key="d1">13078428699608452477</data>
  <data key="d2">https://arxiv.org/abs/2106.08616</data>
  <data key="d3">Out-of-scope intent detection with self-supervision and discriminative training</data>
  <data key="d4">LM Zhan, H Liang, B Liu, L Fan, XM Wu…</data>
  <data key="d5">2021</data>
  <data key="d6">48</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13078428699608452477&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="3308207458495141249">
  <data key="d0">Truthful AI: Developing and governing AI that does not lie</data>
  <data key="d1">3308207458495141249</data>
  <data key="d2">https://arxiv.org/abs/2110.06674</data>
  <data key="d3">Truthful AI: Developing and governing AI that does not lie</data>
  <data key="d4">O Evans, O Cotton-Barratt, L Finnveden…</data>
  <data key="d5">2021</data>
  <data key="d6">48</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3308207458495141249&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="15849255232371913233">
  <data key="d0">Fedaux: Leveraging unlabeled auxiliary data in federated learning</data>
  <data key="d1">15849255232371913233</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9632275/</data>
  <data key="d3">Fedaux: Leveraging unlabeled auxiliary data in federated learning</data>
  <data key="d4">F Sattler, T Korjakow, R Rischke…</data>
  <data key="d5">2021</data>
  <data key="d6">60</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15849255232371913233&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="2231389869816478973">
  <data key="d0">Towards robust explanations for deep neural networks</data>
  <data key="d1">2231389869816478973</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0031320321003769</data>
  <data key="d3">Towards robust explanations for deep neural networks</data>
  <data key="d4">AK Dombrowski, CJ Anders, KR Müller, P Kessel</data>
  <data key="d5">2022</data>
  <data key="d6">48</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2231389869816478973&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="8582043463264170613">
  <data key="d0">Training ood detectors in their natural habitats</data>
  <data key="d1">8582043463264170613</data>
  <data key="d2">https://proceedings.mlr.press/v162/katz-samuels22a.html</data>
  <data key="d3">Training ood detectors in their natural habitats</data>
  <data key="d4">J Katz-Samuels, JB Nakhleh…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8582043463264170613&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="214390233068253722">
  <data key="d0">A hierarchical transformation-discriminating generative model for few shot anomaly detection</data>
  <data key="d1">214390233068253722</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Sheynin_A_Hierarchical_Transformation-Discriminating_Generative_Model_for_Few_Shot_Anomaly_Detection_ICCV_2021_paper.html</data>
  <data key="d3">A hierarchical transformation-discriminating generative model for few shot anomaly detection</data>
  <data key="d4">S Sheynin, S Benaim, L Wolf</data>
  <data key="d5">2021</data>
  <data key="d6">43</data>
  <data key="d7">https://scholar.google.com/scholar?cites=214390233068253722&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="7437266987569487782">
  <data key="d0">Condition monitoring of wind turbine blades based on self-supervised health representation learning: A conducive technique to effective and reliable utilization of wind …</data>
  <data key="d1">7437266987569487782</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0306261922003105</data>
  <data key="d3">Condition monitoring of wind turbine blades based on self-supervised health representation learning: A conducive technique to effective and reliable utilization of wind …</data>
  <data key="d4">S Sun, T Wang, H Yang, F Chu</data>
  <data key="d5">2022</data>
  <data key="d6">24</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7437266987569487782&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="11875282308322336079">
  <data key="d0">Estimating example difficulty using variance of gradients</data>
  <data key="d1">11875282308322336079</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Agarwal_Estimating_Example_Difficulty_Using_Variance_of_Gradients_CVPR_2022_paper.html</data>
  <data key="d3">Estimating example difficulty using variance of gradients</data>
  <data key="d4">C Agarwal, D D'souza…</data>
  <data key="d5">2022</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11875282308322336079&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="3679566789459312121">
  <data key="d0">Latent outlier exposure for anomaly detection with contaminated data</data>
  <data key="d1">3679566789459312121</data>
  <data key="d2">https://proceedings.mlr.press/v162/qiu22b.html</data>
  <data key="d3">Latent outlier exposure for anomaly detection with contaminated data</data>
  <data key="d4">C Qiu, A Li, M Kloft, M Rudolph…</data>
  <data key="d5">2022</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3679566789459312121&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="14353003944710468963">
  <data key="d0">Industrial image anomaly localization based on Gaussian clustering of pretrained feature</data>
  <data key="d1">14353003944710468963</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9479740/</data>
  <data key="d3">Industrial image anomaly localization based on Gaussian clustering of pretrained feature</data>
  <data key="d4">Q Wan, L Gao, X Li, L Wen</data>
  <data key="d5">2021</data>
  <data key="d6">34</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14353003944710468963&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="14214777377381746715">
  <data key="d0">Hyperparameter sensitivity in deep outlier detection: Analysis and a scalable hyper-ensemble solution</data>
  <data key="d1">14214777377381746715</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/3e9113e2bc2e700baa7d765470f140e1-Abstract-Conference.html</data>
  <data key="d3">Hyperparameter sensitivity in deep outlier detection: Analysis and a scalable hyper-ensemble solution</data>
  <data key="d4">X Ding, L Zhao, L Akoglu</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14214777377381746715&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="15256074903033943732">
  <data key="d0">Software for dataset-wide XAI: from local explanations to global insights with Zennit, CoRelAy, and ViRelAy</data>
  <data key="d1">15256074903033943732</data>
  <data key="d2">https://arxiv.org/abs/2106.13200</data>
  <data key="d3">Software for dataset-wide XAI: from local explanations to global insights with Zennit, CoRelAy, and ViRelAy</data>
  <data key="d4">CJ Anders, D Neumann, W Samek, KR Müller…</data>
  <data key="d5">2021</data>
  <data key="d6">42</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15256074903033943732&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="1785343722922140905">
  <data key="d0">The familiarity hypothesis: Explaining the behavior of deep open set methods</data>
  <data key="d1">1785343722922140905</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0031320322004125</data>
  <data key="d3">The familiarity hypothesis: Explaining the behavior of deep open set methods</data>
  <data key="d4">TG Dietterich, A Guyer</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1785343722922140905&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="7248162955817269145">
  <data key="d0">Natural synthetic anomalies for self-supervised anomaly detection and localization</data>
  <data key="d1">7248162955817269145</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19821-2_27</data>
  <data key="d3">Natural synthetic anomalies for self-supervised anomaly detection and localization</data>
  <data key="d4">HM Schlüter, J Tan, B Hou, B Kainz</data>
  <data key="d5">2022</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7248162955817269145&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="12121551931273619933">
  <data key="d0">On the nature and types of anomalies: a review of deviations in data</data>
  <data key="d1">12121551931273619933</data>
  <data key="d2">https://link.springer.com/article/10.1007/s41060-021-00265-1</data>
  <data key="d3">On the nature and types of anomalies: a review of deviations in data</data>
  <data key="d4">R Foorthuis</data>
  <data key="d5">2021</data>
  <data key="d6">59</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12121551931273619933&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="10731212938719398416">
  <data key="d0">Focus your distribution: Coarse-to-fine non-contrastive learning for anomaly detection and localization</data>
  <data key="d1">10731212938719398416</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9859925/</data>
  <data key="d3">Focus your distribution: Coarse-to-fine non-contrastive learning for anomaly detection and localization</data>
  <data key="d4">Y Zheng, X Wang, R Deng, T Bao…</data>
  <data key="d5">2022</data>
  <data key="d6">34</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10731212938719398416&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="5272663799261882882">
  <data key="d0">TFAD: A decomposition time series anomaly detection architecture with time-frequency analysis</data>
  <data key="d1">5272663799261882882</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3511808.3557470</data>
  <data key="d3">TFAD: A decomposition time series anomaly detection architecture with time-frequency analysis</data>
  <data key="d4">C Zhang, T Zhou, Q Wen, L Sun</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5272663799261882882&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="14778056948504220863">
  <data key="d0">Deep isolation forest for anomaly detection</data>
  <data key="d1">14778056948504220863</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10108034/</data>
  <data key="d3">Deep isolation forest for anomaly detection</data>
  <data key="d4">H Xu, G Pang, Y Wang, Y Wang</data>
  <data key="d5">2023</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14778056948504220863&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="16527862481109793893">
  <data key="d0">Gaussian anomaly detection by modeling the distribution of normal data in pretrained deep features</data>
  <data key="d1">16527862481109793893</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9493210/</data>
  <data key="d3">Gaussian anomaly detection by modeling the distribution of normal data in pretrained deep features</data>
  <data key="d4">O Rippel, P Mertens, E König…</data>
  <data key="d5">2021</data>
  <data key="d6">36</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16527862481109793893&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="11326401675293682123">
  <data key="d0">Transfer-based semantic anomaly detection</data>
  <data key="d1">11326401675293682123</data>
  <data key="d2">https://proceedings.mlr.press/v139/deecke21a.html</data>
  <data key="d3">Transfer-based semantic anomaly detection</data>
  <data key="d4">L Deecke, L Ruff…</data>
  <data key="d5">2021</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11326401675293682123&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="5370677025711722393">
  <data key="d0">Anomaly detection for tabular data with internal contrastive learning</data>
  <data key="d1">5370677025711722393</data>
  <data key="d2">https://openreview.net/forum?id=_hszZbt46bT</data>
  <data key="d3">Anomaly detection for tabular data with internal contrastive learning</data>
  <data key="d4">T Shenkar, L Wolf</data>
  <data key="d5">2021</data>
  <data key="d6">37</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5370677025711722393&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="4398874471937260991">
  <data key="d0">Unsupervised image anomaly detection and segmentation based on pretrained feature mapping</data>
  <data key="d1">4398874471937260991</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9795121/</data>
  <data key="d3">Unsupervised image anomaly detection and segmentation based on pretrained feature mapping</data>
  <data key="d4">Q Wan, L Gao, X Li, L Wen</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4398874471937260991&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="6238566466640748238">
  <data key="d0">Perfect density models cannot guarantee anomaly detection</data>
  <data key="d1">6238566466640748238</data>
  <data key="d2">https://www.mdpi.com/1099-4300/23/12/1690</data>
  <data key="d3">Perfect density models cannot guarantee anomaly detection</data>
  <data key="d4">C Le Lan, L Dinh</data>
  <data key="d5">2021</data>
  <data key="d6">36</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6238566466640748238&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="10024501141005307911">
  <data key="d0">Neural contextual anomaly detection for time series</data>
  <data key="d1">10024501141005307911</data>
  <data key="d2">https://arxiv.org/abs/2107.07702</data>
  <data key="d3">Neural contextual anomaly detection for time series</data>
  <data key="d4">CU Carmona, FX Aubet, V Flunkert…</data>
  <data key="d5">2021</data>
  <data key="d6">39</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10024501141005307911&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="3567693488984063272">
  <data key="d0">Out-of-distribution detection and selective generation for conditional language models</data>
  <data key="d1">3567693488984063272</data>
  <data key="d2">https://arxiv.org/abs/2209.15558</data>
  <data key="d3">Out-of-distribution detection and selective generation for conditional language models</data>
  <data key="d4">J Ren, J Luo, Y Zhao, K Krishna, M Saleh…</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3567693488984063272&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="15317793119344835263">
  <data key="d0">A survey on unsupervised industrial anomaly detection algorithms</data>
  <data key="d1">15317793119344835263</data>
  <data key="d2">https://arxiv.org/abs/2204.11161</data>
  <data key="d3">A survey on unsupervised industrial anomaly detection algorithms</data>
  <data key="d4">Y Cui, Z Liu, S Lian</data>
  <data key="d5">2022</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15317793119344835263&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="17363381983025647699">
  <data key="d0">Low-altitude aerial video surveillance via one-class SVM anomaly detection from textural features in UAV images</data>
  <data key="d1">17363381983025647699</data>
  <data key="d2">https://www.mdpi.com/2078-2489/13/1/2</data>
  <data key="d3">Low-altitude aerial video surveillance via one-class SVM anomaly detection from textural features in UAV images</data>
  <data key="d4">D Avola, L Cinque, A Di Mambro, A Diko, A Fagioli…</data>
  <data key="d5">2021</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17363381983025647699&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="9631069152835975454">
  <data key="d0">Self-supervised anomaly detection: A survey and outlook</data>
  <data key="d1">9631069152835975454</data>
  <data key="d2">https://arxiv.org/abs/2205.05173</data>
  <data key="d3">Self-supervised anomaly detection: A survey and outlook</data>
  <data key="d4">H Hojjati, TKK Ho, N Armanfard</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9631069152835975454&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="8772844806290312927">
  <data key="d0">Confidence-based out-of-distribution detection: a comparative study and analysis</data>
  <data key="d1">8772844806290312927</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-87735-4_12</data>
  <data key="d3">Confidence-based out-of-distribution detection: a comparative study and analysis</data>
  <data key="d4">C Berger, M Paschali, B Glocker…</data>
  <data key="d5">2021</data>
  <data key="d6">33</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8772844806290312927&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="16849488672761264795">
  <data key="d0">Task-driven out-of-distribution detection with statistical guarantees for robot learning</data>
  <data key="d1">16849488672761264795</data>
  <data key="d2">https://proceedings.mlr.press/v164/farid22a.html</data>
  <data key="d3">Task-driven out-of-distribution detection with statistical guarantees for robot learning</data>
  <data key="d4">A Farid, S Veer, A Majumdar</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16849488672761264795&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="13607047470867432902">
  <data key="d0">Deep convolutional clustering-based time series anomaly detection</data>
  <data key="d1">13607047470867432902</data>
  <data key="d2">https://www.mdpi.com/1424-8220/21/16/5488</data>
  <data key="d3">Deep convolutional clustering-based time series anomaly detection</data>
  <data key="d4">GS Chadha, I Islam, A Schwung, SX Ding</data>
  <data key="d5">2021</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13607047470867432902&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="10611118271257578362">
  <data key="d0">A call to reflect on evaluation practices for failure detection in image classification</data>
  <data key="d1">10611118271257578362</data>
  <data key="d2">https://arxiv.org/abs/2211.15259</data>
  <data key="d3">A call to reflect on evaluation practices for failure detection in image classification</data>
  <data key="d4">PF Jaeger, CT Lüth, L Klein, TJ Bungert</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10611118271257578362&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="17712406442879515542">
  <data key="d0">Admoe: Anomaly detection with mixture-of-experts from noisy labels</data>
  <data key="d1">17712406442879515542</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/25620</data>
  <data key="d3">Admoe: Anomaly detection with mixture-of-experts from noisy labels</data>
  <data key="d4">Y Zhao, G Zheng, S Mukherjee, R McCann…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17712406442879515542&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="14403707945550137996">
  <data key="d0">Generative probabilistic novelty detection with isometric adversarial autoencoders</data>
  <data key="d1">14403707945550137996</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2022W/WiCV/html/Almohsen_Generative_Probabilistic_Novelty_Detection_With_Isometric_Adversarial_Autoencoders_CVPRW_2022_paper.html</data>
  <data key="d3">Generative probabilistic novelty detection with isometric adversarial autoencoders</data>
  <data key="d4">R Almohsen, MR Keaton…</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14403707945550137996&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="17993515150229039530">
  <data key="d0">Run-time monitoring of machine learning for robotic perception: A survey of emerging trends</data>
  <data key="d1">17993515150229039530</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9336665/</data>
  <data key="d3">Run-time monitoring of machine learning for robotic perception: A survey of emerging trends</data>
  <data key="d4">QM Rahman, P Corke, F Dayoub</data>
  <data key="d5">2021</data>
  <data key="d6">34</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17993515150229039530&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="15191081335956951903">
  <data key="d0">Prototypical residual networks for anomaly detection and localization</data>
  <data key="d1">15191081335956951903</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Prototypical_Residual_Networks_for_Anomaly_Detection_and_Localization_CVPR_2023_paper.html</data>
  <data key="d3">Prototypical residual networks for anomaly detection and localization</data>
  <data key="d4">H Zhang, Z Wu, Z Wang, Z Chen…</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15191081335956951903&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="3634646340176277230">
  <data key="d0">Unmanned aerial vehicle flight data anomaly detection and recovery prediction based on spatio-temporal correlation</data>
  <data key="d1">3634646340176277230</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9667119/</data>
  <data key="d3">Unmanned aerial vehicle flight data anomaly detection and recovery prediction based on spatio-temporal correlation</data>
  <data key="d4">J Zhong, Y Zhang, J Wang, C Luo…</data>
  <data key="d5">2021</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3634646340176277230&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="11311640397140273257">
  <data key="d0">Time series anomaly detection for smart grids: A survey</data>
  <data key="d1">11311640397140273257</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9621752/</data>
  <data key="d3">Time series anomaly detection for smart grids: A survey</data>
  <data key="d4">JE Zhang, D Wu, B Boulet</data>
  <data key="d5">2021</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11311640397140273257&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="2993359408594724286">
  <data key="d0">Zero-shot versus many-shot: Unsupervised texture anomaly detection</data>
  <data key="d1">2993359408594724286</data>
  <data key="d2">https://openaccess.thecvf.com/content/WACV2023/html/Aota_Zero-Shot_Versus_Many-Shot_Unsupervised_Texture_Anomaly_Detection_WACV_2023_paper.html</data>
  <data key="d3">Zero-shot versus many-shot: Unsupervised texture anomaly detection</data>
  <data key="d4">T Aota, LTT Tong, T Okatani</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2993359408594724286&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="15918548619868169194">
  <data key="d0">Using binary classifiers for one-class classification</data>
  <data key="d1">15918548619868169194</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417421012744</data>
  <data key="d3">Using binary classifiers for one-class classification</data>
  <data key="d4">S Kang</data>
  <data key="d5">2022</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15918548619868169194&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="490049363149100661">
  <data key="d0">Self-supervised learning for anomaly detection with dynamic local augmentation</data>
  <data key="d1">490049363149100661</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9597511/</data>
  <data key="d3">Self-supervised learning for anomaly detection with dynamic local augmentation</data>
  <data key="d4">S Yoa, S Lee, C Kim, HJ Kim</data>
  <data key="d5">2021</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=490049363149100661&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="13496204946614385428">
  <data key="d0">Pytorch-ood: A library for out-of-distribution detection based on pytorch</data>
  <data key="d1">13496204946614385428</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2022W/HCIS/html/Kirchheim_PyTorch-OOD_A_Library_for_Out-of-Distribution_Detection_Based_on_PyTorch_CVPRW_2022_paper.html</data>
  <data key="d3">Pytorch-ood: A library for out-of-distribution detection based on pytorch</data>
  <data key="d4">K Kirchheim, M Filax, F Ortmeier</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13496204946614385428&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="1952208765616162210">
  <data key="d0">Next-gen intelligent situational awareness systems for maritime surveillance and autonomous navigation</data>
  <data key="d1">1952208765616162210</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9921394/</data>
  <data key="d3">Next-gen intelligent situational awareness systems for maritime surveillance and autonomous navigation</data>
  <data key="d4">N Forti, E d'Afflisio, P Braca, LM Millefiori…</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1952208765616162210&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="2240462269536330980">
  <data key="d0">Full graph autoencoder for one-class group anomaly detection of IIoT system</data>
  <data key="d1">2240462269536330980</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9792242/</data>
  <data key="d3">Full graph autoencoder for one-class group anomaly detection of IIoT system</data>
  <data key="d4">Y Feng, J Chen, Z Liu, H Lv…</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2240462269536330980&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="10332745316504571260">
  <data key="d0">Deep generative model with hierarchical latent factors for time series anomaly detection</data>
  <data key="d1">10332745316504571260</data>
  <data key="d2">https://proceedings.mlr.press/v151/challu22a.html</data>
  <data key="d3">Deep generative model with hierarchical latent factors for time series anomaly detection</data>
  <data key="d4">CI Challu, P Jiang, YN Wu…</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10332745316504571260&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="4974430546160529512">
  <data key="d0">Graph anomaly detection with unsupervised GNNs</data>
  <data key="d1">4974430546160529512</data>
  <data key="d2">https://arxiv.org/abs/2210.09535</data>
  <data key="d3">Graph anomaly detection with unsupervised GNNs</data>
  <data key="d4">L Zhao, S Sawlani, A Srinivasan, L Akoglu</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4974430546160529512&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="15589620931717671311">
  <data key="d0">Exposing outlier exposure: What can be learned from few, one, and zero outlier images</data>
  <data key="d1">15589620931717671311</data>
  <data key="d2">https://arxiv.org/abs/2205.11474</data>
  <data key="d3">Exposing outlier exposure: What can be learned from few, one, and zero outlier images</data>
  <data key="d4">P Liznerski, L Ruff, RA Vandermeulen…</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15589620931717671311&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="3454373283041760880">
  <data key="d0">A survey on explainable anomaly detection</data>
  <data key="d1">3454373283041760880</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3609333</data>
  <data key="d3">A survey on explainable anomaly detection</data>
  <data key="d4">Z Li, Y Zhu, M Van Leeuwen</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3454373283041760880&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="16811817108630720533">
  <data key="d0">Review on deep learning approaches for anomaly event detection in video surveillance</data>
  <data key="d1">16811817108630720533</data>
  <data key="d2">https://www.mdpi.com/2079-9292/12/1/29</data>
  <data key="d3">Review on deep learning approaches for anomaly event detection in video surveillance</data>
  <data key="d4">SA Jebur, KA Hussein, HK Hoomod, L Alzubaidi…</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16811817108630720533&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="15963090810315431769">
  <data key="d0">Semi-Supervised Anomaly Detection via Neural Process</data>
  <data key="d1">15963090810315431769</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10102264/</data>
  <data key="d3">Semi-Supervised Anomaly Detection via Neural Process</data>
  <data key="d4">F Zhou, G Wang, K Zhang, S Liu…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15963090810315431769&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="15253833027531638311">
  <data key="d0">Raising the bar in graph-level anomaly detection</data>
  <data key="d1">15253833027531638311</data>
  <data key="d2">https://arxiv.org/abs/2205.13845</data>
  <data key="d3">Raising the bar in graph-level anomaly detection</data>
  <data key="d4">C Qiu, M Kloft, S Mandt, M Rudolph</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15253833027531638311&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="17045973692536155067">
  <data key="d0">Inline nondestructive internal disorder detection in pear fruit using explainable deep anomaly detection on X-ray images</data>
  <data key="d1">17045973692536155067</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169922002794</data>
  <data key="d3">Inline nondestructive internal disorder detection in pear fruit using explainable deep anomaly detection on X-ray images</data>
  <data key="d4">T Van De Looverbosch, J He, A Tempelaere…</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17045973692536155067&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="13346469121565650680">
  <data key="d0">Object detection with deep learning: A review</data>
  <data key="d1">13346469121565650680</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/8627998/</data>
  <data key="d3">Object detection with deep learning: A review</data>
  <data key="d4">ZQ Zhao, P Zheng, S Xu, X Wu</data>
  <data key="d5">2019</data>
  <data key="d6">4416</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13346469121565650680&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="8847225001696225074">
  <data key="d0">A survey on deep learning and its applications</data>
  <data key="d1">8847225001696225074</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1574013721000198</data>
  <data key="d3">A survey on deep learning and its applications</data>
  <data key="d4">S Dong, P Wang, K Abbas</data>
  <data key="d5">2021</data>
  <data key="d6">529</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8847225001696225074&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="17325452893759607893">
  <data key="d0">A survey of deep learning techniques for autonomous driving</data>
  <data key="d1">17325452893759607893</data>
  <data key="d2">https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21918</data>
  <data key="d3">A survey of deep learning techniques for autonomous driving</data>
  <data key="d4">S Grigorescu, B Trasnea, T Cocias…</data>
  <data key="d5">2020</data>
  <data key="d6">1198</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17325452893759607893&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="9531339919500160456">
  <data key="d0">Study on artificial intelligence: The state of the art and future prospects</data>
  <data key="d1">9531339919500160456</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2452414X21000248</data>
  <data key="d3">Study on artificial intelligence: The state of the art and future prospects</data>
  <data key="d4">C Zhang, Y Lu</data>
  <data key="d5">2021</data>
  <data key="d6">395</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9531339919500160456&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="7658252996672799137">
  <data key="d0">A survey on performance metrics for object-detection algorithms</data>
  <data key="d1">7658252996672799137</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9145130/</data>
  <data key="d3">A survey on performance metrics for object-detection algorithms</data>
  <data key="d4">R Padilla, SL Netto, EAB Da Silva</data>
  <data key="d5">2020</data>
  <data key="d6">739</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7658252996672799137&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2617964736190327137">
  <data key="d0">Dive into deep learning</data>
  <data key="d1">2617964736190327137</data>
  <data key="d2">https://arxiv.org/abs/2106.11342</data>
  <data key="d3">Dive into deep learning</data>
  <data key="d4">A Zhang, ZC Lipton, M Li, AJ Smola</data>
  <data key="d5">2021</data>
  <data key="d6">867</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2617964736190327137&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="11317456985294514291">
  <data key="d0">A Survey of Autonomous Driving: Common Practices and Emerging Technologies</data>
  <data key="d1">11317456985294514291</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9046805/</data>
  <data key="d3">A Survey of Autonomous Driving: Common Practices and Emerging Technologies</data>
  <data key="d4">E Yurtsever, J Lambert, A Carballo, K Takeda</data>
  <data key="d5">2020</data>
  <data key="d6">1123</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11317456985294514291&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="1629697025352584075">
  <data key="d0">Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: A mini-review, two showcases and beyond</data>
  <data key="d1">1629697025352584075</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1566253521001597</data>
  <data key="d3">Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: A mini-review, two showcases and beyond</data>
  <data key="d4">G Yang, Q Ye, J Xia</data>
  <data key="d5">2022</data>
  <data key="d6">285</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1629697025352584075&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="3387494033233397434">
  <data key="d0">Application of deep learning algorithms in geotechnical engineering: a short critical review</data>
  <data key="d1">3387494033233397434</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10462-021-09967-1</data>
  <data key="d3">Application of deep learning algorithms in geotechnical engineering: a short critical review</data>
  <data key="d4">W Zhang, H Li, Y Li, H Liu, Y Chen, X Ding</data>
  <data key="d5">2021</data>
  <data key="d6">238</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3387494033233397434&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9453884244192340827">
  <data key="d0">Gliding vertex on the horizontal bounding box for multi-oriented object detection</data>
  <data key="d1">9453884244192340827</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9001201/</data>
  <data key="d3">Gliding vertex on the horizontal bounding box for multi-oriented object detection</data>
  <data key="d4">Y Xu, M Fu, Q Wang, Y Wang, K Chen…</data>
  <data key="d5">2020</data>
  <data key="d6">443</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9453884244192340827&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9105258566287239045">
  <data key="d0">Deep learning for chest X-ray analysis: A survey</data>
  <data key="d1">9105258566287239045</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841521001717</data>
  <data key="d3">Deep learning for chest X-ray analysis: A survey</data>
  <data key="d4">E Çallı, E Sogancioglu, B van Ginneken…</data>
  <data key="d5">2021</data>
  <data key="d6">214</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9105258566287239045&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="14930542330832755729">
  <data key="d0">Camouflaged object detection</data>
  <data key="d1">14930542330832755729</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Camouflaged_Object_Detection_CVPR_2020_paper.html</data>
  <data key="d3">Camouflaged object detection</data>
  <data key="d4">DP Fan, GP Ji, G Sun, MM Cheng…</data>
  <data key="d5">2020</data>
  <data key="d6">339</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14930542330832755729&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="14554488295084172370">
  <data key="d0">Applications of deep learning in stock market prediction: recent progress</data>
  <data key="d1">14554488295084172370</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417421009441</data>
  <data key="d3">Applications of deep learning in stock market prediction: recent progress</data>
  <data key="d4">W Jiang</data>
  <data key="d5">2021</data>
  <data key="d6">327</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14554488295084172370&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="212018708743960174">
  <data key="d0">First return, then explore</data>
  <data key="d1">212018708743960174</data>
  <data key="d2">https://www.nature.com/articles/s41586-020-03157-9)</data>
  <data key="d3">First return, then explore</data>
  <data key="d4">A Ecoffet, J Huizinga, J Lehman, KO Stanley, J Clune</data>
  <data key="d5">2021</data>
  <data key="d6">277</data>
  <data key="d7">https://scholar.google.com/scholar?cites=212018708743960174&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="17642928594905005472">
  <data key="d0">Recent advances in small object detection based on deep learning: A review</data>
  <data key="d1">17642928594905005472</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0262885620300421</data>
  <data key="d3">Recent advances in small object detection based on deep learning: A review</data>
  <data key="d4">K Tong, Y Wu, F Zhou</data>
  <data key="d5">2020</data>
  <data key="d6">319</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17642928594905005472&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="17449731775022216088">
  <data key="d0">Concealed object detection</data>
  <data key="d1">17449731775022216088</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9444794/</data>
  <data key="d3">Concealed object detection</data>
  <data key="d4">DP Fan, GP Ji, MM Cheng…</data>
  <data key="d5">2021</data>
  <data key="d6">204</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17449731775022216088&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="14081415941935534928">
  <data key="d0">A deep collocation method for the bending analysis of Kirchhoff plate</data>
  <data key="d1">14081415941935534928</data>
  <data key="d2">https://arxiv.org/abs/2102.02617</data>
  <data key="d3">A deep collocation method for the bending analysis of Kirchhoff plate</data>
  <data key="d4">H Guo, X Zhuang, T Rabczuk</data>
  <data key="d5">2021</data>
  <data key="d6">389</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14081415941935534928&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="12435210238474843309">
  <data key="d0">YOLO v3-Tiny: Object Detection and Recognition using one stage improved model</data>
  <data key="d1">12435210238474843309</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9074315/</data>
  <data key="d3">YOLO v3-Tiny: Object Detection and Recognition using one stage improved model</data>
  <data key="d4">P Adarsh, P Rathi, M Kumar</data>
  <data key="d5">2020</data>
  <data key="d6">339</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12435210238474843309&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="13748943285480844265">
  <data key="d0">Face mask detection using transfer learning of inceptionv3</data>
  <data key="d1">13748943285480844265</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-66665-1_6</data>
  <data key="d3">Face mask detection using transfer learning of inceptionv3</data>
  <data key="d4">G Jignesh Chowdary, NS Punn, SK Sonbhadra…</data>
  <data key="d5">2020</data>
  <data key="d6">267</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13748943285480844265&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="10547178639330987288">
  <data key="d0">Monitoring COVID-19 social distancing with person detection and tracking via fine-tuned YOLO v3 and Deepsort techniques</data>
  <data key="d1">10547178639330987288</data>
  <data key="d2">https://arxiv.org/abs/2005.01385</data>
  <data key="d3">Monitoring COVID-19 social distancing with person detection and tracking via fine-tuned YOLO v3 and Deepsort techniques</data>
  <data key="d4">NS Punn, SK Sonbhadra, S Agarwal, G Rai</data>
  <data key="d5">2020</data>
  <data key="d6">284</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10547178639330987288&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="16888371616033308169">
  <data key="d0">Deep learning for lidar point clouds in autonomous driving: A review</data>
  <data key="d1">16888371616033308169</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9173706/</data>
  <data key="d3">Deep learning for lidar point clouds in autonomous driving: A review</data>
  <data key="d4">Y Li, L Ma, Z Zhong, F Liu…</data>
  <data key="d5">2020</data>
  <data key="d6">273</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16888371616033308169&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="18079286179474516945">
  <data key="d0">Deep learning for safe autonomous driving: Current challenges and future directions</data>
  <data key="d1">18079286179474516945</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9284628/</data>
  <data key="d3">Deep learning for safe autonomous driving: Current challenges and future directions</data>
  <data key="d4">K Muhammad, A Ullah, J Lloret…</data>
  <data key="d5">2020</data>
  <data key="d6">212</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18079286179474516945&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="16601259166110264431">
  <data key="d0">Object detection and image segmentation with deep learning on earth observation data: A review-part i: Evolution and recent trends</data>
  <data key="d1">16601259166110264431</data>
  <data key="d2">https://www.mdpi.com/2072-4292/12/10/1667</data>
  <data key="d3">Object detection and image segmentation with deep learning on earth observation data: A review-part i: Evolution and recent trends</data>
  <data key="d4">T Hoeser, C Kuenzer</data>
  <data key="d5">2020</data>
  <data key="d6">249</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16601259166110264431&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="7566971192202831904">
  <data key="d0">A review of object detection based on deep learning</data>
  <data key="d1">7566971192202831904</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11042-020-08976-6</data>
  <data key="d3">A review of object detection based on deep learning</data>
  <data key="d4">Y Xiao, Z Tian, J Yu, Y Zhang, S Liu, S Du…</data>
  <data key="d5">2020</data>
  <data key="d6">261</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7566971192202831904&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2597379987318853103">
  <data key="d0">Automatic crack classification and segmentation on masonry surfaces using convolutional neural networks and transfer learning</data>
  <data key="d1">2597379987318853103</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0926580521000571</data>
  <data key="d3">Automatic crack classification and segmentation on masonry surfaces using convolutional neural networks and transfer learning</data>
  <data key="d4">D Dais, IE Bal, E Smyrou, V Sarhosis</data>
  <data key="d5">2021</data>
  <data key="d6">156</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2597379987318853103&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="15507377305223603161">
  <data key="d0">RGB-D salient object detection: A survey</data>
  <data key="d1">15507377305223603161</data>
  <data key="d2">https://link.springer.com/article/10.1007/s41095-020-0199-z</data>
  <data key="d3">RGB-D salient object detection: A survey</data>
  <data key="d4">T Zhou, DP Fan, MM Cheng, J Shen, L Shao</data>
  <data key="d5">2021</data>
  <data key="d6">200</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15507377305223603161&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="11504531406673088307">
  <data key="d0">Prediction of groundwater quality using efficient machine learning technique</data>
  <data key="d1">11504531406673088307</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0045653521007347</data>
  <data key="d3">Prediction of groundwater quality using efficient machine learning technique</data>
  <data key="d4">S Singha, S Pasupuleti, SS Singha, R Singh, S Kumar</data>
  <data key="d5">2021</data>
  <data key="d6">129</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11504531406673088307&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="16324732045488656995">
  <data key="d0">Monocular depth estimation based on deep learning: An overview</data>
  <data key="d1">16324732045488656995</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11431-020-1582-8</data>
  <data key="d3">Monocular depth estimation based on deep learning: An overview</data>
  <data key="d4">C Zhao, Q Sun, C Zhang, Y Tang, F Qian</data>
  <data key="d5">2020</data>
  <data key="d6">244</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16324732045488656995&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="12342618065285300259">
  <data key="d0">Deep autoencoder based energy method for the bending, vibration, and buckling analysis of Kirchhoff plates with transfer learning</data>
  <data key="d1">12342618065285300259</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S099775382100019X</data>
  <data key="d3">Deep autoencoder based energy method for the bending, vibration, and buckling analysis of Kirchhoff plates with transfer learning</data>
  <data key="d4">X Zhuang, H Guo, N Alajlan, H Zhu…</data>
  <data key="d5">2021</data>
  <data key="d6">172</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12342618065285300259&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="12928939764513613154">
  <data key="d0">The impact of pre-and post-image processing techniques on deep learning frameworks: A comprehensive review for digital pathology image analysis</data>
  <data key="d1">12928939764513613154</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0010482520304601</data>
  <data key="d3">The impact of pre-and post-image processing techniques on deep learning frameworks: A comprehensive review for digital pathology image analysis</data>
  <data key="d4">M Salvi, UR Acharya, F Molinari…</data>
  <data key="d5">2021</data>
  <data key="d6">159</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12928939764513613154&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="5360881814442745026">
  <data key="d0">Deep learning for change detection in remote sensing images: Comprehensive review and meta-analysis</data>
  <data key="d1">5360881814442745026</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9136674/</data>
  <data key="d3">Deep learning for change detection in remote sensing images: Comprehensive review and meta-analysis</data>
  <data key="d4">L Khelifi, M Mignotte</data>
  <data key="d5">2020</data>
  <data key="d6">205</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5360881814442745026&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="4077468826256393449">
  <data key="d0">A brief review of domain adaptation</data>
  <data key="d1">4077468826256393449</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-71704-9_65</data>
  <data key="d3">A brief review of domain adaptation</data>
  <data key="d4">A Farahani, S Voghoei, K Rasheed…</data>
  <data key="d5">2021</data>
  <data key="d6">218</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4077468826256393449&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="3796249632159372698">
  <data key="d0">Deep learning in multi-object detection and tracking: state of the art</data>
  <data key="d1">3796249632159372698</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10489-021-02293-7</data>
  <data key="d3">Deep learning in multi-object detection and tracking: state of the art</data>
  <data key="d4">SK Pal, A Pramanik, J Maiti, P Mitra</data>
  <data key="d5">2021</data>
  <data key="d6">144</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3796249632159372698&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="6905939594294613143">
  <data key="d0">Automatic detection method of tunnel lining multi‐defects via an enhanced You Only Look Once network</data>
  <data key="d1">6905939594294613143</data>
  <data key="d2">https://onlinelibrary.wiley.com/doi/abs/10.1111/mice.12836</data>
  <data key="d3">Automatic detection method of tunnel lining multi‐defects via an enhanced You Only Look Once network</data>
  <data key="d4">Z Zhou, J Zhang, C Gong</data>
  <data key="d5">2022</data>
  <data key="d6">72</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6905939594294613143&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2142612586528747387">
  <data key="d0">Deep learning in ECG diagnosis: A review</data>
  <data key="d1">2142612586528747387</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0950705121004494</data>
  <data key="d3">Deep learning in ECG diagnosis: A review</data>
  <data key="d4">X Liu, H Wang, Z Li, L Qin</data>
  <data key="d5">2021</data>
  <data key="d6">117</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2142612586528747387&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9106423587240821276">
  <data key="d0">Salient object detection: A survey</data>
  <data key="d1">9106423587240821276</data>
  <data key="d2">https://link.springer.com/article/10.1007/s41095-019-0149-9</data>
  <data key="d3">Salient object detection: A survey</data>
  <data key="d4">A Borji, MM Cheng, Q Hou, H Jiang, J Li</data>
  <data key="d5">2019</data>
  <data key="d6">868</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9106423587240821276&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2983606601337837743">
  <data key="d0">Tectonic evolution and paleoposition of the Baoshan and Lincang blocks of West Yunnan during the Paleozoic</data>
  <data key="d1">2983606601337837743</data>
  <data key="d2">https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019TC006028</data>
  <data key="d3">Tectonic evolution and paleoposition of the Baoshan and Lincang blocks of West Yunnan during the Paleozoic</data>
  <data key="d4">B Liu, T Peng, W Fan, G Zhao, J Gao, X Dong…</data>
  <data key="d5">2020</data>
  <data key="d6">217</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2983606601337837743&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="16360107983645858167">
  <data key="d0">Semi-supervised locality preserving dense graph neural network with ARMA filters and context-aware learning for hyperspectral image classification</data>
  <data key="d1">16360107983645858167</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9506991/</data>
  <data key="d3">Semi-supervised locality preserving dense graph neural network with ARMA filters and context-aware learning for hyperspectral image classification</data>
  <data key="d4">Y Ding, X Zhao, Z Zhang, W Cai…</data>
  <data key="d5">2021</data>
  <data key="d6">115</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16360107983645858167&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="4185948125671434541">
  <data key="d0">Visual-based defect detection and classification approaches for industrial applications—A survey</data>
  <data key="d1">4185948125671434541</data>
  <data key="d2">https://www.mdpi.com/1424-8220/20/5/1459</data>
  <data key="d3">Visual-based defect detection and classification approaches for industrial applications—A survey</data>
  <data key="d4">T Czimmermann, G Ciuti, M Milazzo, M Chiurazzi…</data>
  <data key="d5">2020</data>
  <data key="d6">216</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4185948125671434541&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="15373527129249864261">
  <data key="d0">Deep neural network concepts for background subtraction: A systematic review and comparative evaluation</data>
  <data key="d1">15373527129249864261</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0893608019301303</data>
  <data key="d3">Deep neural network concepts for background subtraction: A systematic review and comparative evaluation</data>
  <data key="d4">T Bouwmans, S Javed, M Sultana, SK Jung</data>
  <data key="d5">2019</data>
  <data key="d6">356</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15373527129249864261&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="3101580748817568884">
  <data key="d0">Multiple object tracking: A literature review</data>
  <data key="d1">3101580748817568884</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0004370220301958</data>
  <data key="d3">Multiple object tracking: A literature review</data>
  <data key="d4">W Luo, J Xing, A Milan, X Zhang, W Liu, TK Kim</data>
  <data key="d5">2021</data>
  <data key="d6">813</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3101580748817568884&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="6067204860221490122">
  <data key="d0">Fedvision: An online visual object detection platform powered by federated learning</data>
  <data key="d1">6067204860221490122</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/7021</data>
  <data key="d3">Fedvision: An online visual object detection platform powered by federated learning</data>
  <data key="d4">Y Liu, A Huang, Y Luo, H Huang, Y Liu…</data>
  <data key="d5">2020</data>
  <data key="d6">226</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6067204860221490122&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="10469230642894389844">
  <data key="d0">Deep learning meets SAR: Concepts, models, pitfalls, and perspectives</data>
  <data key="d1">10469230642894389844</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9351574/</data>
  <data key="d3">Deep learning meets SAR: Concepts, models, pitfalls, and perspectives</data>
  <data key="d4">XX Zhu, S Montazeri, M Ali, Y Hua…</data>
  <data key="d5">2021</data>
  <data key="d6">180</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10469230642894389844&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="11773732314594455999">
  <data key="d0">Convolutional neural networks for image-based high-throughput plant phenotyping: a review</data>
  <data key="d1">11773732314594455999</data>
  <data key="d2">https://spj.science.org/doi/full/10.34133/2020/4152816?adobe_mc=MCMID%3D13000078418609464879081490540568399952%7CMCORGID%3D242B6472541199F70A4C98A6%2540AdobeOrg%7CTS%3D1670976000</data>
  <data key="d3">Convolutional neural networks for image-based high-throughput plant phenotyping: a review</data>
  <data key="d4">Y Jiang, C Li</data>
  <data key="d5">2020</data>
  <data key="d6">182</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11773732314594455999&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="4370422915042112938">
  <data key="d0">Anomaly detection in univariate time-series: A survey on the state-of-the-art</data>
  <data key="d1">4370422915042112938</data>
  <data key="d2">https://arxiv.org/abs/2004.00433</data>
  <data key="d3">Anomaly detection in univariate time-series: A survey on the state-of-the-art</data>
  <data key="d4">M Braei, S Wagner</data>
  <data key="d5">2020</data>
  <data key="d6">189</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4370422915042112938&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9379156628635523145">
  <data key="d0">Object detection algorithm based on improved YOLOv3</data>
  <data key="d1">9379156628635523145</data>
  <data key="d2">https://www.mdpi.com/2079-9292/9/3/537</data>
  <data key="d3">Object detection algorithm based on improved YOLOv3</data>
  <data key="d4">L Zhao, S Li</data>
  <data key="d5">2020</data>
  <data key="d6">193</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9379156628635523145&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2488030068881724873">
  <data key="d0">Recent advances in artificial intelligence and machine learning for nonlinear relationship analysis and process control in drinking water treatment: A review</data>
  <data key="d1">2488030068881724873</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1385894720328011</data>
  <data key="d3">Recent advances in artificial intelligence and machine learning for nonlinear relationship analysis and process control in drinking water treatment: A review</data>
  <data key="d4">L Li, S Rong, R Wang, S Yu</data>
  <data key="d5">2021</data>
  <data key="d6">158</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2488030068881724873&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="5282584135767767517">
  <data key="d0">A review of deep learning with special emphasis on architectures, applications and recent trends</data>
  <data key="d1">5282584135767767517</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S095070512030071X</data>
  <data key="d3">A review of deep learning with special emphasis on architectures, applications and recent trends</data>
  <data key="d4">S Sengupta, S Basak, P Saikia, S Paul…</data>
  <data key="d5">2020</data>
  <data key="d6">306</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5282584135767767517&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="10632950128195096357">
  <data key="d0">Invisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving under physical-world attacks</data>
  <data key="d1">10632950128195096357</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9519442/</data>
  <data key="d3">Invisible for both camera and lidar: Security of multi-sensor fusion based perception in autonomous driving under physical-world attacks</data>
  <data key="d4">Y Cao, N Wang, C Xiao, D Yang, J Fang…</data>
  <data key="d5">2021</data>
  <data key="d6">111</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10632950128195096357&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="708621415733870473">
  <data key="d0">DS-CNN: A pre-trained Xception model based on depth-wise separable convolutional neural network for finger vein recognition</data>
  <data key="d1">708621415733870473</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417421015943</data>
  <data key="d3">DS-CNN: A pre-trained Xception model based on depth-wise separable convolutional neural network for finger vein recognition</data>
  <data key="d4">K Shaheed, A Mao, I Qureshi, M Kumar…</data>
  <data key="d5">2022</data>
  <data key="d6">72</data>
  <data key="d7">https://scholar.google.com/scholar?cites=708621415733870473&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9316980701966772208">
  <data key="d0">An evaluation of deep learning methods for small object detection</data>
  <data key="d1">9316980701966772208</data>
  <data key="d2">https://www.hindawi.com/journals/jece/2020/3189691/</data>
  <data key="d3">An evaluation of deep learning methods for small object detection</data>
  <data key="d4">ND Nguyen, T Do, TD Ngo, DD Le</data>
  <data key="d5">2020</data>
  <data key="d6">163</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9316980701966772208&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="7644063362073502300">
  <data key="d0">Mini-YOLOv3: real-time object detector for embedded applications</data>
  <data key="d1">7644063362073502300</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/8839032/</data>
  <data key="d3">Mini-YOLOv3: real-time object detector for embedded applications</data>
  <data key="d4">QC Mao, HM Sun, YB Liu, RS Jia</data>
  <data key="d5">2019</data>
  <data key="d6">204</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7644063362073502300&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="14163003192753029307">
  <data key="d0">Real-time face mask detection method based on YOLOv3</data>
  <data key="d1">14163003192753029307</data>
  <data key="d2">https://www.mdpi.com/2079-9292/10/7/837</data>
  <data key="d3">Real-time face mask detection method based on YOLOv3</data>
  <data key="d4">X Jiang, T Gao, Z Zhu, Y Zhao</data>
  <data key="d5">2021</data>
  <data key="d6">112</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14163003192753029307&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="18400177295485631651">
  <data key="d0">New generation deep learning for video object detection: A survey</data>
  <data key="d1">18400177295485631651</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9345705/</data>
  <data key="d3">New generation deep learning for video object detection: A survey</data>
  <data key="d4">L Jiao, R Zhang, F Liu, S Yang, B Hou…</data>
  <data key="d5">2021</data>
  <data key="d6">102</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18400177295485631651&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="1644345333320747257">
  <data key="d0">Ship detection in large-scale SAR images via spatial shuffle-group enhance attention</data>
  <data key="d1">1644345333320747257</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9106758/</data>
  <data key="d3">Ship detection in large-scale SAR images via spatial shuffle-group enhance attention</data>
  <data key="d4">Z Cui, X Wang, N Liu, Z Cao…</data>
  <data key="d5">2020</data>
  <data key="d6">147</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1644345333320747257&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="3224984904288274964">
  <data key="d0">Small-object detection in remote sensing images with end-to-end edge-enhanced GAN and object detector network</data>
  <data key="d1">3224984904288274964</data>
  <data key="d2">https://www.mdpi.com/2072-4292/12/9/1432</data>
  <data key="d3">Small-object detection in remote sensing images with end-to-end edge-enhanced GAN and object detector network</data>
  <data key="d4">J Rabbi, N Ray, M Schubert, S Chowdhury, D Chao</data>
  <data key="d5">2020</data>
  <data key="d6">168</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3224984904288274964&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="10691651773549756733">
  <data key="d0">Scop: Scientific control for reliable neural network pruning</data>
  <data key="d1">10691651773549756733</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2020/hash/7bcdf75ad237b8e02e301f4091fb6bc8-Abstract.html</data>
  <data key="d3">Scop: Scientific control for reliable neural network pruning</data>
  <data key="d4">Y Tang, Y Wang, Y Xu, D Tao, C Xu…</data>
  <data key="d5">2020</data>
  <data key="d6">116</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10691651773549756733&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="16945998805068988708">
  <data key="d0">Opportunities and challenges in explainable artificial intelligence (xai): A survey</data>
  <data key="d1">16945998805068988708</data>
  <data key="d2">https://arxiv.org/abs/2006.11371</data>
  <data key="d3">Opportunities and challenges in explainable artificial intelligence (xai): A survey</data>
  <data key="d4">A Das, P Rad</data>
  <data key="d5">2020</data>
  <data key="d6">538</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16945998805068988708&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="2507582518469208253">
  <data key="d0">Deep learning for object detection and scene perception in self-driving cars: Survey, challenges, and open issues</data>
  <data key="d1">2507582518469208253</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2590005621000059</data>
  <data key="d3">Deep learning for object detection and scene perception in self-driving cars: Survey, challenges, and open issues</data>
  <data key="d4">A Gupta, A Anpalagan, L Guan, AS Khwaja</data>
  <data key="d5">2021</data>
  <data key="d6">239</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2507582518469208253&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="15761127917409164495">
  <data key="d0">Machine learning and deep learning</data>
  <data key="d1">15761127917409164495</data>
  <data key="d2">https://link.springer.com/article/10.1007/s12525-021-00475-2</data>
  <data key="d3">Machine learning and deep learning</data>
  <data key="d4">C Janiesch, P Zschech, K Heinrich</data>
  <data key="d5">2021</data>
  <data key="d6">937</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15761127917409164495&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="11127330701624169778">
  <data key="d0">Learning to prompt for continual learning</data>
  <data key="d1">11127330701624169778</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Wang_Learning_To_Prompt_for_Continual_Learning_CVPR_2022_paper.html</data>
  <data key="d3">Learning to prompt for continual learning</data>
  <data key="d4">Z Wang, Z Zhang, CY Lee, H Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">185</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11127330701624169778&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="5823096581545580132">
  <data key="d0">Explainable deep learning: A field guide for the uninitiated</data>
  <data key="d1">5823096581545580132</data>
  <data key="d2">https://www.jair.org/index.php/jair/article/view/13200</data>
  <data key="d3">Explainable deep learning: A field guide for the uninitiated</data>
  <data key="d4">G Ras, N Xie, M Van Gerven, D Doran</data>
  <data key="d5">2022</data>
  <data key="d6">346</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5823096581545580132&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="14750169433894795213">
  <data key="d0">Intelligent driving intelligence test for autonomous vehicles with naturalistic and adversarial environment</data>
  <data key="d1">14750169433894795213</data>
  <data key="d2">https://www.nature.com/articles/s41467-021-21007-8</data>
  <data key="d3">Intelligent driving intelligence test for autonomous vehicles with naturalistic and adversarial environment</data>
  <data key="d4">S Feng, X Yan, H Sun, Y Feng, HX Liu</data>
  <data key="d5">2021</data>
  <data key="d6">179</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14750169433894795213&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="14799418040721278868">
  <data key="d0">Explainable artificial intelligence: objectives, stakeholders, and future research opportunities</data>
  <data key="d1">14799418040721278868</data>
  <data key="d2">https://www.tandfonline.com/doi/abs/10.1080/10580530.2020.1849465</data>
  <data key="d3">Explainable artificial intelligence: objectives, stakeholders, and future research opportunities</data>
  <data key="d4">C Meske, E Bunde, J Schneider…</data>
  <data key="d5">2022</data>
  <data key="d6">231</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14799418040721278868&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="11207367590828257735">
  <data key="d0">Automotive LiDAR technology: A survey</data>
  <data key="d1">11207367590828257735</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9455394/</data>
  <data key="d3">Automotive LiDAR technology: A survey</data>
  <data key="d4">R Roriz, J Cabral, T Gomes</data>
  <data key="d5">2021</data>
  <data key="d6">145</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11207367590828257735&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="16563837428337733383">
  <data key="d0">A survey on deep multimodal learning for computer vision: advances, trends, applications, and datasets</data>
  <data key="d1">16563837428337733383</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00371-021-02166-7</data>
  <data key="d3">A survey on deep multimodal learning for computer vision: advances, trends, applications, and datasets</data>
  <data key="d4">K Bayoudh, R Knani, F Hamdaoui, A Mtibaa</data>
  <data key="d5">2021</data>
  <data key="d6">138</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16563837428337733383&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="3429732511943586845">
  <data key="d0">Video summarization using deep neural networks: A survey</data>
  <data key="d1">3429732511943586845</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9594911/</data>
  <data key="d3">Video summarization using deep neural networks: A survey</data>
  <data key="d4">E Apostolidis, E Adamantidou, AI Metsai…</data>
  <data key="d5">2021</data>
  <data key="d6">137</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3429732511943586845&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="15938347331059363535">
  <data key="d0">Decision making of autonomous vehicles in lane change scenarios: Deep reinforcement learning approaches with risk awareness</data>
  <data key="d1">15938347331059363535</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0968090X21004411</data>
  <data key="d3">Decision making of autonomous vehicles in lane change scenarios: Deep reinforcement learning approaches with risk awareness</data>
  <data key="d4">G Li, Y Yang, S Li, X Qu, N Lyu, SE Li</data>
  <data key="d5">2022</data>
  <data key="d6">85</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15938347331059363535&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="11324300263159808676">
  <data key="d0">Risk assessment based collision avoidance decision-making for autonomous vehicles in multi-scenarios</data>
  <data key="d1">11324300263159808676</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0968090X20307257</data>
  <data key="d3">Risk assessment based collision avoidance decision-making for autonomous vehicles in multi-scenarios</data>
  <data key="d4">G Li, Y Yang, T Zhang, X Qu, D Cao, B Cheng…</data>
  <data key="d5">2021</data>
  <data key="d6">121</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11324300263159808676&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="2878363142274078846">
  <data key="d0">Milestones in autonomous driving and intelligent vehicles: Survey of surveys</data>
  <data key="d1">2878363142274078846</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9963987/</data>
  <data key="d3">Milestones in autonomous driving and intelligent vehicles: Survey of surveys</data>
  <data key="d4">L Chen, Y Li, C Huang, B Li, Y Xing…</data>
  <data key="d5">2022</data>
  <data key="d6">81</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2878363142274078846&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="10459154347245074313">
  <data key="d0">Quo vadis artificial intelligence?</data>
  <data key="d1">10459154347245074313</data>
  <data key="d2">https://link.springer.com/article/10.1007/s44163-022-00022-8</data>
  <data key="d3">Quo vadis artificial intelligence?</data>
  <data key="d4">Y Jiang, X Li, H Luo, S Yin, O Kaynak</data>
  <data key="d5">2022</data>
  <data key="d6">81</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10459154347245074313&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="6067064466586264123">
  <data key="d0">Semantics for robotic mapping, perception and interaction: A survey</data>
  <data key="d1">6067064466586264123</data>
  <data key="d2">https://www.nowpublishers.com/article/Details/ROB-059</data>
  <data key="d3">Semantics for robotic mapping, perception and interaction: A survey</data>
  <data key="d4">S Garg, N Sünderhauf, F Dayoub…</data>
  <data key="d5">2020</data>
  <data key="d6">102</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6067064466586264123&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="8114522470454201714">
  <data key="d0">6G for vehicle-to-everything (V2X) communications: Enabling technologies, challenges, and opportunities</data>
  <data key="d1">8114522470454201714</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9779322/</data>
  <data key="d3">6G for vehicle-to-everything (V2X) communications: Enabling technologies, challenges, and opportunities</data>
  <data key="d4">M Noor-A-Rahim, Z Liu, H Lee, MO Khyam…</data>
  <data key="d5">2022</data>
  <data key="d6">133</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8114522470454201714&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="1131610574501943405">
  <data key="d0">Theory of overparametrization in quantum neural networks</data>
  <data key="d1">1131610574501943405</data>
  <data key="d2">https://www.nature.com/articles/s43588-023-00467-6</data>
  <data key="d3">Theory of overparametrization in quantum neural networks</data>
  <data key="d4">M Larocca, N Ju, D García-Martín, PJ Coles…</data>
  <data key="d5">2023</data>
  <data key="d6">77</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1131610574501943405&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="7068343160162404376">
  <data key="d0">An updated survey of efficient hardware architectures for accelerating deep convolutional neural networks</data>
  <data key="d1">7068343160162404376</data>
  <data key="d2">https://www.mdpi.com/1999-5903/12/7/113</data>
  <data key="d3">An updated survey of efficient hardware architectures for accelerating deep convolutional neural networks</data>
  <data key="d4">M Capra, B Bussolino, A Marchisio, M Shafique…</data>
  <data key="d5">2020</data>
  <data key="d6">136</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7068343160162404376&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="17384033669757226875">
  <data key="d0">Machine learning technologies for secure vehicular communication in internet of vehicles: recent advances and applications</data>
  <data key="d1">17384033669757226875</data>
  <data key="d2">https://www.hindawi.com/journals/scn/2021/8868355/</data>
  <data key="d3">Machine learning technologies for secure vehicular communication in internet of vehicles: recent advances and applications</data>
  <data key="d4">ES Ali, MK Hasan, R Hassan, RA Saeed…</data>
  <data key="d5">2021</data>
  <data key="d6">101</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17384033669757226875&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="12604090720681450553">
  <data key="d0">A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt</data>
  <data key="d1">12604090720681450553</data>
  <data key="d2">https://arxiv.org/abs/2303.04226</data>
  <data key="d3">A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt</data>
  <data key="d4">Y Cao, S Li, Y Liu, Z Yan, Y Dai, PS Yu…</data>
  <data key="d5">2023</data>
  <data key="d6">94</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12604090720681450553&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="8617012105369696231">
  <data key="d0">Learning discriminative features by covering local geometric space for point cloud analysis</data>
  <data key="d1">8617012105369696231</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9762976/</data>
  <data key="d3">Learning discriminative features by covering local geometric space for point cloud analysis</data>
  <data key="d4">C Wang, X Ning, L Sun, L Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">56</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8617012105369696231&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="10783855747646875569">
  <data key="d0">Multifunctional optoelectronic synapse based on ferroelectric van der Waals heterostructure for emulating the entire human visual system</data>
  <data key="d1">10783855747646875569</data>
  <data key="d2">https://onlinelibrary.wiley.com/doi/abs/10.1002/adfm.202108014</data>
  <data key="d3">Multifunctional optoelectronic synapse based on ferroelectric van der Waals heterostructure for emulating the entire human visual system</data>
  <data key="d4">F Guo, M Song, MC Wong, R Ding…</data>
  <data key="d5">2022</data>
  <data key="d6">54</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10783855747646875569&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="15129937235725122381">
  <data key="d0">A survey on autonomous vehicle control in the era of mixed-autonomy: From physics-based to AI-guided driving policy learning</data>
  <data key="d1">15129937235725122381</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0968090X21000401</data>
  <data key="d3">A survey on autonomous vehicle control in the era of mixed-autonomy: From physics-based to AI-guided driving policy learning</data>
  <data key="d4">X Di, R Shi</data>
  <data key="d5">2021</data>
  <data key="d6">114</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15129937235725122381&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="9136495772785862243">
  <data key="d0">Next-generation deep learning based on simulators and synthetic data</data>
  <data key="d1">9136495772785862243</data>
  <data key="d2">https://www.cell.com/trends/cognitive-sciences/fulltext/S1364-6613(21)00293-X</data>
  <data key="d3">Next-generation deep learning based on simulators and synthetic data</data>
  <data key="d4">CM de Melo, A Torralba, L Guibas, J DiCarlo…</data>
  <data key="d5">2022</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9136495772785862243&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="3820824800719710132">
  <data key="d0">Autonomous driving architectures: insights of machine learning and deep learning algorithms</data>
  <data key="d1">3820824800719710132</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2666827021000827</data>
  <data key="d3">Autonomous driving architectures: insights of machine learning and deep learning algorithms</data>
  <data key="d4">MR Bachute, JM Subhedar</data>
  <data key="d5">2021</data>
  <data key="d6">73</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3820824800719710132&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="543244060835176193">
  <data key="d0">A survey of deep RL and IL for autonomous driving policy learning</data>
  <data key="d1">543244060835176193</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9660769/</data>
  <data key="d3">A survey of deep RL and IL for autonomous driving policy learning</data>
  <data key="d4">Z Zhu, H Zhao</data>
  <data key="d5">2021</data>
  <data key="d6">83</data>
  <data key="d7">https://scholar.google.com/scholar?cites=543244060835176193&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="1056529344033078034">
  <data key="d0">Physics-informed attention-based neural network for hyperbolic partial differential equations: application to the Buckley–Leverett problem</data>
  <data key="d1">1056529344033078034</data>
  <data key="d2">https://www.nature.com/articles/s41598-022-11058-2</data>
  <data key="d3">Physics-informed attention-based neural network for hyperbolic partial differential equations: application to the Buckley–Leverett problem</data>
  <data key="d4">R Rodriguez-Torrado, P Ruiz, L Cueto-Felgueroso…</data>
  <data key="d5">2022</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1056529344033078034&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="3290950822311574111">
  <data key="d0">Machine Learning, Deep Learning and Statistical Analysis for forecasting building energy consumption—A systematic review</data>
  <data key="d1">3290950822311574111</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0952197622003372</data>
  <data key="d3">Machine Learning, Deep Learning and Statistical Analysis for forecasting building energy consumption—A systematic review</data>
  <data key="d4">M Khalil, AS McGough, Z Pourmirza…</data>
  <data key="d5">2022</data>
  <data key="d6">44</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3290950822311574111&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="8485046656923809530">
  <data key="d0">Robustart: Benchmarking robustness on architecture design and training techniques</data>
  <data key="d1">8485046656923809530</data>
  <data key="d2">https://arxiv.org/abs/2109.05211</data>
  <data key="d3">Robustart: Benchmarking robustness on architecture design and training techniques</data>
  <data key="d4">S Tang, R Gong, Y Wang, A Liu, J Wang…</data>
  <data key="d5">2021</data>
  <data key="d6">67</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8485046656923809530&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="8810290142290002002">
  <data key="d0">Perturbation-based methods for explaining deep neural networks: A survey</data>
  <data key="d1">8810290142290002002</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0167865521002440</data>
  <data key="d3">Perturbation-based methods for explaining deep neural networks: A survey</data>
  <data key="d4">M Ivanovs, R Kadikis, K Ozols</data>
  <data key="d5">2021</data>
  <data key="d6">75</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8810290142290002002&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="5369236400026867300">
  <data key="d0">A survey of collaborative machine learning using 5G vehicular communications</data>
  <data key="d1">5369236400026867300</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9706268/</data>
  <data key="d3">A survey of collaborative machine learning using 5G vehicular communications</data>
  <data key="d4">SV Balkus, H Wang, BD Cornet…</data>
  <data key="d5">2022</data>
  <data key="d6">35</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5369236400026867300&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="2908516376509801521">
  <data key="d0">Adabits: Neural network quantization with adaptive bit-widths</data>
  <data key="d1">2908516376509801521</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Jin_AdaBits_Neural_Network_Quantization_With_Adaptive_Bit-Widths_CVPR_2020_paper.html</data>
  <data key="d3">Adabits: Neural network quantization with adaptive bit-widths</data>
  <data key="d4">Q Jin, L Yang, Z Liao</data>
  <data key="d5">2020</data>
  <data key="d6">101</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2908516376509801521&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="7820015134418027243">
  <data key="d0">Design and implementation of deep neural network-based control for automatic parking maneuver process</data>
  <data key="d1">7820015134418027243</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9298482/</data>
  <data key="d3">Design and implementation of deep neural network-based control for automatic parking maneuver process</data>
  <data key="d4">R Chai, A Tsourdos, A Savvaris, S Chai…</data>
  <data key="d5">2020</data>
  <data key="d6">78</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7820015134418027243&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="10599118327233610110">
  <data key="d0">Driver anomaly quantification for intelligent vehicles: A contrastive learning approach with representation clustering</data>
  <data key="d1">10599118327233610110</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9745337/</data>
  <data key="d3">Driver anomaly quantification for intelligent vehicles: A contrastive learning approach with representation clustering</data>
  <data key="d4">Z Hu, Y Xing, W Gu, D Cao, C Lv</data>
  <data key="d5">2022</data>
  <data key="d6">37</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10599118327233610110&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="8382044937846336173">
  <data key="d0">The role of artificial intelligence in the mass adoption of electric vehicles</data>
  <data key="d1">8382044937846336173</data>
  <data key="d2">https://www.cell.com/joule/pdf/S2542-4351(21)00350-0.pdf</data>
  <data key="d3">The role of artificial intelligence in the mass adoption of electric vehicles</data>
  <data key="d4">M Ahmed, Y Zheng, A Amine, H Fathiannasab, Z Chen</data>
  <data key="d5">2021</data>
  <data key="d6">41</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8382044937846336173&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="15720662195716475484">
  <data key="d0">Optical coherent dot-product chip for sophisticated deep learning regression</data>
  <data key="d1">15720662195716475484</data>
  <data key="d2">https://www.nature.com/articles/s41377-021-00666-8</data>
  <data key="d3">Optical coherent dot-product chip for sophisticated deep learning regression</data>
  <data key="d4">S Xu, J Wang, H Shu, Z Zhang, S Yi, B Bai…</data>
  <data key="d5">2021</data>
  <data key="d6">47</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15720662195716475484&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="296015877195835414">
  <data key="d0">Diversity matters: Fully exploiting depth clues for reliable monocular 3d object detection</data>
  <data key="d1">296015877195835414</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Li_Diversity_Matters_Fully_Exploiting_Depth_Clues_for_Reliable_Monocular_3D_CVPR_2022_paper.html</data>
  <data key="d3">Diversity matters: Fully exploiting depth clues for reliable monocular 3d object detection</data>
  <data key="d4">Z Li, Z Qu, Y Zhou, J Liu, H Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=296015877195835414&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="11140667219432227939">
  <data key="d0">Super-human performance in gran turismo sport using deep reinforcement learning</data>
  <data key="d1">11140667219432227939</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9372847/</data>
  <data key="d3">Super-human performance in gran turismo sport using deep reinforcement learning</data>
  <data key="d4">F Fuchs, Y Song, E Kaufmann…</data>
  <data key="d5">2021</data>
  <data key="d6">89</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11140667219432227939&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="15889454219146822872">
  <data key="d0">Free lunch for testing: Fuzzing deep-learning libraries from open source</data>
  <data key="d1">15889454219146822872</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3510003.3510041</data>
  <data key="d3">Free lunch for testing: Fuzzing deep-learning libraries from open source</data>
  <data key="d4">A Wei, Y Deng, C Yang, L Zhang</data>
  <data key="d5">2022</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15889454219146822872&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="17919992204175632420">
  <data key="d0">Partially observable markov decision processes in robotics: A survey</data>
  <data key="d1">17919992204175632420</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9899480/</data>
  <data key="d3">Partially observable markov decision processes in robotics: A survey</data>
  <data key="d4">M Lauri, D Hsu, J Pajarinen</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17919992204175632420&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="9829630380879135158">
  <data key="d0">Artificial intelligence governance for businesses</data>
  <data key="d1">9829630380879135158</data>
  <data key="d2">https://www.tandfonline.com/doi/abs/10.1080/10580530.2022.2085825</data>
  <data key="d3">Artificial intelligence governance for businesses</data>
  <data key="d4">J Schneider, R Abraham, C Meske…</data>
  <data key="d5">2023</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9829630380879135158&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="4490548502357617762">
  <data key="d0">Learning-based methods of perception and navigation for ground vehicles in unstructured environments: A review</data>
  <data key="d1">4490548502357617762</data>
  <data key="d2">https://www.mdpi.com/1424-8220/21/1/73</data>
  <data key="d3">Learning-based methods of perception and navigation for ground vehicles in unstructured environments: A review</data>
  <data key="d4">DC Guastella, G Muscato</data>
  <data key="d5">2020</data>
  <data key="d6">57</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4490548502357617762&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="16208419679375763607">
  <data key="d0">Explainable artificial intelligence for autonomous driving: A comprehensive overview and field guide for future research directions</data>
  <data key="d1">16208419679375763607</data>
  <data key="d2">https://arxiv.org/abs/2112.11561</data>
  <data key="d3">Explainable artificial intelligence for autonomous driving: A comprehensive overview and field guide for future research directions</data>
  <data key="d4">S Atakishiyev, M Salameh, H Yao, R Goebel</data>
  <data key="d5">2021</data>
  <data key="d6">49</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16208419679375763607&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="2749681245987987360">
  <data key="d0">A review of high-throughput field phenotyping systems: Focusing on ground robots</data>
  <data key="d1">2749681245987987360</data>
  <data key="d2">https://spj.science.org/doi/full/10.34133/2022/9760269</data>
  <data key="d3">A review of high-throughput field phenotyping systems: Focusing on ground robots</data>
  <data key="d4">R Xu, C Li</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2749681245987987360&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="15770340026262104764">
  <data key="d0">Prospects for multi-agent collaboration and gaming: challenge, technology, and application</data>
  <data key="d1">15770340026262104764</data>
  <data key="d2">https://link.springer.com/article/10.1631/FITEE.2200055</data>
  <data key="d3">Prospects for multi-agent collaboration and gaming: challenge, technology, and application</data>
  <data key="d4">Y Liu, Z Li, Z Jiang, Y He</data>
  <data key="d5">2022</data>
  <data key="d6">32</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15770340026262104764&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="13308776082845550262">
  <data key="d0">Deep learning on monocular object pose detection and tracking: A comprehensive overview</data>
  <data key="d1">13308776082845550262</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3524496</data>
  <data key="d3">Deep learning on monocular object pose detection and tracking: A comprehensive overview</data>
  <data key="d4">Z Fan, Y Zhu, Y He, Q Sun, H Liu, J He</data>
  <data key="d5">2022</data>
  <data key="d6">41</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13308776082845550262&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="17895695761643428844">
  <data key="d0">Decentralized federated learning for extended sensing in 6G connected vehicles</data>
  <data key="d1">17895695761643428844</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2214209621000656</data>
  <data key="d3">Decentralized federated learning for extended sensing in 6G connected vehicles</data>
  <data key="d4">L Barbieri, S Savazzi, M Brambilla, M Nicoli</data>
  <data key="d5">2022</data>
  <data key="d6">41</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17895695761643428844&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="10389594312915615958">
  <data key="d0">Perspectives on future power system control centers for energy transition</data>
  <data key="d1">10389594312915615958</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9744623/</data>
  <data key="d3">Perspectives on future power system control centers for energy transition</data>
  <data key="d4">A Marot, A Kelly, M Naglic, V Barbesant…</data>
  <data key="d5">2022</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10389594312915615958&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="18005903390163138360">
  <data key="d0">The many faces of adversarial risk</data>
  <data key="d1">18005903390163138360</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/52c4608c2f126708211b9e0a60eaf050-Abstract.html</data>
  <data key="d3">The many faces of adversarial risk</data>
  <data key="d4">MS Pydi, V Jog</data>
  <data key="d5">2021</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18005903390163138360&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="15641048523483845755">
  <data key="d0">A survey of image labelling for computer vision applications</data>
  <data key="d1">15641048523483845755</data>
  <data key="d2">https://www.tandfonline.com/doi/abs/10.1080/2573234X.2021.1908861</data>
  <data key="d3">A survey of image labelling for computer vision applications</data>
  <data key="d4">C Sager, C Janiesch, P Zschech</data>
  <data key="d5">2021</data>
  <data key="d6">46</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15641048523483845755&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="3296916295930333686">
  <data key="d0">Challenges of machine learning applied to safety-critical cyber-physical systems</data>
  <data key="d1">3296916295930333686</data>
  <data key="d2">https://www.mdpi.com/2504-4990/2/4/31</data>
  <data key="d3">Challenges of machine learning applied to safety-critical cyber-physical systems</data>
  <data key="d4">A Pereira, C Thomas</data>
  <data key="d5">2020</data>
  <data key="d6">48</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3296916295930333686&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="6053988149774622220">
  <data key="d0">Deep reinforcement learning techniques for vehicular networks: Recent advances and future trends towards 6G</data>
  <data key="d1">6053988149774622220</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S221420962100067X</data>
  <data key="d3">Deep reinforcement learning techniques for vehicular networks: Recent advances and future trends towards 6G</data>
  <data key="d4">A Mekrache, A Bradai, E Moulay, S Dawaliby</data>
  <data key="d5">2022</data>
  <data key="d6">37</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6053988149774622220&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="11971206372978719797">
  <data key="d0">Towards improving calibration in object detection under domain shift</data>
  <data key="d1">11971206372978719797</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/fcd812a51b8f8d05cfea22e3c9c4b369-Abstract-Conference.html</data>
  <data key="d3">Towards improving calibration in object detection under domain shift</data>
  <data key="d4">MA Munir, MH Khan, M Sarfraz…</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11971206372978719797&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="7650029386959535850">
  <data key="d0">A stitch in time saves nine: A train-time regularizing loss for improved neural network calibration</data>
  <data key="d1">7650029386959535850</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Hebbalaguppe_A_Stitch_in_Time_Saves_Nine_A_Train-Time_Regularizing_Loss_CVPR_2022_paper.html</data>
  <data key="d3">A stitch in time saves nine: A train-time regularizing loss for improved neural network calibration</data>
  <data key="d4">R Hebbalaguppe, J Prakash…</data>
  <data key="d5">2022</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7650029386959535850&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="18195869548361214224">
  <data key="d0">A survey on hybrid human-artificial intelligence for autonomous driving</data>
  <data key="d1">18195869548361214224</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9423526/</data>
  <data key="d3">A survey on hybrid human-artificial intelligence for autonomous driving</data>
  <data key="d4">H Ning, R Yin, A Ullah, F Shi</data>
  <data key="d5">2021</data>
  <data key="d6">32</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18195869548361214224&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="17109511764161480588">
  <data key="d0">Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving</data>
  <data key="d1">17109511764161480588</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0921889020304085</data>
  <data key="d3">Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving</data>
  <data key="d4">A Likmeta, AM Metelli, A Tirinzoni, R Giol…</data>
  <data key="d5">2020</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17109511764161480588&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="4773394058091047733">
  <data key="d0">The effect of transparency and trust on intelligent system acceptance: Evidence from a user-based study</data>
  <data key="d1">4773394058091047733</data>
  <data key="d2">https://link.springer.com/article/10.1007/s12525-022-00593-5</data>
  <data key="d3">The effect of transparency and trust on intelligent system acceptance: Evidence from a user-based study</data>
  <data key="d4">J Wanner, LV Herm, K Heinrich, C Janiesch</data>
  <data key="d5">2022</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4773394058091047733&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="7472077363474028067">
  <data key="d0">Offline reinforcement learning as anti-exploration</data>
  <data key="d1">7472077363474028067</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/20783</data>
  <data key="d3">Offline reinforcement learning as anti-exploration</data>
  <data key="d4">S Rezaeifar, R Dadashi, N Vieillard…</data>
  <data key="d5">2022</data>
  <data key="d6">27</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7472077363474028067&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="13254274571229991785">
  <data key="d0">A comprehensive survey on the application of deep and reinforcement learning approaches in autonomous driving</data>
  <data key="d1">13254274571229991785</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1319157822000970</data>
  <data key="d3">A comprehensive survey on the application of deep and reinforcement learning approaches in autonomous driving</data>
  <data key="d4">BB Elallid, N Benamar, AS Hafid, T Rachidi…</data>
  <data key="d5">2022</data>
  <data key="d6">35</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13254274571229991785&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="9602194952104030116">
  <data key="d0">Brain-inspired cognitive model with attention for self-driving cars</data>
  <data key="d1">9602194952104030116</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/7954050/</data>
  <data key="d3">Brain-inspired cognitive model with attention for self-driving cars</data>
  <data key="d4">S Chen, S Zhang, J Shang, B Chen…</data>
  <data key="d5">2017</data>
  <data key="d6">131</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9602194952104030116&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="412550918084992142">
  <data key="d0">Lychee surface defect detection based on deep convolutional neural networks with gan-based data augmentation</data>
  <data key="d1">412550918084992142</data>
  <data key="d2">https://www.mdpi.com/2073-4395/11/8/1500</data>
  <data key="d3">Lychee surface defect detection based on deep convolutional neural networks with gan-based data augmentation</data>
  <data key="d4">C Wang, Z Xiao</data>
  <data key="d5">2021</data>
  <data key="d6">32</data>
  <data key="d7">https://scholar.google.com/scholar?cites=412550918084992142&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="5579084922455744576">
  <data key="d0">Autonomous vehicles and intelligent automation: Applications, challenges, and opportunities</data>
  <data key="d1">5579084922455744576</data>
  <data key="d2">https://www.hindawi.com/journals/misy/2022/7632892/</data>
  <data key="d3">Autonomous vehicles and intelligent automation: Applications, challenges, and opportunities</data>
  <data key="d4">G Bathla, K Bhadane, RK Singh, R Kumar…</data>
  <data key="d5">2022</data>
  <data key="d6">31</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5579084922455744576&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="9463717142610823747">
  <data key="d0">Training uncertainty-aware classifiers with conformalized deep learning</data>
  <data key="d1">9463717142610823747</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/8c96b559340daa7bb29f56ccfbbc9c2f-Abstract-Conference.html</data>
  <data key="d3">Training uncertainty-aware classifiers with conformalized deep learning</data>
  <data key="d4">BS Einbinder, Y Romano, M Sesia…</data>
  <data key="d5">2022</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9463717142610823747&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="521481329840897304">
  <data key="d0">Coverage-guided tensor compiler fuzzing with joint ir-pass mutation</data>
  <data key="d1">521481329840897304</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3527317</data>
  <data key="d3">Coverage-guided tensor compiler fuzzing with joint ir-pass mutation</data>
  <data key="d4">J Liu, Y Wei, S Yang, Y Deng, L Zhang</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=521481329840897304&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="12532017250168790887">
  <data key="d0">TLT: Recurrent fine-tuning transfer learning for water quality long-term prediction</data>
  <data key="d1">12532017250168790887</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0043135422011162</data>
  <data key="d3">TLT: Recurrent fine-tuning transfer learning for water quality long-term prediction</data>
  <data key="d4">L Peng, H Wu, M Gao, H Yi, Q Xiong, L Yang, S Cheng</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12532017250168790887&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="13081138058927339532">
  <data key="d0">Metamaterials: from fundamental physics to intelligent design</data>
  <data key="d1">13081138058927339532</data>
  <data key="d2">https://onlinelibrary.wiley.com/doi/abs/10.1002/idm2.12049</data>
  <data key="d3">Metamaterials: from fundamental physics to intelligent design</data>
  <data key="d4">J Chen, S Hu, S Zhu, T Li</data>
  <data key="d5">2023</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13081138058927339532&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="18151073209899826471">
  <data key="d0">Uncovering and correcting shortcut learning in machine learning models for skin cancer diagnosis</data>
  <data key="d1">18151073209899826471</data>
  <data key="d2">https://www.mdpi.com/2075-4418/12/1/40</data>
  <data key="d3">Uncovering and correcting shortcut learning in machine learning models for skin cancer diagnosis</data>
  <data key="d4">M Nauta, R Walsh, A Dubowski, C Seifert</data>
  <data key="d5">2021</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18151073209899826471&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="621059241978449016">
  <data key="d0">Explaining deep neural networks: A survey on the global interpretation methods</data>
  <data key="d1">621059241978449016</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0925231222012218</data>
  <data key="d3">Explaining deep neural networks: A survey on the global interpretation methods</data>
  <data key="d4">R Saleem, B Yuan, F Kurugollu, A Anjum, L Liu</data>
  <data key="d5">2022</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=621059241978449016&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="1897738753201623147">
  <data key="d0">AI applications in renal pathology</data>
  <data key="d1">1897738753201623147</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0085253821001812</data>
  <data key="d3">AI applications in renal pathology</data>
  <data key="d4">Y Huo, R Deng, Q Liu, AB Fogo, H Yang</data>
  <data key="d5">2021</data>
  <data key="d6">54</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1897738753201623147&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="7436181184954982806">
  <data key="d0">Survey on cooperative perception in an automotive context</data>
  <data key="d1">7436181184954982806</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9732063/</data>
  <data key="d3">Survey on cooperative perception in an automotive context</data>
  <data key="d4">A Caillot, S Ouerghi, P Vasseur…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7436181184954982806&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="13539442582208875227">
  <data key="d0">Moving deep learning to the edge</data>
  <data key="d1">13539442582208875227</data>
  <data key="d2">https://www.mdpi.com/1999-4893/13/5/125</data>
  <data key="d3">Moving deep learning to the edge</data>
  <data key="d4">MP Véstias, RP Duarte, JT de Sousa, HC Neto</data>
  <data key="d5">2020</data>
  <data key="d6">52</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13539442582208875227&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="11794736720683628583">
  <data key="d0">Scale-up: An efficient black-box input-level backdoor detection via analyzing scaled prediction consistency</data>
  <data key="d1">11794736720683628583</data>
  <data key="d2">https://arxiv.org/abs/2302.03251</data>
  <data key="d3">Scale-up: An efficient black-box input-level backdoor detection via analyzing scaled prediction consistency</data>
  <data key="d4">J Guo, Y Li, X Chen, H Guo, L Sun, C Liu</data>
  <data key="d5">2023</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11794736720683628583&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="15738759785276050535">
  <data key="d0">Robust learning for data poisoning attacks</data>
  <data key="d1">15738759785276050535</data>
  <data key="d2">https://proceedings.mlr.press/v139/wang21r.html</data>
  <data key="d3">Robust learning for data poisoning attacks</data>
  <data key="d4">Y Wang, P Mianjy, R Arora</data>
  <data key="d5">2021</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15738759785276050535&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="7649876297714062863">
  <data key="d0">Imagenet-x: Understanding model mistakes with factor of variation annotations</data>
  <data key="d1">7649876297714062863</data>
  <data key="d2">https://arxiv.org/abs/2211.01866</data>
  <data key="d3">Imagenet-x: Understanding model mistakes with factor of variation annotations</data>
  <data key="d4">BY Idrissi, D Bouchacourt, R Balestriero…</data>
  <data key="d5">2022</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7649876297714062863&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="14732420770874304307">
  <data key="d0">Learning collision-free space detection from stereo images: Homography matrix brings better data augmentation</data>
  <data key="d1">14732420770874304307</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9360504/</data>
  <data key="d3">Learning collision-free space detection from stereo images: Homography matrix brings better data augmentation</data>
  <data key="d4">R Fan, H Wang, P Cai, J Wu, MJ Bocus…</data>
  <data key="d5">2021</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14732420770874304307&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="15295068953900215294">
  <data key="d0">Mammogram breast cancer CAD systems for mass detection and classification: a review</data>
  <data key="d1">15295068953900215294</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11042-022-12332-1</data>
  <data key="d3">Mammogram breast cancer CAD systems for mass detection and classification: a review</data>
  <data key="d4">NM Hassan, S Hamad, K Mahar</data>
  <data key="d5">2022</data>
  <data key="d6">35</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15295068953900215294&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="15549291371117213871">
  <data key="d0">Precise single-stage detector</data>
  <data key="d1">15549291371117213871</data>
  <data key="d2">https://arxiv.org/abs/2210.04252</data>
  <data key="d3">Precise single-stage detector</data>
  <data key="d4">A Chandio, G Gui, T Kumar, I Ullah…</data>
  <data key="d5">2022</data>
  <data key="d6">41</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15549291371117213871&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="9884544045603971658">
  <data key="d0">CE-FPN: Enhancing channel information for object detection</data>
  <data key="d1">9884544045603971658</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11042-022-11940-1</data>
  <data key="d3">CE-FPN: Enhancing channel information for object detection</data>
  <data key="d4">Y Luo, X Cao, J Zhang, J Guo, H Shen, T Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">71</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9884544045603971658&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="10628215061802232868">
  <data key="d0">Feature split–merge–enhancement network for remote sensing object detection</data>
  <data key="d1">10628215061802232868</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9673713/</data>
  <data key="d3">Feature split–merge–enhancement network for remote sensing object detection</data>
  <data key="d4">W Ma, N Li, H Zhu, L Jiao, X Tang…</data>
  <data key="d5">2022</data>
  <data key="d6">45</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10628215061802232868&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="9532821550175512200">
  <data key="d0">Guiding pretraining in reinforcement learning with large language models</data>
  <data key="d1">9532821550175512200</data>
  <data key="d2">https://arxiv.org/abs/2302.06692</data>
  <data key="d3">Guiding pretraining in reinforcement learning with large language models</data>
  <data key="d4">Y Du, O Watkins, Z Wang, C Colas, T Darrell…</data>
  <data key="d5">2023</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9532821550175512200&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="12985976504909847163">
  <data key="d0">Integrating deep learning-based iot and fog computing with software-defined networking for detecting weapons in video surveillance systems</data>
  <data key="d1">12985976504909847163</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/14/5075</data>
  <data key="d3">Integrating deep learning-based iot and fog computing with software-defined networking for detecting weapons in video surveillance systems</data>
  <data key="d4">C Fathy, SN Saleh</data>
  <data key="d5">2022</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12985976504909847163&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="2812153438552156646">
  <data key="d0">Backbones-review: Feature extraction networks for deep learning and deep reinforcement learning approaches</data>
  <data key="d1">2812153438552156646</data>
  <data key="d2">https://arxiv.org/abs/2206.08016</data>
  <data key="d3">Backbones-review: Feature extraction networks for deep learning and deep reinforcement learning approaches</data>
  <data key="d4">O Elharrouss, Y Akbari, N Almaadeed…</data>
  <data key="d5">2022</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2812153438552156646&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="14349697475909471320">
  <data key="d0">Small-object detection based on YOLOv5 in autonomous driving systems</data>
  <data key="d1">14349697475909471320</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0167865523000727</data>
  <data key="d3">Small-object detection based on YOLOv5 in autonomous driving systems</data>
  <data key="d4">B Mahaur, KK Mishra</data>
  <data key="d5">2023</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14349697475909471320&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="18351357225093508985">
  <data key="d0">Single-stage uav detection and classification with yolov5: Mosaic data augmentation and panet</data>
  <data key="d1">18351357225093508985</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9663841/</data>
  <data key="d3">Single-stage uav detection and classification with yolov5: Mosaic data augmentation and panet</data>
  <data key="d4">F Dadboud, V Patel, V Mehta, M Bolic…</data>
  <data key="d5">2021</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18351357225093508985&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="11021564807628327569">
  <data key="d0">FPGA-based accelerator for object detection: A comprehensive survey</data>
  <data key="d1">11021564807628327569</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11227-022-04415-5</data>
  <data key="d3">FPGA-based accelerator for object detection: A comprehensive survey</data>
  <data key="d4">K Zeng, Q Ma, JW Wu, Z Chen, T Shen…</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11021564807628327569&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="4655434626952320958">
  <data key="d0">Rail wheel tread defect detection using improved YOLOv3</data>
  <data key="d1">4655434626952320958</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0263224122011551</data>
  <data key="d3">Rail wheel tread defect detection using improved YOLOv3</data>
  <data key="d4">Z Xing, Z Zhang, X Yao, Y Qin, L Jia</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4655434626952320958&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="16079747206913991174">
  <data key="d0">Development of a Low-Power IoMT Portable Pillbox for Medication Adherence Improvement and Remote Treatment Adjustment</data>
  <data key="d1">16079747206913991174</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/15/5818</data>
  <data key="d3">Development of a Low-Power IoMT Portable Pillbox for Medication Adherence Improvement and Remote Treatment Adjustment</data>
  <data key="d4">D Karagiannis, K Mitsis, KS Nikita</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16079747206913991174&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="16432688928939217038">
  <data key="d0">A comprehensive review of object detection with deep learning</data>
  <data key="d1">16432688928939217038</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1051200422004298</data>
  <data key="d3">A comprehensive review of object detection with deep learning</data>
  <data key="d4">R Kaur, S Singh</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16432688928939217038&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="6456248456066525600">
  <data key="d0">A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications</data>
  <data key="d1">6456248456066525600</data>
  <data key="d2">https://link.springer.com/article/10.1186/s40537-023-00727-2</data>
  <data key="d3">A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications</data>
  <data key="d4">L Alzubaidi, J Bai, A Al-Sabaawi, J Santamaría…</data>
  <data key="d5">2023</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6456248456066525600&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="8123745338600640152">
  <data key="d0">Video surveillance using deep transfer learning and deep domain adaptation: Towards better generalization</data>
  <data key="d1">8123745338600640152</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0952197622006881</data>
  <data key="d3">Video surveillance using deep transfer learning and deep domain adaptation: Towards better generalization</data>
  <data key="d4">Y Himeur, S Al-Maadeed, H Kheddar…</data>
  <data key="d5">2023</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8123745338600640152&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="10334732198267289555">
  <data key="d0">Review of recent automated pothole-detection methods</data>
  <data key="d1">10334732198267289555</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/11/5320</data>
  <data key="d3">Review of recent automated pothole-detection methods</data>
  <data key="d4">YM Kim, YG Kim, SY Son, SY Lim, BY Choi, DH Choi</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10334732198267289555&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="5740479134802475221">
  <data key="d0">Towards domain generalization in object detection</data>
  <data key="d1">5740479134802475221</data>
  <data key="d2">https://arxiv.org/abs/2203.14387</data>
  <data key="d3">Towards domain generalization in object detection</data>
  <data key="d4">X Zhang, Z Xu, R Xu, J Liu, P Cui, W Wan…</data>
  <data key="d5">2022</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5740479134802475221&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="6447041552955227244">
  <data key="d0">Allergen30: detecting food items with possible allergens using deep learning-based computer vision</data>
  <data key="d1">6447041552955227244</data>
  <data key="d2">https://link.springer.com/article/10.1007/s12161-022-02353-9</data>
  <data key="d3">Allergen30: detecting food items with possible allergens using deep learning-based computer vision</data>
  <data key="d4">M Mishra, T Sarkar, T Choudhury, N Bansal…</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6447041552955227244&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="11016115370073884103">
  <data key="d0">A systematic study on reinforcement learning based applications</data>
  <data key="d1">11016115370073884103</data>
  <data key="d2">https://www.mdpi.com/1996-1073/16/3/1512</data>
  <data key="d3">A systematic study on reinforcement learning based applications</data>
  <data key="d4">K Sivamayil, E Rajasekar, B Aljafari, S Nikolovski…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11016115370073884103&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="5588318297466755355">
  <data key="d0">Weakly supervised object detection for remote sensing images: A survey</data>
  <data key="d1">5588318297466755355</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/21/5362</data>
  <data key="d3">Weakly supervised object detection for remote sensing images: A survey</data>
  <data key="d4">C Fasana, S Pasini, F Milani, P Fraternali</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5588318297466755355&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="7060227414328690906">
  <data key="d0">Dynamic identification of crane load fall zone: A computer vision approach</data>
  <data key="d1">7060227414328690906</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0925753522002430</data>
  <data key="d3">Dynamic identification of crane load fall zone: A computer vision approach</data>
  <data key="d4">EYT Chian, YM Goh, J Tian, BHW Guo</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7060227414328690906&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="13890326168676881936">
  <data key="d0">Cf-detr: Coarse-to-fine transformers for end-to-end object detection</data>
  <data key="d1">13890326168676881936</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/19893</data>
  <data key="d3">Cf-detr: Coarse-to-fine transformers for end-to-end object detection</data>
  <data key="d4">X Cao, P Yuan, B Feng, K Niu</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13890326168676881936&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="585375656869561806">
  <data key="d0">D-MFPN: A Doppler Feature Matrix Fused with a Multilayer Feature Pyramid Network for SAR Ship Detection</data>
  <data key="d1">585375656869561806</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/3/626</data>
  <data key="d3">D-MFPN: A Doppler Feature Matrix Fused with a Multilayer Feature Pyramid Network for SAR Ship Detection</data>
  <data key="d4">Y Zhou, K Fu, B Han, J Yang, Z Pan, Y Hu, D Yin</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=585375656869561806&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="10256791187069118291">
  <data key="d0">Performance evaluation of deep learning object detectors for weed detection for cotton</data>
  <data key="d1">10256791187069118291</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2772375522000910</data>
  <data key="d3">Performance evaluation of deep learning object detectors for weed detection for cotton</data>
  <data key="d4">A Rahman, Y Lu, H Wang</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10256791187069118291&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="6955302216815830645">
  <data key="d0">RF-Next: Efficient receptive field search for convolutional neural networks</data>
  <data key="d1">6955302216815830645</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9799540/</data>
  <data key="d3">RF-Next: Efficient receptive field search for convolutional neural networks</data>
  <data key="d4">S Gao, ZY Li, Q Han, MM Cheng…</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6955302216815830645&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="9835514645329759223">
  <data key="d0">Towards optimal foreign object debris detection in an airport environment</data>
  <data key="d1">9835514645329759223</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417422018474</data>
  <data key="d3">Towards optimal foreign object debris detection in an airport environment</data>
  <data key="d4">M Noroozi, A Shah</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9835514645329759223&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="12985566362823859744">
  <data key="d0">Machine learning for renal pathologies: an updated survey</data>
  <data key="d1">12985566362823859744</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/13/4989</data>
  <data key="d3">Machine learning for renal pathologies: an updated survey</data>
  <data key="d4">R Magherini, E Mussi, Y Volpe, R Furferi, F Buonamici…</data>
  <data key="d5">2022</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12985566362823859744&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="7984800842432280263">
  <data key="d0">Using artificial intelligence to support marine macrolitter research: A content analysis and an online database</data>
  <data key="d1">7984800842432280263</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0964569122004422</data>
  <data key="d3">Using artificial intelligence to support marine macrolitter research: A content analysis and an online database</data>
  <data key="d4">DV Politikos, A Adamopoulou, G Petasis…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7984800842432280263&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="17521920740408245266">
  <data key="d0">Anatomy of Deep Learning Image Classification and Object Detection on Commercial Edge Devices: A Case Study on Face Mask Detection</data>
  <data key="d1">17521920740408245266</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9918067/</data>
  <data key="d3">Anatomy of Deep Learning Image Classification and Object Detection on Commercial Edge Devices: A Case Study on Face Mask Detection</data>
  <data key="d4">D Kolosov, V Kelefouras, P Kourtessis, I Mporas</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17521920740408245266&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="16420701532688217268">
  <data key="d0">Real-Time Embedded Implementation of Improved Object Detector for Resource-Constrained Devices</data>
  <data key="d1">16420701532688217268</data>
  <data key="d2">https://www.mdpi.com/2079-9268/12/2/21</data>
  <data key="d3">Real-Time Embedded Implementation of Improved Object Detector for Resource-Constrained Devices</data>
  <data key="d4">N Ravi, M El-Sharkawy</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16420701532688217268&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="8989581343974102697">
  <data key="d0">Multimodal data augmentation for visual-infrared person ReID with corrupted data</data>
  <data key="d1">8989581343974102697</data>
  <data key="d2">https://openaccess.thecvf.com/content/WACV2023W/RWS/html/Josi_Multimodal_Data_Augmentation_for_Visual-Infrared_Person_ReID_With_Corrupted_Data_WACVW_2023_paper.html</data>
  <data key="d3">Multimodal data augmentation for visual-infrared person ReID with corrupted data</data>
  <data key="d4">A Josi, M Alehdaghi, RMO Cruz…</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8989581343974102697&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="2488190159038118357">
  <data key="d0">Lightweight multi-scale network for small object detection</data>
  <data key="d1">2488190159038118357</data>
  <data key="d2">https://peerj.com/articles/cs-1145/</data>
  <data key="d3">Lightweight multi-scale network for small object detection</data>
  <data key="d4">L Li, B Li, H Zhou</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2488190159038118357&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="818429456345308176">
  <data key="d0">Towards lightweight neural networks for garbage object detection</data>
  <data key="d1">818429456345308176</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/19/7455</data>
  <data key="d3">Towards lightweight neural networks for garbage object detection</data>
  <data key="d4">X Cai, F Shuang, X Sun, Y Duan, G Cheng</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=818429456345308176&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="18347537605283752941">
  <data key="d0">Inertial Navigation Meets Deep Learning: A Survey of Current Trends and Future Directions</data>
  <data key="d1">18347537605283752941</data>
  <data key="d2">https://arxiv.org/abs/2307.00014</data>
  <data key="d3">Inertial Navigation Meets Deep Learning: A Survey of Current Trends and Future Directions</data>
  <data key="d4">N Cohen, I Klein</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18347537605283752941&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="7447750230007511033">
  <data key="d0">Influence of AVC and HEVC compression on detection of vehicles through Faster R-CNN</data>
  <data key="d1">7447750230007511033</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10247026/</data>
  <data key="d3">Influence of AVC and HEVC compression on detection of vehicles through Faster R-CNN</data>
  <data key="d4">PH Chan, A Huggett, G Souvalioti…</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7447750230007511033&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="17445702357043232709">
  <data key="d0">Recent advances in video analytics for rail network surveillance for security, trespass and suicide prevention—A survey</data>
  <data key="d1">17445702357043232709</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/12/4324</data>
  <data key="d3">Recent advances in video analytics for rail network surveillance for security, trespass and suicide prevention—A survey</data>
  <data key="d4">T Zhang, W Aftab, L Mihaylova, C Langran-Wheeler…</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17445702357043232709&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="10376554157268421149">
  <data key="d0">A comparative review of recent few-shot object detection algorithms</data>
  <data key="d1">10376554157268421149</data>
  <data key="d2">https://arxiv.org/abs/2111.00201</data>
  <data key="d3">A comparative review of recent few-shot object detection algorithms</data>
  <data key="d4">L Jiaxu, C Taiyue, G Xinbo, Y Yongtao, W Ye…</data>
  <data key="d5">2021</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10376554157268421149&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="9972278693063933261">
  <data key="d0">Protein Subcellular Localization Prediction by Concatenation of Convolutional Blocks for Deep Features Extraction from Microscopic Images</data>
  <data key="d1">9972278693063933261</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9999691/</data>
  <data key="d3">Protein Subcellular Localization Prediction by Concatenation of Convolutional Blocks for Deep Features Extraction from Microscopic Images</data>
  <data key="d4">S Aggarwal, S Juneja, J Rashid, D Gupta…</data>
  <data key="d5">2022</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9972278693063933261&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="5102122811255396025">
  <data key="d0">Intelligent weed management based on object detection neural networks in tomato crops</data>
  <data key="d1">5102122811255396025</data>
  <data key="d2">https://www.mdpi.com/2073-4395/12/12/2953</data>
  <data key="d3">Intelligent weed management based on object detection neural networks in tomato crops</data>
  <data key="d4">JM López-Correa, H Moreno, A Ribeiro, D Andújar</data>
  <data key="d5">2022</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5102122811255396025&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="2840095996932934652">
  <data key="d0">Detection and classification of COVID-19 by using faster R-CNN and mask R-CNN on CT images</data>
  <data key="d1">2840095996932934652</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00521-023-08450-y</data>
  <data key="d3">Detection and classification of COVID-19 by using faster R-CNN and mask R-CNN on CT images</data>
  <data key="d4">ME Sahin, H Ulutas, E Yuce, MF Erkoc</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2840095996932934652&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="1768748882307170392">
  <data key="d0">A DCNN-based arbitrarily-oriented object detector with application to quality control and inspection</data>
  <data key="d1">1768748882307170392</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0166361522001348</data>
  <data key="d3">A DCNN-based arbitrarily-oriented object detector with application to quality control and inspection</data>
  <data key="d4">K Yao, A Ortiz, F Bonnin-Pascual</data>
  <data key="d5">2022</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1768748882307170392&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="17587187390595037360">
  <data key="d0">Hybrid features by combining visual and text information to improve spam filtering performance</data>
  <data key="d1">17587187390595037360</data>
  <data key="d2">https://www.mdpi.com/2079-9292/11/13/2053</data>
  <data key="d3">Hybrid features by combining visual and text information to improve spam filtering performance</data>
  <data key="d4">SG Nam, Y Jang, DG Lee, YS Seo</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17587187390595037360&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="188799720050288373">
  <data key="d0">X-ray weld defect detection based on AF-RCNN</data>
  <data key="d1">188799720050288373</data>
  <data key="d2">https://link.springer.com/article/10.1007/s40194-022-01281-w</data>
  <data key="d3">X-ray weld defect detection based on AF-RCNN</data>
  <data key="d4">W Liu, S Shan, H Chen, R Wang, J Sun, Z Zhou</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=188799720050288373&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="12641103106759054130">
  <data key="d0">The study of coal gangue segmentation for location and shape predicts based on multispectral and improved Mask R-CNN</data>
  <data key="d1">12641103106759054130</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0032591022005496</data>
  <data key="d3">The study of coal gangue segmentation for location and shape predicts based on multispectral and improved Mask R-CNN</data>
  <data key="d4">W Lai, F Hu, X Kong, P Yan, K Bian, X Dai</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12641103106759054130&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="10335635097204372178">
  <data key="d0">Crop Node Detection and Internode Length Estimation Using an Improved YOLOv5 Model</data>
  <data key="d1">10335635097204372178</data>
  <data key="d2">https://www.mdpi.com/2077-0472/13/2/473</data>
  <data key="d3">Crop Node Detection and Internode Length Estimation Using an Improved YOLOv5 Model</data>
  <data key="d4">J Hu, G Li, H Mo, Y Lv, T Qian, M Chen, S Lu</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10335635097204372178&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="TwB7dLsYY-MJ">
  <data key="d0">Reconciling Object-Level and Global-Level Objectives for Long-Tail Detection</data>
  <data key="d1">TwB7dLsYY-MJ</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Reconciling_Object-Level_and_Global-Level_Objectives_for_Long-Tail_Detection_ICCV_2023_paper.html</data>
  <data key="d3">Reconciling Object-Level and Global-Level Objectives for Long-Tail Detection</data>
  <data key="d4">S Zhang, C Chen, S Peng</data>
  <data key="d5">2023</data>
  <data key="d8">13</data>
</node>
<node id="8653517720287563782">
  <data key="d0">G-YOLOX: A Lightweight Network for Detecting Vehicle Types</data>
  <data key="d1">8653517720287563782</data>
  <data key="d2">https://www.hindawi.com/journals/js/2022/4488400/</data>
  <data key="d3">G-YOLOX: A Lightweight Network for Detecting Vehicle Types</data>
  <data key="d4">Q Luo, J Wang, M Gao, H Lin, H Zhou, Q Miao</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8653517720287563782&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="5136921629305657035">
  <data key="d0">Bush Detection for Vision-Based UGV Guidance in Blueberry Orchards: Data Set and Methods</data>
  <data key="d1">5136921629305657035</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/Precognition/html/Filipovic_Bush_Detection_for_Vision-Based_UGV_Guidance_in_Blueberry_Orchards_Data_CVPRW_2023_paper.html</data>
  <data key="d3">Bush Detection for Vision-Based UGV Guidance in Blueberry Orchards: Data Set and Methods</data>
  <data key="d4">V Filipović, D Stefanović, N Pajević…</data>
  <data key="d5">2023</data>
  <data key="d8">13</data>
</node>
<node id="11099153551339147297">
  <data key="d0">KBHN: A knowledge-aware bi-hypergraph network based on visual-knowledge features fusion for teaching image annotation</data>
  <data key="d1">11099153551339147297</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0306457322002072</data>
  <data key="d3">KBHN: A knowledge-aware bi-hypergraph network based on visual-knowledge features fusion for teaching image annotation</data>
  <data key="d4">H Li, J Wang, X Du, Z Hu, S Yang</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11099153551339147297&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="5228079008431707389">
  <data key="d0">Simplification of Deep Neural Network-Based Object Detector for Real-Time Edge Computing</data>
  <data key="d1">5228079008431707389</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/7/3777</data>
  <data key="d3">Simplification of Deep Neural Network-Based Object Detector for Real-Time Edge Computing</data>
  <data key="d4">K Choi, SM Wi, HG Jung, JK Suhr</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5228079008431707389&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="13762120871365653169">
  <data key="d0">Large Selective Kernel Network for Remote Sensing Object Detection</data>
  <data key="d1">13762120871365653169</data>
  <data key="d2">https://arxiv.org/abs/2303.09030</data>
  <data key="d3">Large Selective Kernel Network for Remote Sensing Object Detection</data>
  <data key="d4">Y Li, Q Hou, Z Zheng, MM Cheng, J Yang…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13762120871365653169&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="11403420526814911334">
  <data key="d0">Multi-modal 3D Object Detection in Autonomous Driving: A Survey and Taxonomy</data>
  <data key="d1">11403420526814911334</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10093116/</data>
  <data key="d3">Multi-modal 3D Object Detection in Autonomous Driving: A Survey and Taxonomy</data>
  <data key="d4">L Wang, X Zhang, Z Song, J Bi, G Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11403420526814911334&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="3928869340439710212">
  <data key="d0">One-shot doc snippet detection: Powering search in document beyond text</data>
  <data key="d1">3928869340439710212</data>
  <data key="d2">https://openaccess.thecvf.com/content/WACV2023/html/Java_One-Shot_Doc_Snippet_Detection_Powering_Search_in_Document_Beyond_Text_WACV_2023_paper.html</data>
  <data key="d3">One-shot doc snippet detection: Powering search in document beyond text</data>
  <data key="d4">A Java, S Deshmukh, M Aggarwal…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3928869340439710212&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="6432339791137664150">
  <data key="d0">Individual Tree Detection in Coal Mine Afforestation Area Based on Improved Faster RCNN in UAV RGB Images</data>
  <data key="d1">6432339791137664150</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/21/5545</data>
  <data key="d3">Individual Tree Detection in Coal Mine Afforestation Area Based on Improved Faster RCNN in UAV RGB Images</data>
  <data key="d4">M Luo, Y Tian, S Zhang, L Huang, H Wang, Z Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6432339791137664150&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="17407217274075551544">
  <data key="d0">A survey on deep learning-based monocular spacecraft pose estimation: Current state, limitations and prospects</data>
  <data key="d1">17407217274075551544</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0094576523003995</data>
  <data key="d3">A survey on deep learning-based monocular spacecraft pose estimation: Current state, limitations and prospects</data>
  <data key="d4">L Pauly, W Rharbaoui, C Shneider, A Rathinam…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17407217274075551544&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="5708297216613512002">
  <data key="d0">Segmentation of dental restorations on panoramic radiographs using deep learning</data>
  <data key="d1">5708297216613512002</data>
  <data key="d2">https://www.mdpi.com/2075-4418/12/6/1316</data>
  <data key="d3">Segmentation of dental restorations on panoramic radiographs using deep learning</data>
  <data key="d4">C Rohrer, J Krois, J Patel, H Meyer-Lueckel…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5708297216613512002&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="1676138104604729822">
  <data key="d0">Fracture R‐CNN: An anchor‐efficient anti‐interference framework for skull fracture detection in CT images</data>
  <data key="d1">1676138104604729822</data>
  <data key="d2">https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.15809</data>
  <data key="d3">Fracture R‐CNN: An anchor‐efficient anti‐interference framework for skull fracture detection in CT images</data>
  <data key="d4">X Lin, Z Yan, Z Kuang, H Zhang, X Deng…</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1676138104604729822&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="10296222572490103697">
  <data key="d0">Research on Fault Diagnosis of Steel Surface Based on Improved YOLOV5</data>
  <data key="d1">10296222572490103697</data>
  <data key="d2">https://www.mdpi.com/2227-9717/10/11/2274</data>
  <data key="d3">Research on Fault Diagnosis of Steel Surface Based on Improved YOLOV5</data>
  <data key="d4">W Liu, Y Xiao, A Zheng, Z Zheng, X Liu, Z Zhang, C Li</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10296222572490103697&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="12621755854148347362">
  <data key="d0">Benchmarking 2D multi-object detection and tracking algorithms in autonomous vehicle driving scenarios</data>
  <data key="d1">12621755854148347362</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/8/4024</data>
  <data key="d3">Benchmarking 2D multi-object detection and tracking algorithms in autonomous vehicle driving scenarios</data>
  <data key="d4">D Gragnaniello, A Greco, A Saggese, M Vento…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12621755854148347362&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="11159799407205847879">
  <data key="d0">Detecting common coccinellids found in sorghum using deep learning models</data>
  <data key="d1">11159799407205847879</data>
  <data key="d2">https://www.nature.com/articles/s41598-023-36738-5</data>
  <data key="d3">Detecting common coccinellids found in sorghum using deep learning models</data>
  <data key="d4">C Wang, I Grijalva, D Caragea, B McCornack</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11159799407205847879&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="3650044505606796741">
  <data key="d0">Interactive video retrieval in the age of effective joint embedding deep models: lessons from the 11th VBS</data>
  <data key="d1">3650044505606796741</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00530-023-01143-5</data>
  <data key="d3">Interactive video retrieval in the age of effective joint embedding deep models: lessons from the 11th VBS</data>
  <data key="d4">J Lokoč, S Andreadis, W Bailer, A Duane, C Gurrin…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3650044505606796741&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="18214772028547313138">
  <data key="d0">YOLOv5-R: lightweight real-time detection based on improved YOLOv5</data>
  <data key="d1">18214772028547313138</data>
  <data key="d2">https://www.spiedigitallibrary.org/journals/journal-of-electronic-imaging/volume-31/issue-3/033033/YOLOv5-R--lightweight-real-time-detection-based-on-improved/10.1117/1.JEI.31.3.033033.short</data>
  <data key="d3">YOLOv5-R: lightweight real-time detection based on improved YOLOv5</data>
  <data key="d4">J Ren, Z Wang, Y Zhang, L Liao</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18214772028547313138&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="12157739162413628567">
  <data key="d0">Deep learning based whale detection from satellite imagery</data>
  <data key="d1">12157739162413628567</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2210537923000136</data>
  <data key="d3">Deep learning based whale detection from satellite imagery</data>
  <data key="d4">S Kapoor, M Kumar, M Kaushal</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12157739162413628567&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="8535109044975227347">
  <data key="d0">Research on Intelligent Crack Detection in a Deep-Cut Canal Slope in the Chinese South–North Water Transfer Project</data>
  <data key="d1">8535109044975227347</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/21/5384</data>
  <data key="d3">Research on Intelligent Crack Detection in a Deep-Cut Canal Slope in the Chinese South–North Water Transfer Project</data>
  <data key="d4">Q Hu, P Wang, S Li, W Liu, Y Li, W Lu, Y Kou, F Wei…</data>
  <data key="d5">2022</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8535109044975227347&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="11497367426208390181">
  <data key="d0">Wasserstein gan based chest x-ray dataset augmentation for deep learning models: Covid-19 detection use-case</data>
  <data key="d1">11497367426208390181</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9871519/</data>
  <data key="d3">Wasserstein gan based chest x-ray dataset augmentation for deep learning models: Covid-19 detection use-case</data>
  <data key="d4">BZ Hussain, I Andleeb, MS Ansari…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11497367426208390181&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="4643207554767984878">
  <data key="d0">Swin Transformer Combined with Convolution Neural Network for Surface Defect Detection</data>
  <data key="d1">4643207554767984878</data>
  <data key="d2">https://www.mdpi.com/2075-1702/10/11/1083</data>
  <data key="d3">Swin Transformer Combined with Convolution Neural Network for Surface Defect Detection</data>
  <data key="d4">Y Li, Y Xiang, H Guo, P Liu, C Liu</data>
  <data key="d5">2022</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4643207554767984878&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="18334436816345410388">
  <data key="d0">A cell phone app for facial acne severity assessment</data>
  <data key="d1">18334436816345410388</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10489-022-03774-z</data>
  <data key="d3">A cell phone app for facial acne severity assessment</data>
  <data key="d4">J Wang, Y Luo, Z Wang, AH Hounye, C Cao, M Hou…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18334436816345410388&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="4144441762167508574">
  <data key="d0">Comparison of Object Detection Algorithms for Street-level Objects</data>
  <data key="d1">4144441762167508574</data>
  <data key="d2">https://arxiv.org/abs/2208.11315</data>
  <data key="d3">Comparison of Object Detection Algorithms for Street-level Objects</data>
  <data key="d4">MG Naftali, JS Sulistyawan, K Julian</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4144441762167508574&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="13381915677677103405">
  <data key="d0">Educational Innovation Faced with COVID-19: Deep Learning for Online Exam Cheating Detection</data>
  <data key="d1">13381915677677103405</data>
  <data key="d2">https://www.mdpi.com/2227-7102/13/2/194</data>
  <data key="d3">Educational Innovation Faced with COVID-19: Deep Learning for Online Exam Cheating Detection</data>
  <data key="d4">IN Yulita, FA Hariz, I Suryana, AS Prabuwono</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13381915677677103405&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="6827564526345704344">
  <data key="d0">Transformer based tooth classification from cone-beam computed tomography for dental charting</data>
  <data key="d1">6827564526345704344</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0010482522006205</data>
  <data key="d3">Transformer based tooth classification from cone-beam computed tomography for dental charting</data>
  <data key="d4">S Gao, X Li, X Li, Z Li, Y Deng</data>
  <data key="d5">2022</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6827564526345704344&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="4896734917901521680">
  <data key="d0">Improved YOLOv3 Model for Workpiece Stud Leakage Detection</data>
  <data key="d1">4896734917901521680</data>
  <data key="d2">https://www.mdpi.com/2079-9292/11/21/3430</data>
  <data key="d3">Improved YOLOv3 Model for Workpiece Stud Leakage Detection</data>
  <data key="d4">P Cong, K Lv, H Feng, J Zhou</data>
  <data key="d5">2022</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4896734917901521680&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="16683345231471569427">
  <data key="d0">Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection</data>
  <data key="d1">16683345231471569427</data>
  <data key="d2">https://arxiv.org/abs/2303.09026</data>
  <data key="d3">Commonsense Knowledge Assisted Deep Learning for Resource-constrained and Fine-grained Object Detection</data>
  <data key="d4">P Zhang, B Liu</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16683345231471569427&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="18036771643867343218">
  <data key="d0">Research on tire crack detection using image deep learning method</data>
  <data key="d1">18036771643867343218</data>
  <data key="d2">https://www.nature.com/articles/s41598-023-35227-z</data>
  <data key="d3">Research on tire crack detection using image deep learning method</data>
  <data key="d4">SL Lin</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18036771643867343218&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="7625820698997778911">
  <data key="d0">An application of stereo matching algorithm based on transfer learning on robots in multiple scenes</data>
  <data key="d1">7625820698997778911</data>
  <data key="d2">https://www.nature.com/articles/s41598-023-39964-z</data>
  <data key="d3">An application of stereo matching algorithm based on transfer learning on robots in multiple scenes</data>
  <data key="d4">Y Bi, C Li, X Tong, G Wang, H Sun</data>
  <data key="d5">2023</data>
  <data key="d8">13</data>
</node>
<node id="2858579394623725590">
  <data key="d0">ESDDNet: efficient small defect detection network of workpiece surface</data>
  <data key="d1">2858579394623725590</data>
  <data key="d2">https://iopscience.iop.org/article/10.1088/1361-6501/ac793d/meta</data>
  <data key="d3">ESDDNet: efficient small defect detection network of workpiece surface</data>
  <data key="d4">G Chen, F Xu, G Liu, CM Chen, M Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2858579394623725590&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="366512561177278070">
  <data key="d0">A Novel Driver Abnormal Behavior Recognition and Analysis Strategy and Its Application in a Practical Vehicle</data>
  <data key="d1">366512561177278070</data>
  <data key="d2">https://www.mdpi.com/2073-8994/14/10/1956</data>
  <data key="d3">A Novel Driver Abnormal Behavior Recognition and Analysis Strategy and Its Application in a Practical Vehicle</data>
  <data key="d4">S Liu, X Wang, H Ji, L Wang, Z Hou</data>
  <data key="d5">2022</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=366512561177278070&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="5999650257677576183">
  <data key="d0">Cornernet: Detecting objects as paired keypoints</data>
  <data key="d1">5999650257677576183</data>
  <data key="d2">http://openaccess.thecvf.com/content_ECCV_2018/html/Hei_Law_CornerNet_Detecting_Objects_ECCV_2018_paper.html</data>
  <data key="d3">Cornernet: Detecting objects as paired keypoints</data>
  <data key="d4">H Law, J Deng</data>
  <data key="d5">2018</data>
  <data key="d6">3665</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5999650257677576183&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="1026291601187748397">
  <data key="d0">Object detection in optical remote sensing images: A survey and a new benchmark</data>
  <data key="d1">1026291601187748397</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0924271619302825</data>
  <data key="d3">Object detection in optical remote sensing images: A survey and a new benchmark</data>
  <data key="d4">K Li, G Wan, G Cheng, L Meng, J Han</data>
  <data key="d5">2020</data>
  <data key="d6">895</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1026291601187748397&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="16683125965624867998">
  <data key="d0">Vision transformers for dense prediction</data>
  <data key="d1">16683125965624867998</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Ranftl_Vision_Transformers_for_Dense_Prediction_ICCV_2021_paper.html</data>
  <data key="d3">Vision transformers for dense prediction</data>
  <data key="d4">R Ranftl, A Bochkovskiy…</data>
  <data key="d5">2021</data>
  <data key="d6">883</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16683125965624867998&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14703570951981014637">
  <data key="d0">Center-based 3d object detection and tracking</data>
  <data key="d1">14703570951981014637</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Yin_Center-Based_3D_Object_Detection_and_Tracking_CVPR_2021_paper.html</data>
  <data key="d3">Center-based 3d object detection and tracking</data>
  <data key="d4">T Yin, X Zhou, P Krahenbuhl</data>
  <data key="d5">2021</data>
  <data key="d6">945</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14703570951981014637&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="3594606171683130501">
  <data key="d0">Centernet: Keypoint triplets for object detection</data>
  <data key="d1">3594606171683130501</data>
  <data key="d2">http://openaccess.thecvf.com/content_ICCV_2019/html/Duan_CenterNet_Keypoint_Triplets_for_Object_Detection_ICCV_2019_paper.html</data>
  <data key="d3">Centernet: Keypoint triplets for object detection</data>
  <data key="d4">K Duan, S Bai, L Xie, H Qi…</data>
  <data key="d5">2019</data>
  <data key="d6">2357</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3594606171683130501&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4497917248250067674">
  <data key="d0">Objects as points</data>
  <data key="d1">4497917248250067674</data>
  <data key="d2">https://arxiv.org/abs/1904.07850</data>
  <data key="d3">Objects as points</data>
  <data key="d4">X Zhou, D Wang, P Krähenbühl</data>
  <data key="d5">2019</data>
  <data key="d6">3259</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4497917248250067674&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="7515568945301804297">
  <data key="d0">Towards real-time multi-object tracking</data>
  <data key="d1">7515568945301804297</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-58621-8_7</data>
  <data key="d3">Towards real-time multi-object tracking</data>
  <data key="d4">Z Wang, L Zheng, Y Liu, Y Li, S Wang</data>
  <data key="d5">2020</data>
  <data key="d6">755</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7515568945301804297&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="17367410418412289203">
  <data key="d0">Nas-fpn: Learning scalable feature pyramid architecture for object detection</data>
  <data key="d1">17367410418412289203</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2019/html/Ghiasi_NAS-FPN_Learning_Scalable_Feature_Pyramid_Architecture_for_Object_Detection_CVPR_2019_paper.html</data>
  <data key="d3">Nas-fpn: Learning scalable feature pyramid architecture for object detection</data>
  <data key="d4">G Ghiasi, TY Lin, QV Le</data>
  <data key="d5">2019</data>
  <data key="d6">1401</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17367410418412289203&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="13559955005752027345">
  <data key="d0">Libra r-cnn: Towards balanced learning for object detection</data>
  <data key="d1">13559955005752027345</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2019/html/Pang_Libra_R-CNN_Towards_Balanced_Learning_for_Object_Detection_CVPR_2019_paper.html</data>
  <data key="d3">Libra r-cnn: Towards balanced learning for object detection</data>
  <data key="d4">J Pang, K Chen, J Shi, H Feng…</data>
  <data key="d5">2019</data>
  <data key="d6">1321</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13559955005752027345&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="1086168042066320095">
  <data key="d0">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</data>
  <data key="d1">1086168042066320095</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9018080/</data>
  <data key="d3">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</data>
  <data key="d4">S Shi, Z Wang, J Shi, X Wang…</data>
  <data key="d5">2020</data>
  <data key="d6">625</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1086168042066320095&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="15368170985283079245">
  <data key="d0">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</data>
  <data key="d1">15368170985283079245</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_HigherHRNet_Scale-Aware_Representation_Learning_for_Bottom-Up_Human_Pose_Estimation_CVPR_2020_paper.html</data>
  <data key="d3">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</data>
  <data key="d4">B Cheng, B Xiao, J Wang, H Shi…</data>
  <data key="d5">2020</data>
  <data key="d6">584</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15368170985283079245&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="989963634595144692">
  <data key="d0">Empowering things with intelligence: a survey of the progress, challenges, and opportunities in artificial intelligence of things</data>
  <data key="d1">989963634595144692</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9264235/</data>
  <data key="d3">Empowering things with intelligence: a survey of the progress, challenges, and opportunities in artificial intelligence of things</data>
  <data key="d4">J Zhang, D Tao</data>
  <data key="d5">2020</data>
  <data key="d6">341</data>
  <data key="d7">https://scholar.google.com/scholar?cites=989963634595144692&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="6281152651769068818">
  <data key="d0">Probabilistic regression for visual tracking</data>
  <data key="d1">6281152651769068818</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Danelljan_Probabilistic_Regression_for_Visual_Tracking_CVPR_2020_paper.html</data>
  <data key="d3">Probabilistic regression for visual tracking</data>
  <data key="d4">M Danelljan, LV Gool, R Timofte</data>
  <data key="d5">2020</data>
  <data key="d6">462</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6281152651769068818&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="8945175718646989267">
  <data key="d0">Rethinking imagenet pre-training</data>
  <data key="d1">8945175718646989267</data>
  <data key="d2">http://openaccess.thecvf.com/content_ICCV_2019/html/He_Rethinking_ImageNet_Pre-Training_ICCV_2019_paper.html</data>
  <data key="d3">Rethinking imagenet pre-training</data>
  <data key="d4">K He, R Girshick, P Dollár</data>
  <data key="d5">2019</data>
  <data key="d6">1126</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8945175718646989267&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="18246305387780116611">
  <data key="d0">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</data>
  <data key="d1">18246305387780116611</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_Panoptic-DeepLab_A_Simple_Strong_and_Fast_Baseline_for_Bottom-Up_Panoptic_CVPR_2020_paper.html</data>
  <data key="d3">Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</data>
  <data key="d4">B Cheng, MD Collins, Y Zhu, T Liu…</data>
  <data key="d5">2020</data>
  <data key="d6">505</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18246305387780116611&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="18336064849612757553">
  <data key="d0">Bottom-up object detection by grouping extreme and center points</data>
  <data key="d1">18336064849612757553</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2019/html/Zhou_Bottom-Up_Object_Detection_by_Grouping_Extreme_and_Center_Points_CVPR_2019_paper.html</data>
  <data key="d3">Bottom-up object detection by grouping extreme and center points</data>
  <data key="d4">X Zhou, J Zhuo, P Krahenbuhl</data>
  <data key="d5">2019</data>
  <data key="d6">975</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18336064849612757553&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="17369531722515506311">
  <data key="d0">Feature selective anchor-free module for single-shot object detection</data>
  <data key="d1">17369531722515506311</data>
  <data key="d2">http://openaccess.thecvf.com/content_CVPR_2019/html/Zhu_Feature_Selective_Anchor-Free_Module_for_Single-Shot_Object_Detection_CVPR_2019_paper.html</data>
  <data key="d3">Feature selective anchor-free module for single-shot object detection</data>
  <data key="d4">C Zhu, Y He, M Savvides</data>
  <data key="d5">2019</data>
  <data key="d6">840</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17369531722515506311&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="6731983785945015219">
  <data key="d0">Reppoints: Point set representation for object detection</data>
  <data key="d1">6731983785945015219</data>
  <data key="d2">http://openaccess.thecvf.com/content_ICCV_2019/html/Yang_RepPoints_Point_Set_Representation_for_Object_Detection_ICCV_2019_paper.html</data>
  <data key="d3">Reppoints: Point set representation for object detection</data>
  <data key="d4">Z Yang, S Liu, H Hu, L Wang…</data>
  <data key="d5">2019</data>
  <data key="d6">783</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6731983785945015219&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="13359963855282714219">
  <data key="d0">M2det: A single-shot object detector based on multi-level feature pyramid network</data>
  <data key="d1">13359963855282714219</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/4962</data>
  <data key="d3">M2det: A single-shot object detector based on multi-level feature pyramid network</data>
  <data key="d4">Q Zhao, T Sheng, Y Wang, Z Tang, Y Chen…</data>
  <data key="d5">2019</data>
  <data key="d6">785</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13359963855282714219&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="1545024244511440922">
  <data key="d0">High-resolution representations for labeling pixels and regions</data>
  <data key="d1">1545024244511440922</data>
  <data key="d2">https://arxiv.org/abs/1904.04514</data>
  <data key="d3">High-resolution representations for labeling pixels and regions</data>
  <data key="d4">K Sun, Y Zhao, B Jiang, T Cheng, B Xiao, D Liu…</data>
  <data key="d5">2019</data>
  <data key="d6">725</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1545024244511440922&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="5426394606047885053">
  <data key="d0">Embracing single stride 3d object detector with sparse transformer</data>
  <data key="d1">5426394606047885053</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Fan_Embracing_Single_Stride_3D_Object_Detector_With_Sparse_Transformer_CVPR_2022_paper.html</data>
  <data key="d3">Embracing single stride 3d object detector with sparse transformer</data>
  <data key="d4">L Fan, Z Pang, T Zhang, YX Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">114</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5426394606047885053&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="10747326374097383015">
  <data key="d0">Improving 3d object detection with channel-wise transformer</data>
  <data key="d1">10747326374097383015</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021/html/Sheng_Improving_3D_Object_Detection_With_Channel-Wise_Transformer_ICCV_2021_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">Improving 3d object detection with channel-wise transformer</data>
  <data key="d4">H Sheng, S Cai, Y Liu, B Deng…</data>
  <data key="d5">2021</data>
  <data key="d6">159</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10747326374097383015&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="6145323722830593859">
  <data key="d0">A simple semi-supervised learning framework for object detection</data>
  <data key="d1">6145323722830593859</data>
  <data key="d2">https://arxiv.org/abs/2005.04757</data>
  <data key="d3">A simple semi-supervised learning framework for object detection</data>
  <data key="d4">K Sohn, Z Zhang, CL Li, H Zhang, CY Lee…</data>
  <data key="d5">2020</data>
  <data key="d6">357</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6145323722830593859&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14479605661908902222">
  <data key="d0">Arbitrary-oriented object detection with circular smooth label</data>
  <data key="d1">14479605661908902222</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-58598-3_40</data>
  <data key="d3">Arbitrary-oriented object detection with circular smooth label</data>
  <data key="d4">X Yang, J Yan</data>
  <data key="d5">2020</data>
  <data key="d6">327</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14479605661908902222&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11348760748613165934">
  <data key="d0">Bottom-up human pose estimation via disentangled keypoint regression</data>
  <data key="d1">11348760748613165934</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Geng_Bottom-Up_Human_Pose_Estimation_via_Disentangled_Keypoint_Regression_CVPR_2021_paper.html</data>
  <data key="d3">Bottom-up human pose estimation via disentangled keypoint regression</data>
  <data key="d4">Z Geng, K Sun, B Xiao, Z Zhang…</data>
  <data key="d5">2021</data>
  <data key="d6">162</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11348760748613165934&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="12180218794931662709">
  <data key="d0">FAIR1M: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery</data>
  <data key="d1">12180218794931662709</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0924271621003269</data>
  <data key="d3">FAIR1M: A benchmark dataset for fine-grained object recognition in high-resolution remote sensing imagery</data>
  <data key="d4">X Sun, P Wang, Z Yan, F Xu, R Wang, W Diao…</data>
  <data key="d5">2022</data>
  <data key="d6">161</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12180218794931662709&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="2597834166337025404">
  <data key="d0">Computer vision for autonomous vehicles: Problems, datasets and state of the art</data>
  <data key="d1">2597834166337025404</data>
  <data key="d2">https://www.nowpublishers.com/article/Details/CGV-079</data>
  <data key="d3">Computer vision for autonomous vehicles: Problems, datasets and state of the art</data>
  <data key="d4">J Janai, F Güney, A Behl, A Geiger</data>
  <data key="d5">2020</data>
  <data key="d6">850</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2597834166337025404&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="16128994104714577384">
  <data key="d0">A survey on generative diffusion model</data>
  <data key="d1">16128994104714577384</data>
  <data key="d2">https://arxiv.org/abs/2209.02646</data>
  <data key="d3">A survey on generative diffusion model</data>
  <data key="d4">H Cao, C Tan, Z Gao, Y Xu, G Chen, PA Heng…</data>
  <data key="d5">2022</data>
  <data key="d6">73</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16128994104714577384&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="7941234669662371746">
  <data key="d0">Diffusion action segmentation</data>
  <data key="d1">7941234669662371746</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Diffusion_Action_Segmentation_ICCV_2023_paper.html</data>
  <data key="d3">Diffusion action segmentation</data>
  <data key="d4">D Liu, Q Li, AD Dinh, T Jiang…</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7941234669662371746&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="13076252047124330234">
  <data key="d0">Periodically exchange teacher-student for source-free object detection</data>
  <data key="d1">13076252047124330234</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Liu_Periodically_Exchange_Teacher-Student_for_Source-Free_Object_Detection_ICCV_2023_paper.html</data>
  <data key="d3">Periodically exchange teacher-student for source-free object detection</data>
  <data key="d4">Q Liu, L Lin, Z Shen, Z Yang</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13076252047124330234&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="12525771403000822454">
  <data key="d0">Generative prompt model for weakly supervised object localization</data>
  <data key="d1">12525771403000822454</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Zhao_Generative_Prompt_Model_for_Weakly_Supervised_Object_Localization_ICCV_2023_paper.html</data>
  <data key="d3">Generative prompt model for weakly supervised object localization</data>
  <data key="d4">Y Zhao, Q Ye, W Wu, C Shen…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12525771403000822454&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="10942366611650661886">
  <data key="d0">Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation</data>
  <data key="d1">10942366611650661886</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Peng_Diffusion-based_Image_Translation_with_Label_Guidance_for_Domain_Adaptive_Semantic_ICCV_2023_paper.html</data>
  <data key="d3">Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation</data>
  <data key="d4">D Peng, P Hu, Q Ke, J Liu</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10942366611650661886&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="12060121231554906872">
  <data key="d0">Belfusion: Latent diffusion for behavior-driven human motion prediction</data>
  <data key="d1">12060121231554906872</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Barquero_BeLFusion_Latent_Diffusion_for_Behavior-Driven_Human_Motion_Prediction_ICCV_2023_paper.html</data>
  <data key="d3">Belfusion: Latent diffusion for behavior-driven human motion prediction</data>
  <data key="d4">G Barquero, S Escalera…</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12060121231554906872&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="13417650621921820369">
  <data key="d0">Ddp: Diffusion model for dense visual prediction</data>
  <data key="d1">13417650621921820369</data>
  <data key="d2">https://arxiv.org/abs/2303.17559</data>
  <data key="d3">Ddp: Diffusion model for dense visual prediction</data>
  <data key="d4">Y Ji, Z Chen, E Xie, L Hong, X Liu, Z Liu, T Lu…</data>
  <data key="d5">2023</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13417650621921820369&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="6326589465262223216">
  <data key="d0">DiffPose: SpatioTemporal diffusion model for video-based human pose estimation</data>
  <data key="d1">6326589465262223216</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Feng_DiffPose_SpatioTemporal_Diffusion_Model_for_Video-Based_Human_Pose_Estimation_ICCV_2023_paper.html</data>
  <data key="d3">DiffPose: SpatioTemporal diffusion model for video-based human pose estimation</data>
  <data key="d4">R Feng, Y Gao, THE Tse, X Ma…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6326589465262223216&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="1678541804692472087">
  <data key="d0">Unleashing text-to-image diffusion models for visual perception</data>
  <data key="d1">1678541804692472087</data>
  <data key="d2">https://arxiv.org/abs/2303.02153</data>
  <data key="d3">Unleashing text-to-image diffusion models for visual perception</data>
  <data key="d4">W Zhao, Y Rao, Z Liu, B Liu, J Zhou, J Lu</data>
  <data key="d5">2023</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1678541804692472087&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="11360965149573855814">
  <data key="d0">Learning to schedule in diffusion probabilistic models</data>
  <data key="d1">11360965149573855814</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3580305.3599412</data>
  <data key="d3">Learning to schedule in diffusion probabilistic models</data>
  <data key="d4">Y Wang, X Wang, AD Dinh, B Du, C Xu</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11360965149573855814&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="10564325131634348410">
  <data key="d0">Diffusion models in medical imaging: A comprehensive survey</data>
  <data key="d1">10564325131634348410</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841523001068</data>
  <data key="d3">Diffusion models in medical imaging: A comprehensive survey</data>
  <data key="d4">A Kazerouni, EK Aghdam, M Heidari, R Azad…</data>
  <data key="d5">2023</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10564325131634348410&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="9095926209623917067">
  <data key="d0">Exploring diffusion models for unsupervised video anomaly detection</data>
  <data key="d1">9095926209623917067</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10222594/</data>
  <data key="d3">Exploring diffusion models for unsupervised video anomaly detection</data>
  <data key="d4">AO Tur, N Dall'Asen, C Beyan…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9095926209623917067&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="17382263080131354066">
  <data key="d0">Diffusioninst: Diffusion model for instance segmentation</data>
  <data key="d1">17382263080131354066</data>
  <data key="d2">https://arxiv.org/abs/2212.02773</data>
  <data key="d3">Diffusioninst: Diffusion model for instance segmentation</data>
  <data key="d4">Z Gu, H Chen, Z Xu, J Lan, C Meng, W Wang</data>
  <data key="d5">2022</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17382263080131354066&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="8819593779078451823">
  <data key="d0">Dif-fusion: Towards high color fidelity in infrared and visible image fusion with diffusion models</data>
  <data key="d1">8819593779078451823</data>
  <data key="d2">https://arxiv.org/abs/2301.08072</data>
  <data key="d3">Dif-fusion: Towards high color fidelity in infrared and visible image fusion with diffusion models</data>
  <data key="d4">J Yue, L Fang, S Xia, Y Deng, J Ma</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8819593779078451823&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="F4Zoh0oPXE0J">
  <data key="d0">LoTE-Animal: A Long Time-span Dataset for Endangered Animal Behavior Understanding</data>
  <data key="d1">F4Zoh0oPXE0J</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Liu_LoTE-Animal_A_Long_Time-span_Dataset_for_Endangered_Animal_Behavior_Understanding_ICCV_2023_paper.html</data>
  <data key="d3">LoTE-Animal: A Long Time-span Dataset for Endangered Animal Behavior Understanding</data>
  <data key="d4">D Liu, J Hou, S Huang, J Liu, Y He…</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="zMD8xoVPVA0J">
  <data key="d0">Feature Prediction Diffusion Model for Video Anomaly Detection</data>
  <data key="d1">zMD8xoVPVA0J</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Yan_Feature_Prediction_Diffusion_Model_for_Video_Anomaly_Detection_ICCV_2023_paper.html</data>
  <data key="d3">Feature Prediction Diffusion Model for Video Anomaly Detection</data>
  <data key="d4">C Yan, S Zhang, Y Liu, G Pang…</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="13193353915930591338">
  <data key="d0">Diffusionret: Generative text-video retrieval with diffusion model</data>
  <data key="d1">13193353915930591338</data>
  <data key="d2">https://arxiv.org/abs/2303.09867</data>
  <data key="d3">Diffusionret: Generative text-video retrieval with diffusion model</data>
  <data key="d4">P Jin, H Li, Z Cheng, K Li, X Ji, C Liu, L Yuan…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13193353915930591338&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="11287002185478389153">
  <data key="d0">Boundary-denoising for video activity localization</data>
  <data key="d1">11287002185478389153</data>
  <data key="d2">https://arxiv.org/abs/2304.02934</data>
  <data key="d3">Boundary-denoising for video activity localization</data>
  <data key="d4">M Xu, M Soldan, J Gao, S Liu, JM Pérez-Rúa…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11287002185478389153&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="1777455714178307276">
  <data key="d0">Random Boxes Are Open-world Object Detectors</data>
  <data key="d1">1777455714178307276</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Random_Boxes_Are_Open-world_Object_Detectors_ICCV_2023_paper.html</data>
  <data key="d3">Random Boxes Are Open-world Object Detectors</data>
  <data key="d4">Y Wang, Z Yue, XS Hua…</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="3143529597089035400">
  <data key="d0">Diffmic: Dual-guidance diffusion network for medical image classification</data>
  <data key="d1">3143529597089035400</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-43987-2_10</data>
  <data key="d3">Diffmic: Dual-guidance diffusion network for medical image classification</data>
  <data key="d4">Y Yang, H Fu, AI Aviles-Rivero, CB Schönlieb…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3143529597089035400&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="9132989520533932624">
  <data key="d0">Deep Equilibrium Object Detection</data>
  <data key="d1">9132989520533932624</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Deep_Equilibrium_Object_Detection_ICCV_2023_paper.html</data>
  <data key="d3">Deep Equilibrium Object Detection</data>
  <data key="d4">S Wang, Y Teng, L Wang</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="sLUFVBct83MJ">
  <data key="d0">Unsupervised Surface Anomaly Detection with Diffusion Probabilistic Model</data>
  <data key="d1">sLUFVBct83MJ</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Unsupervised_Surface_Anomaly_Detection_with_Diffusion_Probabilistic_Model_ICCV_2023_paper.html</data>
  <data key="d3">Unsupervised Surface Anomaly Detection with Diffusion Probabilistic Model</data>
  <data key="d4">X Zhang, N Li, J Li, T Dai, Y Jiang…</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="5862922647569477938">
  <data key="d0">RecursiveDet: End-to-End Region-based Recursive Object Detection</data>
  <data key="d1">5862922647569477938</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2023/html/Zhao_RecursiveDet_End-to-End_Region-Based_Recursive_Object_Detection_ICCV_2023_paper.html</data>
  <data key="d3">RecursiveDet: End-to-End Region-based Recursive Object Detection</data>
  <data key="d4">J Zhao, L Sun, Q Li</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="6644117076261818901">
  <data key="d0">DiffusionNER: Boundary Diffusion for Named Entity Recognition</data>
  <data key="d1">6644117076261818901</data>
  <data key="d2">https://arxiv.org/abs/2305.13298</data>
  <data key="d3">DiffusionNER: Boundary Diffusion for Named Entity Recognition</data>
  <data key="d4">Y Shen, K Song, X Tan, D Li, W Lu…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6644117076261818901&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="1224216148116518096">
  <data key="d0">Towards Generic and Controllable Attacks Against Object Detection</data>
  <data key="d1">1224216148116518096</data>
  <data key="d2">https://arxiv.org/abs/2307.12342</data>
  <data key="d3">Towards Generic and Controllable Attacks Against Object Detection</data>
  <data key="d4">G Li, Y Xu, J Ding, GS Xia</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1224216148116518096&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="18029852880512974017">
  <data key="d0">Revisiting Table Detection Datasets for Visually Rich Documents</data>
  <data key="d1">18029852880512974017</data>
  <data key="d2">https://arxiv.org/abs/2305.04833</data>
  <data key="d3">Revisiting Table Detection Datasets for Visually Rich Documents</data>
  <data key="d4">B Xiao, M Simsek, B Kantarci, AA Alkheir</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18029852880512974017&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="4088751988372467068">
  <data key="d0">A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</data>
  <data key="d1">4088751988372467068</data>
  <data key="d2">https://arxiv.org/abs/2305.15347</data>
  <data key="d3">A Tale of Two Features: Stable Diffusion Complements DINO for Zero-Shot Semantic Correspondence</data>
  <data key="d4">J Zhang, C Herrmann, J Hur, LP Cabrera…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4088751988372467068&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="16273778234448951456">
  <data key="d0">Structural Pruning for Diffusion Models</data>
  <data key="d1">16273778234448951456</data>
  <data key="d2">https://arxiv.org/abs/2305.10924</data>
  <data key="d3">Structural Pruning for Diffusion Models</data>
  <data key="d4">G Fang, X Ma, X Wang</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16273778234448951456&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="17592923797249586375">
  <data key="d0">Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation</data>
  <data key="d1">17592923797249586375</data>
  <data key="d2">https://arxiv.org/abs/2303.11579</data>
  <data key="d3">Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation</data>
  <data key="d4">W Shan, Z Liu, X Zhang, Z Wang, K Han…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17592923797249586375&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="6142589876195895306">
  <data key="d0">DLT: Conditioned layout generation with Joint Discrete-Continuous Diffusion Layout Transformer</data>
  <data key="d1">6142589876195895306</data>
  <data key="d2">https://arxiv.org/abs/2303.03755</data>
  <data key="d3">DLT: Conditioned layout generation with Joint Discrete-Continuous Diffusion Layout Transformer</data>
  <data key="d4">E Levi, E Brosh, M Mykhailych, M Perez</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6142589876195895306&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="3960507448835907852">
  <data key="d0">Controllable Mind Visual Diffusion Model</data>
  <data key="d1">3960507448835907852</data>
  <data key="d2">https://arxiv.org/abs/2305.10135</data>
  <data key="d3">Controllable Mind Visual Diffusion Model</data>
  <data key="d4">B Zeng, S Li, X Liu, S Gao, X Jiang, X Tang…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3960507448835907852&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="G95UAM_jS08J">
  <data key="d0">SpectralDiff: A Generative Framework for Hyperspectral Image Classification with Diffusion Models</data>
  <data key="d1">G95UAM_jS08J</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10234379/</data>
  <data key="d3">SpectralDiff: A Generative Framework for Hyperspectral Image Classification with Diffusion Models</data>
  <data key="d4">N Chen, J Yue, L Fang, S Xia</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="16273230188622356845">
  <data key="d0">Denoising Diffusion Semantic Segmentation with Mask Prior Modeling</data>
  <data key="d1">16273230188622356845</data>
  <data key="d2">https://arxiv.org/abs/2306.01721</data>
  <data key="d3">Denoising Diffusion Semantic Segmentation with Mask Prior Modeling</data>
  <data key="d4">Z Lai, Y Duan, J Dai, Z Li, Y Fu, H Li, Y Qiao…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16273230188622356845&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="7240823796445100518">
  <data key="d0">Diffusionstr: Diffusion model for scene text recognition</data>
  <data key="d1">7240823796445100518</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10222793/</data>
  <data key="d3">Diffusionstr: Diffusion model for scene text recognition</data>
  <data key="d4">M Fujitake</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7240823796445100518&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="3157779878157184062">
  <data key="d0">The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World</data>
  <data key="d1">3157779878157184062</data>
  <data key="d2">https://arxiv.org/abs/2308.01907</data>
  <data key="d3">The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World</data>
  <data key="d4">W Wang, M Shi, Q Li, W Wang, Z Huang, L Xing…</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="11623041935010349796">
  <data key="d0">Conformal prediction for trustworthy detection of railway signals</data>
  <data key="d1">11623041935010349796</data>
  <data key="d2">https://arxiv.org/abs/2301.11136</data>
  <data key="d3">Conformal prediction for trustworthy detection of railway signals</data>
  <data key="d4">L Andéol, T Fel, F De Grancey, L Mossina</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11623041935010349796&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="14202645124111525490">
  <data key="d0">Enhancing gland segmentation in colon histology images using an instance-aware diffusion model</data>
  <data key="d1">14202645124111525490</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0010482523009927</data>
  <data key="d3">Enhancing gland segmentation in colon histology images using an instance-aware diffusion model</data>
  <data key="d4">M Sun, J Wang, Q Gong, W Huang</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="a96v83mzAUoJ">
  <data key="d0">Dream the Impossible: Outlier Imagination with Diffusion Models</data>
  <data key="d1">a96v83mzAUoJ</data>
  <data key="d2">https://arxiv.org/abs/2309.13415</data>
  <data key="d3">Dream the Impossible: Outlier Imagination with Diffusion Models</data>
  <data key="d4">X Du, Y Sun, X Zhu, Y Li</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="7355981587734255595">
  <data key="d0">DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models</data>
  <data key="d1">7355981587734255595</data>
  <data key="d2">https://arxiv.org/abs/2308.00122</data>
  <data key="d3">DAVIS: High-Quality Audio-Visual Separation with Generative Diffusion Models</data>
  <data key="d4">C Huang, S Liang, Y Tian, A Kumar, C Xu</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="2741005343281217553">
  <data key="d0">Learning proximal operators to discover multiple optima</data>
  <data key="d1">2741005343281217553</data>
  <data key="d2">https://arxiv.org/abs/2201.11945</data>
  <data key="d3">Learning proximal operators to discover multiple optima</data>
  <data key="d4">L Li, N Aigerman, VG Kim, J Li, K Greenewald…</data>
  <data key="d5">2022</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2741005343281217553&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="6909744142839740328">
  <data key="d0">Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt</data>
  <data key="d1">6909744142839740328</data>
  <data key="d2">https://arxiv.org/abs/2306.04607</data>
  <data key="d3">Integrating Geometric Control into Text-to-Image Diffusion Models for High-Quality Detection Data Generation via Text Prompt</data>
  <data key="d4">K Chen, E Xie, Z Chen, L Hong, Z Li…</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="11759926192076555716">
  <data key="d0">DFormer: Diffusion-guided Transformer for Universal Image Segmentation</data>
  <data key="d1">11759926192076555716</data>
  <data key="d2">https://arxiv.org/abs/2306.03437</data>
  <data key="d3">DFormer: Diffusion-guided Transformer for Universal Image Segmentation</data>
  <data key="d4">H Wang, J Cao, RM Anwer, J Xie, FS Khan…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11759926192076555716&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="OaSros3oKKAJ">
  <data key="d0">Diffusion-based 3D Object Detection with Random Boxes</data>
  <data key="d1">OaSros3oKKAJ</data>
  <data key="d2">https://arxiv.org/abs/2309.02049</data>
  <data key="d3">Diffusion-based 3D Object Detection with Random Boxes</data>
  <data key="d4">X Zhou, J Hou, T Yao, D Liang, Z Liu, Z Zou…</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="811326051739083462">
  <data key="d0">Learning A Coarse-to-Fine Diffusion Transformer for Image Restoration</data>
  <data key="d1">811326051739083462</data>
  <data key="d2">https://arxiv.org/abs/2308.08730</data>
  <data key="d3">Learning A Coarse-to-Fine Diffusion Transformer for Image Restoration</data>
  <data key="d4">L Wang, Q Yang, C Wang, W Wang, J Pan…</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="14242124119478297788">
  <data key="d0">Realistic Noise Synthesis with Diffusion Models</data>
  <data key="d1">14242124119478297788</data>
  <data key="d2">https://arxiv.org/abs/2305.14022</data>
  <data key="d3">Realistic Noise Synthesis with Diffusion Models</data>
  <data key="d4">Q Wu, M Han, T Jiang, H Fan, B Zeng, S Liu</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14242124119478297788&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="4554030306715313521">
  <data key="d0">Zero-Shot Image Harmonization with Generative Model Prior</data>
  <data key="d1">4554030306715313521</data>
  <data key="d2">https://arxiv.org/abs/2307.08182</data>
  <data key="d3">Zero-Shot Image Harmonization with Generative Model Prior</data>
  <data key="d4">J Chen, Z Zou, Y Zhang, K Chen, Z Shi</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="4168455509224505377">
  <data key="d0">Table Detection for Visually Rich Document Images</data>
  <data key="d1">4168455509224505377</data>
  <data key="d2">https://arxiv.org/abs/2305.19181</data>
  <data key="d3">Table Detection for Visually Rich Document Images</data>
  <data key="d4">B Xiao, M Simsek, B Kantarci, AA Alkheir</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="13803166602830577984">
  <data key="d0">Ref-Diff: Zero-shot Referring Image Segmentation with Generative Models</data>
  <data key="d1">13803166602830577984</data>
  <data key="d2">https://arxiv.org/abs/2308.16777</data>
  <data key="d3">Ref-Diff: Zero-shot Referring Image Segmentation with Generative Models</data>
  <data key="d4">M Ni, Y Zhang, K Feng, X Li, Y Guo, W Zuo</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="764295440409526857">
  <data key="d0">DiffBEV: Conditional Diffusion Model for Bird's Eye View Perception</data>
  <data key="d1">764295440409526857</data>
  <data key="d2">https://arxiv.org/abs/2303.08333</data>
  <data key="d3">DiffBEV: Conditional Diffusion Model for Bird's Eye View Perception</data>
  <data key="d4">J Zou, Z Zhu, Y Ye, X Wang</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=764295440409526857&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="I3r9FbwuMeoJ">
  <data key="d0">Semi-Supervised Cross Domain Teacher-Student Mutual Training for Damaged Building Detection</data>
  <data key="d1">I3r9FbwuMeoJ</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10175376/</data>
  <data key="d3">Semi-Supervised Cross Domain Teacher-Student Mutual Training for Damaged Building Detection</data>
  <data key="d4">J Pan, P Yin, X Sun, J Tan, W Li</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="3555227715383495415">
  <data key="d0">Learning Structure-Guided Diffusion Model for 2D Human Pose Estimation</data>
  <data key="d1">3555227715383495415</data>
  <data key="d2">https://arxiv.org/abs/2306.17074</data>
  <data key="d3">Learning Structure-Guided Diffusion Model for 2D Human Pose Estimation</data>
  <data key="d4">Z Qiu, Q Yang, J Wang, X Wang, C Xu, D Fu…</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="3070099986647090116">
  <data key="d0">DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion</data>
  <data key="d1">3070099986647090116</data>
  <data key="d2">https://arxiv.org/abs/2303.14863</data>
  <data key="d3">DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion</data>
  <data key="d4">S Nag, X Zhu, J Deng, YZ Song, T Xiang</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3070099986647090116&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="3366278070706666359">
  <data key="d0">SpectralDiff: Hyperspectral Image Classification with Spectral-Spatial Diffusion Models</data>
  <data key="d1">3366278070706666359</data>
  <data key="d2">https://arxiv.org/abs/2304.05961</data>
  <data key="d3">SpectralDiff: Hyperspectral Image Classification with Spectral-Spatial Diffusion Models</data>
  <data key="d4">N Chen, J Yue, L Fang, S Xia</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3366278070706666359&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="7399476248812336314">
  <data key="d0">Diffusion Models Beat GANs on Image Classification</data>
  <data key="d1">7399476248812336314</data>
  <data key="d2">https://arxiv.org/abs/2307.08702</data>
  <data key="d3">Diffusion Models Beat GANs on Image Classification</data>
  <data key="d4">S Mukhopadhyay, M Gwilliam, V Agarwal…</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="6622643112892915762">
  <data key="d0">Diffusion Probabilistic Models for Graph-Structured Prediction</data>
  <data key="d1">6622643112892915762</data>
  <data key="d2">https://arxiv.org/abs/2302.10506</data>
  <data key="d3">Diffusion Probabilistic Models for Graph-Structured Prediction</data>
  <data key="d4">H Jang, S Mo, S Ahn</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6622643112892915762&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="261887705248316913">
  <data key="d0">CLE Diffusion: Controllable Light Enhancement Diffusion Model</data>
  <data key="d1">261887705248316913</data>
  <data key="d2">https://arxiv.org/abs/2308.06725</data>
  <data key="d3">CLE Diffusion: Controllable Light Enhancement Diffusion Model</data>
  <data key="d4">Y Yin, D Xu, C Tan, P Liu, Y Zhao, Y Wei</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="8435158920283992591">
  <data key="d0">A Survey on Deep Learning-based Spatio-temporal Action Detection</data>
  <data key="d1">8435158920283992591</data>
  <data key="d2">https://arxiv.org/abs/2308.01618</data>
  <data key="d3">A Survey on Deep Learning-based Spatio-temporal Action Detection</data>
  <data key="d4">P Wang, F Zeng, Y Qian</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="7539048894844258473">
  <data key="d0">Reference-based Painterly Inpainting via Diffusion: Crossing the Wild Reference Domain Gap</data>
  <data key="d1">7539048894844258473</data>
  <data key="d2">https://arxiv.org/abs/2307.10584</data>
  <data key="d3">Reference-based Painterly Inpainting via Diffusion: Crossing the Wild Reference Domain Gap</data>
  <data key="d4">D Xu, X Xu, W Cong, H Shi, Z Wang</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="17520432466589520461">
  <data key="d0">DiffusionVMR: Diffusion Model for Video Moment Retrieval</data>
  <data key="d1">17520432466589520461</data>
  <data key="d2">https://arxiv.org/abs/2308.15109</data>
  <data key="d3">DiffusionVMR: Diffusion Model for Video Moment Retrieval</data>
  <data key="d4">H Zhao, KQ Lin, R Yan, Z Li</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="2202905144227684777">
  <data key="d0">DiffSED: Sound Event Detection with Denoising Diffusion</data>
  <data key="d1">2202905144227684777</data>
  <data key="d2">https://arxiv.org/abs/2308.07293</data>
  <data key="d3">DiffSED: Sound Event Detection with Denoising Diffusion</data>
  <data key="d4">S Bhosale, S Nag, D Kanojia, J Deng, X Zhu</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="3941086606225563243">
  <data key="d0">DiffusionTrack: Diffusion Model For Multi-Object Tracking</data>
  <data key="d1">3941086606225563243</data>
  <data key="d2">https://arxiv.org/abs/2308.09905</data>
  <data key="d3">DiffusionTrack: Diffusion Model For Multi-Object Tracking</data>
  <data key="d4">R Luo, Z Song, L Ma, J Wei, W Yang…</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="SOWfTYHvJj8J">
  <data key="d0">PSDiff: Diffusion Model for Person Search with Iterative and Collaborative Refinement</data>
  <data key="d1">SOWfTYHvJj8J</data>
  <data key="d2">https://arxiv.org/abs/2309.11125</data>
  <data key="d3">PSDiff: Diffusion Model for Person Search with Iterative and Collaborative Refinement</data>
  <data key="d4">C Jia, M Luo, Z Dang, G Dai, X Chang, J Wang…</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="7227275169875605471">
  <data key="d0">Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling</data>
  <data key="d1">7227275169875605471</data>
  <data key="d2">https://arxiv.org/abs/2308.10705</data>
  <data key="d3">Unsupervised 3D Pose Estimation with Non-Rigid Structure-from-Motion Modeling</data>
  <data key="d4">H Ji, H Deng, Y Dai, H Li</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="6835716193545665416">
  <data key="d0">DENTEX: An Abnormal Tooth Detection with Dental Enumeration and Diagnosis Benchmark for Panoramic X-rays</data>
  <data key="d1">6835716193545665416</data>
  <data key="d2">https://arxiv.org/abs/2305.19112</data>
  <data key="d3">DENTEX: An Abnormal Tooth Detection with Dental Enumeration and Diagnosis Benchmark for Panoramic X-rays</data>
  <data key="d4">IE Hamamci, S Er, E Simsar, AE Yuksel…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6835716193545665416&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="17019817652517638697">
  <data key="d0">Confident Object Detection via Conformal Prediction and Conformal Risk Control: an Application to Railway Signaling</data>
  <data key="d1">17019817652517638697</data>
  <data key="d2">https://arxiv.org/abs/2304.06052</data>
  <data key="d3">Confident Object Detection via Conformal Prediction and Conformal Risk Control: an Application to Railway Signaling</data>
  <data key="d4">L Andéol, T Fel, F De Grancey, L Mossina</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="16289864856633093116">
  <data key="d0">VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models</data>
  <data key="d1">16289864856633093116</data>
  <data key="d2">https://arxiv.org/abs/2306.06874</data>
  <data key="d3">VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion Models</data>
  <data key="d4">SY Chou, PY Chen, TY Ho</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16289864856633093116&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="4733234527636485458">
  <data key="d0">DiffMatch: Diffusion Model for Dense Matching</data>
  <data key="d1">4733234527636485458</data>
  <data key="d2">https://arxiv.org/abs/2305.19094</data>
  <data key="d3">DiffMatch: Diffusion Model for Dense Matching</data>
  <data key="d4">J Nam, G Lee, S Kim, H Kim, H Cho, S Kim…</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<node id="9801496609815990936">
  <data key="d0">Real-World Denoising via Diffusion Model</data>
  <data key="d1">9801496609815990936</data>
  <data key="d2">https://arxiv.org/abs/2305.04457</data>
  <data key="d3">Real-World Denoising via Diffusion Model</data>
  <data key="d4">C Yang, L Liang, Z Su</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9801496609815990936&amp;as_sdt=20000005&amp;sciodt=0,21&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="n4rt-tP6doUJ">
  <data key="d0">Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images</data>
  <data key="d1">n4rt-tP6doUJ</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-43987-2_64</data>
  <data key="d3">Instance-Aware Diffusion Model for Gland Segmentation in Colon Histology Images</data>
  <data key="d4">M Sun, W Huang, Y Zheng</data>
  <data key="d5">2023</data>
  <data key="d8">18</data>
</node>
<edge source="7522504961268153944" target="6382612685700818764"/>
<edge source="7522504961268153944" target="1672665553767281734"/>
<edge source="15456065911372617945" target="7522504961268153944"/>
<edge source="15456065911372617945" target="1672665553767281734"/>
<edge source="7749897961068121501" target="7522504961268153944"/>
<edge source="7749897961068121501" target="1672665553767281734"/>
<edge source="7749897961068121501" target="1431633311194488622"/>
<edge source="16431204865977056518" target="7522504961268153944"/>
<edge source="16431204865977056518" target="1672665553767281734"/>
<edge source="10110104334022835757" target="7522504961268153944"/>
<edge source="14988305211504802629" target="7522504961268153944"/>
<edge source="15635397108812213817" target="7522504961268153944"/>
<edge source="15635397108812213817" target="1672665553767281734"/>
<edge source="15172072370662904150" target="7522504961268153944"/>
<edge source="14311400318178337111" target="7522504961268153944"/>
<edge source="14311400318178337111" target="1672665553767281734"/>
<edge source="14311400318178337111" target="12563424232401784595"/>
<edge source="14311400318178337111" target="18210588638302847093"/>
<edge source="14311400318178337111" target="4550423614333066109"/>
<edge source="14311400318178337111" target="13055852169785823920"/>
<edge source="14311400318178337111" target="13346469121565650680"/>
<edge source="14311400318178337111" target="6382612685700818764"/>
<edge source="17327663970405370182" target="7522504961268153944"/>
<edge source="17327663970405370182" target="1672665553767281734"/>
<edge source="6784655767122395745" target="7522504961268153944"/>
<edge source="6784655767122395745" target="1672665553767281734"/>
<edge source="10445431151558976110" target="7522504961268153944"/>
<edge source="17272204730668948165" target="7522504961268153944"/>
<edge source="17272204730668948165" target="1672665553767281734"/>
<edge source="17272204730668948165" target="2104521862399993019"/>
<edge source="17272204730668948165" target="11324600873272743514"/>
<edge source="4397240948250887492" target="7522504961268153944"/>
<edge source="4397240948250887492" target="17325452893759607893"/>
<edge source="982391967541643955" target="7522504961268153944"/>
<edge source="2524690293389739454" target="7522504961268153944"/>
<edge source="8226119387953108124" target="7522504961268153944"/>
<edge source="8226119387953108124" target="4041041901496425203"/>
<edge source="8226119387953108124" target="1431633311194488622"/>
<edge source="4109211049740650760" target="7522504961268153944"/>
<edge source="13405798017275933263" target="7522504961268153944"/>
<edge source="7207323120779432545" target="7522504961268153944"/>
<edge source="7207323120779432545" target="535744563039386055"/>
<edge source="572712567324091714" target="7522504961268153944"/>
<edge source="12569071048746881465" target="7522504961268153944"/>
<edge source="10894599314289744077" target="7522504961268153944"/>
<edge source="13410997068826787348" target="7522504961268153944"/>
<edge source="8678112700060654133" target="7522504961268153944"/>
<edge source="14580693445961229120" target="7522504961268153944"/>
<edge source="8840944912676876934" target="7522504961268153944"/>
<edge source="9207086502844896554" target="7522504961268153944"/>
<edge source="10761248177036470713" target="7522504961268153944"/>
<edge source="10761248177036470713" target="6004268348151288098"/>
<edge source="7877485587962972033" target="7522504961268153944"/>
<edge source="10471521790809284341" target="7522504961268153944"/>
<edge source="12867511582517934835" target="7522504961268153944"/>
<edge source="12867511582517934835" target="2104521862399993019"/>
<edge source="12867511582517934835" target="6004268348151288098"/>
<edge source="12867511582517934835" target="11324600873272743514"/>
<edge source="2898208138417725333" target="7522504961268153944"/>
<edge source="5446507817827610731" target="7522504961268153944"/>
<edge source="14136709172791920331" target="7522504961268153944"/>
<edge source="14136709172791920331" target="2104521862399993019"/>
<edge source="14136709172791920331" target="13346469121565650680"/>
<edge source="568699136349469348" target="7522504961268153944"/>
<edge source="4431578198915484435" target="7522504961268153944"/>
<edge source="9280402251304953591" target="7522504961268153944"/>
<edge source="9280402251304953591" target="4490918786976048296"/>
<edge source="5601871542106060008" target="7522504961268153944"/>
<edge source="10835602930051801712" target="7522504961268153944"/>
<edge source="10835602930051801712" target="17325452893759607893"/>
<edge source="12895029264791091547" target="7522504961268153944"/>
<edge source="4988077150684599939" target="7522504961268153944"/>
<edge source="4988077150684599939" target="535744563039386055"/>
<edge source="5670723070535415123" target="7522504961268153944"/>
<edge source="7525849990631181493" target="7522504961268153944"/>
<edge source="2246956610590692796" target="7522504961268153944"/>
<edge source="10743434876113045449" target="7522504961268153944"/>
<edge source="9897783945226246229" target="7522504961268153944"/>
<edge source="278490733091580759" target="7522504961268153944"/>
<edge source="12384531505899273984" target="7522504961268153944"/>
<edge source="8053588590478703627" target="7522504961268153944"/>
<edge source="2012981774095049918" target="7522504961268153944"/>
<edge source="17245415599430658769" target="7522504961268153944"/>
<edge source="10019448087059497602" target="7522504961268153944"/>
<edge source="11308628305789992240" target="7522504961268153944"/>
<edge source="14281222646840020216" target="7522504961268153944"/>
<edge source="14281222646840020216" target="1273811038957334386"/>
<edge source="350651523509452369" target="7522504961268153944"/>
<edge source="14640072730760349377" target="7522504961268153944"/>
<edge source="16207837178250254474" target="7522504961268153944"/>
<edge source="13079493271785323872" target="7522504961268153944"/>
<edge source="7347143883946966195" target="7522504961268153944"/>
<edge source="10437338697213367930" target="7522504961268153944"/>
<edge source="17121643192375450290" target="7522504961268153944"/>
<edge source="906783918798836468" target="7522504961268153944"/>
<edge source="17970566394936146777" target="7522504961268153944"/>
<edge source="1797517254712068026" target="7522504961268153944"/>
<edge source="2971359940394370469" target="7522504961268153944"/>
<edge source="13482994153555216848" target="7522504961268153944"/>
<edge source="3062420136055973965" target="7522504961268153944"/>
<edge source="3062420136055973965" target="13055852169785823920"/>
<edge source="12187111677452062889" target="7522504961268153944"/>
<edge source="9999087805969499080" target="7522504961268153944"/>
<edge source="3774217736601927222" target="7522504961268153944"/>
<edge source="12443165844301288148" target="7522504961268153944"/>
<edge source="8750779121680802066" target="7522504961268153944"/>
<edge source="8737500900907252911" target="7522504961268153944"/>
<edge source="15425036751013855629" target="7522504961268153944"/>
<edge source="6637879260771394229" target="7522504961268153944"/>
<edge source="17981792349028675992" target="7522504961268153944"/>
<edge source="18080630766028543102" target="7522504961268153944"/>
<edge source="18080630766028543102" target="1273811038957334386"/>
<edge source="15758326914658017418" target="7522504961268153944"/>
<edge source="15758326914658017418" target="2104521862399993019"/>
<edge source="12478683589701856519" target="7522504961268153944"/>
<edge source="8623435992513029102" target="7522504961268153944"/>
<edge source="9927551479269048485" target="7522504961268153944"/>
<edge source="6583545798277065500" target="7522504961268153944"/>
<edge source="2547364804279732726" target="7522504961268153944"/>
<edge source="3241342143336277180" target="7522504961268153944"/>
<edge source="1309943264288419550" target="7522504961268153944"/>
<edge source="10901034885667795659" target="7522504961268153944"/>
<edge source="15926311935982020340" target="7522504961268153944"/>
<edge source="15926311935982020340" target="1273811038957334386"/>
<edge source="5301766735590672032" target="7522504961268153944"/>
<edge source="5301766735590672032" target="4490918786976048296"/>
<edge source="16583630453825645332" target="7522504961268153944"/>
<edge source="3738295254278174876" target="7522504961268153944"/>
<edge source="3738295254278174876" target="1273811038957334386"/>
<edge source="9591435289724766439" target="7522504961268153944"/>
<edge source="9591435289724766439" target="8448943115025253905"/>
<edge source="9591435289724766439" target="14311400318178337111"/>
<edge source="3333668025330401477" target="7522504961268153944"/>
<edge source="3333668025330401477" target="18210588638302847093"/>
<edge source="5952922111315863760" target="7522504961268153944"/>
<edge source="1101546884380916291" target="7522504961268153944"/>
<edge source="8994048266291075499" target="7522504961268153944"/>
<edge source="15220408395767693757" target="7522504961268153944"/>
<edge source="5423828611786306174" target="7522504961268153944"/>
<edge source="5888510936612783579" target="7522504961268153944"/>
<edge source="5888510936612783579" target="4490918786976048296"/>
<edge source="17171842121702147403" target="7522504961268153944"/>
<edge source="8071202295861113716" target="7522504961268153944"/>
<edge source="8071202295861113716" target="1273811038957334386"/>
<edge source="4041041901496425203" target="6382612685700818764"/>
<edge source="8136644755673586417" target="4041041901496425203"/>
<edge source="8136644755673586417" target="13346469121565650680"/>
<edge source="6141939658324331542" target="4041041901496425203"/>
<edge source="7770442917120891581" target="4041041901496425203"/>
<edge source="8793029896395507010" target="4041041901496425203"/>
<edge source="8793029896395507010" target="1672665553767281734"/>
<edge source="8793029896395507010" target="2104521862399993019"/>
<edge source="8793029896395507010" target="9538084449875791919"/>
<edge source="9595110325981705564" target="4041041901496425203"/>
<edge source="12987945369444025427" target="4041041901496425203"/>
<edge source="16770513324417061228" target="4041041901496425203"/>
<edge source="9063880872255850171" target="4041041901496425203"/>
<edge source="2321980635951558135" target="4041041901496425203"/>
<edge source="2321980635951558135" target="12563424232401784595"/>
<edge source="2321980635951558135" target="4550423614333066109"/>
<edge source="862169174991977666" target="4041041901496425203"/>
<edge source="7227117659365205945" target="4041041901496425203"/>
<edge source="16564777441869762048" target="4041041901496425203"/>
<edge source="9003206964118171143" target="4041041901496425203"/>
<edge source="10054528338033032937" target="4041041901496425203"/>
<edge source="7358070883597795569" target="4041041901496425203"/>
<edge source="13927538846757401282" target="4041041901496425203"/>
<edge source="11310550941481896506" target="4041041901496425203"/>
<edge source="13436034306365145381" target="4041041901496425203"/>
<edge source="11885254696629287118" target="4041041901496425203"/>
<edge source="7309961819207216748" target="4041041901496425203"/>
<edge source="7309961819207216748" target="12644740604491229422"/>
<edge source="4984340879175316146" target="4041041901496425203"/>
<edge source="4984340879175316146" target="13346469121565650680"/>
<edge source="8302576934258228524" target="4041041901496425203"/>
<edge source="10911957018595078026" target="4041041901496425203"/>
<edge source="17777612268280748847" target="4041041901496425203"/>
<edge source="4325500781447230872" target="4041041901496425203"/>
<edge source="13909256571563279522" target="4041041901496425203"/>
<edge source="14260054837347093009" target="4041041901496425203"/>
<edge source="2022703968544749835" target="4041041901496425203"/>
<edge source="18421653793757811360" target="4041041901496425203"/>
<edge source="639736007842008588" target="4041041901496425203"/>
<edge source="11748945572016694012" target="4041041901496425203"/>
<edge source="18253746969450725752" target="4041041901496425203"/>
<edge source="1091474511097006762" target="4041041901496425203"/>
<edge source="1091474511097006762" target="535744563039386055"/>
<edge source="3667217905602891315" target="4041041901496425203"/>
<edge source="12913547344242152073" target="4041041901496425203"/>
<edge source="10803661783624345607" target="4041041901496425203"/>
<edge source="6953811760252391041" target="4041041901496425203"/>
<edge source="7183832294030210754" target="4041041901496425203"/>
<edge source="7183832294030210754" target="13346469121565650680"/>
<edge source="7119350279654084346" target="4041041901496425203"/>
<edge source="7119350279654084346" target="1431633311194488622"/>
<edge source="4506444976781600964" target="4041041901496425203"/>
<edge source="10215776754955167080" target="4041041901496425203"/>
<edge source="6018697739026892882" target="4041041901496425203"/>
<edge source="14158261027181687094" target="4041041901496425203"/>
<edge source="5892850021726448686" target="4041041901496425203"/>
<edge source="14228395058672942618" target="4041041901496425203"/>
<edge source="18043841524050979844" target="4041041901496425203"/>
<edge source="16629143072867731417" target="4041041901496425203"/>
<edge source="370769816745103063" target="4041041901496425203"/>
<edge source="370769816745103063" target="1431633311194488622"/>
<edge source="1555581554615315719" target="4041041901496425203"/>
<edge source="7294969416203662369" target="4041041901496425203"/>
<edge source="9634990485617283026" target="4041041901496425203"/>
<edge source="9634990485617283026" target="1431633311194488622"/>
<edge source="5055262884129010690" target="4041041901496425203"/>
<edge source="3398859238063529749" target="4041041901496425203"/>
<edge source="14585779787909883755" target="4041041901496425203"/>
<edge source="4956847805201710311" target="4041041901496425203"/>
<edge source="6542176543643613250" target="4041041901496425203"/>
<edge source="13844417272486663442" target="4041041901496425203"/>
<edge source="1995042310766600523" target="4041041901496425203"/>
<edge source="17455667949400521995" target="4041041901496425203"/>
<edge source="1824518314930278626" target="4041041901496425203"/>
<edge source="16973471166771569958" target="4041041901496425203"/>
<edge source="11854062718992351025" target="4041041901496425203"/>
<edge source="14217745962583788184" target="4041041901496425203"/>
<edge source="5951026065039086409" target="4041041901496425203"/>
<edge source="7826621618102468948" target="4041041901496425203"/>
<edge source="7802246301550866837" target="4041041901496425203"/>
<edge source="10296237619596505984" target="4041041901496425203"/>
<edge source="2178700053782973603" target="4041041901496425203"/>
<edge source="10604152332073933497" target="4041041901496425203"/>
<edge source="7188590031243869377" target="4041041901496425203"/>
<edge source="15803341694212540345" target="4041041901496425203"/>
<edge source="6256455976929564913" target="4041041901496425203"/>
<edge source="15580172266410736369" target="4041041901496425203"/>
<edge source="15580172266410736369" target="12563424232401784595"/>
<edge source="15580172266410736369" target="4550423614333066109"/>
<edge source="6628585655867076014" target="4041041901496425203"/>
<edge source="8885814579435600701" target="4041041901496425203"/>
<edge source="1656744800823975080" target="4041041901496425203"/>
<edge source="10374132085208051787" target="4041041901496425203"/>
<edge source="10374132085208051787" target="4550423614333066109"/>
<edge source="10940775338620708972" target="4041041901496425203"/>
<edge source="17945544379535649211" target="4041041901496425203"/>
<edge source="16412654433518658547" target="4041041901496425203"/>
<edge source="804636322301015973" target="4041041901496425203"/>
<edge source="6794503273897899990" target="4041041901496425203"/>
<edge source="5744336008744333374" target="4041041901496425203"/>
<edge source="11739296977766500505" target="4041041901496425203"/>
<edge source="9595578074718261885" target="4041041901496425203"/>
<edge source="7437652212406869439" target="4041041901496425203"/>
<edge source="14190759885589230911" target="4041041901496425203"/>
<edge source="15052277191896050356" target="4041041901496425203"/>
<edge source="8770863820400864485" target="4041041901496425203"/>
<edge source="11914760532013868738" target="4041041901496425203"/>
<edge source="8884460723513624148" target="4041041901496425203"/>
<edge source="16779378878512616320" target="4041041901496425203"/>
<edge source="17666447766930405698" target="4041041901496425203"/>
<edge source="18148219906827393870" target="4041041901496425203"/>
<edge source="2194257343717484563" target="4041041901496425203"/>
<edge source="9750378231570611023" target="4041041901496425203"/>
<edge source="8336676790606133743" target="4041041901496425203"/>
<edge source="9023365447299077306" target="4041041901496425203"/>
<edge source="9023365447299077306" target="12563424232401784595"/>
<edge source="9023365447299077306" target="4550423614333066109"/>
<edge source="9023365447299077306" target="13055852169785823920"/>
<edge source="448552700743310645" target="4041041901496425203"/>
<edge source="2026726248513085794" target="6382612685700818764"/>
<edge source="2026726248513085794" target="11531242419091815801"/>
<edge source="2026726248513085794" target="4490918786976048296"/>
<edge source="2026726248513085794" target="1672665553767281734"/>
<edge source="2026726248513085794" target="12563424232401784595"/>
<edge source="2026726248513085794" target="4550423614333066109"/>
<edge source="2026726248513085794" target="2104521862399993019"/>
<edge source="2026726248513085794" target="13055852169785823920"/>
<edge source="2026726248513085794" target="9538084449875791919"/>
<edge source="2026726248513085794" target="11324600873272743514"/>
<edge source="2026726248513085794" target="12644740604491229422"/>
<edge source="9850512646184180167" target="2026726248513085794"/>
<edge source="9850512646184180167" target="1672665553767281734"/>
<edge source="9850512646184180167" target="4550423614333066109"/>
<edge source="9850512646184180167" target="13055852169785823920"/>
<edge source="9850512646184180167" target="9538084449875791919"/>
<edge source="9850512646184180167" target="12644740604491229422"/>
<edge source="9850512646184180167" target="13346469121565650680"/>
<edge source="9850512646184180167" target="5999650257677576183"/>
<edge source="14638466021176544465" target="2026726248513085794"/>
<edge source="14638466021176544465" target="11531242419091815801"/>
<edge source="14638466021176544465" target="1672665553767281734"/>
<edge source="14638466021176544465" target="4550423614333066109"/>
<edge source="14638466021176544465" target="2104521862399993019"/>
<edge source="13702720529764835843" target="2026726248513085794"/>
<edge source="13702720529764835843" target="11531242419091815801"/>
<edge source="13702720529764835843" target="4550423614333066109"/>
<edge source="13702720529764835843" target="13055852169785823920"/>
<edge source="13702720529764835843" target="9538084449875791919"/>
<edge source="13702720529764835843" target="11324600873272743514"/>
<edge source="13702720529764835843" target="12644740604491229422"/>
<edge source="13702720529764835843" target="5999650257677576183"/>
<edge source="17646032300901507453" target="2026726248513085794"/>
<edge source="17646032300901507453" target="12563424232401784595"/>
<edge source="17127958062872744853" target="2026726248513085794"/>
<edge source="17127958062872744853" target="11531242419091815801"/>
<edge source="17127958062872744853" target="12563424232401784595"/>
<edge source="17127958062872744853" target="13055852169785823920"/>
<edge source="17127958062872744853" target="8367071398417962813"/>
<edge source="13222462680960111198" target="2026726248513085794"/>
<edge source="8594852448598418616" target="2026726248513085794"/>
<edge source="16827683962441657873" target="2026726248513085794"/>
<edge source="16827683962441657873" target="18210588638302847093"/>
<edge source="11112203079270473115" target="2026726248513085794"/>
<edge source="17690753104303819861" target="2026726248513085794"/>
<edge source="10264464849626080964" target="2026726248513085794"/>
<edge source="10264464849626080964" target="17325452893759607893"/>
<edge source="17785166320928157068" target="2026726248513085794"/>
<edge source="17785166320928157068" target="11531242419091815801"/>
<edge source="7665772481323515367" target="2026726248513085794"/>
<edge source="7665772481323515367" target="11531242419091815801"/>
<edge source="5875186414038374408" target="2026726248513085794"/>
<edge source="5875186414038374408" target="18210588638302847093"/>
<edge source="9420485880557840183" target="2026726248513085794"/>
<edge source="14658447611882211088" target="2026726248513085794"/>
<edge source="10221363205500346797" target="2026726248513085794"/>
<edge source="16559564215385655731" target="2026726248513085794"/>
<edge source="958587697277086390" target="2026726248513085794"/>
<edge source="958587697277086390" target="11531242419091815801"/>
<edge source="17898472391686934449" target="2026726248513085794"/>
<edge source="17898472391686934449" target="8367071398417962813"/>
<edge source="6352382094902298941" target="2026726248513085794"/>
<edge source="14183845217069566705" target="2026726248513085794"/>
<edge source="14052692408086479292" target="2026726248513085794"/>
<edge source="16133934620382970642" target="2026726248513085794"/>
<edge source="15271212594475018431" target="2026726248513085794"/>
<edge source="8819196546794680544" target="2026726248513085794"/>
<edge source="8819196546794680544" target="11531242419091815801"/>
<edge source="8819196546794680544" target="12563424232401784595"/>
<edge source="12345969996209231071" target="2026726248513085794"/>
<edge source="13723544815291086075" target="2026726248513085794"/>
<edge source="13723544815291086075" target="11531242419091815801"/>
<edge source="7938131059737931281" target="2026726248513085794"/>
<edge source="4384676216153265937" target="2026726248513085794"/>
<edge source="4384676216153265937" target="18210588638302847093"/>
<edge source="10116304010765610972" target="2026726248513085794"/>
<edge source="10116304010765610972" target="18210588638302847093"/>
<edge source="2663039362134560569" target="2026726248513085794"/>
<edge source="3281046445393215005" target="2026726248513085794"/>
<edge source="3281046445393215005" target="11531242419091815801"/>
<edge source="3281046445393215005" target="12563424232401784595"/>
<edge source="176548951834254449" target="2026726248513085794"/>
<edge source="7090138214741207408" target="2026726248513085794"/>
<edge source="14151669589159930339" target="2026726248513085794"/>
<edge source="6535827589324635187" target="2026726248513085794"/>
<edge source="6535827589324635187" target="8367071398417962813"/>
<edge source="12472359179257688469" target="2026726248513085794"/>
<edge source="9925896961448568187" target="2026726248513085794"/>
<edge source="14205155098487362799" target="2026726248513085794"/>
<edge source="14205155098487362799" target="18210588638302847093"/>
<edge source="14205155098487362799" target="8367071398417962813"/>
<edge source="7644674706913288304" target="2026726248513085794"/>
<edge source="15845207782709112329" target="2026726248513085794"/>
<edge source="15845207782709112329" target="8367071398417962813"/>
<edge source="2749712196276682618" target="2026726248513085794"/>
<edge source="8263847837619800805" target="2026726248513085794"/>
<edge source="7690447212830750827" target="2026726248513085794"/>
<edge source="10608878509050050554" target="2026726248513085794"/>
<edge source="7076299391791953686" target="2026726248513085794"/>
<edge source="7291687913883151332" target="2026726248513085794"/>
<edge source="7099002288627031897" target="2026726248513085794"/>
<edge source="15184762675201395065" target="2026726248513085794"/>
<edge source="6327480080772807927" target="2026726248513085794"/>
<edge source="8866059787183806571" target="2026726248513085794"/>
<edge source="13803925855903140317" target="2026726248513085794"/>
<edge source="16632032221621362841" target="2026726248513085794"/>
<edge source="18391486949977622277" target="2026726248513085794"/>
<edge source="18391486949977622277" target="6004268348151288098"/>
<edge source="10392802398675002497" target="2026726248513085794"/>
<edge source="9903488889701687950" target="2026726248513085794"/>
<edge source="8851123877575968338" target="2026726248513085794"/>
<edge source="1702973244756968520" target="2026726248513085794"/>
<edge source="14037392503982667044" target="2026726248513085794"/>
<edge source="2410476445007210626" target="2026726248513085794"/>
<edge source="17386293721154871155" target="2026726248513085794"/>
<edge source="17386293721154871155" target="18210588638302847093"/>
<edge source="9015347870265085472" target="2026726248513085794"/>
<edge source="12337878440731894992" target="2026726248513085794"/>
<edge source="6147088247534052612" target="2026726248513085794"/>
<edge source="4526718419682475090" target="2026726248513085794"/>
<edge source="6128611593700706618" target="2026726248513085794"/>
<edge source="17237114188030762025" target="2026726248513085794"/>
<edge source="15542278339082031875" target="2026726248513085794"/>
<edge source="15542278339082031875" target="18210588638302847093"/>
<edge source="8569975234173607623" target="2026726248513085794"/>
<edge source="14110793700557908078" target="2026726248513085794"/>
<edge source="388784122271071816" target="2026726248513085794"/>
<edge source="388784122271071816" target="18210588638302847093"/>
<edge source="8471323315563528709" target="2026726248513085794"/>
<edge source="13825486577349279839" target="2026726248513085794"/>
<edge source="4419171281200457557" target="2026726248513085794"/>
<edge source="4043189257734549657" target="2026726248513085794"/>
<edge source="12115561072014802341" target="2026726248513085794"/>
<edge source="5444293888769597917" target="2026726248513085794"/>
<edge source="13381092205432192534" target="2026726248513085794"/>
<edge source="14213127820813811957" target="2026726248513085794"/>
<edge source="3527582028524441499" target="2026726248513085794"/>
<edge source="310549142066083798" target="2026726248513085794"/>
<edge source="310549142066083798" target="8367071398417962813"/>
<edge source="2795686517415303571" target="2026726248513085794"/>
<edge source="12602640083300493507" target="2026726248513085794"/>
<edge source="12602640083300493507" target="14311400318178337111"/>
<edge source="2199006774138818312" target="2026726248513085794"/>
<edge source="1068983787928302396" target="2026726248513085794"/>
<edge source="13704777355910523310" target="2026726248513085794"/>
<edge source="8849990741625315400" target="2026726248513085794"/>
<edge source="8849990741625315400" target="18210588638302847093"/>
<edge source="14014258077591706555" target="2026726248513085794"/>
<edge source="14014258077591706555" target="18210588638302847093"/>
<edge source="10603972393777456896" target="2026726248513085794"/>
<edge source="5046561428623654374" target="2026726248513085794"/>
<edge source="3550218616048655433" target="2026726248513085794"/>
<edge source="5900725104936277094" target="2026726248513085794"/>
<edge source="12974822052652996066" target="2026726248513085794"/>
<edge source="f2PZtE-BhD0J" target="2026726248513085794"/>
<edge source="2868755694884485530" target="2026726248513085794"/>
<edge source="5981499027157381733" target="2026726248513085794"/>
<edge source="5981499027157381733" target="18210588638302847093"/>
<edge source="16205265336717732975" target="2026726248513085794"/>
<edge source="7572686774487370911" target="2026726248513085794"/>
<edge source="3244016085706305630" target="2026726248513085794"/>
<edge source="11531242419091815801" target="6382612685700818764"/>
<edge source="11531242419091815801" target="1672665553767281734"/>
<edge source="11531242419091815801" target="12563424232401784595"/>
<edge source="11531242419091815801" target="4550423614333066109"/>
<edge source="11531242419091815801" target="9538084449875791919"/>
<edge source="11531242419091815801" target="8367071398417962813"/>
<edge source="11531242419091815801" target="11324600873272743514"/>
<edge source="11531242419091815801" target="5999650257677576183"/>
<edge source="18210588638302847093" target="11531242419091815801"/>
<edge source="18210588638302847093" target="12563424232401784595"/>
<edge source="18210588638302847093" target="6382612685700818764"/>
<edge source="18210588638302847093" target="4550423614333066109"/>
<edge source="18210588638302847093" target="9538084449875791919"/>
<edge source="18210588638302847093" target="5999650257677576183"/>
<edge source="11452477192554017570" target="11531242419091815801"/>
<edge source="11452477192554017570" target="1672665553767281734"/>
<edge source="11452477192554017570" target="2104521862399993019"/>
<edge source="11452477192554017570" target="9538084449875791919"/>
<edge source="11452477192554017570" target="12644740604491229422"/>
<edge source="11452477192554017570" target="6382612685700818764"/>
<edge source="7039269942427062691" target="11531242419091815801"/>
<edge source="7039269942427062691" target="1672665553767281734"/>
<edge source="7039269942427062691" target="4550423614333066109"/>
<edge source="7039269942427062691" target="6004268348151288098"/>
<edge source="7039269942427062691" target="9538084449875791919"/>
<edge source="7039269942427062691" target="12644740604491229422"/>
<edge source="4488471448867589825" target="11531242419091815801"/>
<edge source="4488471448867589825" target="4550423614333066109"/>
<edge source="4488471448867589825" target="13055852169785823920"/>
<edge source="5295462520886771746" target="11531242419091815801"/>
<edge source="2093389731615411975" target="11531242419091815801"/>
<edge source="2093389731615411975" target="13055852169785823920"/>
<edge source="2093389731615411975" target="12644740604491229422"/>
<edge source="11789051068432887660" target="11531242419091815801"/>
<edge source="11789051068432887660" target="12563424232401784595"/>
<edge source="11789051068432887660" target="8367071398417962813"/>
<edge source="11789051068432887660" target="14311400318178337111"/>
<edge source="11838073149065061192" target="11531242419091815801"/>
<edge source="11838073149065061192" target="4550423614333066109"/>
<edge source="11838073149065061192" target="2104521862399993019"/>
<edge source="11838073149065061192" target="9538084449875791919"/>
<edge source="11838073149065061192" target="12644740604491229422"/>
<edge source="14151291256828319904" target="11531242419091815801"/>
<edge source="16069829188377130053" target="11531242419091815801"/>
<edge source="16069829188377130053" target="12644740604491229422"/>
<edge source="14300935760162828522" target="11531242419091815801"/>
<edge source="14300935760162828522" target="2104521862399993019"/>
<edge source="7664894228759942842" target="11531242419091815801"/>
<edge source="7664894228759942842" target="4550423614333066109"/>
<edge source="7664894228759942842" target="2104521862399993019"/>
<edge source="7664894228759942842" target="6004268348151288098"/>
<edge source="7664894228759942842" target="8367071398417962813"/>
<edge source="7664894228759942842" target="11324600873272743514"/>
<edge source="7664894228759942842" target="12644740604491229422"/>
<edge source="14167857935364818481" target="11531242419091815801"/>
<edge source="13684672788136223420" target="11531242419091815801"/>
<edge source="5598971766222052345" target="11531242419091815801"/>
<edge source="5598971766222052345" target="12644740604491229422"/>
<edge source="9529319158101525799" target="11531242419091815801"/>
<edge source="7104781172538541114" target="11531242419091815801"/>
<edge source="7104781172538541114" target="18210588638302847093"/>
<edge source="7104781172538541114" target="4550423614333066109"/>
<edge source="7104781172538541114" target="13055852169785823920"/>
<edge source="7104781172538541114" target="8367071398417962813"/>
<edge source="7104781172538541114" target="11324600873272743514"/>
<edge source="3501401895928296969" target="11531242419091815801"/>
<edge source="3501401895928296969" target="12563424232401784595"/>
<edge source="3501401895928296969" target="11324600873272743514"/>
<edge source="3501401895928296969" target="12644740604491229422"/>
<edge source="3753161556540928995" target="11531242419091815801"/>
<edge source="3753161556540928995" target="11324600873272743514"/>
<edge source="4241401890946035983" target="11531242419091815801"/>
<edge source="4241401890946035983" target="12563424232401784595"/>
<edge source="4241401890946035983" target="4550423614333066109"/>
<edge source="10239175441885037567" target="11531242419091815801"/>
<edge source="10239175441885037567" target="4550423614333066109"/>
<edge source="11442601300141569813" target="11531242419091815801"/>
<edge source="11442601300141569813" target="2104521862399993019"/>
<edge source="12358278621343852999" target="11531242419091815801"/>
<edge source="12358278621343852999" target="11324600873272743514"/>
<edge source="14575314968965851624" target="11531242419091815801"/>
<edge source="14575314968965851624" target="2104521862399993019"/>
<edge source="8851003302500796244" target="11531242419091815801"/>
<edge source="10246639746398567269" target="11531242419091815801"/>
<edge source="9808171637715574951" target="11531242419091815801"/>
<edge source="3318911338829069796" target="11531242419091815801"/>
<edge source="10139464426827633103" target="11531242419091815801"/>
<edge source="5177357115845082072" target="11531242419091815801"/>
<edge source="5177357115845082072" target="13055852169785823920"/>
<edge source="503273143199070187" target="11531242419091815801"/>
<edge source="503273143199070187" target="13055852169785823920"/>
<edge source="17928092202816562244" target="11531242419091815801"/>
<edge source="17928092202816562244" target="4490918786976048296"/>
<edge source="17928092202816562244" target="2104521862399993019"/>
<edge source="17928092202816562244" target="6004268348151288098"/>
<edge source="17207953557216789132" target="11531242419091815801"/>
<edge source="17207953557216789132" target="12563424232401784595"/>
<edge source="1858398396401784628" target="11531242419091815801"/>
<edge source="12280368052734223898" target="11531242419091815801"/>
<edge source="5901510358391583323" target="11531242419091815801"/>
<edge source="5901510358391583323" target="13055852169785823920"/>
<edge source="7280187709203663599" target="11531242419091815801"/>
<edge source="8918294861982063782" target="11531242419091815801"/>
<edge source="9002295544688722648" target="11531242419091815801"/>
<edge source="9002295544688722648" target="4490918786976048296"/>
<edge source="9002295544688722648" target="8448943115025253905"/>
<edge source="4172110649540745529" target="11531242419091815801"/>
<edge source="1182214878703167432" target="11531242419091815801"/>
<edge source="5085325784414234300" target="11531242419091815801"/>
<edge source="5085325784414234300" target="4490918786976048296"/>
<edge source="5085325784414234300" target="1273811038957334386"/>
<edge source="8537681662983932237" target="11531242419091815801"/>
<edge source="1803748338102694409" target="11531242419091815801"/>
<edge source="16254437278639517990" target="11531242419091815801"/>
<edge source="9097004020017827588" target="11531242419091815801"/>
<edge source="12877767864993853066" target="11531242419091815801"/>
<edge source="12877767864993853066" target="12563424232401784595"/>
<edge source="2089165535494063327" target="11531242419091815801"/>
<edge source="3119590478708025935" target="11531242419091815801"/>
<edge source="679650553160689765" target="11531242419091815801"/>
<edge source="1907607333359949293" target="11531242419091815801"/>
<edge source="18256102101708736597" target="11531242419091815801"/>
<edge source="9917302194767651380" target="11531242419091815801"/>
<edge source="7439091361445822478" target="11531242419091815801"/>
<edge source="13668398880115037252" target="11531242419091815801"/>
<edge source="9399655475680081198" target="11531242419091815801"/>
<edge source="3905038671402584496" target="11531242419091815801"/>
<edge source="4526443549077206496" target="11531242419091815801"/>
<edge source="7753649110726260014" target="11531242419091815801"/>
<edge source="48995825530336826" target="11531242419091815801"/>
<edge source="16929187721477555329" target="11531242419091815801"/>
<edge source="13672856354810391005" target="11531242419091815801"/>
<edge source="13672856354810391005" target="12563424232401784595"/>
<edge source="13672856354810391005" target="1273811038957334386"/>
<edge source="13672856354810391005" target="8367071398417962813"/>
<edge source="7436725887660345544" target="11531242419091815801"/>
<edge source="7436725887660345544" target="8367071398417962813"/>
<edge source="8487537285655290697" target="11531242419091815801"/>
<edge source="5327297247516466432" target="11531242419091815801"/>
<edge source="1845681316412466465" target="11531242419091815801"/>
<edge source="15573361464256666532" target="11531242419091815801"/>
<edge source="1380225769636433327" target="11531242419091815801"/>
<edge source="17206319611652371856" target="11531242419091815801"/>
<edge source="410585140321607722" target="11531242419091815801"/>
<edge source="2499885218349084470" target="11531242419091815801"/>
<edge source="15199713057680873086" target="11531242419091815801"/>
<edge source="5063651843298506539" target="11531242419091815801"/>
<edge source="5063651843298506539" target="18210588638302847093"/>
<edge source="17116877368626144169" target="11531242419091815801"/>
<edge source="11206281819202318260" target="11531242419091815801"/>
<edge source="11206281819202318260" target="12563424232401784595"/>
<edge source="12932718969286495303" target="11531242419091815801"/>
<edge source="12932718969286495303" target="18210588638302847093"/>
<edge source="3883959449895642650" target="11531242419091815801"/>
<edge source="3883959449895642650" target="18210588638302847093"/>
<edge source="1435298664216281195" target="11531242419091815801"/>
<edge source="1435298664216281195" target="8367071398417962813"/>
<edge source="5279305686836034362" target="11531242419091815801"/>
<edge source="2900647541574704523" target="11531242419091815801"/>
<edge source="9358192350302074225" target="11531242419091815801"/>
<edge source="7333075876299418187" target="11531242419091815801"/>
<edge source="7333075876299418187" target="4490918786976048296"/>
<edge source="41036872912386481" target="11531242419091815801"/>
<edge source="3142941948497874730" target="11531242419091815801"/>
<edge source="12459243040456605693" target="11531242419091815801"/>
<edge source="6585235751137549645" target="11531242419091815801"/>
<edge source="6585235751137549645" target="18210588638302847093"/>
<edge source="16425348670839745439" target="11531242419091815801"/>
<edge source="16435089417528679481" target="11531242419091815801"/>
<edge source="7917884940240812522" target="11531242419091815801"/>
<edge source="14459465598979164145" target="11531242419091815801"/>
<edge source="4490918786976048296" target="6382612685700818764"/>
<edge source="4490918786976048296" target="1672665553767281734"/>
<edge source="4490918786976048296" target="1273811038957334386"/>
<edge source="4490918786976048296" target="9538084449875791919"/>
<edge source="4490918786976048296" target="8367071398417962813"/>
<edge source="4490918786976048296" target="5999650257677576183"/>
<edge source="6243645967630982889" target="4490918786976048296"/>
<edge source="9370947851501467347" target="4490918786976048296"/>
<edge source="15741444728855576863" target="4490918786976048296"/>
<edge source="15741444728855576863" target="1672665553767281734"/>
<edge source="10588342779298269046" target="4490918786976048296"/>
<edge source="10588342779298269046" target="6004268348151288098"/>
<edge source="10588342779298269046" target="1273811038957334386"/>
<edge source="6118595289890500680" target="4490918786976048296"/>
<edge source="17997891498068622933" target="4490918786976048296"/>
<edge source="17997891498068622933" target="13055852169785823920"/>
<edge source="17997891498068622933" target="12644740604491229422"/>
<edge source="9439766841533136382" target="4490918786976048296"/>
<edge source="9439766841533136382" target="1273811038957334386"/>
<edge source="12938213222665733645" target="4490918786976048296"/>
<edge source="12938213222665733645" target="1273811038957334386"/>
<edge source="14316138733173377966" target="4490918786976048296"/>
<edge source="14316138733173377966" target="2104521862399993019"/>
<edge source="14316138733173377966" target="1273811038957334386"/>
<edge source="14316138733173377966" target="11324600873272743514"/>
<edge source="12550135891789567985" target="4490918786976048296"/>
<edge source="12550135891789567985" target="1273811038957334386"/>
<edge source="12550135891789567985" target="11324600873272743514"/>
<edge source="4555711365675635781" target="4490918786976048296"/>
<edge source="4555711365675635781" target="1273811038957334386"/>
<edge source="14680303082655356082" target="4490918786976048296"/>
<edge source="640071900436442539" target="4490918786976048296"/>
<edge source="9830572971874050892" target="4490918786976048296"/>
<edge source="9830572971874050892" target="8448943115025253905"/>
<edge source="18387677115510147109" target="4490918786976048296"/>
<edge source="18387677115510147109" target="11324600873272743514"/>
<edge source="7734774232157964979" target="4490918786976048296"/>
<edge source="7734774232157964979" target="1273811038957334386"/>
<edge source="14613261815949086918" target="4490918786976048296"/>
<edge source="6979440938086217563" target="4490918786976048296"/>
<edge source="5337226914153811562" target="4490918786976048296"/>
<edge source="5337226914153811562" target="8448943115025253905"/>
<edge source="7134900495559356797" target="4490918786976048296"/>
<edge source="5030683813836303943" target="4490918786976048296"/>
<edge source="11522546362437060713" target="4490918786976048296"/>
<edge source="11522546362437060713" target="1273811038957334386"/>
<edge source="12195536569534646845" target="4490918786976048296"/>
<edge source="12195536569534646845" target="1273811038957334386"/>
<edge source="5962552400166695847" target="4490918786976048296"/>
<edge source="14300246766552093616" target="4490918786976048296"/>
<edge source="8055277125890470910" target="4490918786976048296"/>
<edge source="8055277125890470910" target="6004268348151288098"/>
<edge source="7111056020821706590" target="4490918786976048296"/>
<edge source="1052062078621162321" target="4490918786976048296"/>
<edge source="7380604299331262465" target="4490918786976048296"/>
<edge source="17776221386292352341" target="4490918786976048296"/>
<edge source="8429597570092570925" target="4490918786976048296"/>
<edge source="8815839713273959723" target="4490918786976048296"/>
<edge source="4731455256244956465" target="4490918786976048296"/>
<edge source="8388230239628727224" target="4490918786976048296"/>
<edge source="13876321258274905912" target="4490918786976048296"/>
<edge source="3429308917664707403" target="4490918786976048296"/>
<edge source="3429308917664707403" target="6004268348151288098"/>
<edge source="3429308917664707403" target="1273811038957334386"/>
<edge source="16202141712210963294" target="4490918786976048296"/>
<edge source="16202141712210963294" target="8448943115025253905"/>
<edge source="3360913791146424633" target="4490918786976048296"/>
<edge source="3360913791146424633" target="6004268348151288098"/>
<edge source="11213926060739194818" target="4490918786976048296"/>
<edge source="352322076591852020" target="4490918786976048296"/>
<edge source="352322076591852020" target="1273811038957334386"/>
<edge source="13906564416939487459" target="4490918786976048296"/>
<edge source="12626688018988097647" target="4490918786976048296"/>
<edge source="12626688018988097647" target="6004268348151288098"/>
<edge source="15264491751168338591" target="4490918786976048296"/>
<edge source="1684205177621612046" target="4490918786976048296"/>
<edge source="9289927810203044884" target="4490918786976048296"/>
<edge source="4028732696004816605" target="4490918786976048296"/>
<edge source="2922049850680781929" target="4490918786976048296"/>
<edge source="6877116560747276387" target="4490918786976048296"/>
<edge source="6877116560747276387" target="1273811038957334386"/>
<edge source="4504943264737055020" target="4490918786976048296"/>
<edge source="8057404930177375775" target="4490918786976048296"/>
<edge source="8100725342405011358" target="4490918786976048296"/>
<edge source="8100725342405011358" target="1273811038957334386"/>
<edge source="1673777623717824835" target="4490918786976048296"/>
<edge source="526538470553837028" target="4490918786976048296"/>
<edge source="11594648656724394854" target="4490918786976048296"/>
<edge source="11594648656724394854" target="6004268348151288098"/>
<edge source="4578573813216045589" target="4490918786976048296"/>
<edge source="9793040987893379450" target="4490918786976048296"/>
<edge source="9793040987893379450" target="12563424232401784595"/>
<edge source="10898983420399357710" target="4490918786976048296"/>
<edge source="11821895939908774894" target="4490918786976048296"/>
<edge source="10214171113959087100" target="4490918786976048296"/>
<edge source="6493028014197162041" target="4490918786976048296"/>
<edge source="14312905429445779769" target="4490918786976048296"/>
<edge source="4706041545647768300" target="4490918786976048296"/>
<edge source="11889722873208015946" target="4490918786976048296"/>
<edge source="11889722873208015946" target="1273811038957334386"/>
<edge source="RR4ndOlcdmUJ" target="4490918786976048296"/>
<edge source="RR4ndOlcdmUJ" target="1273811038957334386"/>
<edge source="7490603673765775284" target="4490918786976048296"/>
<edge source="117882505525518941" target="4490918786976048296"/>
<edge source="117882505525518941" target="6004268348151288098"/>
<edge source="117882505525518941" target="1273811038957334386"/>
<edge source="16343611144020544659" target="4490918786976048296"/>
<edge source="16343611144020544659" target="1273811038957334386"/>
<edge source="11178483353620714191" target="4490918786976048296"/>
<edge source="vTRbp0ZWND4J" target="4490918786976048296"/>
<edge source="vTRbp0ZWND4J" target="1273811038957334386"/>
<edge source="17890971807751741818" target="4490918786976048296"/>
<edge source="7816124690949941974" target="4490918786976048296"/>
<edge source="7897280292740363837" target="4490918786976048296"/>
<edge source="7897280292740363837" target="1273811038957334386"/>
<edge source="306282026774466293" target="4490918786976048296"/>
<edge source="306282026774466293" target="1273811038957334386"/>
<edge source="3818577016458576609" target="4490918786976048296"/>
<edge source="2272102423598794653" target="4490918786976048296"/>
<edge source="15646051846564298047" target="4490918786976048296"/>
<edge source="647200095634493159" target="4490918786976048296"/>
<edge source="647200095634493159" target="1273811038957334386"/>
<edge source="8153900683485623660" target="4490918786976048296"/>
<edge source="8153900683485623660" target="1273811038957334386"/>
<edge source="mT3lyH5QtvIJ" target="4490918786976048296"/>
<edge source="15577220087848933491" target="4490918786976048296"/>
<edge source="15577220087848933491" target="1273811038957334386"/>
<edge source="cTJB_-Obg3YJ" target="4490918786976048296"/>
<edge source="5523520554880035051" target="4490918786976048296"/>
<edge source="7501011475036280832" target="4490918786976048296"/>
<edge source="7424513001270707708" target="4490918786976048296"/>
<edge source="10014472325439300361" target="4490918786976048296"/>
<edge source="16089508268500048635" target="4490918786976048296"/>
<edge source="14381170671853204535" target="4490918786976048296"/>
<edge source="7564591903506430655" target="4490918786976048296"/>
<edge source="16928381517970009960" target="4490918786976048296"/>
<edge source="11996337183205116538" target="4490918786976048296"/>
<edge source="18182863446124159795" target="4490918786976048296"/>
<edge source="56664849184651828" target="4490918786976048296"/>
<edge source="1672665553767281734" target="6382612685700818764"/>
<edge source="1672665553767281734" target="9538084449875791919"/>
<edge source="1672665553767281734" target="11324600873272743514"/>
<edge source="1672665553767281734" target="12644740604491229422"/>
<edge source="2325917221075842848" target="1672665553767281734"/>
<edge source="11165298458048562314" target="1672665553767281734"/>
<edge source="15816136068893942524" target="1672665553767281734"/>
<edge source="3458396398389387877" target="1672665553767281734"/>
<edge source="3458396398389387877" target="4550423614333066109"/>
<edge source="3458396398389387877" target="2104521862399993019"/>
<edge source="3458396398389387877" target="11324600873272743514"/>
<edge source="7329647594369932315" target="1672665553767281734"/>
<edge source="16235705232339507184" target="1672665553767281734"/>
<edge source="2013933719074368496" target="1672665553767281734"/>
<edge source="11517447940529951525" target="1672665553767281734"/>
<edge source="4461602603986165987" target="1672665553767281734"/>
<edge source="1788827408361087894" target="1672665553767281734"/>
<edge source="3844087685498854388" target="1672665553767281734"/>
<edge source="10375739191012965737" target="1672665553767281734"/>
<edge source="17465500328468068642" target="1672665553767281734"/>
<edge source="7911999856845003856" target="1672665553767281734"/>
<edge source="7911999856845003856" target="9538084449875791919"/>
<edge source="7911999856845003856" target="11324600873272743514"/>
<edge source="8508636578765152299" target="1672665553767281734"/>
<edge source="6828425192739736056" target="1672665553767281734"/>
<edge source="1018521690946850362" target="1672665553767281734"/>
<edge source="15154755818357511167" target="1672665553767281734"/>
<edge source="7704492432415173786" target="1672665553767281734"/>
<edge source="13209348926291080860" target="1672665553767281734"/>
<edge source="5512802662340022027" target="1672665553767281734"/>
<edge source="14421942083121350206" target="1672665553767281734"/>
<edge source="13501013621324561884" target="1672665553767281734"/>
<edge source="13810965122904729211" target="1672665553767281734"/>
<edge source="5060121065165184210" target="1672665553767281734"/>
<edge source="15783325521916683415" target="1672665553767281734"/>
<edge source="10722665686456251008" target="1672665553767281734"/>
<edge source="10722665686456251008" target="12644740604491229422"/>
<edge source="8140812159859442226" target="1672665553767281734"/>
<edge source="4072300244718161964" target="1672665553767281734"/>
<edge source="5447961710166142858" target="1672665553767281734"/>
<edge source="5447961710166142858" target="9538084449875791919"/>
<edge source="12908708535020454138" target="1672665553767281734"/>
<edge source="817698272872287436" target="1672665553767281734"/>
<edge source="8704237515510088771" target="1672665553767281734"/>
<edge source="4431453089685809340" target="1672665553767281734"/>
<edge source="2104521862399993019" target="1672665553767281734"/>
<edge source="2104521862399993019" target="12563424232401784595"/>
<edge source="2104521862399993019" target="6382612685700818764"/>
<edge source="2104521862399993019" target="9538084449875791919"/>
<edge source="2104521862399993019" target="11324600873272743514"/>
<edge source="2104521862399993019" target="12644740604491229422"/>
<edge source="2104521862399993019" target="5999650257677576183"/>
<edge source="14911027486580949315" target="1672665553767281734"/>
<edge source="4278610892084589339" target="1672665553767281734"/>
<edge source="4278610892084589339" target="9538084449875791919"/>
<edge source="3868256673952367025" target="1672665553767281734"/>
<edge source="915917310659134030" target="1672665553767281734"/>
<edge source="915917310659134030" target="12644740604491229422"/>
<edge source="6004268348151288098" target="1672665553767281734"/>
<edge source="6004268348151288098" target="6382612685700818764"/>
<edge source="6004268348151288098" target="9538084449875791919"/>
<edge source="7327595990658945420" target="1672665553767281734"/>
<edge source="7623662567011711547" target="1672665553767281734"/>
<edge source="10424310778620231784" target="1672665553767281734"/>
<edge source="10424310778620231784" target="12644740604491229422"/>
<edge source="8367071398417962813" target="1672665553767281734"/>
<edge source="8367071398417962813" target="4550423614333066109"/>
<edge source="8367071398417962813" target="9538084449875791919"/>
<edge source="8367071398417962813" target="6382612685700818764"/>
<edge source="8367071398417962813" target="11324600873272743514"/>
<edge source="8367071398417962813" target="12644740604491229422"/>
<edge source="8367071398417962813" target="5999650257677576183"/>
<edge source="5445650339708387168" target="1672665553767281734"/>
<edge source="5445650339708387168" target="12644740604491229422"/>
<edge source="8639836755629224484" target="1672665553767281734"/>
<edge source="1440121271646678581" target="1672665553767281734"/>
<edge source="3380151090492446607" target="1672665553767281734"/>
<edge source="17454749727813031882" target="1672665553767281734"/>
<edge source="17454749727813031882" target="12644740604491229422"/>
<edge source="11808828478262249051" target="1672665553767281734"/>
<edge source="11460907858590036405" target="1672665553767281734"/>
<edge source="11460907858590036405" target="12644740604491229422"/>
<edge source="610621467807251926" target="1672665553767281734"/>
<edge source="9233219793813194550" target="1672665553767281734"/>
<edge source="17780356579077510129" target="1672665553767281734"/>
<edge source="18216985783540724512" target="1672665553767281734"/>
<edge source="10842331006678867208" target="1672665553767281734"/>
<edge source="10842331006678867208" target="4550423614333066109"/>
<edge source="10842331006678867208" target="9538084449875791919"/>
<edge source="10842331006678867208" target="11324600873272743514"/>
<edge source="10842331006678867208" target="12644740604491229422"/>
<edge source="10842331006678867208" target="5999650257677576183"/>
<edge source="3320528631702187603" target="1672665553767281734"/>
<edge source="4773463079530656035" target="1672665553767281734"/>
<edge source="4773463079530656035" target="2104521862399993019"/>
<edge source="4773463079530656035" target="11324600873272743514"/>
<edge source="3799744009906269739" target="1672665553767281734"/>
<edge source="3799744009906269739" target="1273811038957334386"/>
<edge source="3799744009906269739" target="9538084449875791919"/>
<edge source="11112701312974631954" target="1672665553767281734"/>
<edge source="12911220675230413174" target="1672665553767281734"/>
<edge source="12911220675230413174" target="2104521862399993019"/>
<edge source="14185047449981394536" target="1672665553767281734"/>
<edge source="9393703958705125224" target="1672665553767281734"/>
<edge source="14030709993426908081" target="1672665553767281734"/>
<edge source="12956114412627599893" target="1672665553767281734"/>
<edge source="17712252571307454824" target="1672665553767281734"/>
<edge source="9646853108847192407" target="1672665553767281734"/>
<edge source="5792560058636996973" target="1672665553767281734"/>
<edge source="5792560058636996973" target="11324600873272743514"/>
<edge source="7843478121381660661" target="1672665553767281734"/>
<edge source="8730617665338917129" target="1672665553767281734"/>
<edge source="10828507269261066901" target="1672665553767281734"/>
<edge source="10828507269261066901" target="9538084449875791919"/>
<edge source="10828507269261066901" target="11324600873272743514"/>
<edge source="10828507269261066901" target="5999650257677576183"/>
<edge source="1986765630077984342" target="1672665553767281734"/>
<edge source="1986765630077984342" target="2104521862399993019"/>
<edge source="1986765630077984342" target="9538084449875791919"/>
<edge source="1319826694668830613" target="1672665553767281734"/>
<edge source="1319826694668830613" target="2104521862399993019"/>
<edge source="1319826694668830613" target="9538084449875791919"/>
<edge source="1319826694668830613" target="11324600873272743514"/>
<edge source="4416527254888837916" target="1672665553767281734"/>
<edge source="12372357504406827550" target="1672665553767281734"/>
<edge source="14889255110023347971" target="1672665553767281734"/>
<edge source="14889255110023347971" target="12563424232401784595"/>
<edge source="8562591691593838404" target="1672665553767281734"/>
<edge source="17870066505440679476" target="1672665553767281734"/>
<edge source="14766365221923861675" target="1672665553767281734"/>
<edge source="966567457136989804" target="1672665553767281734"/>
<edge source="14768862339001694574" target="1672665553767281734"/>
<edge source="3435520687143059291" target="1672665553767281734"/>
<edge source="12563424232401784595" target="6382612685700818764"/>
<edge source="12563424232401784595" target="4550423614333066109"/>
<edge source="12563424232401784595" target="9538084449875791919"/>
<edge source="12563424232401784595" target="11324600873272743514"/>
<edge source="12563424232401784595" target="5999650257677576183"/>
<edge source="1747429016739752835" target="12563424232401784595"/>
<edge source="8455459026871994587" target="12563424232401784595"/>
<edge source="1529894457156192581" target="12563424232401784595"/>
<edge source="1529894457156192581" target="4550423614333066109"/>
<edge source="1529894457156192581" target="11324600873272743514"/>
<edge source="1529894457156192581" target="12644740604491229422"/>
<edge source="3460125397015656161" target="12563424232401784595"/>
<edge source="3460125397015656161" target="4550423614333066109"/>
<edge source="3460125397015656161" target="11324600873272743514"/>
<edge source="17632693958287984620" target="12563424232401784595"/>
<edge source="17632693958287984620" target="1273811038957334386"/>
<edge source="12528683203894514853" target="12563424232401784595"/>
<edge source="7857274260714324520" target="12563424232401784595"/>
<edge source="7857274260714324520" target="4550423614333066109"/>
<edge source="14032891689439695129" target="12563424232401784595"/>
<edge source="14032891689439695129" target="4550423614333066109"/>
<edge source="14032891689439695129" target="13055852169785823920"/>
<edge source="14685699400456064504" target="12563424232401784595"/>
<edge source="14685699400456064504" target="13055852169785823920"/>
<edge source="1836634166646364702" target="12563424232401784595"/>
<edge source="9800203968264557275" target="12563424232401784595"/>
<edge source="4801397296227909937" target="12563424232401784595"/>
<edge source="4801397296227909937" target="13055852169785823920"/>
<edge source="17733214079678161609" target="12563424232401784595"/>
<edge source="17275460023441273996" target="12563424232401784595"/>
<edge source="9099615620722636165" target="12563424232401784595"/>
<edge source="9099615620722636165" target="18210588638302847093"/>
<edge source="9099615620722636165" target="13055852169785823920"/>
<edge source="15832601975079045141" target="12563424232401784595"/>
<edge source="15832601975079045141" target="13055852169785823920"/>
<edge source="501800524296721407" target="12563424232401784595"/>
<edge source="13429408729973378271" target="12563424232401784595"/>
<edge source="16749039856030657986" target="12563424232401784595"/>
<edge source="16749039856030657986" target="13055852169785823920"/>
<edge source="11425593478470212697" target="12563424232401784595"/>
<edge source="11425593478470212697" target="13055852169785823920"/>
<edge source="11107660116184346387" target="12563424232401784595"/>
<edge source="11107660116184346387" target="13055852169785823920"/>
<edge source="12615305976817605176" target="12563424232401784595"/>
<edge source="15647727552153509563" target="12563424232401784595"/>
<edge source="15014850471536980328" target="12563424232401784595"/>
<edge source="6434716222653444990" target="12563424232401784595"/>
<edge source="6434716222653444990" target="2104521862399993019"/>
<edge source="17859848890141888541" target="12563424232401784595"/>
<edge source="3847442534681122442" target="12563424232401784595"/>
<edge source="3175561385815325856" target="12563424232401784595"/>
<edge source="3175561385815325856" target="13055852169785823920"/>
<edge source="6897858680399954297" target="12563424232401784595"/>
<edge source="17212634534495365627" target="12563424232401784595"/>
<edge source="5912943256082429583" target="12563424232401784595"/>
<edge source="16607894208431157893" target="12563424232401784595"/>
<edge source="14615147074360053768" target="12563424232401784595"/>
<edge source="14615147074360053768" target="2104521862399993019"/>
<edge source="8195426956859072339" target="12563424232401784595"/>
<edge source="3657623424699616492" target="12563424232401784595"/>
<edge source="7400981380759084546" target="12563424232401784595"/>
<edge source="3490367220831301861" target="12563424232401784595"/>
<edge source="15727602987444865635" target="12563424232401784595"/>
<edge source="1805226328438105949" target="12563424232401784595"/>
<edge source="1805226328438105949" target="2104521862399993019"/>
<edge source="14442074394232253245" target="12563424232401784595"/>
<edge source="15900202697333596864" target="12563424232401784595"/>
<edge source="3843578070760729914" target="12563424232401784595"/>
<edge source="7176502249404124990" target="12563424232401784595"/>
<edge source="1170441202157905831" target="12563424232401784595"/>
<edge source="12063573584218762374" target="12563424232401784595"/>
<edge source="2762494041692657615" target="12563424232401784595"/>
<edge source="5067198574099017621" target="12563424232401784595"/>
<edge source="10920476727045815687" target="12563424232401784595"/>
<edge source="618726556668923706" target="12563424232401784595"/>
<edge source="10392290641755897502" target="12563424232401784595"/>
<edge source="16867947394585746384" target="12563424232401784595"/>
<edge source="7958224277211121064" target="12563424232401784595"/>
<edge source="14994183852412960494" target="12563424232401784595"/>
<edge source="8743963594325005332" target="12563424232401784595"/>
<edge source="3716397009677337300" target="12563424232401784595"/>
<edge source="11651296702682936875" target="12563424232401784595"/>
<edge source="491588394875958513" target="12563424232401784595"/>
<edge source="2613459347497379531" target="12563424232401784595"/>
<edge source="6340122554340223367" target="12563424232401784595"/>
<edge source="6724179115405406423" target="12563424232401784595"/>
<edge source="14372476051154245137" target="12563424232401784595"/>
<edge source="18310514353510448588" target="12563424232401784595"/>
<edge source="14757740639617181116" target="12563424232401784595"/>
<edge source="3625588465481963755" target="12563424232401784595"/>
<edge source="1358307963221369570" target="12563424232401784595"/>
<edge source="17729334233569922217" target="12563424232401784595"/>
<edge source="5997947092271436772" target="12563424232401784595"/>
<edge source="10928542084451319450" target="12563424232401784595"/>
<edge source="10928542084451319450" target="8367071398417962813"/>
<edge source="14498566121599741556" target="12563424232401784595"/>
<edge source="10132132318136226957" target="12563424232401784595"/>
<edge source="4082625592026884096" target="12563424232401784595"/>
<edge source="6948486903139323825" target="12563424232401784595"/>
<edge source="18135864541368653593" target="12563424232401784595"/>
<edge source="7748241482703179571" target="12563424232401784595"/>
<edge source="3294374866555392652" target="12563424232401784595"/>
<edge source="5386784097479583414" target="12563424232401784595"/>
<edge source="6661086806896725511" target="12563424232401784595"/>
<edge source="15115558022538513237" target="12563424232401784595"/>
<edge source="14159121989210618633" target="12563424232401784595"/>
<edge source="15045952046135698930" target="18210588638302847093"/>
<edge source="1223563817431199857" target="18210588638302847093"/>
<edge source="1223563817431199857" target="4550423614333066109"/>
<edge source="1223563817431199857" target="13055852169785823920"/>
<edge source="1223563817431199857" target="12644740604491229422"/>
<edge source="3743433833463586545" target="18210588638302847093"/>
<edge source="2805320302601413924" target="18210588638302847093"/>
<edge source="12502648671025662060" target="18210588638302847093"/>
<edge source="11992929173642808953" target="18210588638302847093"/>
<edge source="6429502500328305488" target="18210588638302847093"/>
<edge source="4628199491611022446" target="18210588638302847093"/>
<edge source="17059844040400317816" target="18210588638302847093"/>
<edge source="2901021971175385417" target="18210588638302847093"/>
<edge source="5838860470680827023" target="18210588638302847093"/>
<edge source="16864394826962881925" target="18210588638302847093"/>
<edge source="7269756958028651926" target="18210588638302847093"/>
<edge source="6832920879012580854" target="18210588638302847093"/>
<edge source="12145681576382105820" target="18210588638302847093"/>
<edge source="18188852084265258548" target="18210588638302847093"/>
<edge source="795370128396508846" target="18210588638302847093"/>
<edge source="17094049497238104059" target="18210588638302847093"/>
<edge source="8167187293724692556" target="18210588638302847093"/>
<edge source="1708442884848326195" target="18210588638302847093"/>
<edge source="10621171041712014560" target="18210588638302847093"/>
<edge source="282841756216791105" target="18210588638302847093"/>
<edge source="1020146708014614449" target="18210588638302847093"/>
<edge source="10294172557164487522" target="18210588638302847093"/>
<edge source="14493401188532558995" target="18210588638302847093"/>
<edge source="915175193869878529" target="18210588638302847093"/>
<edge source="4712339159260231874" target="18210588638302847093"/>
<edge source="10634144542916924319" target="18210588638302847093"/>
<edge source="10634144542916924319" target="14311400318178337111"/>
<edge source="15151751976029410131" target="18210588638302847093"/>
<edge source="3619283013073038983" target="18210588638302847093"/>
<edge source="3619283013073038983" target="8367071398417962813"/>
<edge source="14298330466237469464" target="18210588638302847093"/>
<edge source="17746736907900692263" target="18210588638302847093"/>
<edge source="17746736907900692263" target="8367071398417962813"/>
<edge source="7490525040340169893" target="18210588638302847093"/>
<edge source="1646852206964174022" target="18210588638302847093"/>
<edge source="3566720882072534084" target="18210588638302847093"/>
<edge source="6060221142406415982" target="18210588638302847093"/>
<edge source="7686098348117464607" target="18210588638302847093"/>
<edge source="10112787672038528081" target="18210588638302847093"/>
<edge source="12467383398842524346" target="18210588638302847093"/>
<edge source="17781373599130948810" target="18210588638302847093"/>
<edge source="10321528843008389282" target="18210588638302847093"/>
<edge source="3073101641983057594" target="18210588638302847093"/>
<edge source="15258815906138889567" target="18210588638302847093"/>
<edge source="12958131567968188905" target="18210588638302847093"/>
<edge source="11183708110410802287" target="18210588638302847093"/>
<edge source="14682032714717312533" target="18210588638302847093"/>
<edge source="11101126821023653833" target="18210588638302847093"/>
<edge source="2547097814727899492" target="18210588638302847093"/>
<edge source="9478150375568593008" target="18210588638302847093"/>
<edge source="1144856546777335713" target="18210588638302847093"/>
<edge source="5373184172069649878" target="18210588638302847093"/>
<edge source="1630814355216789856" target="18210588638302847093"/>
<edge source="5480963051895971527" target="18210588638302847093"/>
<edge source="14031173575587899194" target="18210588638302847093"/>
<edge source="16620415112774447142" target="18210588638302847093"/>
<edge source="16620415112774447142" target="8367071398417962813"/>
<edge source="16166479600523370143" target="18210588638302847093"/>
<edge source="12327522076345093055" target="18210588638302847093"/>
<edge source="14339464007846837517" target="18210588638302847093"/>
<edge source="7455995568870583888" target="18210588638302847093"/>
<edge source="7455995568870583888" target="8367071398417962813"/>
<edge source="3242865566534850692" target="18210588638302847093"/>
<edge source="10190720705683005400" target="18210588638302847093"/>
<edge source="3066843940507119154" target="18210588638302847093"/>
<edge source="6332829983840302405" target="18210588638302847093"/>
<edge source="13500090513888098729" target="18210588638302847093"/>
<edge source="12621962626789350377" target="18210588638302847093"/>
<edge source="16002571822077945928" target="18210588638302847093"/>
<edge source="3232219129862852847" target="18210588638302847093"/>
<edge source="5354452319526960123" target="18210588638302847093"/>
<edge source="14918519107862734365" target="18210588638302847093"/>
<edge source="17165120566620849856" target="18210588638302847093"/>
<edge source="17165120566620849856" target="8367071398417962813"/>
<edge source="4741106364027764952" target="18210588638302847093"/>
<edge source="11222483890632287401" target="18210588638302847093"/>
<edge source="12142856329509817511" target="18210588638302847093"/>
<edge source="12142856329509817511" target="8367071398417962813"/>
<edge source="11083218512495776976" target="18210588638302847093"/>
<edge source="3110755508632090464" target="18210588638302847093"/>
<edge source="4388463373166231389" target="18210588638302847093"/>
<edge source="11986345078302880571" target="18210588638302847093"/>
<edge source="943904167170865655" target="18210588638302847093"/>
<edge source="5907315131746612745" target="18210588638302847093"/>
<edge source="2505454209938753585" target="18210588638302847093"/>
<edge source="1928444155270241570" target="18210588638302847093"/>
<edge source="4550423614333066109" target="6382612685700818764"/>
<edge source="4550423614333066109" target="13055852169785823920"/>
<edge source="4550423614333066109" target="9538084449875791919"/>
<edge source="4550423614333066109" target="11324600873272743514"/>
<edge source="4550423614333066109" target="12644740604491229422"/>
<edge source="4550423614333066109" target="5999650257677576183"/>
<edge source="2143047335646200982" target="4550423614333066109"/>
<edge source="9374811277911686866" target="4550423614333066109"/>
<edge source="2641915666092458000" target="4550423614333066109"/>
<edge source="6847238147022651878" target="4550423614333066109"/>
<edge source="15073015961806262305" target="4550423614333066109"/>
<edge source="15073015961806262305" target="13346469121565650680"/>
<edge source="4893668343000496575" target="4550423614333066109"/>
<edge source="2257115641228924434" target="4550423614333066109"/>
<edge source="8384042348747306199" target="4550423614333066109"/>
<edge source="8384042348747306199" target="13346469121565650680"/>
<edge source="8384042348747306199" target="5999650257677576183"/>
<edge source="2188347889974787509" target="4550423614333066109"/>
<edge source="2188347889974787509" target="9538084449875791919"/>
<edge source="2188347889974787509" target="5999650257677576183"/>
<edge source="4220770694777717094" target="4550423614333066109"/>
<edge source="4220770694777717094" target="13346469121565650680"/>
<edge source="8949706078964164796" target="4550423614333066109"/>
<edge source="17957175255238019436" target="4550423614333066109"/>
<edge source="6960142602186458983" target="4550423614333066109"/>
<edge source="6960142602186458983" target="13055852169785823920"/>
<edge source="6960142602186458983" target="9538084449875791919"/>
<edge source="6960142602186458983" target="11324600873272743514"/>
<edge source="6960142602186458983" target="12644740604491229422"/>
<edge source="6960142602186458983" target="5999650257677576183"/>
<edge source="14469340457726874491" target="4550423614333066109"/>
<edge source="1333110857552676288" target="4550423614333066109"/>
<edge source="1333110857552676288" target="13055852169785823920"/>
<edge source="1333110857552676288" target="9538084449875791919"/>
<edge source="1333110857552676288" target="5999650257677576183"/>
<edge source="2870654243860368694" target="4550423614333066109"/>
<edge source="2870654243860368694" target="13055852169785823920"/>
<edge source="2870654243860368694" target="9538084449875791919"/>
<edge source="2870654243860368694" target="12644740604491229422"/>
<edge source="14520782850205069410" target="4550423614333066109"/>
<edge source="8856983795724243302" target="4550423614333066109"/>
<edge source="14463958793485183598" target="4550423614333066109"/>
<edge source="14463958793485183598" target="9538084449875791919"/>
<edge source="11047031223001120005" target="4550423614333066109"/>
<edge source="11047031223001120005" target="6004268348151288098"/>
<edge source="10949414717109961618" target="4550423614333066109"/>
<edge source="10684080371797193170" target="4550423614333066109"/>
<edge source="2381712952938130950" target="4550423614333066109"/>
<edge source="2381712952938130950" target="13055852169785823920"/>
<edge source="2381712952938130950" target="12644740604491229422"/>
<edge source="2381712952938130950" target="13346469121565650680"/>
<edge source="599070989090902392" target="4550423614333066109"/>
<edge source="8128999273420313759" target="4550423614333066109"/>
<edge source="8128999273420313759" target="13346469121565650680"/>
<edge source="6959485123048444799" target="4550423614333066109"/>
<edge source="6959485123048444799" target="13346469121565650680"/>
<edge source="15019162986849498399" target="4550423614333066109"/>
<edge source="15019162986849498399" target="13055852169785823920"/>
<edge source="15019162986849498399" target="12644740604491229422"/>
<edge source="14023415170451607918" target="4550423614333066109"/>
<edge source="2956947656254895097" target="4550423614333066109"/>
<edge source="2956947656254895097" target="1431633311194488622"/>
<edge source="4134683771649593563" target="4550423614333066109"/>
<edge source="9683136153549357939" target="4550423614333066109"/>
<edge source="9683136153549357939" target="13055852169785823920"/>
<edge source="9683136153549357939" target="12644740604491229422"/>
<edge source="9631020688074423843" target="4550423614333066109"/>
<edge source="9631020688074423843" target="12644740604491229422"/>
<edge source="12262835944335888935" target="4550423614333066109"/>
<edge source="12262835944335888935" target="13055852169785823920"/>
<edge source="12301499295205287615" target="4550423614333066109"/>
<edge source="12301499295205287615" target="13055852169785823920"/>
<edge source="12301499295205287615" target="12644740604491229422"/>
<edge source="4438492435191836535" target="4550423614333066109"/>
<edge source="4438492435191836535" target="13055852169785823920"/>
<edge source="13337861536124971139" target="4550423614333066109"/>
<edge source="12863020002569949671" target="4550423614333066109"/>
<edge source="3656418176994524008" target="4550423614333066109"/>
<edge source="2452866517197292093" target="4550423614333066109"/>
<edge source="2452866517197292093" target="8367071398417962813"/>
<edge source="15799285439221317870" target="4550423614333066109"/>
<edge source="9207765007843851263" target="4550423614333066109"/>
<edge source="9207765007843851263" target="13346469121565650680"/>
<edge source="6026250830209566504" target="4550423614333066109"/>
<edge source="6026250830209566504" target="2104521862399993019"/>
<edge source="11882550127852592683" target="4550423614333066109"/>
<edge source="9014029637884314032" target="4550423614333066109"/>
<edge source="9014029637884314032" target="13055852169785823920"/>
<edge source="9014029637884314032" target="13346469121565650680"/>
<edge source="3909123747085405380" target="4550423614333066109"/>
<edge source="3909123747085405380" target="2104521862399993019"/>
<edge source="3909123747085405380" target="12644740604491229422"/>
<edge source="15281565077085720279" target="4550423614333066109"/>
<edge source="15281565077085720279" target="13346469121565650680"/>
<edge source="14672720829595281606" target="4550423614333066109"/>
<edge source="7222787612695411352" target="4550423614333066109"/>
<edge source="2241015688519496377" target="4550423614333066109"/>
<edge source="2241015688519496377" target="13346469121565650680"/>
<edge source="15226748689642581218" target="4550423614333066109"/>
<edge source="16799196534333096171" target="4550423614333066109"/>
<edge source="16799196534333096171" target="13346469121565650680"/>
<edge source="9519448529520784463" target="4550423614333066109"/>
<edge source="8269996013982993565" target="4550423614333066109"/>
<edge source="12292417492888691772" target="4550423614333066109"/>
<edge source="3469053983010224891" target="4550423614333066109"/>
<edge source="3469053983010224891" target="13055852169785823920"/>
<edge source="3469053983010224891" target="12644740604491229422"/>
<edge source="15326684253758327032" target="4550423614333066109"/>
<edge source="11839204226610748442" target="4550423614333066109"/>
<edge source="7244644999756957350" target="4550423614333066109"/>
<edge source="7244644999756957350" target="13055852169785823920"/>
<edge source="15430727510373100145" target="4550423614333066109"/>
<edge source="2484140027818923745" target="4550423614333066109"/>
<edge source="2484140027818923745" target="11324600873272743514"/>
<edge source="10402998279197599439" target="4550423614333066109"/>
<edge source="16564571492344788038" target="4550423614333066109"/>
<edge source="16564571492344788038" target="13346469121565650680"/>
<edge source="17634164049223670343" target="4550423614333066109"/>
<edge source="19561195188667465" target="4550423614333066109"/>
<edge source="19561195188667465" target="13055852169785823920"/>
<edge source="19561195188667465" target="13346469121565650680"/>
<edge source="15167698914301610121" target="4550423614333066109"/>
<edge source="9817510568104192073" target="4550423614333066109"/>
<edge source="9665171368816644761" target="4550423614333066109"/>
<edge source="15329030602586300416" target="4550423614333066109"/>
<edge source="15329030602586300416" target="13055852169785823920"/>
<edge source="7085275218926344408" target="4550423614333066109"/>
<edge source="7085275218926344408" target="11324600873272743514"/>
<edge source="2700093867622300626" target="4550423614333066109"/>
<edge source="12979999094083269078" target="4550423614333066109"/>
<edge source="12979999094083269078" target="13055852169785823920"/>
<edge source="12527627239009594881" target="4550423614333066109"/>
<edge source="16290848704501469564" target="4550423614333066109"/>
<edge source="9060832344926997444" target="2104521862399993019"/>
<edge source="1659700362637596040" target="2104521862399993019"/>
<edge source="1659700362637596040" target="11324600873272743514"/>
<edge source="8662214348531793431" target="2104521862399993019"/>
<edge source="17314410080623672897" target="2104521862399993019"/>
<edge source="17314410080623672897" target="5999650257677576183"/>
<edge source="10274815087550872131" target="2104521862399993019"/>
<edge source="10274815087550872131" target="12644740604491229422"/>
<edge source="8124073977598762954" target="2104521862399993019"/>
<edge source="8124073977598762954" target="8448943115025253905"/>
<edge source="14069730115218722114" target="2104521862399993019"/>
<edge source="14069730115218722114" target="9538084449875791919"/>
<edge source="14069730115218722114" target="11324600873272743514"/>
<edge source="977587388012773361" target="2104521862399993019"/>
<edge source="977587388012773361" target="9538084449875791919"/>
<edge source="977587388012773361" target="8367071398417962813"/>
<edge source="977587388012773361" target="11324600873272743514"/>
<edge source="15814018180296500902" target="2104521862399993019"/>
<edge source="17436807480659026891" target="2104521862399993019"/>
<edge source="7616713930375867128" target="2104521862399993019"/>
<edge source="7616713930375867128" target="9538084449875791919"/>
<edge source="17891879498080154736" target="2104521862399993019"/>
<edge source="8825090207674893811" target="2104521862399993019"/>
<edge source="8825090207674893811" target="1273811038957334386"/>
<edge source="11825390691537168558" target="2104521862399993019"/>
<edge source="4586155361346152499" target="2104521862399993019"/>
<edge source="9894263145711509588" target="2104521862399993019"/>
<edge source="9894263145711509588" target="11324600873272743514"/>
<edge source="1322906163224921925" target="2104521862399993019"/>
<edge source="10270081680505767341" target="2104521862399993019"/>
<edge source="10270081680505767341" target="11324600873272743514"/>
<edge source="10270081680505767341" target="12644740604491229422"/>
<edge source="1534689713476232636" target="2104521862399993019"/>
<edge source="1528825781662204302" target="2104521862399993019"/>
<edge source="1528825781662204302" target="11324600873272743514"/>
<edge source="1528825781662204302" target="12644740604491229422"/>
<edge source="1852377411269249881" target="2104521862399993019"/>
<edge source="2803945007747990107" target="2104521862399993019"/>
<edge source="2803945007747990107" target="8367071398417962813"/>
<edge source="15824157200018556836" target="2104521862399993019"/>
<edge source="16816930780976029141" target="2104521862399993019"/>
<edge source="16816930780976029141" target="6004268348151288098"/>
<edge source="9209014176820981155" target="2104521862399993019"/>
<edge source="5598507831422168925" target="2104521862399993019"/>
<edge source="17182921757850029040" target="2104521862399993019"/>
<edge source="17182921757850029040" target="11324600873272743514"/>
<edge source="17304148947425718239" target="2104521862399993019"/>
<edge source="9105314878135623559" target="2104521862399993019"/>
<edge source="15674075284060635644" target="2104521862399993019"/>
<edge source="5425241495538385765" target="2104521862399993019"/>
<edge source="5425241495538385765" target="11324600873272743514"/>
<edge source="8214441654299017644" target="2104521862399993019"/>
<edge source="16490222600712608071" target="2104521862399993019"/>
<edge source="16490222600712608071" target="8367071398417962813"/>
<edge source="16490222600712608071" target="11324600873272743514"/>
<edge source="12517757346728435037" target="2104521862399993019"/>
<edge source="16726572534871887028" target="2104521862399993019"/>
<edge source="17039945862035123188" target="2104521862399993019"/>
<edge source="17039945862035123188" target="11324600873272743514"/>
<edge source="16118065418965645421" target="2104521862399993019"/>
<edge source="16118065418965645421" target="13055852169785823920"/>
<edge source="10812290025544152705" target="2104521862399993019"/>
<edge source="18413127590718653617" target="2104521862399993019"/>
<edge source="18413127590718653617" target="11324600873272743514"/>
<edge source="6989674085539462525" target="2104521862399993019"/>
<edge source="12121457393934602812" target="2104521862399993019"/>
<edge source="4303557091610097560" target="2104521862399993019"/>
<edge source="932093633137744803" target="2104521862399993019"/>
<edge source="3247052077202185212" target="2104521862399993019"/>
<edge source="4349671636320753508" target="2104521862399993019"/>
<edge source="18249743012593625562" target="2104521862399993019"/>
<edge source="11513198882440237429" target="2104521862399993019"/>
<edge source="17487592973794344865" target="2104521862399993019"/>
<edge source="17487592973794344865" target="8367071398417962813"/>
<edge source="14517707013240558642" target="2104521862399993019"/>
<edge source="17247852947552745742" target="2104521862399993019"/>
<edge source="10634998792902995201" target="2104521862399993019"/>
<edge source="10884589459641707712" target="2104521862399993019"/>
<edge source="7760609266921230809" target="2104521862399993019"/>
<edge source="88270295968296053" target="2104521862399993019"/>
<edge source="88270295968296053" target="8367071398417962813"/>
<edge source="16950256669529671184" target="2104521862399993019"/>
<edge source="9968669049352955949" target="2104521862399993019"/>
<edge source="13477863999935708391" target="2104521862399993019"/>
<edge source="17378883393050270914" target="2104521862399993019"/>
<edge source="9787590110579187483" target="2104521862399993019"/>
<edge source="9787590110579187483" target="8448943115025253905"/>
<edge source="1851007110321536767" target="2104521862399993019"/>
<edge source="16309997597335697837" target="2104521862399993019"/>
<edge source="16309997597335697837" target="8367071398417962813"/>
<edge source="12695099468525271301" target="2104521862399993019"/>
<edge source="4296423219815701790" target="2104521862399993019"/>
<edge source="3432971989735915692" target="2104521862399993019"/>
<edge source="1436259968817456407" target="2104521862399993019"/>
<edge source="1436259968817456407" target="11452477192554017570"/>
<edge source="9723387705619955263" target="2104521862399993019"/>
<edge source="7127843590064680446" target="2104521862399993019"/>
<edge source="10574929869960191427" target="2104521862399993019"/>
<edge source="15062612983329704744" target="2104521862399993019"/>
<edge source="15062612983329704744" target="6004268348151288098"/>
<edge source="15276230008077862137" target="2104521862399993019"/>
<edge source="7534423418396554672" target="2104521862399993019"/>
<edge source="2519052790779402959" target="2104521862399993019"/>
<edge source="4642654167987856293" target="2104521862399993019"/>
<edge source="12417942115314361068" target="2104521862399993019"/>
<edge source="3862789668091273745" target="2104521862399993019"/>
<edge source="13055852169785823920" target="6382612685700818764"/>
<edge source="13055852169785823920" target="9538084449875791919"/>
<edge source="13055852169785823920" target="12644740604491229422"/>
<edge source="13055852169785823920" target="5999650257677576183"/>
<edge source="10191050231224943760" target="13055852169785823920"/>
<edge source="12829565963573040312" target="13055852169785823920"/>
<edge source="12829565963573040312" target="12644740604491229422"/>
<edge source="17430652832439326106" target="13055852169785823920"/>
<edge source="17430652832439326106" target="12644740604491229422"/>
<edge source="17430652832439326106" target="13346469121565650680"/>
<edge source="17430652832439326106" target="5999650257677576183"/>
<edge source="7988129384525076421" target="13055852169785823920"/>
<edge source="7988129384525076421" target="11324600873272743514"/>
<edge source="7988129384525076421" target="12644740604491229422"/>
<edge source="7487171131395966182" target="13055852169785823920"/>
<edge source="7487171131395966182" target="9538084449875791919"/>
<edge source="7487171131395966182" target="11324600873272743514"/>
<edge source="7487171131395966182" target="12644740604491229422"/>
<edge source="9458084216549029781" target="13055852169785823920"/>
<edge source="9458084216549029781" target="12644740604491229422"/>
<edge source="15795399494869889077" target="13055852169785823920"/>
<edge source="15795399494869889077" target="9538084449875791919"/>
<edge source="15795399494869889077" target="12644740604491229422"/>
<edge source="8762454778937977659" target="13055852169785823920"/>
<edge source="8762454778937977659" target="12644740604491229422"/>
<edge source="8762454778937977659" target="5999650257677576183"/>
<edge source="9575584799952236610" target="13055852169785823920"/>
<edge source="13728347746290657939" target="13055852169785823920"/>
<edge source="13222542974410927057" target="13055852169785823920"/>
<edge source="1654562328017701106" target="13055852169785823920"/>
<edge source="13553623797919213094" target="13055852169785823920"/>
<edge source="13816665854585781255" target="13055852169785823920"/>
<edge source="5767715645618653984" target="13055852169785823920"/>
<edge source="15818286087987627641" target="13055852169785823920"/>
<edge source="16403067771390791423" target="13055852169785823920"/>
<edge source="5845461691817459977" target="13055852169785823920"/>
<edge source="14373500618604811887" target="13055852169785823920"/>
<edge source="14509614284638697910" target="13055852169785823920"/>
<edge source="2822888602570219347" target="13055852169785823920"/>
<edge source="2124912804095600023" target="13055852169785823920"/>
<edge source="16401671466632432676" target="13055852169785823920"/>
<edge source="18286028574106509245" target="13055852169785823920"/>
<edge source="7167680945390018074" target="13055852169785823920"/>
<edge source="7167680945390018074" target="11324600873272743514"/>
<edge source="4019899772206366950" target="13055852169785823920"/>
<edge source="3815706068371757718" target="13055852169785823920"/>
<edge source="3273783590798016013" target="13055852169785823920"/>
<edge source="249972322094479786" target="13055852169785823920"/>
<edge source="7697764778571254929" target="13055852169785823920"/>
<edge source="16919329449044452538" target="13055852169785823920"/>
<edge source="6421431120514564979" target="13055852169785823920"/>
<edge source="13529499341139137951" target="13055852169785823920"/>
<edge source="15464155576948087293" target="13055852169785823920"/>
<edge source="14962706247913538159" target="13055852169785823920"/>
<edge source="7855712559170638610" target="13055852169785823920"/>
<edge source="302515988675284467" target="13055852169785823920"/>
<edge source="17326028169687643062" target="13055852169785823920"/>
<edge source="4612145705765974530" target="13055852169785823920"/>
<edge source="6077963581280858299" target="13055852169785823920"/>
<edge source="6155236788302466355" target="13055852169785823920"/>
<edge source="4790015723049709885" target="13055852169785823920"/>
<edge source="4059997502371762707" target="13055852169785823920"/>
<edge source="5682759315963442168" target="13055852169785823920"/>
<edge source="17354835527964479444" target="13055852169785823920"/>
<edge source="13938085636022527711" target="13055852169785823920"/>
<edge source="4094576052109847144" target="13055852169785823920"/>
<edge source="10745444013023171384" target="13055852169785823920"/>
<edge source="6927002374951805343" target="13055852169785823920"/>
<edge source="5562955281835677624" target="6004268348151288098"/>
<edge source="8448388555539304854" target="6004268348151288098"/>
<edge source="4160517527641475312" target="6004268348151288098"/>
<edge source="4160517527641475312" target="9538084449875791919"/>
<edge source="2958442793928445860" target="6004268348151288098"/>
<edge source="10156380014983934707" target="6004268348151288098"/>
<edge source="1828958259233079068" target="6004268348151288098"/>
<edge source="9556534593478071448" target="6004268348151288098"/>
<edge source="2013709511789422934" target="6004268348151288098"/>
<edge source="2013709511789422934" target="11324600873272743514"/>
<edge source="2013709511789422934" target="12644740604491229422"/>
<edge source="213109028691722316" target="6004268348151288098"/>
<edge source="9083483030705185424" target="6004268348151288098"/>
<edge source="8163168166663635565" target="6004268348151288098"/>
<edge source="8163168166663635565" target="1273811038957334386"/>
<edge source="477874232529254013" target="6004268348151288098"/>
<edge source="14173651844217137684" target="6004268348151288098"/>
<edge source="7539527092820284785" target="6004268348151288098"/>
<edge source="7539527092820284785" target="11324600873272743514"/>
<edge source="10493115215078116970" target="6004268348151288098"/>
<edge source="2135826005582191986" target="6004268348151288098"/>
<edge source="238317474783907025" target="6004268348151288098"/>
<edge source="5712399572413041920" target="6004268348151288098"/>
<edge source="4463177057029404685" target="6004268348151288098"/>
<edge source="527701684105539342" target="6004268348151288098"/>
<edge source="11009706863402152282" target="6004268348151288098"/>
<edge source="427793634810618701" target="6004268348151288098"/>
<edge source="3710797496089968926" target="6004268348151288098"/>
<edge source="16719721271879905477" target="6004268348151288098"/>
<edge source="4650814090908712272" target="6004268348151288098"/>
<edge source="4803373243514933274" target="6004268348151288098"/>
<edge source="13480193018115242663" target="6004268348151288098"/>
<edge source="8802764110576830272" target="6004268348151288098"/>
<edge source="15826539500910476875" target="6004268348151288098"/>
<edge source="15237439848602268466" target="6004268348151288098"/>
<edge source="11416070587823639118" target="6004268348151288098"/>
<edge source="1637260800010177981" target="6004268348151288098"/>
<edge source="10981280129508603338" target="6004268348151288098"/>
<edge source="4328967688191824247" target="6004268348151288098"/>
<edge source="7638029543193686772" target="6004268348151288098"/>
<edge source="13377052506571091555" target="6004268348151288098"/>
<edge source="1437976452746021768" target="6004268348151288098"/>
<edge source="6353566274637563919" target="6004268348151288098"/>
<edge source="10393073040573897009" target="6004268348151288098"/>
<edge source="8912529817908967060" target="6004268348151288098"/>
<edge source="6499513093496013030" target="6004268348151288098"/>
<edge source="12164426444376333064" target="6004268348151288098"/>
<edge source="12516751945568333636" target="6004268348151288098"/>
<edge source="4093024291231214911" target="6004268348151288098"/>
<edge source="4093024291231214911" target="8448943115025253905"/>
<edge source="8402450993508627791" target="6004268348151288098"/>
<edge source="17253288657998487561" target="6004268348151288098"/>
<edge source="17311003549850461359" target="6004268348151288098"/>
<edge source="12473981189594605407" target="6004268348151288098"/>
<edge source="5975344897755588707" target="6004268348151288098"/>
<edge source="11304746998376087883" target="6004268348151288098"/>
<edge source="347260239260204132" target="6004268348151288098"/>
<edge source="11224588046887428191" target="6004268348151288098"/>
<edge source="6014107160773218396" target="6004268348151288098"/>
<edge source="15281327127593550121" target="6004268348151288098"/>
<edge source="2811646611673890603" target="6004268348151288098"/>
<edge source="12439397764083500705" target="6004268348151288098"/>
<edge source="5897644011303594318" target="6004268348151288098"/>
<edge source="8565593079230529362" target="6004268348151288098"/>
<edge source="3496963086485613469" target="6004268348151288098"/>
<edge source="7990405874739376119" target="6004268348151288098"/>
<edge source="14743366429652248033" target="6004268348151288098"/>
<edge source="10883123266865516659" target="6004268348151288098"/>
<edge source="3817102158084191521" target="6004268348151288098"/>
<edge source="17782492311519284315" target="6004268348151288098"/>
<edge source="10132989367820823432" target="6004268348151288098"/>
<edge source="13594362999251312694" target="6004268348151288098"/>
<edge source="5126083329506829425" target="6004268348151288098"/>
<edge source="841216030252728204" target="6004268348151288098"/>
<edge source="8BD3FPNAIHoJ" target="6004268348151288098"/>
<edge source="12075572838672919294" target="6004268348151288098"/>
<edge source="11630631256493667109" target="6004268348151288098"/>
<edge source="8636762778479383554" target="6004268348151288098"/>
<edge source="8814192339048828786" target="6004268348151288098"/>
<edge source="b5DIgD2ZqeQJ" target="6004268348151288098"/>
<edge source="1273811038957334386" target="6382612685700818764"/>
<edge source="12617218192144474322" target="1273811038957334386"/>
<edge source="5215096183189163093" target="1273811038957334386"/>
<edge source="17757348919061164318" target="1273811038957334386"/>
<edge source="12692106295877813680" target="1273811038957334386"/>
<edge source="1144066736404687657" target="1273811038957334386"/>
<edge source="12972864106896201781" target="1273811038957334386"/>
<edge source="15930910090552432609" target="1273811038957334386"/>
<edge source="16057670792750577500" target="1273811038957334386"/>
<edge source="3876751087281584278" target="1273811038957334386"/>
<edge source="467066746241725706" target="1273811038957334386"/>
<edge source="5506925343943935516" target="1273811038957334386"/>
<edge source="2000389979125404276" target="1273811038957334386"/>
<edge source="17534197269359092183" target="1273811038957334386"/>
<edge source="5763129354275940103" target="1273811038957334386"/>
<edge source="13233494379811120690" target="1273811038957334386"/>
<edge source="9867831262985002468" target="1273811038957334386"/>
<edge source="8988041508983958224" target="1273811038957334386"/>
<edge source="13961698310318632817" target="1273811038957334386"/>
<edge source="17264780080323807782" target="1273811038957334386"/>
<edge source="7521998026481226724" target="1273811038957334386"/>
<edge source="14292753405609492066" target="1273811038957334386"/>
<edge source="15701650176909360920" target="1273811038957334386"/>
<edge source="7083069404926056970" target="1273811038957334386"/>
<edge source="16345516296958117809" target="1273811038957334386"/>
<edge source="18352250779745802826" target="1273811038957334386"/>
<edge source="12521341369829058727" target="1273811038957334386"/>
<edge source="18278928779930263666" target="1273811038957334386"/>
<edge source="11435180467185856094" target="1273811038957334386"/>
<edge source="297116116872847031" target="1273811038957334386"/>
<edge source="10401914588824198986" target="1273811038957334386"/>
<edge source="1636570585907882285" target="1273811038957334386"/>
<edge source="12251423014012987213" target="1273811038957334386"/>
<edge source="16223923155940056199" target="1273811038957334386"/>
<edge source="1475939297738772704" target="1273811038957334386"/>
<edge source="163853940069191532" target="1273811038957334386"/>
<edge source="17353513126312288026" target="1273811038957334386"/>
<edge source="4553933449540878013" target="1273811038957334386"/>
<edge source="7548052650525629965" target="1273811038957334386"/>
<edge source="13374404244463301147" target="1273811038957334386"/>
<edge source="11917368591479581577" target="1273811038957334386"/>
<edge source="18135230557293693450" target="1273811038957334386"/>
<edge source="2687678320604906488" target="1273811038957334386"/>
<edge source="7457602010468600062" target="1273811038957334386"/>
<edge source="14160811618017343674" target="1273811038957334386"/>
<edge source="7586367217993538099" target="1273811038957334386"/>
<edge source="k2InbV7B6EkJ" target="1273811038957334386"/>
<edge source="8672277335438437194" target="1273811038957334386"/>
<edge source="8783919417057800696" target="1273811038957334386"/>
<edge source="17020241982343048724" target="1273811038957334386"/>
<edge source="6135323271371562232" target="1273811038957334386"/>
<edge source="9897568004629027598" target="1273811038957334386"/>
<edge source="iPZOpP141k4J" target="1273811038957334386"/>
<edge source="5346646808921915555" target="1273811038957334386"/>
<edge source="14765226393767823271" target="1273811038957334386"/>
<edge source="5678901956168261131" target="1273811038957334386"/>
<edge source="1431633311194488622" target="6382612685700818764"/>
<edge source="17457133269069328267" target="1431633311194488622"/>
<edge source="5640775539800892593" target="1431633311194488622"/>
<edge source="5640775539800892593" target="535744563039386055"/>
<edge source="17228116273163901194" target="1431633311194488622"/>
<edge source="5656297883023258429" target="1431633311194488622"/>
<edge source="2381021153585051804" target="1431633311194488622"/>
<edge source="11131425815493661687" target="1431633311194488622"/>
<edge source="16559789848821467555" target="1431633311194488622"/>
<edge source="13748354740225969894" target="1431633311194488622"/>
<edge source="17348071344751182786" target="1431633311194488622"/>
<edge source="2846334625875347669" target="1431633311194488622"/>
<edge source="2846334625875347669" target="12644740604491229422"/>
<edge source="2846334625875347669" target="5999650257677576183"/>
<edge source="7012076773417880547" target="1431633311194488622"/>
<edge source="14188108076981434930" target="1431633311194488622"/>
<edge source="10560769186027812935" target="1431633311194488622"/>
<edge source="17598489487193491440" target="1431633311194488622"/>
<edge source="824590046745083591" target="1431633311194488622"/>
<edge source="3987152815255001944" target="1431633311194488622"/>
<edge source="1405916394303361407" target="1431633311194488622"/>
<edge source="3929066136257329737" target="1431633311194488622"/>
<edge source="6969950659727941449" target="1431633311194488622"/>
<edge source="4712708126956095125" target="1431633311194488622"/>
<edge source="5884209795367025285" target="1431633311194488622"/>
<edge source="16286153053570566579" target="1431633311194488622"/>
<edge source="8218634905233683023" target="1431633311194488622"/>
<edge source="8173578619526272020" target="1431633311194488622"/>
<edge source="15628822607882875170" target="1431633311194488622"/>
<edge source="13519623104352135728" target="1431633311194488622"/>
<edge source="11740996352054171303" target="1431633311194488622"/>
<edge source="13319126561466992426" target="1431633311194488622"/>
<edge source="18175359140763832369" target="1431633311194488622"/>
<edge source="8609729441168460418" target="1431633311194488622"/>
<edge source="12338916099901568166" target="1431633311194488622"/>
<edge source="10361934383634103378" target="1431633311194488622"/>
<edge source="10034248469511743317" target="1431633311194488622"/>
<edge source="13094368588973025667" target="1431633311194488622"/>
<edge source="14507474565925582559" target="1431633311194488622"/>
<edge source="8608635600889702315" target="1431633311194488622"/>
<edge source="3480504377185349195" target="1431633311194488622"/>
<edge source="8264388784225550416" target="1431633311194488622"/>
<edge source="16185498123764366373" target="1431633311194488622"/>
<edge source="3692815723362239451" target="1431633311194488622"/>
<edge source="17960766448265380456" target="1431633311194488622"/>
<edge source="3243408560707200262" target="1431633311194488622"/>
<edge source="11612055028743993917" target="1431633311194488622"/>
<edge source="942093327978373135" target="1431633311194488622"/>
<edge source="6584430158722490128" target="1431633311194488622"/>
<edge source="204598726516650366" target="1431633311194488622"/>
<edge source="17047112888909822506" target="1431633311194488622"/>
<edge source="7072607443445254562" target="1431633311194488622"/>
<edge source="7755255229432407884" target="1431633311194488622"/>
<edge source="6916383400010189934" target="1431633311194488622"/>
<edge source="13318009799245280479" target="1431633311194488622"/>
<edge source="3632941331539070968" target="1431633311194488622"/>
<edge source="16803457256664781582" target="1431633311194488622"/>
<edge source="18317541865909512163" target="1431633311194488622"/>
<edge source="18185187817423073352" target="1431633311194488622"/>
<edge source="4523751644674550881" target="1431633311194488622"/>
<edge source="16804172820179337006" target="1431633311194488622"/>
<edge source="1939691424133781861" target="1431633311194488622"/>
<edge source="6018075345772831348" target="1431633311194488622"/>
<edge source="17854740042612276767" target="1431633311194488622"/>
<edge source="1656421726766747841" target="1431633311194488622"/>
<edge source="9732428124783939302" target="1431633311194488622"/>
<edge source="7651463217590835983" target="1431633311194488622"/>
<edge source="7594913044183235646" target="1431633311194488622"/>
<edge source="11598438983746935209" target="1431633311194488622"/>
<edge source="8359089573172587095" target="1431633311194488622"/>
<edge source="531500407384902218" target="1431633311194488622"/>
<edge source="15397526244877086732" target="1431633311194488622"/>
<edge source="18161333932150601045" target="1431633311194488622"/>
<edge source="12908963143152569355" target="1431633311194488622"/>
<edge source="4499233656145369438" target="1431633311194488622"/>
<edge source="8291535141920945881" target="1431633311194488622"/>
<edge source="16222652166929144204" target="1431633311194488622"/>
<edge source="11491875380349289404" target="1431633311194488622"/>
<edge source="3720958980403020816" target="1431633311194488622"/>
<edge source="9486832350006291839" target="1431633311194488622"/>
<edge source="1051374793971189386" target="1431633311194488622"/>
<edge source="3658250324739476352" target="1431633311194488622"/>
<edge source="16313647976714156830" target="1431633311194488622"/>
<edge source="2604085176257305633" target="1431633311194488622"/>
<edge source="17770512424346032016" target="1431633311194488622"/>
<edge source="11601132339555571470" target="1431633311194488622"/>
<edge source="1211835406473191863" target="1431633311194488622"/>
<edge source="4347083033764848435" target="1431633311194488622"/>
<edge source="9538084449875791919" target="6382612685700818764"/>
<edge source="9538084449875791919" target="5999650257677576183"/>
<edge source="11408603882397693774" target="9538084449875791919"/>
<edge source="11408603882397693774" target="13346469121565650680"/>
<edge source="11408603882397693774" target="5999650257677576183"/>
<edge source="12500443856801763727" target="9538084449875791919"/>
<edge source="16138254679061222132" target="9538084449875791919"/>
<edge source="16138254679061222132" target="5999650257677576183"/>
<edge source="15505711795824202735" target="9538084449875791919"/>
<edge source="15505711795824202735" target="5999650257677576183"/>
<edge source="11324600873272743514" target="9538084449875791919"/>
<edge source="11324600873272743514" target="6382612685700818764"/>
<edge source="11324600873272743514" target="12644740604491229422"/>
<edge source="11324600873272743514" target="5999650257677576183"/>
<edge source="1564404986299438483" target="9538084449875791919"/>
<edge source="1564404986299438483" target="5999650257677576183"/>
<edge source="4116193485073279318" target="9538084449875791919"/>
<edge source="4116193485073279318" target="12644740604491229422"/>
<edge source="14550673639887731046" target="9538084449875791919"/>
<edge source="5146522853377591327" target="9538084449875791919"/>
<edge source="5146522853377591327" target="11324600873272743514"/>
<edge source="18039062144864581963" target="9538084449875791919"/>
<edge source="5669364687087032958" target="9538084449875791919"/>
<edge source="5669364687087032958" target="12644740604491229422"/>
<edge source="5669364687087032958" target="5999650257677576183"/>
<edge source="9040151999224305592" target="9538084449875791919"/>
<edge source="9040151999224305592" target="12644740604491229422"/>
<edge source="9040151999224305592" target="5999650257677576183"/>
<edge source="3720837586692381467" target="9538084449875791919"/>
<edge source="16050906463468676059" target="9538084449875791919"/>
<edge source="16050906463468676059" target="5999650257677576183"/>
<edge source="18267268624032061291" target="9538084449875791919"/>
<edge source="18267268624032061291" target="11324600873272743514"/>
<edge source="18267268624032061291" target="12644740604491229422"/>
<edge source="18267268624032061291" target="5999650257677576183"/>
<edge source="4993232610053036190" target="9538084449875791919"/>
<edge source="3466946125865762121" target="9538084449875791919"/>
<edge source="3466946125865762121" target="5999650257677576183"/>
<edge source="16305632232773240100" target="9538084449875791919"/>
<edge source="16305632232773240100" target="11324600873272743514"/>
<edge source="16305632232773240100" target="12644740604491229422"/>
<edge source="16305632232773240100" target="5999650257677576183"/>
<edge source="10517065804384853053" target="9538084449875791919"/>
<edge source="10517065804384853053" target="5999650257677576183"/>
<edge source="14858856697824684994" target="9538084449875791919"/>
<edge source="14858856697824684994" target="11324600873272743514"/>
<edge source="14858856697824684994" target="12644740604491229422"/>
<edge source="14858856697824684994" target="5999650257677576183"/>
<edge source="2731814227384441305" target="9538084449875791919"/>
<edge source="10020660180667111529" target="9538084449875791919"/>
<edge source="3426844125187450693" target="9538084449875791919"/>
<edge source="4790520819340140445" target="9538084449875791919"/>
<edge source="4790520819340140445" target="5999650257677576183"/>
<edge source="12328462444653843486" target="9538084449875791919"/>
<edge source="12541147144211358106" target="9538084449875791919"/>
<edge source="11565832587997700911" target="9538084449875791919"/>
<edge source="13904438580236939165" target="9538084449875791919"/>
<edge source="13904438580236939165" target="8448943115025253905"/>
<edge source="15407313785451367942" target="9538084449875791919"/>
<edge source="6294085712262467065" target="9538084449875791919"/>
<edge source="6294085712262467065" target="11324600873272743514"/>
<edge source="6294085712262467065" target="12644740604491229422"/>
<edge source="6294085712262467065" target="5999650257677576183"/>
<edge source="15376920793982085069" target="9538084449875791919"/>
<edge source="3308996211432317749" target="9538084449875791919"/>
<edge source="9190599054932808738" target="9538084449875791919"/>
<edge source="9190599054932808738" target="5999650257677576183"/>
<edge source="6999319186345681083" target="9538084449875791919"/>
<edge source="4793705234619070656" target="9538084449875791919"/>
<edge source="4793705234619070656" target="5999650257677576183"/>
<edge source="11352324708624125387" target="9538084449875791919"/>
<edge source="11352324708624125387" target="11324600873272743514"/>
<edge source="11352324708624125387" target="12644740604491229422"/>
<edge source="760283713428624225" target="9538084449875791919"/>
<edge source="760283713428624225" target="12644740604491229422"/>
<edge source="3077479873636042159" target="9538084449875791919"/>
<edge source="3077479873636042159" target="5999650257677576183"/>
<edge source="15278380461775262117" target="9538084449875791919"/>
<edge source="15278380461775262117" target="5999650257677576183"/>
<edge source="10187246864084975926" target="9538084449875791919"/>
<edge source="10187246864084975926" target="12644740604491229422"/>
<edge source="12308813102675740697" target="9538084449875791919"/>
<edge source="17049037070556303909" target="9538084449875791919"/>
<edge source="17049037070556303909" target="11324600873272743514"/>
<edge source="17049037070556303909" target="5999650257677576183"/>
<edge source="14304438718574157071" target="9538084449875791919"/>
<edge source="11305281026744071790" target="9538084449875791919"/>
<edge source="11305281026744071790" target="11324600873272743514"/>
<edge source="2210620764419829906" target="9538084449875791919"/>
<edge source="2210620764419829906" target="11324600873272743514"/>
<edge source="2210620764419829906" target="12644740604491229422"/>
<edge source="2210620764419829906" target="5999650257677576183"/>
<edge source="1321793621715878501" target="9538084449875791919"/>
<edge source="1321793621715878501" target="5999650257677576183"/>
<edge source="15248432581458880980" target="9538084449875791919"/>
<edge source="15847441662530625185" target="9538084449875791919"/>
<edge source="890350355816522800" target="9538084449875791919"/>
<edge source="890350355816522800" target="5999650257677576183"/>
<edge source="7794016071296671172" target="9538084449875791919"/>
<edge source="7794016071296671172" target="5999650257677576183"/>
<edge source="14672664302977559137" target="9538084449875791919"/>
<edge source="13259842884566962187" target="9538084449875791919"/>
<edge source="12458152692195612427" target="9538084449875791919"/>
<edge source="12458152692195612427" target="11324600873272743514"/>
<edge source="12458152692195612427" target="5999650257677576183"/>
<edge source="18416642050963457459" target="9538084449875791919"/>
<edge source="18416642050963457459" target="11324600873272743514"/>
<edge source="18416642050963457459" target="12644740604491229422"/>
<edge source="18416642050963457459" target="5999650257677576183"/>
<edge source="3028370674147212605" target="8367071398417962813"/>
<edge source="11978445553624214380" target="8367071398417962813"/>
<edge source="949946091219062353" target="8367071398417962813"/>
<edge source="9132276678803270854" target="8367071398417962813"/>
<edge source="6230352926792784452" target="8367071398417962813"/>
<edge source="12916943626500149813" target="8367071398417962813"/>
<edge source="282502688385514417" target="8367071398417962813"/>
<edge source="13126613410294485366" target="8367071398417962813"/>
<edge source="7011964847765802199" target="8367071398417962813"/>
<edge source="8654903568547047842" target="8367071398417962813"/>
<edge source="5278530239378148719" target="8367071398417962813"/>
<edge source="2885916562392086198" target="8367071398417962813"/>
<edge source="283347957351040289" target="8367071398417962813"/>
<edge source="2620378263331322045" target="8367071398417962813"/>
<edge source="8136487588159115675" target="8367071398417962813"/>
<edge source="3137631071934908410" target="8367071398417962813"/>
<edge source="1587380561678705672" target="8367071398417962813"/>
<edge source="18269904853006408245" target="8367071398417962813"/>
<edge source="8884306980263647674" target="8367071398417962813"/>
<edge source="11272776230560973281" target="8367071398417962813"/>
<edge source="1187656217922278561" target="8367071398417962813"/>
<edge source="17222739228813142360" target="8367071398417962813"/>
<edge source="9386562677703754376" target="8367071398417962813"/>
<edge source="18231280624475386436" target="8367071398417962813"/>
<edge source="18294362283448718927" target="8367071398417962813"/>
<edge source="9346886979579970159" target="8367071398417962813"/>
<edge source="1810771332585372148" target="8367071398417962813"/>
<edge source="1147425233131687163" target="8367071398417962813"/>
<edge source="4888428022255023575" target="8367071398417962813"/>
<edge source="18156777187430112022" target="8367071398417962813"/>
<edge source="4470676289942389642" target="8367071398417962813"/>
<edge source="6294936790423161188" target="8367071398417962813"/>
<edge source="10175988650775937306" target="8367071398417962813"/>
<edge source="12727040761225033710" target="8367071398417962813"/>
<edge source="13204595328459365591" target="8367071398417962813"/>
<edge source="17237009222277721047" target="8367071398417962813"/>
<edge source="17855267959511452037" target="8367071398417962813"/>
<edge source="3563115954885327440" target="8367071398417962813"/>
<edge source="7854236750952897970" target="8367071398417962813"/>
<edge source="7711517380639147333" target="8367071398417962813"/>
<edge source="1089623869034577539" target="8367071398417962813"/>
<edge source="8366499969990031131" target="8367071398417962813"/>
<edge source="3451836672949095488" target="8367071398417962813"/>
<edge source="Lq8Umk34rDIJ" target="8367071398417962813"/>
<edge source="17178883444476643109" target="8367071398417962813"/>
<edge source="15088715437227844190" target="8367071398417962813"/>
<edge source="2642922896219719634" target="8367071398417962813"/>
<edge source="4505986135620301670" target="8367071398417962813"/>
<edge source="4657133348792854073" target="8367071398417962813"/>
<edge source="4748746639276173174" target="8367071398417962813"/>
<edge source="1365022424711941058" target="8367071398417962813"/>
<edge source="4381569612155749359" target="8367071398417962813"/>
<edge source="9304804814105874282" target="8367071398417962813"/>
<edge source="745746885882128143" target="8367071398417962813"/>
<edge source="3394653354596589290" target="8367071398417962813"/>
<edge source="2190081206030913404" target="8367071398417962813"/>
<edge source="9343841596799945450" target="8367071398417962813"/>
<edge source="8353648664327356722" target="8367071398417962813"/>
<edge source="4120546132280571779" target="8367071398417962813"/>
<edge source="13843016997180263542" target="8367071398417962813"/>
<edge source="18113388340145752965" target="8367071398417962813"/>
<edge source="10442483506407696721" target="8367071398417962813"/>
<edge source="728770006208310182" target="11324600873272743514"/>
<edge source="728770006208310182" target="13346469121565650680"/>
<edge source="728770006208310182" target="5999650257677576183"/>
<edge source="2381336199697874290" target="11324600873272743514"/>
<edge source="8767327019590446489" target="11324600873272743514"/>
<edge source="5174677530138095658" target="11324600873272743514"/>
<edge source="5174677530138095658" target="12644740604491229422"/>
<edge source="5174677530138095658" target="13346469121565650680"/>
<edge source="10604255508526386376" target="11324600873272743514"/>
<edge source="6196888269192586051" target="11324600873272743514"/>
<edge source="2965311170207987165" target="11324600873272743514"/>
<edge source="11140668418037577536" target="11324600873272743514"/>
<edge source="17675636447045263550" target="11324600873272743514"/>
<edge source="1878991916435892092" target="11324600873272743514"/>
<edge source="1878991916435892092" target="12644740604491229422"/>
<edge source="4988547052049874893" target="11324600873272743514"/>
<edge source="4350254585552726003" target="11324600873272743514"/>
<edge source="4350254585552726003" target="12644740604491229422"/>
<edge source="124174586124414315" target="11324600873272743514"/>
<edge source="6486278392250669413" target="11324600873272743514"/>
<edge source="11137887749868459671" target="11324600873272743514"/>
<edge source="2275159817454841223" target="11324600873272743514"/>
<edge source="7436704720048829343" target="11324600873272743514"/>
<edge source="10062072141692223467" target="11324600873272743514"/>
<edge source="14766840776929327982" target="11324600873272743514"/>
<edge source="10951300284096917451" target="11324600873272743514"/>
<edge source="15129909426072124916" target="11324600873272743514"/>
<edge source="12867622177717661888" target="11324600873272743514"/>
<edge source="625028681591269424" target="11324600873272743514"/>
<edge source="9394900835405963044" target="11324600873272743514"/>
<edge source="4089155253019708361" target="11324600873272743514"/>
<edge source="4089155253019708361" target="8448943115025253905"/>
<edge source="17297670040926232009" target="11324600873272743514"/>
<edge source="14843700105251392523" target="11324600873272743514"/>
<edge source="8225197904370189459" target="11324600873272743514"/>
<edge source="4527666642194648577" target="11324600873272743514"/>
<edge source="11128139934147031880" target="11324600873272743514"/>
<edge source="3406496252840731715" target="11324600873272743514"/>
<edge source="13568718297544497544" target="11324600873272743514"/>
<edge source="12711419494084177503" target="11324600873272743514"/>
<edge source="9396440723658947289" target="6382612685700818764"/>
<edge source="9396440723658947289" target="5999650257677576183"/>
<edge source="17659240868782185606" target="9396440723658947289"/>
<edge source="1330128288162812959" target="9396440723658947289"/>
<edge source="12461780382342897766" target="9396440723658947289"/>
<edge source="2894457065609999138" target="9396440723658947289"/>
<edge source="3367852488199289115" target="9396440723658947289"/>
<edge source="10959765252077951834" target="9396440723658947289"/>
<edge source="10959765252077951834" target="17325452893759607893"/>
<edge source="6330853643103469907" target="9396440723658947289"/>
<edge source="11264330757232304805" target="9396440723658947289"/>
<edge source="104652044397290954" target="9396440723658947289"/>
<edge source="8405812116415915225" target="9396440723658947289"/>
<edge source="11817815354857298574" target="9396440723658947289"/>
<edge source="16381610666186844878" target="9396440723658947289"/>
<edge source="3245855105634480603" target="9396440723658947289"/>
<edge source="3529628295378086556" target="9396440723658947289"/>
<edge source="7737148347777017891" target="9396440723658947289"/>
<edge source="7585820726866106888" target="9396440723658947289"/>
<edge source="16752346668949596807" target="9396440723658947289"/>
<edge source="9766170352567939917" target="9396440723658947289"/>
<edge source="13409251986329658691" target="9396440723658947289"/>
<edge source="473530282656916811" target="9396440723658947289"/>
<edge source="1190843868981231273" target="9396440723658947289"/>
<edge source="11786070565691042647" target="9396440723658947289"/>
<edge source="9568123547245923264" target="9396440723658947289"/>
<edge source="9275353594998479018" target="9396440723658947289"/>
<edge source="674676834371939814" target="9396440723658947289"/>
<edge source="16147182476848743143" target="9396440723658947289"/>
<edge source="6604331675399620890" target="9396440723658947289"/>
<edge source="17835922521314202374" target="9396440723658947289"/>
<edge source="8491094163353345221" target="9396440723658947289"/>
<edge source="2776188056280590893" target="9396440723658947289"/>
<edge source="12148457517294144158" target="9396440723658947289"/>
<edge source="162944317775498564" target="9396440723658947289"/>
<edge source="7022412739527890738" target="9396440723658947289"/>
<edge source="16729230970251043916" target="9396440723658947289"/>
<edge source="7643365245113467622" target="9396440723658947289"/>
<edge source="11258068837507688644" target="9396440723658947289"/>
<edge source="17820547851845778893" target="9396440723658947289"/>
<edge source="13749289119625037185" target="9396440723658947289"/>
<edge source="7991192741706114520" target="9396440723658947289"/>
<edge source="7647425146852033954" target="9396440723658947289"/>
<edge source="11663125968410326892" target="9396440723658947289"/>
<edge source="166903673990182366" target="9396440723658947289"/>
<edge source="9882802714230086643" target="9396440723658947289"/>
<edge source="15888192171340754642" target="9396440723658947289"/>
<edge source="12874330746450003457" target="9396440723658947289"/>
<edge source="11163389046080764508" target="9396440723658947289"/>
<edge source="11069243657064968619" target="9396440723658947289"/>
<edge source="8285293036267904321" target="9396440723658947289"/>
<edge source="10942600018958755822" target="9396440723658947289"/>
<edge source="14522233362639662098" target="9396440723658947289"/>
<edge source="5432221077116828342" target="9396440723658947289"/>
<edge source="12109569770609306860" target="9396440723658947289"/>
<edge source="9800786600952641254" target="9396440723658947289"/>
<edge source="3851853212057137221" target="9396440723658947289"/>
<edge source="5116550664858304460" target="9396440723658947289"/>
<edge source="6524394779160024104" target="9396440723658947289"/>
<edge source="9586487638855151494" target="9396440723658947289"/>
<edge source="409384955469315628" target="9396440723658947289"/>
<edge source="9288583655021028647" target="9396440723658947289"/>
<edge source="15152142901961614455" target="9396440723658947289"/>
<edge source="17659066342463746102" target="9396440723658947289"/>
<edge source="3079241190308487697" target="9396440723658947289"/>
<edge source="5544945261975848615" target="9396440723658947289"/>
<edge source="8932281142246040268" target="9396440723658947289"/>
<edge source="4804878180815758179" target="9396440723658947289"/>
<edge source="12494571714440080437" target="9396440723658947289"/>
<edge source="12934231027519550777" target="9396440723658947289"/>
<edge source="4286407019373546306" target="9396440723658947289"/>
<edge source="12934860564933811755" target="9396440723658947289"/>
<edge source="6471952400524995967" target="9396440723658947289"/>
<edge source="2936922659962418453" target="9396440723658947289"/>
<edge source="5343150201557663140" target="9396440723658947289"/>
<edge source="288293461476145835" target="9396440723658947289"/>
<edge source="1946486029826048685" target="9396440723658947289"/>
<edge source="13147547687598130899" target="9396440723658947289"/>
<edge source="1188818162646880118" target="9396440723658947289"/>
<edge source="8069973205387436662" target="9396440723658947289"/>
<edge source="7181641912111691081" target="9396440723658947289"/>
<edge source="18387253888735733879" target="9396440723658947289"/>
<edge source="14318958312133554657" target="9396440723658947289"/>
<edge source="6617971465675626792" target="9396440723658947289"/>
<edge source="16908822867886701327" target="9396440723658947289"/>
<edge source="3307690701392927370" target="9396440723658947289"/>
<edge source="11526591086958196796" target="9396440723658947289"/>
<edge source="12934682784061910585" target="9396440723658947289"/>
<edge source="7854799250999961474" target="9396440723658947289"/>
<edge source="5383924793672854664" target="9396440723658947289"/>
<edge source="10139905476651083212" target="9396440723658947289"/>
<edge source="7149880764356096625" target="9396440723658947289"/>
<edge source="17663852253493236753" target="9396440723658947289"/>
<edge source="12644740604491229422" target="6382612685700818764"/>
<edge source="13495361512235022596" target="12644740604491229422"/>
<edge source="13495361512235022596" target="5999650257677576183"/>
<edge source="15866259573388647937" target="12644740604491229422"/>
<edge source="5167657309745527366" target="12644740604491229422"/>
<edge source="4582080155258437560" target="12644740604491229422"/>
<edge source="4582080155258437560" target="5999650257677576183"/>
<edge source="17229720160752682638" target="12644740604491229422"/>
<edge source="11676336857650567467" target="12644740604491229422"/>
<edge source="2733828785339010244" target="12644740604491229422"/>
<edge source="6724748843400977919" target="12644740604491229422"/>
<edge source="16852040746567843557" target="12644740604491229422"/>
<edge source="9897780157833606863" target="12644740604491229422"/>
<edge source="1516895187053369438" target="12644740604491229422"/>
<edge source="5255251082771415776" target="12644740604491229422"/>
<edge source="8435222954649090273" target="12644740604491229422"/>
<edge source="2447420766194254891" target="12644740604491229422"/>
<edge source="12087685277533101192" target="12644740604491229422"/>
<edge source="1340805047015841402" target="12644740604491229422"/>
<edge source="1340805047015841402" target="13346469121565650680"/>
<edge source="8119995839638175849" target="12644740604491229422"/>
<edge source="15843316366415866499" target="12644740604491229422"/>
<edge source="9376665948461610119" target="12644740604491229422"/>
<edge source="8315666909540964380" target="12644740604491229422"/>
<edge source="10407924981028323116" target="12644740604491229422"/>
<edge source="14661055790907170296" target="12644740604491229422"/>
<edge source="11874667315223245954" target="12644740604491229422"/>
<edge source="12353159982052869953" target="12644740604491229422"/>
<edge source="2040091967364618062" target="12644740604491229422"/>
<edge source="10434862692373421904" target="12644740604491229422"/>
<edge source="8448943115025253905" target="6382612685700818764"/>
<edge source="6668235945473015803" target="8448943115025253905"/>
<edge source="12719486318898360519" target="8448943115025253905"/>
<edge source="16431656661146854759" target="8448943115025253905"/>
<edge source="3484260075890953852" target="8448943115025253905"/>
<edge source="12396058489435840533" target="8448943115025253905"/>
<edge source="15469437604545198809" target="8448943115025253905"/>
<edge source="3898607331439114201" target="8448943115025253905"/>
<edge source="11133634648290997125" target="8448943115025253905"/>
<edge source="4695735279498312558" target="8448943115025253905"/>
<edge source="12107148927806619132" target="8448943115025253905"/>
<edge source="10949691838250411133" target="8448943115025253905"/>
<edge source="2183335709889759560" target="8448943115025253905"/>
<edge source="5084328956172259959" target="8448943115025253905"/>
<edge source="8779019959052555626" target="8448943115025253905"/>
<edge source="12960288265914815983" target="8448943115025253905"/>
<edge source="10341392145675802868" target="8448943115025253905"/>
<edge source="9757750069113028831" target="8448943115025253905"/>
<edge source="33927402341244667" target="8448943115025253905"/>
<edge source="14524693012131206782" target="8448943115025253905"/>
<edge source="532394135980020430" target="8448943115025253905"/>
<edge source="17284983713036766691" target="8448943115025253905"/>
<edge source="633087071652505085" target="8448943115025253905"/>
<edge source="633087071652505085" target="535744563039386055"/>
<edge source="14101725080609270309" target="8448943115025253905"/>
<edge source="17624445298553964188" target="8448943115025253905"/>
<edge source="10023428544428348799" target="8448943115025253905"/>
<edge source="3726545131723476858" target="8448943115025253905"/>
<edge source="12927720534552603420" target="8448943115025253905"/>
<edge source="10551610679278163665" target="8448943115025253905"/>
<edge source="8481021928568633762" target="8448943115025253905"/>
<edge source="7983433851687883883" target="8448943115025253905"/>
<edge source="13592875691592926608" target="8448943115025253905"/>
<edge source="9426593234701255657" target="8448943115025253905"/>
<edge source="1924986113269211163" target="8448943115025253905"/>
<edge source="17074925293404018328" target="8448943115025253905"/>
<edge source="5868001138380150922" target="8448943115025253905"/>
<edge source="5575721172969217810" target="8448943115025253905"/>
<edge source="11947642466448713378" target="8448943115025253905"/>
<edge source="6750819842213893633" target="8448943115025253905"/>
<edge source="11904217105191322390" target="8448943115025253905"/>
<edge source="1775977373440976785" target="8448943115025253905"/>
<edge source="1318173233917005478" target="8448943115025253905"/>
<edge source="14529545999567875627" target="8448943115025253905"/>
<edge source="1661264717430846222" target="8448943115025253905"/>
<edge source="15793792687497316784" target="8448943115025253905"/>
<edge source="8981892223562201199" target="8448943115025253905"/>
<edge source="10833379900462253180" target="8448943115025253905"/>
<edge source="1047156362824031830" target="8448943115025253905"/>
<edge source="735436327696041641" target="8448943115025253905"/>
<edge source="9214065931586807664" target="8448943115025253905"/>
<edge source="16212063836582850356" target="8448943115025253905"/>
<edge source="14964804060777605755" target="8448943115025253905"/>
<edge source="10173276721588368719" target="8448943115025253905"/>
<edge source="16722403537302150812" target="8448943115025253905"/>
<edge source="8555368968556268636" target="8448943115025253905"/>
<edge source="33793130511872188" target="8448943115025253905"/>
<edge source="15449354411100450148" target="8448943115025253905"/>
<edge source="9049923034415496270" target="8448943115025253905"/>
<edge source="432444698543425795" target="8448943115025253905"/>
<edge source="9021031822777383648" target="8448943115025253905"/>
<edge source="8430936739774837199" target="8448943115025253905"/>
<edge source="5638477121687230939" target="8448943115025253905"/>
<edge source="6099615041197158188" target="8448943115025253905"/>
<edge source="8773982247708585699" target="8448943115025253905"/>
<edge source="6776914221737060740" target="8448943115025253905"/>
<edge source="9515469045960583745" target="8448943115025253905"/>
<edge source="18400465047273061750" target="8448943115025253905"/>
<edge source="16150052612237999167" target="8448943115025253905"/>
<edge source="2476318727654891437" target="8448943115025253905"/>
<edge source="9361339446003315812" target="8448943115025253905"/>
<edge source="2507058243298497028" target="8448943115025253905"/>
<edge source="8173455362624893467" target="8448943115025253905"/>
<edge source="5902003836142023006" target="8448943115025253905"/>
<edge source="4005108257678361504" target="8448943115025253905"/>
<edge source="7085743912913543637" target="8448943115025253905"/>
<edge source="8011144875870384891" target="8448943115025253905"/>
<edge source="2931986225761676831" target="8448943115025253905"/>
<edge source="11156672461051367506" target="8448943115025253905"/>
<edge source="6354529551574600680" target="8448943115025253905"/>
<edge source="12655218227669740570" target="8448943115025253905"/>
<edge source="248902349160897339" target="8448943115025253905"/>
<edge source="535744563039386055" target="6382612685700818764"/>
<edge source="12768464336489196273" target="535744563039386055"/>
<edge source="16299082709654826538" target="535744563039386055"/>
<edge source="18092310435120949940" target="535744563039386055"/>
<edge source="17544397226639680198" target="535744563039386055"/>
<edge source="411578991690355580" target="535744563039386055"/>
<edge source="4407607921916219597" target="535744563039386055"/>
<edge source="5644429611266863213" target="535744563039386055"/>
<edge source="9152627594047007367" target="535744563039386055"/>
<edge source="9152627594047007367" target="17325452893759607893"/>
<edge source="6458276904017990971" target="535744563039386055"/>
<edge source="4245256336048193979" target="535744563039386055"/>
<edge source="1382712243609022780" target="535744563039386055"/>
<edge source="11441760293318611832" target="535744563039386055"/>
<edge source="1292087033558963213" target="535744563039386055"/>
<edge source="10712076956509058252" target="535744563039386055"/>
<edge source="16040050996847565363" target="535744563039386055"/>
<edge source="2307421743935657923" target="535744563039386055"/>
<edge source="678143692114235571" target="535744563039386055"/>
<edge source="2021435255736482892" target="535744563039386055"/>
<edge source="10864892511232457463" target="535744563039386055"/>
<edge source="9579459676985496387" target="535744563039386055"/>
<edge source="8190770042431966238" target="535744563039386055"/>
<edge source="5768724617166869747" target="535744563039386055"/>
<edge source="6162716714810201955" target="535744563039386055"/>
<edge source="13078428699608452477" target="535744563039386055"/>
<edge source="3308207458495141249" target="535744563039386055"/>
<edge source="15849255232371913233" target="535744563039386055"/>
<edge source="2231389869816478973" target="535744563039386055"/>
<edge source="8582043463264170613" target="535744563039386055"/>
<edge source="214390233068253722" target="535744563039386055"/>
<edge source="7437266987569487782" target="535744563039386055"/>
<edge source="11875282308322336079" target="535744563039386055"/>
<edge source="3679566789459312121" target="535744563039386055"/>
<edge source="14353003944710468963" target="535744563039386055"/>
<edge source="14214777377381746715" target="535744563039386055"/>
<edge source="15256074903033943732" target="535744563039386055"/>
<edge source="1785343722922140905" target="535744563039386055"/>
<edge source="7248162955817269145" target="535744563039386055"/>
<edge source="12121551931273619933" target="535744563039386055"/>
<edge source="10731212938719398416" target="535744563039386055"/>
<edge source="5272663799261882882" target="535744563039386055"/>
<edge source="14778056948504220863" target="535744563039386055"/>
<edge source="16527862481109793893" target="535744563039386055"/>
<edge source="11326401675293682123" target="535744563039386055"/>
<edge source="5370677025711722393" target="535744563039386055"/>
<edge source="4398874471937260991" target="535744563039386055"/>
<edge source="6238566466640748238" target="535744563039386055"/>
<edge source="10024501141005307911" target="535744563039386055"/>
<edge source="3567693488984063272" target="535744563039386055"/>
<edge source="15317793119344835263" target="535744563039386055"/>
<edge source="17363381983025647699" target="535744563039386055"/>
<edge source="9631069152835975454" target="535744563039386055"/>
<edge source="8772844806290312927" target="535744563039386055"/>
<edge source="16849488672761264795" target="535744563039386055"/>
<edge source="13607047470867432902" target="535744563039386055"/>
<edge source="10611118271257578362" target="535744563039386055"/>
<edge source="17712406442879515542" target="535744563039386055"/>
<edge source="14403707945550137996" target="535744563039386055"/>
<edge source="17993515150229039530" target="535744563039386055"/>
<edge source="15191081335956951903" target="535744563039386055"/>
<edge source="3634646340176277230" target="535744563039386055"/>
<edge source="11311640397140273257" target="535744563039386055"/>
<edge source="2993359408594724286" target="535744563039386055"/>
<edge source="15918548619868169194" target="535744563039386055"/>
<edge source="490049363149100661" target="535744563039386055"/>
<edge source="13496204946614385428" target="535744563039386055"/>
<edge source="1952208765616162210" target="535744563039386055"/>
<edge source="2240462269536330980" target="535744563039386055"/>
<edge source="10332745316504571260" target="535744563039386055"/>
<edge source="4974430546160529512" target="535744563039386055"/>
<edge source="15589620931717671311" target="535744563039386055"/>
<edge source="3454373283041760880" target="535744563039386055"/>
<edge source="16811817108630720533" target="535744563039386055"/>
<edge source="15963090810315431769" target="535744563039386055"/>
<edge source="15253833027531638311" target="535744563039386055"/>
<edge source="17045973692536155067" target="535744563039386055"/>
<edge source="13346469121565650680" target="6382612685700818764"/>
<edge source="8847225001696225074" target="13346469121565650680"/>
<edge source="17325452893759607893" target="13346469121565650680"/>
<edge source="17325452893759607893" target="6382612685700818764"/>
<edge source="17325452893759607893" target="5999650257677576183"/>
<edge source="9531339919500160456" target="13346469121565650680"/>
<edge source="7658252996672799137" target="13346469121565650680"/>
<edge source="2617964736190327137" target="13346469121565650680"/>
<edge source="11317456985294514291" target="13346469121565650680"/>
<edge source="1629697025352584075" target="13346469121565650680"/>
<edge source="3387494033233397434" target="13346469121565650680"/>
<edge source="9453884244192340827" target="13346469121565650680"/>
<edge source="9453884244192340827" target="5999650257677576183"/>
<edge source="9105258566287239045" target="13346469121565650680"/>
<edge source="14930542330832755729" target="13346469121565650680"/>
<edge source="14554488295084172370" target="13346469121565650680"/>
<edge source="212018708743960174" target="13346469121565650680"/>
<edge source="17642928594905005472" target="13346469121565650680"/>
<edge source="17642928594905005472" target="5999650257677576183"/>
<edge source="17449731775022216088" target="13346469121565650680"/>
<edge source="14081415941935534928" target="13346469121565650680"/>
<edge source="12435210238474843309" target="13346469121565650680"/>
<edge source="13748943285480844265" target="13346469121565650680"/>
<edge source="10547178639330987288" target="13346469121565650680"/>
<edge source="16888371616033308169" target="13346469121565650680"/>
<edge source="18079286179474516945" target="13346469121565650680"/>
<edge source="16601259166110264431" target="13346469121565650680"/>
<edge source="7566971192202831904" target="13346469121565650680"/>
<edge source="2597379987318853103" target="13346469121565650680"/>
<edge source="15507377305223603161" target="13346469121565650680"/>
<edge source="11504531406673088307" target="13346469121565650680"/>
<edge source="16324732045488656995" target="13346469121565650680"/>
<edge source="12342618065285300259" target="13346469121565650680"/>
<edge source="12928939764513613154" target="13346469121565650680"/>
<edge source="5360881814442745026" target="13346469121565650680"/>
<edge source="4077468826256393449" target="13346469121565650680"/>
<edge source="3796249632159372698" target="13346469121565650680"/>
<edge source="6905939594294613143" target="13346469121565650680"/>
<edge source="2142612586528747387" target="13346469121565650680"/>
<edge source="9106423587240821276" target="13346469121565650680"/>
<edge source="2983606601337837743" target="13346469121565650680"/>
<edge source="16360107983645858167" target="13346469121565650680"/>
<edge source="4185948125671434541" target="13346469121565650680"/>
<edge source="15373527129249864261" target="13346469121565650680"/>
<edge source="3101580748817568884" target="13346469121565650680"/>
<edge source="6067204860221490122" target="13346469121565650680"/>
<edge source="10469230642894389844" target="13346469121565650680"/>
<edge source="11773732314594455999" target="13346469121565650680"/>
<edge source="4370422915042112938" target="13346469121565650680"/>
<edge source="9379156628635523145" target="13346469121565650680"/>
<edge source="2488030068881724873" target="13346469121565650680"/>
<edge source="5282584135767767517" target="13346469121565650680"/>
<edge source="10632950128195096357" target="13346469121565650680"/>
<edge source="708621415733870473" target="13346469121565650680"/>
<edge source="9316980701966772208" target="13346469121565650680"/>
<edge source="7644063362073502300" target="13346469121565650680"/>
<edge source="14163003192753029307" target="13346469121565650680"/>
<edge source="18400177295485631651" target="13346469121565650680"/>
<edge source="1644345333320747257" target="13346469121565650680"/>
<edge source="3224984904288274964" target="13346469121565650680"/>
<edge source="10691651773549756733" target="13346469121565650680"/>
<edge source="16945998805068988708" target="17325452893759607893"/>
<edge source="2507582518469208253" target="17325452893759607893"/>
<edge source="15761127917409164495" target="17325452893759607893"/>
<edge source="11127330701624169778" target="17325452893759607893"/>
<edge source="5823096581545580132" target="17325452893759607893"/>
<edge source="14750169433894795213" target="17325452893759607893"/>
<edge source="14799418040721278868" target="17325452893759607893"/>
<edge source="11207367590828257735" target="17325452893759607893"/>
<edge source="16563837428337733383" target="17325452893759607893"/>
<edge source="3429732511943586845" target="17325452893759607893"/>
<edge source="15938347331059363535" target="17325452893759607893"/>
<edge source="11324300263159808676" target="17325452893759607893"/>
<edge source="2878363142274078846" target="17325452893759607893"/>
<edge source="10459154347245074313" target="17325452893759607893"/>
<edge source="6067064466586264123" target="17325452893759607893"/>
<edge source="8114522470454201714" target="17325452893759607893"/>
<edge source="1131610574501943405" target="17325452893759607893"/>
<edge source="7068343160162404376" target="17325452893759607893"/>
<edge source="17384033669757226875" target="17325452893759607893"/>
<edge source="12604090720681450553" target="17325452893759607893"/>
<edge source="8617012105369696231" target="17325452893759607893"/>
<edge source="10783855747646875569" target="17325452893759607893"/>
<edge source="15129937235725122381" target="17325452893759607893"/>
<edge source="9136495772785862243" target="17325452893759607893"/>
<edge source="3820824800719710132" target="17325452893759607893"/>
<edge source="543244060835176193" target="17325452893759607893"/>
<edge source="1056529344033078034" target="17325452893759607893"/>
<edge source="3290950822311574111" target="17325452893759607893"/>
<edge source="8485046656923809530" target="17325452893759607893"/>
<edge source="8810290142290002002" target="17325452893759607893"/>
<edge source="5369236400026867300" target="17325452893759607893"/>
<edge source="2908516376509801521" target="17325452893759607893"/>
<edge source="7820015134418027243" target="17325452893759607893"/>
<edge source="10599118327233610110" target="17325452893759607893"/>
<edge source="8382044937846336173" target="17325452893759607893"/>
<edge source="15720662195716475484" target="17325452893759607893"/>
<edge source="296015877195835414" target="17325452893759607893"/>
<edge source="11140667219432227939" target="17325452893759607893"/>
<edge source="15889454219146822872" target="17325452893759607893"/>
<edge source="17919992204175632420" target="17325452893759607893"/>
<edge source="9829630380879135158" target="17325452893759607893"/>
<edge source="4490548502357617762" target="17325452893759607893"/>
<edge source="16208419679375763607" target="17325452893759607893"/>
<edge source="2749681245987987360" target="17325452893759607893"/>
<edge source="15770340026262104764" target="17325452893759607893"/>
<edge source="13308776082845550262" target="17325452893759607893"/>
<edge source="17895695761643428844" target="17325452893759607893"/>
<edge source="10389594312915615958" target="17325452893759607893"/>
<edge source="18005903390163138360" target="17325452893759607893"/>
<edge source="15641048523483845755" target="17325452893759607893"/>
<edge source="3296916295930333686" target="17325452893759607893"/>
<edge source="6053988149774622220" target="17325452893759607893"/>
<edge source="11971206372978719797" target="17325452893759607893"/>
<edge source="7650029386959535850" target="17325452893759607893"/>
<edge source="18195869548361214224" target="17325452893759607893"/>
<edge source="17109511764161480588" target="17325452893759607893"/>
<edge source="4773394058091047733" target="17325452893759607893"/>
<edge source="7472077363474028067" target="17325452893759607893"/>
<edge source="13254274571229991785" target="17325452893759607893"/>
<edge source="9602194952104030116" target="17325452893759607893"/>
<edge source="412550918084992142" target="17325452893759607893"/>
<edge source="5579084922455744576" target="17325452893759607893"/>
<edge source="9463717142610823747" target="17325452893759607893"/>
<edge source="521481329840897304" target="17325452893759607893"/>
<edge source="12532017250168790887" target="17325452893759607893"/>
<edge source="13081138058927339532" target="17325452893759607893"/>
<edge source="18151073209899826471" target="17325452893759607893"/>
<edge source="621059241978449016" target="17325452893759607893"/>
<edge source="1897738753201623147" target="17325452893759607893"/>
<edge source="7436181184954982806" target="17325452893759607893"/>
<edge source="13539442582208875227" target="17325452893759607893"/>
<edge source="11794736720683628583" target="17325452893759607893"/>
<edge source="15738759785276050535" target="17325452893759607893"/>
<edge source="7649876297714062863" target="17325452893759607893"/>
<edge source="14732420770874304307" target="17325452893759607893"/>
<edge source="15295068953900215294" target="14311400318178337111"/>
<edge source="15549291371117213871" target="14311400318178337111"/>
<edge source="9884544045603971658" target="14311400318178337111"/>
<edge source="10628215061802232868" target="14311400318178337111"/>
<edge source="9532821550175512200" target="14311400318178337111"/>
<edge source="12985976504909847163" target="14311400318178337111"/>
<edge source="2812153438552156646" target="14311400318178337111"/>
<edge source="14349697475909471320" target="14311400318178337111"/>
<edge source="18351357225093508985" target="14311400318178337111"/>
<edge source="11021564807628327569" target="14311400318178337111"/>
<edge source="4655434626952320958" target="14311400318178337111"/>
<edge source="16079747206913991174" target="14311400318178337111"/>
<edge source="16432688928939217038" target="14311400318178337111"/>
<edge source="6456248456066525600" target="14311400318178337111"/>
<edge source="8123745338600640152" target="14311400318178337111"/>
<edge source="10334732198267289555" target="14311400318178337111"/>
<edge source="5740479134802475221" target="14311400318178337111"/>
<edge source="6447041552955227244" target="14311400318178337111"/>
<edge source="11016115370073884103" target="14311400318178337111"/>
<edge source="5588318297466755355" target="14311400318178337111"/>
<edge source="7060227414328690906" target="14311400318178337111"/>
<edge source="13890326168676881936" target="14311400318178337111"/>
<edge source="585375656869561806" target="14311400318178337111"/>
<edge source="10256791187069118291" target="14311400318178337111"/>
<edge source="6955302216815830645" target="14311400318178337111"/>
<edge source="9835514645329759223" target="14311400318178337111"/>
<edge source="12985566362823859744" target="14311400318178337111"/>
<edge source="7984800842432280263" target="14311400318178337111"/>
<edge source="17521920740408245266" target="14311400318178337111"/>
<edge source="16420701532688217268" target="14311400318178337111"/>
<edge source="8989581343974102697" target="14311400318178337111"/>
<edge source="2488190159038118357" target="14311400318178337111"/>
<edge source="818429456345308176" target="14311400318178337111"/>
<edge source="18347537605283752941" target="14311400318178337111"/>
<edge source="7447750230007511033" target="14311400318178337111"/>
<edge source="17445702357043232709" target="14311400318178337111"/>
<edge source="10376554157268421149" target="14311400318178337111"/>
<edge source="9972278693063933261" target="14311400318178337111"/>
<edge source="5102122811255396025" target="14311400318178337111"/>
<edge source="2840095996932934652" target="14311400318178337111"/>
<edge source="1768748882307170392" target="14311400318178337111"/>
<edge source="17587187390595037360" target="14311400318178337111"/>
<edge source="188799720050288373" target="14311400318178337111"/>
<edge source="12641103106759054130" target="14311400318178337111"/>
<edge source="10335635097204372178" target="14311400318178337111"/>
<edge source="TwB7dLsYY-MJ" target="14311400318178337111"/>
<edge source="8653517720287563782" target="14311400318178337111"/>
<edge source="5136921629305657035" target="14311400318178337111"/>
<edge source="11099153551339147297" target="14311400318178337111"/>
<edge source="5228079008431707389" target="14311400318178337111"/>
<edge source="13762120871365653169" target="14311400318178337111"/>
<edge source="11403420526814911334" target="14311400318178337111"/>
<edge source="3928869340439710212" target="14311400318178337111"/>
<edge source="6432339791137664150" target="14311400318178337111"/>
<edge source="17407217274075551544" target="14311400318178337111"/>
<edge source="5708297216613512002" target="14311400318178337111"/>
<edge source="1676138104604729822" target="14311400318178337111"/>
<edge source="10296222572490103697" target="14311400318178337111"/>
<edge source="12621755854148347362" target="14311400318178337111"/>
<edge source="11159799407205847879" target="14311400318178337111"/>
<edge source="3650044505606796741" target="14311400318178337111"/>
<edge source="18214772028547313138" target="14311400318178337111"/>
<edge source="12157739162413628567" target="14311400318178337111"/>
<edge source="8535109044975227347" target="14311400318178337111"/>
<edge source="11497367426208390181" target="14311400318178337111"/>
<edge source="4643207554767984878" target="14311400318178337111"/>
<edge source="18334436816345410388" target="14311400318178337111"/>
<edge source="4144441762167508574" target="14311400318178337111"/>
<edge source="13381915677677103405" target="14311400318178337111"/>
<edge source="6827564526345704344" target="14311400318178337111"/>
<edge source="4896734917901521680" target="14311400318178337111"/>
<edge source="16683345231471569427" target="14311400318178337111"/>
<edge source="18036771643867343218" target="14311400318178337111"/>
<edge source="7625820698997778911" target="14311400318178337111"/>
<edge source="2858579394623725590" target="14311400318178337111"/>
<edge source="366512561177278070" target="14311400318178337111"/>
<edge source="5999650257677576183" target="6382612685700818764"/>
<edge source="1026291601187748397" target="5999650257677576183"/>
<edge source="16683125965624867998" target="5999650257677576183"/>
<edge source="14703570951981014637" target="5999650257677576183"/>
<edge source="3594606171683130501" target="5999650257677576183"/>
<edge source="4497917248250067674" target="5999650257677576183"/>
<edge source="7515568945301804297" target="5999650257677576183"/>
<edge source="17367410418412289203" target="5999650257677576183"/>
<edge source="13559955005752027345" target="5999650257677576183"/>
<edge source="1086168042066320095" target="5999650257677576183"/>
<edge source="15368170985283079245" target="5999650257677576183"/>
<edge source="989963634595144692" target="5999650257677576183"/>
<edge source="6281152651769068818" target="5999650257677576183"/>
<edge source="8945175718646989267" target="5999650257677576183"/>
<edge source="18246305387780116611" target="5999650257677576183"/>
<edge source="18336064849612757553" target="5999650257677576183"/>
<edge source="17369531722515506311" target="5999650257677576183"/>
<edge source="6731983785945015219" target="5999650257677576183"/>
<edge source="13359963855282714219" target="5999650257677576183"/>
<edge source="1545024244511440922" target="5999650257677576183"/>
<edge source="5426394606047885053" target="5999650257677576183"/>
<edge source="10747326374097383015" target="5999650257677576183"/>
<edge source="6145323722830593859" target="5999650257677576183"/>
<edge source="14479605661908902222" target="5999650257677576183"/>
<edge source="11348760748613165934" target="5999650257677576183"/>
<edge source="12180218794931662709" target="5999650257677576183"/>
<edge source="2597834166337025404" target="5999650257677576183"/>
<edge source="16128994104714577384" target="11452477192554017570"/>
<edge source="7941234669662371746" target="11452477192554017570"/>
<edge source="13076252047124330234" target="11452477192554017570"/>
<edge source="12525771403000822454" target="11452477192554017570"/>
<edge source="10942366611650661886" target="11452477192554017570"/>
<edge source="12060121231554906872" target="11452477192554017570"/>
<edge source="13417650621921820369" target="11452477192554017570"/>
<edge source="6326589465262223216" target="11452477192554017570"/>
<edge source="1678541804692472087" target="11452477192554017570"/>
<edge source="11360965149573855814" target="11452477192554017570"/>
<edge source="10564325131634348410" target="11452477192554017570"/>
<edge source="9095926209623917067" target="11452477192554017570"/>
<edge source="17382263080131354066" target="11452477192554017570"/>
<edge source="8819593779078451823" target="11452477192554017570"/>
<edge source="F4Zoh0oPXE0J" target="11452477192554017570"/>
<edge source="zMD8xoVPVA0J" target="11452477192554017570"/>
<edge source="13193353915930591338" target="11452477192554017570"/>
<edge source="11287002185478389153" target="11452477192554017570"/>
<edge source="1777455714178307276" target="11452477192554017570"/>
<edge source="3143529597089035400" target="11452477192554017570"/>
<edge source="9132989520533932624" target="11452477192554017570"/>
<edge source="sLUFVBct83MJ" target="11452477192554017570"/>
<edge source="5862922647569477938" target="11452477192554017570"/>
<edge source="6644117076261818901" target="11452477192554017570"/>
<edge source="1224216148116518096" target="11452477192554017570"/>
<edge source="18029852880512974017" target="11452477192554017570"/>
<edge source="4088751988372467068" target="11452477192554017570"/>
<edge source="16273778234448951456" target="11452477192554017570"/>
<edge source="17592923797249586375" target="11452477192554017570"/>
<edge source="6142589876195895306" target="11452477192554017570"/>
<edge source="3960507448835907852" target="11452477192554017570"/>
<edge source="G95UAM_jS08J" target="11452477192554017570"/>
<edge source="16273230188622356845" target="11452477192554017570"/>
<edge source="7240823796445100518" target="11452477192554017570"/>
<edge source="3157779878157184062" target="11452477192554017570"/>
<edge source="11623041935010349796" target="11452477192554017570"/>
<edge source="14202645124111525490" target="11452477192554017570"/>
<edge source="a96v83mzAUoJ" target="11452477192554017570"/>
<edge source="7355981587734255595" target="11452477192554017570"/>
<edge source="2741005343281217553" target="11452477192554017570"/>
<edge source="6909744142839740328" target="11452477192554017570"/>
<edge source="11759926192076555716" target="11452477192554017570"/>
<edge source="OaSros3oKKAJ" target="11452477192554017570"/>
<edge source="811326051739083462" target="11452477192554017570"/>
<edge source="14242124119478297788" target="11452477192554017570"/>
<edge source="4554030306715313521" target="11452477192554017570"/>
<edge source="4168455509224505377" target="11452477192554017570"/>
<edge source="13803166602830577984" target="11452477192554017570"/>
<edge source="764295440409526857" target="11452477192554017570"/>
<edge source="I3r9FbwuMeoJ" target="11452477192554017570"/>
<edge source="3555227715383495415" target="11452477192554017570"/>
<edge source="3070099986647090116" target="11452477192554017570"/>
<edge source="3366278070706666359" target="11452477192554017570"/>
<edge source="7399476248812336314" target="11452477192554017570"/>
<edge source="6622643112892915762" target="11452477192554017570"/>
<edge source="261887705248316913" target="11452477192554017570"/>
<edge source="8435158920283992591" target="11452477192554017570"/>
<edge source="7539048894844258473" target="11452477192554017570"/>
<edge source="17520432466589520461" target="11452477192554017570"/>
<edge source="2202905144227684777" target="11452477192554017570"/>
<edge source="3941086606225563243" target="11452477192554017570"/>
<edge source="SOWfTYHvJj8J" target="11452477192554017570"/>
<edge source="7227275169875605471" target="11452477192554017570"/>
<edge source="6835716193545665416" target="11452477192554017570"/>
<edge source="17019817652517638697" target="11452477192554017570"/>
<edge source="16289864856633093116" target="11452477192554017570"/>
<edge source="4733234527636485458" target="11452477192554017570"/>
<edge source="9801496609815990936" target="11452477192554017570"/>
<edge source="n4rt-tP6doUJ" target="11452477192554017570"/>
</graph></graphml>