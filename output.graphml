<?xml version='1.0' encoding='utf-8'?>
<graphml xmlns="http://graphml.graphdrawing.org/xmlns" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://graphml.graphdrawing.org/xmlns http://graphml.graphdrawing.org/xmlns/1.0/graphml.xsd"><key id="d8" for="node" attr.name="modularity" attr.type="long"/>
<key id="d7" for="node" attr.name="cited_by_url" attr.type="string"/>
<key id="d6" for="node" attr.name="cited_by" attr.type="long"/>
<key id="d5" for="node" attr.name="year" attr.type="string"/>
<key id="d4" for="node" attr.name="authors" attr.type="string"/>
<key id="d3" for="node" attr.name="title" attr.type="string"/>
<key id="d2" for="node" attr.name="url" attr.type="string"/>
<key id="d1" for="node" attr.name="id" attr.type="string"/>
<key id="d0" for="node" attr.name="label" attr.type="string"/>
<graph edgedefault="directed"><node id="7522504961268153944">
  <data key="d0">Transformers in vision: A survey</data>
  <data key="d1">7522504961268153944</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3505244</data>
  <data key="d3">Transformers in vision: A survey</data>
  <data key="d4">S Khan, M Naseer, M Hayat, SW Zamir…</data>
  <data key="d5">2022</data>
  <data key="d6">1298</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7522504961268153944&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="6382612685700818764">
  <data key="d0">You only look once: Unified, real-time object detection</data>
  <data key="d1">6382612685700818764</data>
  <data key="d3">You only look once: Unified, real-time object detection</data>
  <data key="d6">39258</data>
  <data key="d8">1</data>
</node>
<node id="15456065911372617945">
  <data key="d0">Attention mechanisms in computer vision: A survey</data>
  <data key="d1">15456065911372617945</data>
  <data key="d2">https://link.springer.com/article/10.1007/s41095-022-0271-y</data>
  <data key="d3">Attention mechanisms in computer vision: A survey</data>
  <data key="d4">MH Guo, TX Xu, JJ Liu, ZN Liu, PT Jiang, TJ Mu…</data>
  <data key="d5">2022</data>
  <data key="d6">620</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15456065911372617945&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="982391967541643955">
  <data key="d0">Transformers in medical imaging: A survey</data>
  <data key="d1">982391967541643955</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841523000634</data>
  <data key="d3">Transformers in medical imaging: A survey</data>
  <data key="d4">F Shamshad, S Khan, SW Zamir, MH Khan…</data>
  <data key="d5">2023</data>
  <data key="d6">183</data>
  <data key="d7">https://scholar.google.com/scholar?cites=982391967541643955&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">20</data>
</node>
<node id="2452866517197292093">
  <data key="d0">A comprehensive survey on pretrained foundation models: A history from bert to chatgpt</data>
  <data key="d1">2452866517197292093</data>
  <data key="d2">https://arxiv.org/abs/2302.09419</data>
  <data key="d3">A comprehensive survey on pretrained foundation models: A history from bert to chatgpt</data>
  <data key="d4">C Zhou, Q Li, C Li, J Yu, Y Liu, G Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">112</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2452866517197292093&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4773463079530656035">
  <data key="d0">Visual attention network</data>
  <data key="d1">4773463079530656035</data>
  <data key="d2">https://link.springer.com/article/10.1007/s41095-023-0364-2</data>
  <data key="d3">Visual attention network</data>
  <data key="d4">MH Guo, CZ Lu, ZN Liu, MM Cheng, SM Hu</data>
  <data key="d5">2023</data>
  <data key="d6">244</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4773463079530656035&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="761718241536208511">
  <data key="d0">Segnext: Rethinking convolutional attention design for semantic segmentation</data>
  <data key="d1">761718241536208511</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/08050f40fff41616ccfc3080e60a301a-Abstract-Conference.html</data>
  <data key="d3">Segnext: Rethinking convolutional attention design for semantic segmentation</data>
  <data key="d4">MH Guo, CZ Lu, Q Hou, Z Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">116</data>
  <data key="d7">https://scholar.google.com/scholar?cites=761718241536208511&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="7104781172538541114">
  <data key="d0">YOLOv5-Tassel: Detecting tassels in RGB UAV imagery with improved YOLOv5 based on transfer learning</data>
  <data key="d1">7104781172538541114</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9889182/</data>
  <data key="d3">YOLOv5-Tassel: Detecting tassels in RGB UAV imagery with improved YOLOv5 based on transfer learning</data>
  <data key="d4">W Liu, K Quijano, MM Crawford</data>
  <data key="d5">2022</data>
  <data key="d6">119</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7104781172538541114&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="6491078858607146383">
  <data key="d0">Towards an end-to-end framework for flow-guided video inpainting</data>
  <data key="d1">6491078858607146383</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Li_Towards_an_End-to-End_Framework_for_Flow-Guided_Video_Inpainting_CVPR_2022_paper.html</data>
  <data key="d3">Towards an end-to-end framework for flow-guided video inpainting</data>
  <data key="d4">Z Li, CZ Lu, J Qin, CL Guo…</data>
  <data key="d5">2022</data>
  <data key="d6">43</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6491078858607146383&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11978445553624214380">
  <data key="d0">ISNet: Towards improving separability for remote sensing image change detection</data>
  <data key="d1">11978445553624214380</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9772654/</data>
  <data key="d3">ISNet: Towards improving separability for remote sensing image change detection</data>
  <data key="d4">G Cheng, G Wang, J Han</data>
  <data key="d5">2022</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11978445553624214380&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="5228146784334715443">
  <data key="d0">Diagnosis of brain diseases in fusion of neuroimaging modalities using deep learning: A review</data>
  <data key="d1">5228146784334715443</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1566253522002573</data>
  <data key="d3">Diagnosis of brain diseases in fusion of neuroimaging modalities using deep learning: A review</data>
  <data key="d4">A Shoeibi, M Khodatars, M Jafari, N Ghassemi…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5228146784334715443&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="9099615620722636165">
  <data key="d0">Swin-transformer-enabled YOLOv5 with attention mechanism for small object detection on satellite images</data>
  <data key="d1">9099615620722636165</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/12/2861</data>
  <data key="d3">Swin-transformer-enabled YOLOv5 with attention mechanism for small object detection on satellite images</data>
  <data key="d4">H Gong, T Mu, Q Li, H Dai, C Li, Z He, W Wang, F Han…</data>
  <data key="d5">2022</data>
  <data key="d6">52</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9099615620722636165&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="10884589459641707712">
  <data key="d0">Beyond self-attention: External attention using two linear layers for visual tasks</data>
  <data key="d1">10884589459641707712</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9912362/</data>
  <data key="d3">Beyond self-attention: External attention using two linear layers for visual tasks</data>
  <data key="d4">MH Guo, ZN Liu, TJ Mu, SM Hu</data>
  <data key="d5">2022</data>
  <data key="d6">272</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10884589459641707712&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="7749897961068121501">
  <data key="d0">A survey of transformers</data>
  <data key="d1">7749897961068121501</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2666651022000146</data>
  <data key="d3">A survey of transformers</data>
  <data key="d4">T Lin, Y Wang, X Liu, X Qiu</data>
  <data key="d5">2022</data>
  <data key="d6">486</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7749897961068121501&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="1539076789580815483">
  <data key="d0">Pre-trained models for natural language processing: A survey</data>
  <data key="d1">1539076789580815483</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11431-020-1647-3</data>
  <data key="d3">Pre-trained models for natural language processing: A survey</data>
  <data key="d4">X Qiu, T Sun, Y Xu, Y Shao, N Dai, X Huang</data>
  <data key="d5">2020</data>
  <data key="d6">1184</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1539076789580815483&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="966567457136989804">
  <data key="d0">Pre-trained models: Past, present and future</data>
  <data key="d1">966567457136989804</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2666651021000231</data>
  <data key="d3">Pre-trained models: Past, present and future</data>
  <data key="d4">X Han, Z Zhang, N Ding, Y Gu, X Liu, Y Huo, J Qiu…</data>
  <data key="d5">2021</data>
  <data key="d6">366</data>
  <data key="d7">https://scholar.google.com/scholar?cites=966567457136989804&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="10761248177036470713">
  <data key="d0">Multimodal learning with transformers: A survey</data>
  <data key="d1">10761248177036470713</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10123038/</data>
  <data key="d3">Multimodal learning with transformers: A survey</data>
  <data key="d4">P Xu, X Zhu, DA Clifton</data>
  <data key="d5">2023</data>
  <data key="d6">110</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10761248177036470713&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="14136709172791920331">
  <data key="d0">A survey of visual transformers</data>
  <data key="d1">14136709172791920331</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10088164/</data>
  <data key="d3">A survey of visual transformers</data>
  <data key="d4">Y Liu, Y Zhang, Y Wang, F Hou, J Yuan…</data>
  <data key="d5">2023</data>
  <data key="d6">108</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14136709172791920331&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="13597902966753793310">
  <data key="d0">Recent advances in deep learning based dialogue systems: A systematic survey</data>
  <data key="d1">13597902966753793310</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10462-022-10248-8</data>
  <data key="d3">Recent advances in deep learning based dialogue systems: A systematic survey</data>
  <data key="d4">J Ni, T Young, V Pandelea, F Xue…</data>
  <data key="d5">2023</data>
  <data key="d6">122</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13597902966753793310&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="4431578198915484435">
  <data key="d0">Ammus: A survey of transformer-based pretrained models in natural language processing</data>
  <data key="d1">4431578198915484435</data>
  <data key="d2">https://arxiv.org/abs/2108.05542</data>
  <data key="d3">Ammus: A survey of transformer-based pretrained models in natural language processing</data>
  <data key="d4">KS Kalyan, A Rajasekharan, S Sangeetha</data>
  <data key="d5">2021</data>
  <data key="d6">145</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4431578198915484435&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="2600515932282922845">
  <data key="d0">ChatGPT: Jack of all trades, master of none</data>
  <data key="d1">2600515932282922845</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S156625352300177X</data>
  <data key="d3">ChatGPT: Jack of all trades, master of none</data>
  <data key="d4">J Kocoń, I Cichecki, O Kaszyca, M Kochanek, D Szydło…</data>
  <data key="d5">2023</data>
  <data key="d6">66</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2600515932282922845&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="8053588590478703627">
  <data key="d0">Physformer: Facial video-based physiological measurement with temporal difference transformer</data>
  <data key="d1">8053588590478703627</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Yu_PhysFormer_Facial_Video-Based_Physiological_Measurement_With_Temporal_Difference_Transformer_CVPR_2022_paper.html</data>
  <data key="d3">Physformer: Facial video-based physiological measurement with temporal difference transformer</data>
  <data key="d4">Z Yu, Y Shen, J Shi, H Zhao…</data>
  <data key="d5">2022</data>
  <data key="d6">54</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8053588590478703627&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="15393921212791157727">
  <data key="d0">ChatGPT is not all you need. A State of the Art Review of large Generative AI models</data>
  <data key="d1">15393921212791157727</data>
  <data key="d2">https://arxiv.org/abs/2301.04655</data>
  <data key="d3">ChatGPT is not all you need. A State of the Art Review of large Generative AI models</data>
  <data key="d4">R Gozalo-Brizuela, EC Garrido-Merchan</data>
  <data key="d5">2023</data>
  <data key="d6">83</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15393921212791157727&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="16431204865977056518">
  <data key="d0">Restormer: Efficient transformer for high-resolution image restoration</data>
  <data key="d1">16431204865977056518</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zamir_Restormer_Efficient_Transformer_for_High-Resolution_Image_Restoration_CVPR_2022_paper.html</data>
  <data key="d3">Restormer: Efficient transformer for high-resolution image restoration</data>
  <data key="d4">SW Zamir, A Arora, S Khan, M Hayat…</data>
  <data key="d5">2022</data>
  <data key="d6">700</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16431204865977056518&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="16932998913230259066">
  <data key="d0">NTIRE 2023 challenge on efficient super-resolution: Methods and results</data>
  <data key="d1">16932998913230259066</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Li_NTIRE_2023_Challenge_on_Efficient_Super-Resolution_Methods_and_Results_CVPRW_2023_paper.html</data>
  <data key="d3">NTIRE 2023 challenge on efficient super-resolution: Methods and results</data>
  <data key="d4">Y Li, Y Zhang, R Timofte, L Van Gool…</data>
  <data key="d5">2023</data>
  <data key="d6">73</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16932998913230259066&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="498268664873674535">
  <data key="d0">Simple baselines for image restoration</data>
  <data key="d1">498268664873674535</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20071-7_2</data>
  <data key="d3">Simple baselines for image restoration</data>
  <data key="d4">L Chen, X Chu, X Zhang, J Sun</data>
  <data key="d5">2022</data>
  <data key="d6">242</data>
  <data key="d7">https://scholar.google.com/scholar?cites=498268664873674535&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="9110410135628850117">
  <data key="d0">Lens-to-lens bokeh effect transformation. NTIRE 2023 challenge report</data>
  <data key="d1">9110410135628850117</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Conde_Lens-to-Lens_Bokeh_Effect_Transformation._NTIRE_2023_Challenge_Report_CVPRW_2023_paper.html</data>
  <data key="d3">Lens-to-lens bokeh effect transformation. NTIRE 2023 challenge report</data>
  <data key="d4">MV Conde, M Kolmet, T Seizinger…</data>
  <data key="d5">2023</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9110410135628850117&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="7710701073211724386">
  <data key="d0">NTIRE 2023 video colorization challenge</data>
  <data key="d1">7710701073211724386</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Kang_NTIRE_2023_Video_Colorization_Challenge_CVPRW_2023_paper.html</data>
  <data key="d3">NTIRE 2023 video colorization challenge</data>
  <data key="d4">X Kang, X Lin, K Zhang, Z Hui…</data>
  <data key="d5">2023</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7710701073211724386&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="13272942409666364496">
  <data key="d0">NTIRE 2023 Challenge on 360deg Omnidirectional Image and Video Super-Resolution: Datasets, Methods and Results</data>
  <data key="d1">13272942409666364496</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Cao_NTIRE_2023_Challenge_on_360deg_Omnidirectional_Image_and_Video_Super-Resolution_CVPRW_2023_paper.html</data>
  <data key="d3">NTIRE 2023 Challenge on 360deg Omnidirectional Image and Video Super-Resolution: Datasets, Methods and Results</data>
  <data key="d4">M Cao, C Mou, F Yu, X Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13272942409666364496&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="12297468854729392143">
  <data key="d0">NTIRE 2023 challenge on image denoising: Methods and results</data>
  <data key="d1">12297468854729392143</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Li_NTIRE_2023_Challenge_on_Image_Denoising_Methods_and_Results_CVPRW_2023_paper.html</data>
  <data key="d3">NTIRE 2023 challenge on image denoising: Methods and results</data>
  <data key="d4">Y Li, Y Zhang, R Timofte, L Van Gool…</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12297468854729392143&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="5364344454304770774">
  <data key="d0">NTIRE 2023 challenge on night photography rendering</data>
  <data key="d1">5364344454304770774</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Shutova_NTIRE_2023_Challenge_on_Night_Photography_Rendering_CVPRW_2023_paper.html</data>
  <data key="d3">NTIRE 2023 challenge on night photography rendering</data>
  <data key="d4">A Shutova, E Ershov…</data>
  <data key="d5">2023</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5364344454304770774&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="3464402829187061665">
  <data key="d0">Efficient long-range attention network for image super-resolution</data>
  <data key="d1">3464402829187061665</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19790-1_39</data>
  <data key="d3">Efficient long-range attention network for image super-resolution</data>
  <data key="d4">X Zhang, H Zeng, S Guo, L Zhang</data>
  <data key="d5">2022</data>
  <data key="d6">74</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3464402829187061665&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="15226748689642581218">
  <data key="d0">Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion</data>
  <data key="d1">15226748689642581218</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.html</data>
  <data key="d3">Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion</data>
  <data key="d4">Z Zhao, H Bai, J Zhang, Y Zhang, S Xu…</data>
  <data key="d5">2023</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15226748689642581218&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="10110104334022835757">
  <data key="d0">Coatnet: Marrying convolution and attention for all data sizes</data>
  <data key="d1">10110104334022835757</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/20568692db622456cc42a2e853ca21f8-Abstract.html</data>
  <data key="d3">Coatnet: Marrying convolution and attention for all data sizes</data>
  <data key="d4">Z Dai, H Liu, QV Le, M Tan</data>
  <data key="d5">2021</data>
  <data key="d6">711</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10110104334022835757&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14443907969977981621">
  <data key="d0">A convnet for the 2020s</data>
  <data key="d1">14443907969977981621</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html</data>
  <data key="d3">A convnet for the 2020s</data>
  <data key="d4">Z Liu, H Mao, CY Wu, C Feichtenhofer…</data>
  <data key="d5">2022</data>
  <data key="d6">2184</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14443907969977981621&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="13501013621324561884">
  <data key="d0">Scaling vision transformers</data>
  <data key="d1">13501013621324561884</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhai_Scaling_Vision_Transformers_CVPR_2022_paper.html</data>
  <data key="d3">Scaling vision transformers</data>
  <data key="d4">X Zhai, A Kolesnikov, N Houlsby…</data>
  <data key="d5">2022</data>
  <data key="d6">570</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13501013621324561884&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="13629397900287809862">
  <data key="d0">Coca: Contrastive captioners are image-text foundation models</data>
  <data key="d1">13629397900287809862</data>
  <data key="d2">https://arxiv.org/abs/2205.01917</data>
  <data key="d3">Coca: Contrastive captioners are image-text foundation models</data>
  <data key="d4">J Yu, Z Wang, V Vasudevan, L Yeung…</data>
  <data key="d5">2022</data>
  <data key="d6">483</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13629397900287809862&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="1273811038957334386">
  <data key="d0">Mvitv2: Improved multiscale vision transformers for classification and detection</data>
  <data key="d1">1273811038957334386</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.html</data>
  <data key="d3">Mvitv2: Improved multiscale vision transformers for classification and detection</data>
  <data key="d4">Y Li, CY Wu, H Fan, K Mangalam…</data>
  <data key="d5">2022</data>
  <data key="d6">293</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1273811038957334386&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="610621467807251926">
  <data key="d0">Inception transformer</data>
  <data key="d1">610621467807251926</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/94e85561a342de88b559b72c9b29f638-Abstract-Conference.html</data>
  <data key="d3">Inception transformer</data>
  <data key="d4">C Si, W Yu, P Zhou, Y Zhou…</data>
  <data key="d5">2022</data>
  <data key="d6">152</data>
  <data key="d7">https://scholar.google.com/scholar?cites=610621467807251926&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="18380770342348927722">
  <data key="d0">Lit: Zero-shot transfer with locked-image text tuning</data>
  <data key="d1">18380770342348927722</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhai_LiT_Zero-Shot_Transfer_With_Locked-Image_Text_Tuning_CVPR_2022_paper.html</data>
  <data key="d3">Lit: Zero-shot transfer with locked-image text tuning</data>
  <data key="d4">X Zhai, X Wang, B Mustafa, A Steiner…</data>
  <data key="d5">2022</data>
  <data key="d6">229</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18380770342348927722&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="9618435703828650575">
  <data key="d0">Simvlm: Simple visual language model pretraining with weak supervision</data>
  <data key="d1">9618435703828650575</data>
  <data key="d2">https://arxiv.org/abs/2108.10904</data>
  <data key="d3">Simvlm: Simple visual language model pretraining with weak supervision</data>
  <data key="d4">Z Wang, J Yu, AW Yu, Z Dai, Y Tsvetkov…</data>
  <data key="d5">2021</data>
  <data key="d6">456</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9618435703828650575&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14988305211504802629">
  <data key="d0">Multi-stage progressive image restoration</data>
  <data key="d1">14988305211504802629</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Zamir_Multi-Stage_Progressive_Image_Restoration_CVPR_2021_paper.html</data>
  <data key="d3">Multi-stage progressive image restoration</data>
  <data key="d4">SW Zamir, A Arora, S Khan, M Hayat…</data>
  <data key="d5">2021</data>
  <data key="d6">873</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14988305211504802629&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="14031000766044293652">
  <data key="d0">Uformer: A general u-shaped transformer for image restoration</data>
  <data key="d1">14031000766044293652</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Wang_Uformer_A_General_U-Shaped_Transformer_for_Image_Restoration_CVPR_2022_paper.html</data>
  <data key="d3">Uformer: A general u-shaped transformer for image restoration</data>
  <data key="d4">Z Wang, X Cun, J Bao, W Zhou…</data>
  <data key="d5">2022</data>
  <data key="d6">613</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14031000766044293652&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="16381083981417715623">
  <data key="d0">Ntire 2022 spectral recovery challenge and data set</data>
  <data key="d1">16381083981417715623</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Arad_NTIRE_2022_Spectral_Recovery_Challenge_and_Data_Set_CVPRW_2022_paper.html</data>
  <data key="d3">Ntire 2022 spectral recovery challenge and data set</data>
  <data key="d4">B Arad, R Timofte, R Yahel, N Morag…</data>
  <data key="d5">2022</data>
  <data key="d6">41</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16381083981417715623&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="6999508588420552953">
  <data key="d0">NTIRE 2021 challenge on image deblurring</data>
  <data key="d1">6999508588420552953</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Nah_NTIRE_2021_Challenge_on_Image_Deblurring_CVPRW_2021_paper.html</data>
  <data key="d3">NTIRE 2021 challenge on image deblurring</data>
  <data key="d4">S Nah, S Son, S Lee, R Timofte…</data>
  <data key="d5">2021</data>
  <data key="d6">60</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6999508588420552953&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="18275282813589182456">
  <data key="d0">Maxim: Multi-axis mlp for image processing</data>
  <data key="d1">18275282813589182456</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Tu_MAXIM_Multi-Axis_MLP_for_Image_Processing_CVPR_2022_paper.html</data>
  <data key="d3">Maxim: Multi-axis mlp for image processing</data>
  <data key="d4">Z Tu, H Talebi, H Zhang, F Yang…</data>
  <data key="d5">2022</data>
  <data key="d6">192</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18275282813589182456&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11948207786179445379">
  <data key="d0">Rethinking coarse-to-fine approach in single image deblurring</data>
  <data key="d1">11948207786179445379</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021/html/Cho_Rethinking_Coarse-To-Fine_Approach_in_Single_Image_Deblurring_ICCV_2021_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">Rethinking coarse-to-fine approach in single image deblurring</data>
  <data key="d4">SJ Cho, SW Ji, JP Hong, SW Jung…</data>
  <data key="d5">2021</data>
  <data key="d6">276</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11948207786179445379&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="2731814227384441305">
  <data key="d0">Hinet: Half instance normalization network for image restoration</data>
  <data key="d1">2731814227384441305</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Chen_HINet_Half_Instance_Normalization_Network_for_Image_Restoration_CVPRW_2021_paper.html</data>
  <data key="d3">Hinet: Half instance normalization network for image restoration</data>
  <data key="d4">L Chen, X Lu, J Zhang, X Chu…</data>
  <data key="d5">2021</data>
  <data key="d6">247</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2731814227384441305&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="563900006853828372">
  <data key="d0">Transweather: Transformer-based restoration of images degraded by adverse weather conditions</data>
  <data key="d1">563900006853828372</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Valanarasu_TransWeather_Transformer-Based_Restoration_of_Images_Degraded_by_Adverse_Weather_Conditions_CVPR_2022_paper.html</data>
  <data key="d3">Transweather: Transformer-based restoration of images degraded by adverse weather conditions</data>
  <data key="d4">JMJ Valanarasu, R Yasarla…</data>
  <data key="d5">2022</data>
  <data key="d6">93</data>
  <data key="d7">https://scholar.google.com/scholar?cites=563900006853828372&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="2892557376887066282">
  <data key="d0">Deblurring via stochastic refinement</data>
  <data key="d1">2892557376887066282</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Whang_Deblurring_via_Stochastic_Refinement_CVPR_2022_paper.html</data>
  <data key="d3">Deblurring via stochastic refinement</data>
  <data key="d4">J Whang, M Delbracio, H Talebi…</data>
  <data key="d5">2022</data>
  <data key="d6">89</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2892557376887066282&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="10807347079650485654">
  <data key="d0">All-in-one image restoration for unknown corruption</data>
  <data key="d1">10807347079650485654</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Li_All-in-One_Image_Restoration_for_Unknown_Corruption_CVPR_2022_paper.html</data>
  <data key="d3">All-in-one image restoration for unknown corruption</data>
  <data key="d4">B Li, X Liu, P Hu, Z Wu, J Lv…</data>
  <data key="d5">2022</data>
  <data key="d6">73</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10807347079650485654&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="15635397108812213817">
  <data key="d0">Transreid: Transformer-based object re-identification</data>
  <data key="d1">15635397108812213817</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/He_TransReID_Transformer-Based_Object_Re-Identification_ICCV_2021_paper.html</data>
  <data key="d3">Transreid: Transformer-based object re-identification</data>
  <data key="d4">S He, H Luo, P Wang, F Wang, H Li…</data>
  <data key="d5">2021</data>
  <data key="d6">499</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15635397108812213817&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="7329647594369932315">
  <data key="d0">Multiscale vision transformers</data>
  <data key="d1">7329647594369932315</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Fan_Multiscale_Vision_Transformers_ICCV_2021_paper.html</data>
  <data key="d3">Multiscale vision transformers</data>
  <data key="d4">H Fan, B Xiong, K Mangalam, Y Li…</data>
  <data key="d5">2021</data>
  <data key="d6">1621</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7329647594369932315&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4431453089685809340">
  <data key="d0">Cswin transformer: A general vision transformer backbone with cross-shaped windows</data>
  <data key="d1">4431453089685809340</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Dong_CSWin_Transformer_A_General_Vision_Transformer_Backbone_With_Cross-Shaped_Windows_CVPR_2022_paper.html</data>
  <data key="d3">Cswin transformer: A general vision transformer backbone with cross-shaped windows</data>
  <data key="d4">X Dong, J Bao, D Chen, W Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">488</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4431453089685809340&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="4278610892084589339">
  <data key="d0">A survey on vision transformer</data>
  <data key="d1">4278610892084589339</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9716741/</data>
  <data key="d3">A survey on vision transformer</data>
  <data key="d4">K Han, Y Wang, H Chen, X Chen, J Guo…</data>
  <data key="d5">2022</data>
  <data key="d6">698</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4278610892084589339&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="601129416962130879">
  <data key="d0">Transfg: A transformer architecture for fine-grained recognition</data>
  <data key="d1">601129416962130879</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/19967</data>
  <data key="d3">Transfg: A transformer architecture for fine-grained recognition</data>
  <data key="d4">J He, JN Chen, S Liu, A Kortylewski, C Yang…</data>
  <data key="d5">2022</data>
  <data key="d6">220</data>
  <data key="d7">https://scholar.google.com/scholar?cites=601129416962130879&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="4326704403467340422">
  <data key="d0">Multi-animal pose estimation, identification and tracking with DeepLabCut</data>
  <data key="d1">4326704403467340422</data>
  <data key="d2">https://www.nature.com/articles/s41592-022-01443-0</data>
  <data key="d3">Multi-animal pose estimation, identification and tracking with DeepLabCut</data>
  <data key="d4">J Lauer, M Zhou, S Ye, W Menegas, S Schneider…</data>
  <data key="d5">2022</data>
  <data key="d6">169</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4326704403467340422&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="18177167198432349205">
  <data key="d0">Mhformer: Multi-hypothesis transformer for 3d human pose estimation</data>
  <data key="d1">18177167198432349205</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Li_MHFormer_Multi-Hypothesis_Transformer_for_3D_Human_Pose_Estimation_CVPR_2022_paper.html</data>
  <data key="d3">Mhformer: Multi-hypothesis transformer for 3d human pose estimation</data>
  <data key="d4">W Li, H Liu, H Tang, P Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">121</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18177167198432349205&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="342346550438939327">
  <data key="d0">Uniformer: Unifying convolution and self-attention for visual recognition</data>
  <data key="d1">342346550438939327</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10143709/</data>
  <data key="d3">Uniformer: Unifying convolution and self-attention for visual recognition</data>
  <data key="d4">K Li, Y Wang, J Zhang, P Gao, G Song…</data>
  <data key="d5">2023</data>
  <data key="d6">102</data>
  <data key="d7">https://scholar.google.com/scholar?cites=342346550438939327&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="9632085609200326616">
  <data key="d0">Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition</data>
  <data key="d1">9632085609200326616</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/64517d8435994992e682b3e4aa0a0661-Abstract.html</data>
  <data key="d3">Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition</data>
  <data key="d4">Y Wang, R Huang, S Song…</data>
  <data key="d5">2021</data>
  <data key="d6">101</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9632085609200326616&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="15172072370662904150">
  <data key="d0">Intriguing properties of vision transformers</data>
  <data key="d1">15172072370662904150</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html</data>
  <data key="d3">Intriguing properties of vision transformers</data>
  <data key="d4">MM Naseer, K Ranasinghe, SH Khan…</data>
  <data key="d5">2021</data>
  <data key="d6">366</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15172072370662904150&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">20</data>
</node>
<node id="6668235945473015803">
  <data key="d0">ibot: Image bert pre-training with online tokenizer</data>
  <data key="d1">6668235945473015803</data>
  <data key="d2">https://arxiv.org/abs/2111.07832</data>
  <data key="d3">ibot: Image bert pre-training with online tokenizer</data>
  <data key="d4">J Zhou, C Wei, H Wang, W Shen, C Xie, A Yuille…</data>
  <data key="d5">2021</data>
  <data key="d6">377</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6668235945473015803&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">20</data>
</node>
<node id="2316302132679082774">
  <data key="d0">Are transformers more robust than cnns?</data>
  <data key="d1">2316302132679082774</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/e19347e1c3ca0c0b97de5fb3b690855a-Abstract.html</data>
  <data key="d3">Are transformers more robust than cnns?</data>
  <data key="d4">Y Bai, J Mei, AL Yuille, C Xie</data>
  <data key="d5">2021</data>
  <data key="d6">168</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2316302132679082774&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">20</data>
</node>
<node id="4388759310460601633">
  <data key="d0">Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation</data>
  <data key="d1">4388759310460601633</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Hoyer_DAFormer_Improving_Network_Architectures_and_Training_Strategies_for_Domain-Adaptive_Semantic_CVPR_2022_paper.html</data>
  <data key="d3">Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation</data>
  <data key="d4">L Hoyer, D Dai, L Van Gool</data>
  <data key="d5">2022</data>
  <data key="d6">182</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4388759310460601633&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">20</data>
</node>
<node id="3041067607452518927">
  <data key="d0">Understanding the robustness in vision transformers</data>
  <data key="d1">3041067607452518927</data>
  <data key="d2">https://proceedings.mlr.press/v162/zhou22m.html</data>
  <data key="d3">Understanding the robustness in vision transformers</data>
  <data key="d4">D Zhou, Z Yu, E Xie, C Xiao…</data>
  <data key="d5">2022</data>
  <data key="d6">82</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3041067607452518927&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">20</data>
</node>
<node id="875131557547078483">
  <data key="d0">Partial success in closing the gap between human and machine vision</data>
  <data key="d1">875131557547078483</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/c8877cff22082a16395a57e97232bb6f-Abstract.html</data>
  <data key="d3">Partial success in closing the gap between human and machine vision</data>
  <data key="d4">R Geirhos, K Narayanappa, B Mitzkus…</data>
  <data key="d5">2021</data>
  <data key="d6">110</data>
  <data key="d7">https://scholar.google.com/scholar?cites=875131557547078483&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">20</data>
</node>
<node id="17891879498080154736">
  <data key="d0">Efficient training of visual transformers with small datasets</data>
  <data key="d1">17891879498080154736</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/c81e155d85dae5430a8cee6f2242e82c-Abstract.html</data>
  <data key="d3">Efficient training of visual transformers with small datasets</data>
  <data key="d4">Y Liu, E Sangineto, W Bi, N Sebe…</data>
  <data key="d5">2021</data>
  <data key="d6">108</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17891879498080154736&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">20</data>
</node>
<node id="2028336304446280911">
  <data key="d0">Assaying out-of-distribution generalization in transfer learning</data>
  <data key="d1">2028336304446280911</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/2f5acc925919209370a3af4eac5cad4a-Abstract-Conference.html</data>
  <data key="d3">Assaying out-of-distribution generalization in transfer learning</data>
  <data key="d4">F Wenzel, A Dittadi, P Gehler…</data>
  <data key="d5">2022</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2028336304446280911&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">20</data>
</node>
<node id="5601871542106060008">
  <data key="d0">Ow-detr: Open-world detection transformer</data>
  <data key="d1">5601871542106060008</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Gupta_OW-DETR_Open-World_Detection_Transformer_CVPR_2022_paper.html</data>
  <data key="d3">Ow-detr: Open-world detection transformer</data>
  <data key="d4">A Gupta, S Narayan, KJ Joseph…</data>
  <data key="d5">2022</data>
  <data key="d6">68</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5601871542106060008&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">20</data>
</node>
<node id="14311400318178337111">
  <data key="d0">A survey of modern deep learning based object detection models</data>
  <data key="d1">14311400318178337111</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1051200422001312</data>
  <data key="d3">A survey of modern deep learning based object detection models</data>
  <data key="d4">SSA Zaidi, MS Ansari, A Aslam, N Kanwal…</data>
  <data key="d5">2022</data>
  <data key="d6">435</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14311400318178337111&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="11789051068432887660">
  <data key="d0">Deep learning methods for object detection in smart manufacturing: A survey</data>
  <data key="d1">11789051068432887660</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0278612522001066</data>
  <data key="d3">Deep learning methods for object detection in smart manufacturing: A survey</data>
  <data key="d4">HM Ahmad, A Rahimi</data>
  <data key="d5">2022</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11789051068432887660&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="15295068953900215294">
  <data key="d0">Mammogram breast cancer CAD systems for mass detection and classification: a review</data>
  <data key="d1">15295068953900215294</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11042-022-12332-1</data>
  <data key="d3">Mammogram breast cancer CAD systems for mass detection and classification: a review</data>
  <data key="d4">NM Hassan, S Hamad, K Mahar</data>
  <data key="d5">2022</data>
  <data key="d6">33</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15295068953900215294&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="15549291371117213871">
  <data key="d0">Precise single-stage detector</data>
  <data key="d1">15549291371117213871</data>
  <data key="d2">https://arxiv.org/abs/2210.04252</data>
  <data key="d3">Precise single-stage detector</data>
  <data key="d4">A Chandio, G Gui, T Kumar, I Ullah…</data>
  <data key="d5">2022</data>
  <data key="d6">41</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15549291371117213871&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="9884544045603971658">
  <data key="d0">CE-FPN: Enhancing channel information for object detection</data>
  <data key="d1">9884544045603971658</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11042-022-11940-1</data>
  <data key="d3">CE-FPN: Enhancing channel information for object detection</data>
  <data key="d4">Y Luo, X Cao, J Zhang, J Guo, H Shen, T Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">68</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9884544045603971658&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="9591435289724766439">
  <data key="d0">A survey of self-supervised and few-shot object detection</data>
  <data key="d1">9591435289724766439</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9860087/</data>
  <data key="d3">A survey of self-supervised and few-shot object detection</data>
  <data key="d4">G Huang, I Laradji, D Vazquez…</data>
  <data key="d5">2022</data>
  <data key="d6">43</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9591435289724766439&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="10628215061802232868">
  <data key="d0">Feature split–merge–enhancement network for remote sensing object detection</data>
  <data key="d1">10628215061802232868</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9673713/</data>
  <data key="d3">Feature split–merge–enhancement network for remote sensing object detection</data>
  <data key="d4">W Ma, N Li, H Zhu, L Jiao, X Tang…</data>
  <data key="d5">2022</data>
  <data key="d6">42</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10628215061802232868&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="9532821550175512200">
  <data key="d0">Guiding pretraining in reinforcement learning with large language models</data>
  <data key="d1">9532821550175512200</data>
  <data key="d2">https://arxiv.org/abs/2302.06692</data>
  <data key="d3">Guiding pretraining in reinforcement learning with large language models</data>
  <data key="d4">Y Du, O Watkins, Z Wang, C Colas, T Darrell…</data>
  <data key="d5">2023</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9532821550175512200&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="12985976504909847163">
  <data key="d0">Integrating deep learning-based iot and fog computing with software-defined networking for detecting weapons in video surveillance systems</data>
  <data key="d1">12985976504909847163</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/14/5075</data>
  <data key="d3">Integrating deep learning-based iot and fog computing with software-defined networking for detecting weapons in video surveillance systems</data>
  <data key="d4">C Fathy, SN Saleh</data>
  <data key="d5">2022</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12985976504909847163&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="2812153438552156646">
  <data key="d0">Backbones-review: Feature extraction networks for deep learning and deep reinforcement learning approaches</data>
  <data key="d1">2812153438552156646</data>
  <data key="d2">https://arxiv.org/abs/2206.08016</data>
  <data key="d3">Backbones-review: Feature extraction networks for deep learning and deep reinforcement learning approaches</data>
  <data key="d4">O Elharrouss, Y Akbari, N Almaadeed…</data>
  <data key="d5">2022</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2812153438552156646&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="14349697475909471320">
  <data key="d0">Small-object detection based on YOLOv5 in autonomous driving systems</data>
  <data key="d1">14349697475909471320</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0167865523000727</data>
  <data key="d3">Small-object detection based on YOLOv5 in autonomous driving systems</data>
  <data key="d4">B Mahaur, KK Mishra</data>
  <data key="d5">2023</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14349697475909471320&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="17327663970405370182">
  <data key="d0">Point-bert: Pre-training 3d point cloud transformers with masked point modeling</data>
  <data key="d1">17327663970405370182</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Yu_Point-BERT_Pre-Training_3D_Point_Cloud_Transformers_With_Masked_Point_Modeling_CVPR_2022_paper.html</data>
  <data key="d3">Point-bert: Pre-training 3d point cloud transformers with masked point modeling</data>
  <data key="d4">X Yu, L Tang, Y Rao, T Huang…</data>
  <data key="d5">2022</data>
  <data key="d6">219</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17327663970405370182&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="1930827783125608869">
  <data key="d0">Masked autoencoders for point cloud self-supervised learning</data>
  <data key="d1">1930827783125608869</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20086-1_35</data>
  <data key="d3">Masked autoencoders for point cloud self-supervised learning</data>
  <data key="d4">Y Pang, W Wang, FEH Tay, W Liu, Y Tian…</data>
  <data key="d5">2022</data>
  <data key="d6">143</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1930827783125608869&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="14072888861532659606">
  <data key="d0">Pointnext: Revisiting pointnet++ with improved training and scaling strategies</data>
  <data key="d1">14072888861532659606</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/9318763d049edf9a1f2779b2a59911d3-Abstract-Conference.html</data>
  <data key="d3">Pointnext: Revisiting pointnet++ with improved training and scaling strategies</data>
  <data key="d4">G Qian, Y Li, H Peng, J Mai…</data>
  <data key="d5">2022</data>
  <data key="d6">152</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14072888861532659606&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="1118274744677998915">
  <data key="d0">Unsupervised point cloud representation learning with deep neural networks: A survey</data>
  <data key="d1">1118274744677998915</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10086697/</data>
  <data key="d3">Unsupervised point cloud representation learning with deep neural networks: A survey</data>
  <data key="d4">A Xiao, J Huang, D Guan, X Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">31</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1118274744677998915&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="8230127879015912569">
  <data key="d0">Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training</data>
  <data key="d1">8230127879015912569</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/ad1d7a4df30a9c0c46b387815a774a84-Abstract-Conference.html</data>
  <data key="d3">Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training</data>
  <data key="d4">R Zhang, Z Guo, P Gao, R Fang…</data>
  <data key="d5">2022</data>
  <data key="d6">60</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8230127879015912569&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="12182702384297284737">
  <data key="d0">Masked discrimination for self-supervised learning on point clouds</data>
  <data key="d1">12182702384297284737</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20086-1_38</data>
  <data key="d3">Masked discrimination for self-supervised learning on point clouds</data>
  <data key="d4">H Liu, M Cai, YJ Lee</data>
  <data key="d5">2022</data>
  <data key="d6">57</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12182702384297284737&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="17896705208502572006">
  <data key="d0">Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders</data>
  <data key="d1">17896705208502572006</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Learning_3D_Representations_From_2D_Pre-Trained_Models_via_Image-to-Point_Masked_CVPR_2023_paper.html</data>
  <data key="d3">Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders</data>
  <data key="d4">R Zhang, L Wang, Y Qiao, P Gao…</data>
  <data key="d5">2023</data>
  <data key="d6">32</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17896705208502572006&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="477874232529254013">
  <data key="d0">ULIP: Learning a unified representation of language, images, and point clouds for 3D understanding</data>
  <data key="d1">477874232529254013</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Gao_ULIP_Learning_a_Unified_Representation_of_Language_Images_and_Point_CVPR_2023_paper.html</data>
  <data key="d3">ULIP: Learning a unified representation of language, images, and point clouds for 3D understanding</data>
  <data key="d4">L Xue, M Gao, C Xing, R Martín-Martín…</data>
  <data key="d5">2023</data>
  <data key="d6">37</data>
  <data key="d7">https://scholar.google.com/scholar?cites=477874232529254013&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="16387925596110304701">
  <data key="d0">P2p: Tuning pre-trained image models for point cloud analysis with point-to-pixel prompting</data>
  <data key="d1">16387925596110304701</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/5cd6dc946ccc37ae6c9f4fc6b6181e1d-Abstract-Conference.html</data>
  <data key="d3">P2p: Tuning pre-trained image models for point cloud analysis with point-to-pixel prompting</data>
  <data key="d4">Z Wang, X Yu, Y Rao, J Zhou…</data>
  <data key="d5">2022</data>
  <data key="d6">31</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16387925596110304701&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="7193625896865391995">
  <data key="d0">Shapeformer: Transformer-based shape completion via sparse representation</data>
  <data key="d1">7193625896865391995</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Yan_ShapeFormer_Transformer-Based_Shape_Completion_via_Sparse_Representation_CVPR_2022_paper.html</data>
  <data key="d3">Shapeformer: Transformer-based shape completion via sparse representation</data>
  <data key="d4">X Yan, L Lin, NJ Mitra, D Lischinski…</data>
  <data key="d5">2022</data>
  <data key="d6">57</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7193625896865391995&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">4</data>
</node>
<node id="6784655767122395745">
  <data key="d0">Maxvit: Multi-axis vision transformer</data>
  <data key="d1">6784655767122395745</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20053-3_27</data>
  <data key="d3">Maxvit: Multi-axis vision transformer</data>
  <data key="d4">Z Tu, H Talebi, H Zhang, F Yang, P Milanfar…</data>
  <data key="d5">2022</data>
  <data key="d6">166</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6784655767122395745&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="10588342779298269046">
  <data key="d0">Eva: Exploring the limits of masked visual representation learning at scale</data>
  <data key="d1">10588342779298269046</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.html</data>
  <data key="d3">Eva: Exploring the limits of masked visual representation learning at scale</data>
  <data key="d4">Y Fang, W Wang, B Xie, Q Sun, L Wu…</data>
  <data key="d5">2023</data>
  <data key="d6">102</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10588342779298269046&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12938213222665733645">
  <data key="d0">Hornet: Efficient high-order spatial interactions with recursive gated convolutions</data>
  <data key="d1">12938213222665733645</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/436d042b2dd81214d23ae43eb196b146-Abstract-Conference.html</data>
  <data key="d3">Hornet: Efficient high-order spatial interactions with recursive gated convolutions</data>
  <data key="d4">Y Rao, W Zhao, Y Tang, J Zhou…</data>
  <data key="d5">2022</data>
  <data key="d6">101</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12938213222665733645&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="1388490151733704334">
  <data key="d0">Convnext v2: Co-designing and scaling convnets with masked autoencoders</data>
  <data key="d1">1388490151733704334</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Woo_ConvNeXt_V2_Co-Designing_and_Scaling_ConvNets_With_Masked_Autoencoders_CVPR_2023_paper.html</data>
  <data key="d3">Convnext v2: Co-designing and scaling convnets with masked autoencoders</data>
  <data key="d4">S Woo, S Debnath, R Hu, X Chen…</data>
  <data key="d5">2023</data>
  <data key="d6">63</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1388490151733704334&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="15088728781552938978">
  <data key="d0">V2x-vit: Vehicle-to-everything cooperative perception with vision transformer</data>
  <data key="d1">15088728781552938978</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19842-7_7</data>
  <data key="d3">V2x-vit: Vehicle-to-everything cooperative perception with vision transformer</data>
  <data key="d4">R Xu, H Xiang, Z Tu, X Xia, MH Yang, J Ma</data>
  <data key="d5">2022</data>
  <data key="d6">111</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15088728781552938978&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="12692106295877813680">
  <data key="d0">Efficientformer: Vision transformers at mobilenet speed</data>
  <data key="d1">12692106295877813680</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/5452ad8ee6ea6e7dc41db1cbd31ba0b8-Abstract-Conference.html</data>
  <data key="d3">Efficientformer: Vision transformers at mobilenet speed</data>
  <data key="d4">Y Li, G Yuan, Y Wen, J Hu…</data>
  <data key="d5">2022</data>
  <data key="d6">87</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12692106295877813680&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="10239175441885037567">
  <data key="d0">Tracking objects as pixel-wise distributions</data>
  <data key="d1">10239175441885037567</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20047-2_5</data>
  <data key="d3">Tracking objects as pixel-wise distributions</data>
  <data key="d4">Z Zhao, Z Wu, Y Zhuang, B Li, J Jia</data>
  <data key="d5">2022</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10239175441885037567&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="2000389979125404276">
  <data key="d0">CoBEVT: Cooperative bird's eye view semantic segmentation with sparse transformers</data>
  <data key="d1">2000389979125404276</data>
  <data key="d2">https://arxiv.org/abs/2207.02202</data>
  <data key="d3">CoBEVT: Cooperative bird's eye view semantic segmentation with sparse transformers</data>
  <data key="d4">R Xu, Z Tu, H Xiang, W Shao, B Zhou, J Ma</data>
  <data key="d5">2022</data>
  <data key="d6">61</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2000389979125404276&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="14170076594522259195">
  <data key="d0">Multi-agent reinforcement learning is a sequence modeling problem</data>
  <data key="d1">14170076594522259195</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/69413f87e5a34897cd010ca698097d0a-Abstract-Conference.html</data>
  <data key="d3">Multi-agent reinforcement learning is a sequence modeling problem</data>
  <data key="d4">M Wen, J Kuba, R Lin, W Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">39</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14170076594522259195&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="5425241495538385765">
  <data key="d0">Dual vision transformer</data>
  <data key="d1">5425241495538385765</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10105499/</data>
  <data key="d3">Dual vision transformer</data>
  <data key="d4">T Yao, Y Li, Y Pan, Y Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">31</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5425241495538385765&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="4041041901496425203">
  <data key="d0">A survey on image data augmentation for deep learning</data>
  <data key="d1">4041041901496425203</data>
  <data key="d2">https://journalofbigdata.springeropen.com/track/pdf/10.1186/s40537-019-0197-0.pdf</data>
  <data key="d3">A survey on image data augmentation for deep learning</data>
  <data key="d4">C Shorten, TM Khoshgoftaar</data>
  <data key="d5">2019</data>
  <data key="d6">7696</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4041041901496425203&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="8136644755673586417">
  <data key="d0">Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions</data>
  <data key="d1">8136644755673586417</data>
  <data key="d2">https://link.springer.com/article/10.1186/s40537-021-00444-8</data>
  <data key="d3">Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions</data>
  <data key="d4">L Alzubaidi, J Zhang, AJ Humaidi, A Al-Dujaili…</data>
  <data key="d5">2021</data>
  <data key="d6">2391</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8136644755673586417&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="7358070883597795569">
  <data key="d0">Multi-sensor information fusion based on machine learning for real applications in human activity recognition: State-of-the-art and research challenges</data>
  <data key="d1">7358070883597795569</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1566253521002311</data>
  <data key="d3">Multi-sensor information fusion based on machine learning for real applications in human activity recognition: State-of-the-art and research challenges</data>
  <data key="d4">S Qiu, H Zhao, N Jiang, Z Wang, L Liu, Y An, H Zhao…</data>
  <data key="d5">2022</data>
  <data key="d6">215</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7358070883597795569&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="2188347889974787509">
  <data key="d0">Deep learning in computer vision: A critical review of emerging techniques and application scenarios</data>
  <data key="d1">2188347889974787509</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2666827021000670</data>
  <data key="d3">Deep learning in computer vision: A critical review of emerging techniques and application scenarios</data>
  <data key="d4">J Chai, H Zeng, A Li, EWT Ngai</data>
  <data key="d5">2021</data>
  <data key="d6">252</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2188347889974787509&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="17352312443164396046">
  <data key="d0">Machine learning for structural engineering: A state-of-the-art review</data>
  <data key="d1">17352312443164396046</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2352012422000947</data>
  <data key="d3">Machine learning for structural engineering: A state-of-the-art review</data>
  <data key="d4">HT Thai</data>
  <data key="d5">2022</data>
  <data key="d6">136</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17352312443164396046&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="17997900354107908438">
  <data key="d0">Measuring biological age using omics data</data>
  <data key="d1">17997900354107908438</data>
  <data key="d2">https://www.nature.com/articles/s41576-022-00511-7</data>
  <data key="d3">Measuring biological age using omics data</data>
  <data key="d4">J Rutledge, H Oh, T Wyss-Coray</data>
  <data key="d5">2022</data>
  <data key="d6">83</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17997900354107908438&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="9888405418392239400">
  <data key="d0">Transmed: Transformers advance multi-modal medical image classification</data>
  <data key="d1">9888405418392239400</data>
  <data key="d2">https://www.mdpi.com/2075-4418/11/8/1384</data>
  <data key="d3">Transmed: Transformers advance multi-modal medical image classification</data>
  <data key="d4">Y Dai, Y Gao, F Liu</data>
  <data key="d5">2021</data>
  <data key="d6">155</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9888405418392239400&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="14217745962583788184">
  <data key="d0">Machine learning in aerodynamic shape optimization</data>
  <data key="d1">14217745962583788184</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0376042122000410</data>
  <data key="d3">Machine learning in aerodynamic shape optimization</data>
  <data key="d4">J Li, X Du, JRRA Martins</data>
  <data key="d5">2022</data>
  <data key="d6">59</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14217745962583788184&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="3134740227865659264">
  <data key="d0">Part of speech tagging: a systematic review of deep learning and machine learning approaches</data>
  <data key="d1">3134740227865659264</data>
  <data key="d2">https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00561-y</data>
  <data key="d3">Part of speech tagging: a systematic review of deep learning and machine learning approaches</data>
  <data key="d4">A Chiche, B Yitagesu</data>
  <data key="d5">2022</data>
  <data key="d6">61</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3134740227865659264&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="1681999640612811907">
  <data key="d0">Deep learning in analytical chemistry</data>
  <data key="d1">1681999640612811907</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S016599362100282X</data>
  <data key="d3">Deep learning in analytical chemistry</data>
  <data key="d4">B Debus, H Parastar, P Harrington…</data>
  <data key="d5">2021</data>
  <data key="d6">66</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1681999640612811907&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="7025715449086074571">
  <data key="d0">Making radiomics more reproducible across scanner and imaging protocol variations: a review of harmonization methods</data>
  <data key="d1">7025715449086074571</data>
  <data key="d2">https://www.mdpi.com/2075-4426/11/9/842</data>
  <data key="d3">Making radiomics more reproducible across scanner and imaging protocol variations: a review of harmonization methods</data>
  <data key="d4">SA Mali, A Ibrahim, HC Woodruff…</data>
  <data key="d5">2021</data>
  <data key="d6">76</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7025715449086074571&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="4162777562069804924">
  <data key="d0">An overview of deep learning methods for multimodal medical data mining</data>
  <data key="d1">4162777562069804924</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417422004249</data>
  <data key="d3">An overview of deep learning methods for multimodal medical data mining</data>
  <data key="d4">F Behrad, MS Abadeh</data>
  <data key="d5">2022</data>
  <data key="d6">43</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4162777562069804924&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">10</data>
</node>
<node id="6141939658324331542">
  <data key="d0">Review on Convolutional Neural Networks (CNN) in vegetation remote sensing</data>
  <data key="d1">6141939658324331542</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0924271620303488</data>
  <data key="d3">Review on Convolutional Neural Networks (CNN) in vegetation remote sensing</data>
  <data key="d4">T Kattenborn, J Leitloff, F Schiefer, S Hinz</data>
  <data key="d5">2021</data>
  <data key="d6">601</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6141939658324331542&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="563969953758433323">
  <data key="d0">UAV-based forest health monitoring: A systematic review</data>
  <data key="d1">563969953758433323</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/13/3205</data>
  <data key="d3">UAV-based forest health monitoring: A systematic review</data>
  <data key="d4">S Ecke, J Dempewolf, J Frey, A Schwaller, E Endres…</data>
  <data key="d5">2022</data>
  <data key="d6">46</data>
  <data key="d7">https://scholar.google.com/scholar?cites=563969953758433323&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="2331420292369119754">
  <data key="d0">Effect of attention mechanism in deep learning-based remote sensing image processing: A systematic literature review</data>
  <data key="d1">2331420292369119754</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/15/2965</data>
  <data key="d3">Effect of attention mechanism in deep learning-based remote sensing image processing: A systematic literature review</data>
  <data key="d4">S Ghaffarian, J Valente, M Van Der Voort…</data>
  <data key="d5">2021</data>
  <data key="d6">71</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2331420292369119754&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="13012757373810467293">
  <data key="d0">Automated tree-crown and height detection in a young forest plantation using mask region-based convolutional neural network (Mask R-CNN)</data>
  <data key="d1">13012757373810467293</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0924271621001611</data>
  <data key="d3">Automated tree-crown and height detection in a young forest plantation using mask region-based convolutional neural network (Mask R-CNN)</data>
  <data key="d4">Z Hao, L Lin, CJ Post, EA Mikhailova, M Li…</data>
  <data key="d5">2021</data>
  <data key="d6">74</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13012757373810467293&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="1751297388377231685">
  <data key="d0">Global wheat head detection 2021: An improved dataset for benchmarking wheat head detection methods</data>
  <data key="d1">1751297388377231685</data>
  <data key="d2">https://spj.science.org/doi/full/10.34133/2021/9846158</data>
  <data key="d3">Global wheat head detection 2021: An improved dataset for benchmarking wheat head detection methods</data>
  <data key="d4">E David, M Serouart, D Smith, S Madec…</data>
  <data key="d5">2021</data>
  <data key="d6">54</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1751297388377231685&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="5520588759774186774">
  <data key="d0">Deep learning in forestry using uav-acquired rgb data: A practical review</data>
  <data key="d1">5520588759774186774</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/14/2837</data>
  <data key="d3">Deep learning in forestry using uav-acquired rgb data: A practical review</data>
  <data key="d4">Y Diez, S Kentsch, M Fukuda, MLL Caceres…</data>
  <data key="d5">2021</data>
  <data key="d6">59</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5520588759774186774&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="11053439912877888530">
  <data key="d0">Land use land cover classification with U-net: Advantages of combining sentinel-1 and sentinel-2 imagery</data>
  <data key="d1">11053439912877888530</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/18/3600</data>
  <data key="d3">Land use land cover classification with U-net: Advantages of combining sentinel-1 and sentinel-2 imagery</data>
  <data key="d4">JV Solórzano, JF Mas, Y Gao, JA Gallardo-Cruz</data>
  <data key="d5">2021</data>
  <data key="d6">51</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11053439912877888530&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="4856010013404526375">
  <data key="d0">A high-resolution canopy height model of the Earth</data>
  <data key="d1">4856010013404526375</data>
  <data key="d2">https://arxiv.org/abs/2204.08322</data>
  <data key="d3">A high-resolution canopy height model of the Earth</data>
  <data key="d4">N Lang, W Jetz, K Schindler, JD Wegner</data>
  <data key="d5">2022</data>
  <data key="d6">50</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4856010013404526375&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="3951350311895905492">
  <data key="d0">Mowing event detection in permanent grasslands: Systematic evaluation of input features from Sentinel-1, Sentinel-2, and Landsat 8 time series</data>
  <data key="d1">3951350311895905492</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0034425721004715</data>
  <data key="d3">Mowing event detection in permanent grasslands: Systematic evaluation of input features from Sentinel-1, Sentinel-2, and Landsat 8 time series</data>
  <data key="d4">F Lobert, AK Holtgrave, M Schwieder, M Pause…</data>
  <data key="d5">2021</data>
  <data key="d6">27</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3951350311895905492&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="9663211362112210725">
  <data key="d0">Deep neural networks to detect weeds from crops in agricultural environments in real-time: A review</data>
  <data key="d1">9663211362112210725</data>
  <data key="d2">https://www.mdpi.com/2072-4292/13/21/4486</data>
  <data key="d3">Deep neural networks to detect weeds from crops in agricultural environments in real-time: A review</data>
  <data key="d4">I Rakhmatulin, A Kamilaris, C Andreasen</data>
  <data key="d5">2021</data>
  <data key="d6">33</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9663211362112210725&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="727216592699071177">
  <data key="d0">A review of landcover classification with very-high resolution remotely sensed optical images—Analysis unit, model scalability and transferability</data>
  <data key="d1">727216592699071177</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/3/646</data>
  <data key="d3">A review of landcover classification with very-high resolution remotely sensed optical images—Analysis unit, model scalability and transferability</data>
  <data key="d4">R Qin, T Liu</data>
  <data key="d5">2022</data>
  <data key="d6">32</data>
  <data key="d7">https://scholar.google.com/scholar?cites=727216592699071177&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">11</data>
</node>
<node id="7770442917120891581">
  <data key="d0">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation</data>
  <data key="d1">7770442917120891581</data>
  <data key="d2">https://proceedings.mlr.press/v162/li22n.html</data>
  <data key="d3">Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation</data>
  <data key="d4">J Li, D Li, C Xiong, S Hoi</data>
  <data key="d5">2022</data>
  <data key="d6">790</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7770442917120891581&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="11430520233869250304">
  <data key="d0">Survey of hallucination in natural language generation</data>
  <data key="d1">11430520233869250304</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3571730</data>
  <data key="d3">Survey of hallucination in natural language generation</data>
  <data key="d4">Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii…</data>
  <data key="d5">2023</data>
  <data key="d6">472</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11430520233869250304&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="5562955281835677624">
  <data key="d0">Vision-language pre-training: Basics, recent advances, and future trends</data>
  <data key="d1">5562955281835677624</data>
  <data key="d2">https://www.nowpublishers.com/article/Details/CGV-105</data>
  <data key="d3">Vision-language pre-training: Basics, recent advances, and future trends</data>
  <data key="d4">Z Gan, L Li, C Li, L Wang, Z Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">55</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5562955281835677624&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="2325917221075842848">
  <data key="d0">Flamingo: a visual language model for few-shot learning</data>
  <data key="d1">2325917221075842848</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html</data>
  <data key="d3">Flamingo: a visual language model for few-shot learning</data>
  <data key="d4">JB Alayrac, J Donahue, P Luc…</data>
  <data key="d5">2022</data>
  <data key="d6">781</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2325917221075842848&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="8018158103125985189">
  <data key="d0">Laion-5b: An open large-scale dataset for training next generation image-text models</data>
  <data key="d1">8018158103125985189</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/a1859debfb3b59d094f3504d5ebb6c25-Abstract-Datasets_and_Benchmarks.html</data>
  <data key="d3">Laion-5b: An open large-scale dataset for training next generation image-text models</data>
  <data key="d4">C Schuhmann, R Beaumont, R Vencu…</data>
  <data key="d5">2022</data>
  <data key="d6">513</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8018158103125985189&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="11255276306904968426">
  <data key="d0">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</data>
  <data key="d1">11255276306904968426</data>
  <data key="d2">https://arxiv.org/abs/2301.12597</data>
  <data key="d3">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models</data>
  <data key="d4">J Li, D Li, S Savarese, S Hoi</data>
  <data key="d5">2023</data>
  <data key="d6">376</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11255276306904968426&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="5385256443321262916">
  <data key="d0">Git: A generative image-to-text transformer for vision and language</data>
  <data key="d1">5385256443321262916</data>
  <data key="d2">https://arxiv.org/abs/2205.14100</data>
  <data key="d3">Git: A generative image-to-text transformer for vision and language</data>
  <data key="d4">J Wang, Z Yang, X Hu, L Li, K Lin, Z Gan, Z Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">151</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5385256443321262916&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="17485588102904105060">
  <data key="d0">Socratic models: Composing zero-shot multimodal reasoning with language</data>
  <data key="d1">17485588102904105060</data>
  <data key="d2">https://arxiv.org/abs/2204.00598</data>
  <data key="d3">Socratic models: Composing zero-shot multimodal reasoning with language</data>
  <data key="d4">A Zeng, M Attarian, B Ichter, K Choromanski…</data>
  <data key="d5">2022</data>
  <data key="d6">158</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17485588102904105060&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="5354355722706795291">
  <data key="d0">Latent-nerf for shape-guided generation of 3d shapes and textures</data>
  <data key="d1">5354355722706795291</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Metzer_Latent-NeRF_for_Shape-Guided_Generation_of_3D_Shapes_and_Textures_CVPR_2023_paper.html</data>
  <data key="d3">Latent-nerf for shape-guided generation of 3d shapes and textures</data>
  <data key="d4">G Metzer, E Richardson, O Patashnik…</data>
  <data key="d5">2023</data>
  <data key="d6">62</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5354355722706795291&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="972963521258961554">
  <data key="d0">Omnivl: One foundation model for image-language and video-language tasks</data>
  <data key="d1">972963521258961554</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/259a5df46308d60f8454bd4adcc3b462-Abstract-Conference.html</data>
  <data key="d3">Omnivl: One foundation model for image-language and video-language tasks</data>
  <data key="d4">J Wang, D Chen, Z Wu, C Luo, L Zhou…</data>
  <data key="d5">2022</data>
  <data key="d6">46</data>
  <data key="d7">https://scholar.google.com/scholar?cites=972963521258961554&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="8793029896395507010">
  <data key="d0">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</data>
  <data key="d1">8793029896395507010</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.html</data>
  <data key="d3">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</data>
  <data key="d4">W Wang, E Xie, X Li, DP Fan, K Song…</data>
  <data key="d5">2021</data>
  <data key="d6">2304</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8793029896395507010&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="3458396398389387877">
  <data key="d0">Swin transformer: Hierarchical vision transformer using shifted windows</data>
  <data key="d1">3458396398389387877</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper</data>
  <data key="d3">Swin transformer: Hierarchical vision transformer using shifted windows</data>
  <data key="d4">Z Liu, Y Lin, Y Cao, H Hu, Y Wei…</data>
  <data key="d5">2021</data>
  <data key="d6">10359</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3458396398389387877&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="10553738615668616847">
  <data key="d0">Mlp-mixer: An all-mlp architecture for vision</data>
  <data key="d1">10553738615668616847</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html</data>
  <data key="d3">Mlp-mixer: An all-mlp architecture for vision</data>
  <data key="d4">IO Tolstikhin, N Houlsby, A Kolesnikov…</data>
  <data key="d5">2021</data>
  <data key="d6">1439</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10553738615668616847&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11517447940529951525">
  <data key="d0">Cvt: Introducing convolutions to vision transformers</data>
  <data key="d1">11517447940529951525</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Wu_CvT_Introducing_Convolutions_to_Vision_Transformers_ICCV_2021_paper.html</data>
  <data key="d3">Cvt: Introducing convolutions to vision transformers</data>
  <data key="d4">H Wu, B Xiao, N Codella, M Liu, X Dai…</data>
  <data key="d5">2021</data>
  <data key="d6">1208</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11517447940529951525&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4461602603986165987">
  <data key="d0">Swin-unet: Unet-like pure transformer for medical image segmentation</data>
  <data key="d1">4461602603986165987</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-25066-8_9</data>
  <data key="d3">Swin-unet: Unet-like pure transformer for medical image segmentation</data>
  <data key="d4">H Cao, Y Wang, J Chen, D Jiang, X Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">1176</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4461602603986165987&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="1788827408361087894">
  <data key="d0">Vivit: A video vision transformer</data>
  <data key="d1">1788827408361087894</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021/html/Arnab_ViViT_A_Video_Vision_Transformer_ICCV_2021_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">Vivit: A video vision transformer</data>
  <data key="d4">A Arnab, M Dehghani, G Heigold…</data>
  <data key="d5">2021</data>
  <data key="d6">1184</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1788827408361087894&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="15154755818357511167">
  <data key="d0">Transformer in transformer</data>
  <data key="d1">15154755818357511167</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/854d9fca60b4bd07f9bb215d59ef5561-Abstract.html</data>
  <data key="d3">Transformer in transformer</data>
  <data key="d4">K Han, A Xiao, E Wu, J Guo, C Xu…</data>
  <data key="d5">2021</data>
  <data key="d6">910</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15154755818357511167&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="9595110325981705564">
  <data key="d0">On the opportunities and risks of foundation models</data>
  <data key="d1">9595110325981705564</data>
  <data key="d2">https://arxiv.org/abs/2108.07258</data>
  <data key="d3">On the opportunities and risks of foundation models</data>
  <data key="d4">R Bommasani, DA Hudson, E Adeli, R Altman…</data>
  <data key="d5">2021</data>
  <data key="d6">1520</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9595110325981705564&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="3497017024792502078">
  <data key="d0">Large language models in medicine</data>
  <data key="d1">3497017024792502078</data>
  <data key="d2">https://www.nature.com/articles/s41591-023-02448-8</data>
  <data key="d3">Large language models in medicine</data>
  <data key="d4">AJ Thirunavukarasu, DSJ Ting, K Elangovan…</data>
  <data key="d5">2023</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3497017024792502078&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="12550806676654513546">
  <data key="d0">Sustainable ai: Environmental implications, challenges and opportunities</data>
  <data key="d1">12550806676654513546</data>
  <data key="d2">https://proceedings.mlsys.org/paper_files/paper/2022/hash/462211f67c7d858f663355eff93b745e-Abstract.html</data>
  <data key="d3">Sustainable ai: Environmental implications, challenges and opportunities</data>
  <data key="d4">CJ Wu, R Raghavendra, U Gupta…</data>
  <data key="d5">2022</data>
  <data key="d6">132</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12550806676654513546&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="12979976309017799162">
  <data key="d0">Training language models to follow instructions with human feedback</data>
  <data key="d1">12979976309017799162</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html</data>
  <data key="d3">Training language models to follow instructions with human feedback</data>
  <data key="d4">L Ouyang, J Wu, X Jiang, D Almeida…</data>
  <data key="d5">2022</data>
  <data key="d6">2206</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12979976309017799162&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="10258451456074526049">
  <data key="d0">Palm: Scaling language modeling with pathways</data>
  <data key="d1">10258451456074526049</data>
  <data key="d2">https://arxiv.org/abs/2204.02311</data>
  <data key="d3">Palm: Scaling language modeling with pathways</data>
  <data key="d4">A Chowdhery, S Narang, J Devlin, M Bosma…</data>
  <data key="d5">2022</data>
  <data key="d6">1519</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10258451456074526049&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="11775513497668487533">
  <data key="d0">Scaling autoregressive models for content-rich text-to-image generation</data>
  <data key="d1">11775513497668487533</data>
  <data key="d2">https://3dvar.com/Yu2022Scaling.pdf</data>
  <data key="d3">Scaling autoregressive models for content-rich text-to-image generation</data>
  <data key="d4">J Yu, Y Xu, JY Koh, T Luong, G Baid, Z Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">340</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11775513497668487533&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="16117828918544644907">
  <data key="d0">Learning to prompt for vision-language models</data>
  <data key="d1">16117828918544644907</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11263-022-01653-1</data>
  <data key="d3">Learning to prompt for vision-language models</data>
  <data key="d4">K Zhou, J Yang, CC Loy, Z Liu</data>
  <data key="d5">2022</data>
  <data key="d6">644</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16117828918544644907&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="14421942083121350206">
  <data key="d0">Visual prompt tuning</data>
  <data key="d1">14421942083121350206</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19827-4_41</data>
  <data key="d3">Visual prompt tuning</data>
  <data key="d4">M Jia, L Tang, BC Chen, C Cardie, S Belongie…</data>
  <data key="d5">2022</data>
  <data key="d6">375</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14421942083121350206&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="737352693355482724">
  <data key="d0">Conditional prompt learning for vision-language models</data>
  <data key="d1">737352693355482724</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.html</data>
  <data key="d3">Conditional prompt learning for vision-language models</data>
  <data key="d4">K Zhou, J Yang, CC Loy, Z Liu</data>
  <data key="d5">2022</data>
  <data key="d6">350</data>
  <data key="d7">https://scholar.google.com/scholar?cites=737352693355482724&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="2829133918492221101">
  <data key="d0">A generalist agent</data>
  <data key="d1">2829133918492221101</data>
  <data key="d2">https://arxiv.org/abs/2205.06175</data>
  <data key="d3">A generalist agent</data>
  <data key="d4">S Reed, K Zolna, E Parisotto, SG Colmenarejo…</data>
  <data key="d5">2022</data>
  <data key="d6">395</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2829133918492221101&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="12987945369444025427">
  <data key="d0">Vilt: Vision-and-language transformer without convolution or region supervision</data>
  <data key="d1">12987945369444025427</data>
  <data key="d2">https://proceedings.mlr.press/v139/kim21k.html</data>
  <data key="d3">Vilt: Vision-and-language transformer without convolution or region supervision</data>
  <data key="d4">W Kim, B Son, I Kim</data>
  <data key="d5">2021</data>
  <data key="d6">829</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12987945369444025427&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="12604090720681450553">
  <data key="d0">A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt</data>
  <data key="d1">12604090720681450553</data>
  <data key="d2">https://arxiv.org/abs/2303.04226</data>
  <data key="d3">A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt</data>
  <data key="d4">Y Cao, S Li, Y Liu, Z Yan, Y Dai, PS Yu…</data>
  <data key="d5">2023</data>
  <data key="d6">86</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12604090720681450553&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="1440121271646678581">
  <data key="d0">Flava: A foundational language and vision alignment model</data>
  <data key="d1">1440121271646678581</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.html</data>
  <data key="d3">Flava: A foundational language and vision alignment model</data>
  <data key="d4">A Singh, R Hu, V Goswami…</data>
  <data key="d5">2022</data>
  <data key="d6">261</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1440121271646678581&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="9718581961231347788">
  <data key="d0">An empirical study of training end-to-end vision-and-language transformers</data>
  <data key="d1">9718581961231347788</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Dou_An_Empirical_Study_of_Training_End-to-End_Vision-and-Language_Transformers_CVPR_2022_paper.html</data>
  <data key="d3">An empirical study of training end-to-end vision-and-language transformers</data>
  <data key="d4">ZY Dou, Y Xu, Z Gan, J Wang, S Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">196</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9718581961231347788&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="4791913395909773486">
  <data key="d0">Vlmo: Unified vision-language pre-training with mixture-of-modality-experts</data>
  <data key="d1">4791913395909773486</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/d46662aa53e78a62afd980a29e0c37ed-Abstract-Conference.html</data>
  <data key="d3">Vlmo: Unified vision-language pre-training with mixture-of-modality-experts</data>
  <data key="d4">H Bao, W Wang, L Dong, Q Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">174</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4791913395909773486&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="15973126860844704292">
  <data key="d0">Merlot: Multimodal neural script knowledge models</data>
  <data key="d1">15973126860844704292</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/c6d4eb15f1e84a36eff58eca3627c82e-Abstract.html</data>
  <data key="d3">Merlot: Multimodal neural script knowledge models</data>
  <data key="d4">R Zellers, X Lu, J Hessel, Y Yu…</data>
  <data key="d5">2021</data>
  <data key="d6">217</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15973126860844704292&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="3641665820178422490">
  <data key="d0">Winoground: Probing vision and language models for visio-linguistic compositionality</data>
  <data key="d1">3641665820178422490</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Thrush_Winoground_Probing_Vision_and_Language_Models_for_Visio-Linguistic_Compositionality_CVPR_2022_paper.html</data>
  <data key="d3">Winoground: Probing vision and language models for visio-linguistic compositionality</data>
  <data key="d4">T Thrush, R Jiang, M Bartolo, A Singh…</data>
  <data key="d5">2022</data>
  <data key="d6">124</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3641665820178422490&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="3085868247096884162">
  <data key="d0">Scaling up vision-language pre-training for image captioning</data>
  <data key="d1">3085868247096884162</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Hu_Scaling_Up_Vision-Language_Pre-Training_for_Image_Captioning_CVPR_2022_paper.html</data>
  <data key="d3">Scaling up vision-language pre-training for image captioning</data>
  <data key="d4">X Hu, Z Gan, J Wang, Z Yang, Z Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">142</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3085868247096884162&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="16770513324417061228">
  <data key="d0">A survey of uncertainty in deep neural networks</data>
  <data key="d1">16770513324417061228</data>
  <data key="d2">https://link.springer.com/article/10.1007/s10462-023-10562-9</data>
  <data key="d3">A survey of uncertainty in deep neural networks</data>
  <data key="d4">J Gawlikowski, CRN Tassi, M Ali, J Lee, M Humt…</data>
  <data key="d5">2023</data>
  <data key="d6">442</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16770513324417061228&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="7086855047075759373">
  <data key="d0">Remote patient monitoring using artificial intelligence: Current state, applications, and challenges</data>
  <data key="d1">7086855047075759373</data>
  <data key="d2">https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1485</data>
  <data key="d3">Remote patient monitoring using artificial intelligence: Current state, applications, and challenges</data>
  <data key="d4">T Shaik, X Tao, N Higgins, L Li…</data>
  <data key="d5">2023</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7086855047075759373&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="669840749336753525">
  <data key="d0">A comprehensive review of digital twin—part 2: roles of uncertainty quantification and optimization, a battery digital twin, and perspectives</data>
  <data key="d1">669840749336753525</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00158-022-03410-x</data>
  <data key="d3">A comprehensive review of digital twin—part 2: roles of uncertainty quantification and optimization, a battery digital twin, and perspectives</data>
  <data key="d4">A Thelen, X Zhang, O Fink, Y Lu, S Ghosh…</data>
  <data key="d5">2023</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=669840749336753525&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="11718687852373920511">
  <data key="d0">Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons</data>
  <data key="d1">11718687852373920511</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0021999122009652</data>
  <data key="d3">Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons</data>
  <data key="d4">AF Psaros, X Meng, Z Zou, L Guo…</data>
  <data key="d5">2023</data>
  <data key="d6">78</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11718687852373920511&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="2894691460777216219">
  <data key="d0">Towards trustworthy machine fault diagnosis: A probabilistic Bayesian deep learning framework</data>
  <data key="d1">2894691460777216219</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S095183202200179X</data>
  <data key="d3">Towards trustworthy machine fault diagnosis: A probabilistic Bayesian deep learning framework</data>
  <data key="d4">T Zhou, T Han, EL Droguett</data>
  <data key="d5">2022</data>
  <data key="d6">68</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2894691460777216219&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="10327828086531941187">
  <data key="d0">Out-of-distribution detection-assisted trustworthy machinery fault diagnosis approach with uncertainty-aware deep ensembles</data>
  <data key="d1">10327828086531941187</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0951832022002836</data>
  <data key="d3">Out-of-distribution detection-assisted trustworthy machinery fault diagnosis approach with uncertainty-aware deep ensembles</data>
  <data key="d4">T Han, YF Li</data>
  <data key="d5">2022</data>
  <data key="d6">64</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10327828086531941187&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="10956894200939947900">
  <data key="d0">Rambo-rl: Robust adversarial model-based offline reinforcement learning</data>
  <data key="d1">10956894200939947900</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/6691c5e4a199b72dffd9c90acb63bcd6-Abstract-Conference.html</data>
  <data key="d3">Rambo-rl: Robust adversarial model-based offline reinforcement learning</data>
  <data key="d4">M Rigter, B Lacerda, N Hawes</data>
  <data key="d5">2022</data>
  <data key="d6">39</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10956894200939947900&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="6038180113926801086">
  <data key="d0">Uncertainty-aware multiview deep learning for internet of things applications</data>
  <data key="d1">6038180113926801086</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9906001/</data>
  <data key="d3">Uncertainty-aware multiview deep learning for internet of things applications</data>
  <data key="d4">C Xu, W Zhao, J Zhao, Z Guan…</data>
  <data key="d5">2022</data>
  <data key="d6">34</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6038180113926801086&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="14636569081444625016">
  <data key="d0">Generative time series forecasting with diffusion, denoise, and disentanglement</data>
  <data key="d1">14636569081444625016</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/91a85f3fb8f570e6be52b333b5ab017a-Abstract-Conference.html</data>
  <data key="d3">Generative time series forecasting with diffusion, denoise, and disentanglement</data>
  <data key="d4">Y Li, X Lu, Y Wang, D Dou</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14636569081444625016&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="9643084522903856394">
  <data key="d0">Probabilistic deep learning for real-time large deformation simulations</data>
  <data key="d1">9643084522903856394</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S004578252200411X</data>
  <data key="d3">Probabilistic deep learning for real-time large deformation simulations</data>
  <data key="d4">S Deshpande, J Lengiewicz, SPA Bordas</data>
  <data key="d5">2022</data>
  <data key="d6">31</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9643084522903856394&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="17266336177445157331">
  <data key="d0">Uncertainty quantification in DenseNet model using myocardial infarction ECG signals</data>
  <data key="d1">17266336177445157331</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0169260722006897</data>
  <data key="d3">Uncertainty quantification in DenseNet model using myocardial infarction ECG signals</data>
  <data key="d4">V Jahmunah, EYK Ng, RS Tan, SL Oh…</data>
  <data key="d5">2023</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17266336177445157331&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">12</data>
</node>
<node id="9063880872255850171">
  <data key="d0">Training generative adversarial networks with limited data</data>
  <data key="d1">9063880872255850171</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2020/hash/8d30aa96e72440759f74bd2306c1fa3d-Abstract.html</data>
  <data key="d3">Training generative adversarial networks with limited data</data>
  <data key="d4">T Karras, M Aittala, J Hellsten, S Laine…</data>
  <data key="d5">2020</data>
  <data key="d6">1303</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9063880872255850171&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="13909256571563279522">
  <data key="d0">Text data augmentation for deep learning</data>
  <data key="d1">13909256571563279522</data>
  <data key="d2">https://link.springer.com/article/10.1186/s40537-021-00492-0</data>
  <data key="d3">Text data augmentation for deep learning</data>
  <data key="d4">C Shorten, TM Khoshgoftaar, B Furht</data>
  <data key="d5">2021</data>
  <data key="d6">215</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13909256571563279522&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="6594876225658804365">
  <data key="d0">Deep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models</data>
  <data key="d1">6594876225658804365</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9555209/</data>
  <data key="d3">Deep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models</data>
  <data key="d4">S Bond-Taylor, A Leach, Y Long…</data>
  <data key="d5">2021</data>
  <data key="d6">238</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6594876225658804365&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="17368705487922251039">
  <data key="d0">Alias-free generative adversarial networks</data>
  <data key="d1">17368705487922251039</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/076ccd93ad68be51f23707988e934906-Abstract.html</data>
  <data key="d3">Alias-free generative adversarial networks</data>
  <data key="d4">T Karras, M Aittala, S Laine…</data>
  <data key="d5">2021</data>
  <data key="d6">923</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17368705487922251039&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="5258718823597512255">
  <data key="d0">Elucidating the design space of diffusion-based generative models</data>
  <data key="d1">5258718823597512255</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html</data>
  <data key="d3">Elucidating the design space of diffusion-based generative models</data>
  <data key="d4">T Karras, M Aittala, T Aila…</data>
  <data key="d5">2022</data>
  <data key="d6">283</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5258718823597512255&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="7080475698203068086">
  <data key="d0">Efficient geometry-aware 3D generative adversarial networks</data>
  <data key="d1">7080475698203068086</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Chan_Efficient_Geometry-Aware_3D_Generative_Adversarial_Networks_CVPR_2022_paper.html</data>
  <data key="d3">Efficient geometry-aware 3D generative adversarial networks</data>
  <data key="d4">ER Chan, CZ Lin, MA Chan…</data>
  <data key="d5">2022</data>
  <data key="d6">451</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7080475698203068086&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="622631041436591387">
  <data key="d0">Denoising diffusion probabilistic models</data>
  <data key="d1">622631041436591387</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html</data>
  <data key="d3">Denoising diffusion probabilistic models</data>
  <data key="d4">J Ho, A Jain, P Abbeel</data>
  <data key="d5">2020</data>
  <data key="d6">3757</data>
  <data key="d7">https://scholar.google.com/scholar?cites=622631041436591387&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="12650581806598225058">
  <data key="d0">Taming transformers for high-resolution image synthesis</data>
  <data key="d1">12650581806598225058</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">Taming transformers for high-resolution image synthesis</data>
  <data key="d4">P Esser, R Rombach, B Ommer</data>
  <data key="d5">2021</data>
  <data key="d6">1103</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12650581806598225058&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="13161498921981862309">
  <data key="d0">Card: Classification and regression diffusion models</data>
  <data key="d1">13161498921981862309</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/72dad95a24fae750f8ab1cb3dab5e58d-Abstract-Conference.html</data>
  <data key="d3">Card: Classification and regression diffusion models</data>
  <data key="d4">X Han, H Zheng, M Zhou</data>
  <data key="d5">2022</data>
  <data key="d6">372</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13161498921981862309&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="14592788616550656262">
  <data key="d0">Score-based generative modeling through stochastic differential equations</data>
  <data key="d1">14592788616550656262</data>
  <data key="d2">https://arxiv.org/abs/2011.13456</data>
  <data key="d3">Score-based generative modeling through stochastic differential equations</data>
  <data key="d4">Y Song, J Sohl-Dickstein, DP Kingma, A Kumar…</data>
  <data key="d5">2020</data>
  <data key="d6">1619</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14592788616550656262&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="4031590341915143464">
  <data key="d0">Styleclip: Text-driven manipulation of stylegan imagery</data>
  <data key="d1">4031590341915143464</data>
  <data key="d2">http://openaccess.thecvf.com/content/ICCV2021/html/Patashnik_StyleCLIP_Text-Driven_Manipulation_of_StyleGAN_Imagery_ICCV_2021_paper.html</data>
  <data key="d3">Styleclip: Text-driven manipulation of stylegan imagery</data>
  <data key="d4">O Patashnik, Z Wu, E Shechtman…</data>
  <data key="d5">2021</data>
  <data key="d6">699</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4031590341915143464&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">13</data>
</node>
<node id="2321980635951558135">
  <data key="d0">Simple copy-paste is a strong data augmentation method for instance segmentation</data>
  <data key="d1">2321980635951558135</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2021/html/Ghiasi_Simple_Copy-Paste_Is_a_Strong_Data_Augmentation_Method_for_Instance_CVPR_2021_paper.html?ref=https://githubhelp.com</data>
  <data key="d3">Simple copy-paste is a strong data augmentation method for instance segmentation</data>
  <data key="d4">G Ghiasi, Y Cui, A Srinivas, R Qian…</data>
  <data key="d5">2021</data>
  <data key="d6">670</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2321980635951558135&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="15580172266410736369">
  <data key="d0">A comprehensive survey of image augmentation techniques for deep learning</data>
  <data key="d1">15580172266410736369</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0031320323000481</data>
  <data key="d3">A comprehensive survey of image augmentation techniques for deep learning</data>
  <data key="d4">M Xu, S Yoon, A Fuentes, DS Park</data>
  <data key="d5">2023</data>
  <data key="d6">71</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15580172266410736369&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="1177161480824869016">
  <data key="d0">Deep learning for neuroimaging-based diagnosis and rehabilitation of autism spectrum disorder: a review</data>
  <data key="d1">1177161480824869016</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0010482521007435</data>
  <data key="d3">Deep learning for neuroimaging-based diagnosis and rehabilitation of autism spectrum disorder: a review</data>
  <data key="d4">M Khodatars, A Shoeibi, D Sadeghi…</data>
  <data key="d5">2021</data>
  <data key="d6">130</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1177161480824869016&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="11531242419091815801">
  <data key="d0">Yolox: Exceeding yolo series in 2021</data>
  <data key="d1">11531242419091815801</data>
  <data key="d2">https://arxiv.org/abs/2107.08430</data>
  <data key="d3">Yolox: Exceeding yolo series in 2021</data>
  <data key="d4">Z Ge, S Liu, F Wang, Z Li, J Sun</data>
  <data key="d5">2021</data>
  <data key="d6">2232</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11531242419091815801&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="15329990143169836178">
  <data key="d0">Swin transformer v2: Scaling up capacity and resolution</data>
  <data key="d1">15329990143169836178</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.html</data>
  <data key="d3">Swin transformer v2: Scaling up capacity and resolution</data>
  <data key="d4">Z Liu, H Hu, Y Lin, Z Yao, Z Xie, Y Wei…</data>
  <data key="d5">2022</data>
  <data key="d6">669</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15329990143169836178&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="17018195497378444438">
  <data key="d0">Simmim: A simple framework for masked image modeling</data>
  <data key="d1">17018195497378444438</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Xie_SimMIM_A_Simple_Framework_for_Masked_Image_Modeling_CVPR_2022_paper.html</data>
  <data key="d3">Simmim: A simple framework for masked image modeling</data>
  <data key="d4">Z Xie, Z Zhang, Y Cao, Y Lin, J Bao…</data>
  <data key="d5">2022</data>
  <data key="d6">563</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17018195497378444438&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="10375739191012965737">
  <data key="d0">Masked-attention mask transformer for universal image segmentation</data>
  <data key="d1">10375739191012965737</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.html</data>
  <data key="d3">Masked-attention mask transformer for universal image segmentation</data>
  <data key="d4">B Cheng, I Misra, AG Schwing…</data>
  <data key="d5">2022</data>
  <data key="d6">603</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10375739191012965737&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4490918786976048296">
  <data key="d0">Exploring plain vision transformer backbones for object detection</data>
  <data key="d1">4490918786976048296</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20077-9_17</data>
  <data key="d3">Exploring plain vision transformer backbones for object detection</data>
  <data key="d4">Y Li, H Mao, R Girshick, K He</data>
  <data key="d5">2022</data>
  <data key="d6">285</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4490918786976048296&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="6004268348151288098">
  <data key="d0">Grounded language-image pre-training</data>
  <data key="d1">6004268348151288098</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2022/html/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.html?ref=blog.roboflow.com</data>
  <data key="d3">Grounded language-image pre-training</data>
  <data key="d4">LH Li, P Zhang, H Zhang, J Yang, C Li…</data>
  <data key="d5">2022</data>
  <data key="d6">318</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6004268348151288098&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="862169174991977666">
  <data key="d0">A survey of data augmentation approaches for NLP</data>
  <data key="d1">862169174991977666</data>
  <data key="d2">https://arxiv.org/abs/2105.03075</data>
  <data key="d3">A survey of data augmentation approaches for NLP</data>
  <data key="d4">SY Feng, V Gangal, J Wei, S Chandar…</data>
  <data key="d5">2021</data>
  <data key="d6">490</data>
  <data key="d7">https://scholar.google.com/scholar?cites=862169174991977666&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="11748945572016694012">
  <data key="d0">Data augmentation approaches in natural language processing: A survey</data>
  <data key="d1">11748945572016694012</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2666651022000080</data>
  <data key="d3">Data augmentation approaches in natural language processing: A survey</data>
  <data key="d4">B Li, Y Hou, W Che</data>
  <data key="d5">2022</data>
  <data key="d6">113</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11748945572016694012&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="3667217905602891315">
  <data key="d0">A survey on data augmentation for text classification</data>
  <data key="d1">3667217905602891315</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3544558</data>
  <data key="d3">A survey on data augmentation for text classification</data>
  <data key="d4">M Bayer, MA Kaufhold, C Reuter</data>
  <data key="d5">2022</data>
  <data key="d6">160</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3667217905602891315&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="2501577020384058220">
  <data key="d0">Natural language processing applied to mental illness detection: a narrative review</data>
  <data key="d1">2501577020384058220</data>
  <data key="d2">https://www.nature.com/articles/s41746-022-00589-7</data>
  <data key="d3">Natural language processing applied to mental illness detection: a narrative review</data>
  <data key="d4">T Zhang, AM Schoene, S Ji, S Ananiadou</data>
  <data key="d5">2022</data>
  <data key="d6">68</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2501577020384058220&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="8511009105279544089">
  <data key="d0">Data augmentation as feature manipulation</data>
  <data key="d1">8511009105279544089</data>
  <data key="d2">https://proceedings.mlr.press/v162/shen22a.html</data>
  <data key="d3">Data augmentation as feature manipulation</data>
  <data key="d4">R Shen, S Bubeck…</data>
  <data key="d5">2022</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8511009105279544089&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="13145127891619293871">
  <data key="d0">An empirical survey of data augmentation for limited data learning in NLP</data>
  <data key="d1">13145127891619293871</data>
  <data key="d2">https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00542/115238</data>
  <data key="d3">An empirical survey of data augmentation for limited data learning in NLP</data>
  <data key="d4">J Chen, D Tam, C Raffel, M Bansal…</data>
  <data key="d5">2023</data>
  <data key="d6">67</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13145127891619293871&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="6584420285295374856">
  <data key="d0">Graph data augmentation for graph machine learning: A survey</data>
  <data key="d1">6584420285295374856</data>
  <data key="d2">https://arxiv.org/abs/2202.08871</data>
  <data key="d3">Graph data augmentation for graph machine learning: A survey</data>
  <data key="d4">T Zhao, W Jin, Y Liu, Y Wang, G Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">54</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6584420285295374856&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="11074972199329338101">
  <data key="d0">Measure and improve robustness in nlp models: A survey</data>
  <data key="d1">11074972199329338101</data>
  <data key="d2">https://arxiv.org/abs/2112.08313</data>
  <data key="d3">Measure and improve robustness in nlp models: A survey</data>
  <data key="d4">X Wang, H Wang, D Yang</data>
  <data key="d5">2021</data>
  <data key="d6">45</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11074972199329338101&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="15873591660981920322">
  <data key="d0">Spectral feature augmentation for graph contrastive learning and beyond</data>
  <data key="d1">15873591660981920322</data>
  <data key="d2">https://ojs.aaai.org/index.php/AAAI/article/view/26336</data>
  <data key="d3">Spectral feature augmentation for graph contrastive learning and beyond</data>
  <data key="d4">Y Zhang, H Zhu, Z Song, P Koniusz…</data>
  <data key="d5">2023</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15873591660981920322&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="13102079179802671182">
  <data key="d0">Chataug: Leveraging chatgpt for text data augmentation</data>
  <data key="d1">13102079179802671182</data>
  <data key="d2">https://arxiv.org/abs/2302.13007</data>
  <data key="d3">Chataug: Leveraging chatgpt for text data augmentation</data>
  <data key="d4">H Dai, Z Liu, W Liao, X Huang, Z Wu, L Zhao…</data>
  <data key="d5">2023</data>
  <data key="d6">42</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13102079179802671182&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="3720496172950573884">
  <data key="d0">Curriculum contrastive context denoising for few-shot conversational dense retrieval</data>
  <data key="d1">3720496172950573884</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3477495.3531961</data>
  <data key="d3">Curriculum contrastive context denoising for few-shot conversational dense retrieval</data>
  <data key="d4">K Mao, Z Dou, H Qian</data>
  <data key="d5">2022</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3720496172950573884&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">14</data>
</node>
<node id="2026726248513085794">
  <data key="d0">YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</data>
  <data key="d1">2026726248513085794</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Wang_YOLOv7_Trainable_Bag-of-Freebies_Sets_New_State-of-the-Art_for_Real-Time_Object_Detectors_CVPR_2023_paper.html</data>
  <data key="d3">YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</data>
  <data key="d4">CY Wang, A Bochkovskiy…</data>
  <data key="d5">2023</data>
  <data key="d6">1822</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2026726248513085794&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="9850512646184180167">
  <data key="d0">Object detection in 20 years: A survey</data>
  <data key="d1">9850512646184180167</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10028728/</data>
  <data key="d3">Object detection in 20 years: A survey</data>
  <data key="d4">Z Zou, K Chen, Z Shi, Y Guo, J Ye</data>
  <data key="d5">2023</data>
  <data key="d6">1601</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9850512646184180167&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="13405798017275933263">
  <data key="d0">A survey of human-in-the-loop for machine learning</data>
  <data key="d1">13405798017275933263</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0167739X22001790</data>
  <data key="d3">A survey of human-in-the-loop for machine learning</data>
  <data key="d4">X Wu, L Xiao, Y Sun, J Zhang, T Ma, L He</data>
  <data key="d5">2022</data>
  <data key="d6">214</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13405798017275933263&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="5669364687087032958">
  <data key="d0">A survey of deep learning-based object detection</data>
  <data key="d1">5669364687087032958</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/8825470/</data>
  <data key="d3">A survey of deep learning-based object detection</data>
  <data key="d4">L Jiao, F Zhang, F Liu, S Yang, L Li, Z Feng…</data>
  <data key="d5">2019</data>
  <data key="d6">1054</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5669364687087032958&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="1319826694668830613">
  <data key="d0">Dynamic head: Unifying object detection heads with attentions</data>
  <data key="d1">1319826694668830613</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2021/html/Dai_Dynamic_Head_Unifying_Object_Detection_Heads_With_Attentions_CVPR_2021_paper.html</data>
  <data key="d3">Dynamic head: Unifying object detection heads with attentions</data>
  <data key="d4">X Dai, Y Chen, B Xiao, D Chen, M Liu…</data>
  <data key="d5">2021</data>
  <data key="d6">274</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1319826694668830613&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="8384042348747306199">
  <data key="d0">A comparative analysis of object detection metrics with a companion open-source toolkit</data>
  <data key="d1">8384042348747306199</data>
  <data key="d2">https://www.mdpi.com/2079-9292/10/3/279</data>
  <data key="d3">A comparative analysis of object detection metrics with a companion open-source toolkit</data>
  <data key="d4">R Padilla, WL Passos, TLB Dias, SL Netto…</data>
  <data key="d5">2021</data>
  <data key="d6">343</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8384042348747306199&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="17642928594905005472">
  <data key="d0">Recent advances in small object detection based on deep learning: A review</data>
  <data key="d1">17642928594905005472</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0262885620300421</data>
  <data key="d3">Recent advances in small object detection based on deep learning: A review</data>
  <data key="d4">K Tong, Y Wu, F Zhou</data>
  <data key="d5">2020</data>
  <data key="d6">316</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17642928594905005472&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="8762454778937977659">
  <data key="d0">Imbalance problems in object detection: A review</data>
  <data key="d1">8762454778937977659</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9042296/</data>
  <data key="d3">Imbalance problems in object detection: A review</data>
  <data key="d4">K Oksuz, BC Cam, S Kalkan…</data>
  <data key="d5">2020</data>
  <data key="d6">369</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8762454778937977659&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="10949414717109961618">
  <data key="d0">YOLOv4-5D: An effective and efficient object detector for autonomous driving</data>
  <data key="d1">10949414717109961618</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9374990/</data>
  <data key="d3">YOLOv4-5D: An effective and efficient object detector for autonomous driving</data>
  <data key="d4">Y Cai, T Luan, H Gao, H Wang, L Chen…</data>
  <data key="d5">2021</data>
  <data key="d6">200</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10949414717109961618&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="4438492435191836535">
  <data key="d0">Real-time detection of uneaten feed pellets in underwater images for aquaculture using an improved YOLO-V4 network</data>
  <data key="d1">4438492435191836535</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169921001538</data>
  <data key="d3">Real-time detection of uneaten feed pellets in underwater images for aquaculture using an improved YOLO-V4 network</data>
  <data key="d4">X Hu, Y Liu, Z Zhao, J Liu, X Yang, C Sun…</data>
  <data key="d5">2021</data>
  <data key="d6">129</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4438492435191836535&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="2241015688519496377">
  <data key="d0">Deep learning-based object detection in low-altitude UAV datasets: A survey</data>
  <data key="d1">2241015688519496377</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0262885620301785</data>
  <data key="d3">Deep learning-based object detection in low-altitude UAV datasets: A survey</data>
  <data key="d4">P Mittal, R Singh, A Sharma</data>
  <data key="d5">2020</data>
  <data key="d6">150</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2241015688519496377&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="14638466021176544465">
  <data key="d0">Bytetrack: Multi-object tracking by associating every detection box</data>
  <data key="d1">14638466021176544465</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20047-2_1</data>
  <data key="d3">Bytetrack: Multi-object tracking by associating every detection box</data>
  <data key="d4">Y Zhang, P Sun, Y Jiang, D Yu, F Weng, Z Yuan…</data>
  <data key="d5">2022</data>
  <data key="d6">515</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14638466021176544465&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="15866259573388647937">
  <data key="d0">Motr: End-to-end multiple-object tracking with transformer</data>
  <data key="d1">15866259573388647937</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19812-0_38</data>
  <data key="d3">Motr: End-to-end multiple-object tracking with transformer</data>
  <data key="d4">F Zeng, B Dong, Y Zhang, T Wang, X Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">231</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15866259573388647937&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="2093389731615411975">
  <data key="d0">Observation-centric sort: Rethinking sort for robust multi-object tracking</data>
  <data key="d1">2093389731615411975</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Cao_Observation-Centric_SORT_Rethinking_SORT_for_Robust_Multi-Object_Tracking_CVPR_2023_paper.html</data>
  <data key="d3">Observation-centric sort: Rethinking sort for robust multi-object tracking</data>
  <data key="d4">J Cao, J Pang, X Weng…</data>
  <data key="d5">2023</data>
  <data key="d6">136</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2093389731615411975&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="6724748843400977919">
  <data key="d0">Aiatrack: Attention in attention for transformer visual tracking</data>
  <data key="d1">6724748843400977919</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-20047-2_9</data>
  <data key="d3">Aiatrack: Attention in attention for transformer visual tracking</data>
  <data key="d4">S Gao, C Zhou, C Ma, X Wang, J Yuan</data>
  <data key="d5">2022</data>
  <data key="d6">66</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6724748843400977919&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="16069829188377130053">
  <data key="d0">In defense of online models for video instance segmentation</data>
  <data key="d1">16069829188377130053</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19815-1_34</data>
  <data key="d3">In defense of online models for video instance segmentation</data>
  <data key="d4">J Wu, Q Liu, Y Jiang, S Bai, A Yuille, X Bai</data>
  <data key="d5">2022</data>
  <data key="d6">49</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16069829188377130053&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="14300935760162828522">
  <data key="d0">Towards grand unification of object tracking</data>
  <data key="d1">14300935760162828522</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19803-8_43</data>
  <data key="d3">Towards grand unification of object tracking</data>
  <data key="d4">B Yan, Y Jiang, P Sun, D Wang, Z Yuan, P Luo…</data>
  <data key="d5">2022</data>
  <data key="d6">68</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14300935760162828522&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="14167857935364818481">
  <data key="d0">Strongsort: Make deepsort great again</data>
  <data key="d1">14167857935364818481</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10032656/</data>
  <data key="d3">Strongsort: Make deepsort great again</data>
  <data key="d4">Y Du, Z Zhao, Y Song, Y Zhao, F Su…</data>
  <data key="d5">2023</data>
  <data key="d6">145</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14167857935364818481&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="17632693958287984620">
  <data key="d0">The 7th AI City Challenge</data>
  <data key="d1">17632693958287984620</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Naphade_The_7th_AI_City_Challenge_CVPRW_2023_paper.html</data>
  <data key="d3">The 7th AI City Challenge</data>
  <data key="d4">M Naphade, S Wang, DC Anastasiu…</data>
  <data key="d5">2023</data>
  <data key="d6">188</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17632693958287984620&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="9529319158101525799">
  <data key="d0">Dancetrack: Multi-object tracking in uniform appearance and diverse motion</data>
  <data key="d1">9529319158101525799</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Sun_DanceTrack_Multi-Object_Tracking_in_Uniform_Appearance_and_Diverse_Motion_CVPR_2022_paper.html</data>
  <data key="d3">Dancetrack: Multi-object tracking in uniform appearance and diverse motion</data>
  <data key="d4">P Sun, J Cao, Y Jiang, Z Yuan, S Bai…</data>
  <data key="d5">2022</data>
  <data key="d6">70</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9529319158101525799&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="13702720529764835843">
  <data key="d0">YOLOv6: A single-stage object detection framework for industrial applications</data>
  <data key="d1">13702720529764835843</data>
  <data key="d2">https://arxiv.org/abs/2209.02976</data>
  <data key="d3">YOLOv6: A single-stage object detection framework for industrial applications</data>
  <data key="d4">C Li, L Li, H Jiang, K Weng, Y Geng, L Li, Z Ke…</data>
  <data key="d5">2022</data>
  <data key="d6">464</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13702720529764835843&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">21</data>
</node>
<node id="17646032300901507453">
  <data key="d0">Optimization strategies of fruit detection to overcome the challenge of unstructured background in field orchard environment: A review</data>
  <data key="d1">17646032300901507453</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11119-023-10009-9</data>
  <data key="d3">Optimization strategies of fruit detection to overcome the challenge of unstructured background in field orchard environment: A review</data>
  <data key="d4">Y Tang, J Qiu, Y Zhang, D Wu, Y Cao, K Zhao…</data>
  <data key="d5">2023</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17646032300901507453&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="8819196546794680544">
  <data key="d0">A comprehensive review of YOLO: From YOLOv1 to YOLOv8 and beyond</data>
  <data key="d1">8819196546794680544</data>
  <data key="d2">https://arxiv.org/abs/2304.00501</data>
  <data key="d3">A comprehensive review of YOLO: From YOLOv1 to YOLOv8 and beyond</data>
  <data key="d4">J Terven, D Cordova-Esparza</data>
  <data key="d5">2023</data>
  <data key="d6">84</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8819196546794680544&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">21</data>
</node>
<node id="17785166320928157068">
  <data key="d0">Rtmdet: An empirical study of designing real-time object detectors</data>
  <data key="d1">17785166320928157068</data>
  <data key="d2">https://arxiv.org/abs/2212.07784</data>
  <data key="d3">Rtmdet: An empirical study of designing real-time object detectors</data>
  <data key="d4">C Lyu, W Zhang, H Huang, Y Zhou, Y Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">36</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17785166320928157068&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">21</data>
</node>
<node id="5875186414038374408">
  <data key="d0">Hybrid-YOLO for classification of insulators defects in transmission lines based on UAV</data>
  <data key="d1">5875186414038374408</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S014206152300039X</data>
  <data key="d3">Hybrid-YOLO for classification of insulators defects in transmission lines based on UAV</data>
  <data key="d4">BJ Souza, SF Stefenon, G Singh, RZ Freire</data>
  <data key="d5">2023</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5875186414038374408&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">21</data>
</node>
<node id="958587697277086390">
  <data key="d0">Yolov6 v3. 0: A full-scale reloading</data>
  <data key="d1">958587697277086390</data>
  <data key="d2">https://arxiv.org/abs/2301.05586</data>
  <data key="d3">Yolov6 v3. 0: A full-scale reloading</data>
  <data key="d4">C Li, L Li, Y Geng, H Jiang, M Cheng, B Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">41</data>
  <data key="d7">https://scholar.google.com/scholar?cites=958587697277086390&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">21</data>
</node>
<node id="17898472391686934449">
  <data key="d0">Deep object detection of crop weeds: Performance of YOLOv7 on a real case dataset from UAV images</data>
  <data key="d1">17898472391686934449</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/2/539?ref=blog.roboflow.com</data>
  <data key="d3">Deep object detection of crop weeds: Performance of YOLOv7 on a real case dataset from UAV images</data>
  <data key="d4">I Gallo, AU Rehman, RH Dehkordi, N Landro…</data>
  <data key="d5">2023</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17898472391686934449&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">21</data>
</node>
<node id="15900202697333596864">
  <data key="d0">Helmet wearing detection of motorcycle drivers using deep learning network with residual transformer-spatial attention</data>
  <data key="d1">15900202697333596864</data>
  <data key="d2">https://www.mdpi.com/2504-446X/6/12/415</data>
  <data key="d3">Helmet wearing detection of motorcycle drivers using deep learning network with residual transformer-spatial attention</data>
  <data key="d4">S Chen, J Lan, H Liu, C Chen, X Wang</data>
  <data key="d5">2022</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15900202697333596864&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">21</data>
</node>
<node id="16133934620382970642">
  <data key="d0">Stall number detection of cow teats key frames</data>
  <data key="d1">16133934620382970642</data>
  <data key="d2">https://arxiv.org/abs/2303.10444</data>
  <data key="d3">Stall number detection of cow teats key frames</data>
  <data key="d4">Y Zhang</data>
  <data key="d5">2023</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16133934620382970642&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">21</data>
</node>
<node id="15271212594475018431">
  <data key="d0">Adversarial patch attack on multi-scale object detection for uav remote sensing images</data>
  <data key="d1">15271212594475018431</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/21/5298</data>
  <data key="d3">Adversarial patch attack on multi-scale object detection for uav remote sensing images</data>
  <data key="d4">Y Zhang, Y Zhang, J Qi, K Bin, H Wen, X Tong…</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15271212594475018431&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">21</data>
</node>
<node id="10116304010765610972">
  <data key="d0">Gbh-yolov5: Ghost convolution with bottleneckcsp and tiny target prediction head incorporating yolov5 for pv panel defect detection</data>
  <data key="d1">10116304010765610972</data>
  <data key="d2">https://www.mdpi.com/2079-9292/12/3/561</data>
  <data key="d3">Gbh-yolov5: Ghost convolution with bottleneckcsp and tiny target prediction head incorporating yolov5 for pv panel defect detection</data>
  <data key="d4">L Li, Z Wang, T Zhang</data>
  <data key="d5">2023</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10116304010765610972&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">21</data>
</node>
<node id="18022128353994651475">
  <data key="d0">Spatial-Temporal Semantic Perception Network for Remote Sensing Image Semantic Change Detection</data>
  <data key="d1">18022128353994651475</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/16/4095</data>
  <data key="d3">Spatial-Temporal Semantic Perception Network for Remote Sensing Image Semantic Change Detection</data>
  <data key="d4">Y He, H Zhang, X Ning, R Zhang, D Chang, M Hao</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18022128353994651475&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="2DdH1BLUQaIJ">
  <data key="d0">Towards the synthesis of spectral imaging and machine learning-based approaches for non-invasive phenotyping of plants</data>
  <data key="d1">2DdH1BLUQaIJ</data>
  <data key="d2">https://link.springer.com/article/10.1007/s12551-023-01125-x</data>
  <data key="d3">Towards the synthesis of spectral imaging and machine learning-based approaches for non-invasive phenotyping of plants</data>
  <data key="d4">A Solovchenko, B Shurygin, DA Nesterov…</data>
  <data key="d5">2023</data>
  <data key="d8">5</data>
</node>
<node id="4749208375417131840">
  <data key="d0">An Accurate Forest Fire Recognition Method Based on Improved BPNN and IoT</data>
  <data key="d1">4749208375417131840</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/9/2365</data>
  <data key="d3">An Accurate Forest Fire Recognition Method Based on Improved BPNN and IoT</data>
  <data key="d4">S Zheng, P Gao, Y Zhou, Z Wu, L Wan, F Hu, W Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4749208375417131840&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="7487952808605114076">
  <data key="d0">Unstructured road extraction and roadside fruit recognition in grape orchards based on a synchronous detection algorithm</data>
  <data key="d1">7487952808605114076</data>
  <data key="d2">https://www.frontiersin.org/articles/10.3389/fpls.2023.1103276/full</data>
  <data key="d3">Unstructured road extraction and roadside fruit recognition in grape orchards based on a synchronous detection algorithm</data>
  <data key="d4">X Zhou, X Zou, W Tang, Z Yan, H Meng…</data>
  <data key="d5">2023</data>
  <data key="d8">5</data>
</node>
<node id="10897720152285769060">
  <data key="d0">Grape-bunch identification and location of picking points on occluded fruit axis based on YOLOv5-GAP</data>
  <data key="d1">10897720152285769060</data>
  <data key="d2">https://www.mdpi.com/2311-7524/9/4/498</data>
  <data key="d3">Grape-bunch identification and location of picking points on occluded fruit axis based on YOLOv5-GAP</data>
  <data key="d4">T Zhang, F Wu, M Wang, Z Chen, L Li, X Zou</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10897720152285769060&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="pCHwc0wrutcJ">
  <data key="d0">SOD head: A network for locating small fruits from top to bottom in layers of feature maps</data>
  <data key="d1">pCHwc0wrutcJ</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923005215</data>
  <data key="d3">SOD head: A network for locating small fruits from top to bottom in layers of feature maps</data>
  <data key="d4">Y Lu, M Sun, Y Guan, J Lian, Z Ji, X Yin…</data>
  <data key="d5">2023</data>
  <data key="d8">5</data>
</node>
<node id="AtRUthsSjPcJ">
  <data key="d0">Nondestructive Detection of Egg Freshness Based on Infrared Thermal Imaging</data>
  <data key="d1">AtRUthsSjPcJ</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/12/5530</data>
  <data key="d3">Nondestructive Detection of Egg Freshness Based on Infrared Thermal Imaging</data>
  <data key="d4">J Zhang, W Lu, X Jian, Q Hu, D Dai</data>
  <data key="d5">2023</data>
  <data key="d8">5</data>
</node>
<node id="1291063805327774306">
  <data key="d0">DenseNet-201 and Xception Pre-Trained Deep Learning Models for Fruit Recognition</data>
  <data key="d1">1291063805327774306</data>
  <data key="d2">https://www.mdpi.com/2079-9292/12/14/3132</data>
  <data key="d3">DenseNet-201 and Xception Pre-Trained Deep Learning Models for Fruit Recognition</data>
  <data key="d4">F Salim, F Saeed, S Basurra, SN Qasem…</data>
  <data key="d5">2023</data>
  <data key="d8">5</data>
</node>
<node id="17567167024662762007">
  <data key="d0">Deep Learning in Precision Agriculture: Artificially Generated VNIR Images Segmentation for Early Postharvest Decay Prediction in Apples</data>
  <data key="d1">17567167024662762007</data>
  <data key="d2">https://www.mdpi.com/1099-4300/25/7/987</data>
  <data key="d3">Deep Learning in Precision Agriculture: Artificially Generated VNIR Images Segmentation for Early Postharvest Decay Prediction in Apples</data>
  <data key="d4">N Stasenko, I Shukhratov, M Savinov, D Shadrin…</data>
  <data key="d5">2023</data>
  <data key="d8">5</data>
</node>
<node id="10347681032777129532">
  <data key="d0">Rapid detection of Yunnan Xiaomila based on lightweight YOLOv7 algorithm</data>
  <data key="d1">10347681032777129532</data>
  <data key="d2">https://www.frontiersin.org/articles/10.3389/fpls.2023.1200144/full</data>
  <data key="d3">Rapid detection of Yunnan Xiaomila based on lightweight YOLOv7 algorithm</data>
  <data key="d4">F Wang, J Jiang, Y Chen, Z Sun, Y Tang…</data>
  <data key="d5">2023</data>
  <data key="d8">5</data>
</node>
<node id="17127958062872744853">
  <data key="d0">YOLOWeeds: a novel benchmark of YOLO object detectors for multi-class weed detection in cotton production systems</data>
  <data key="d1">17127958062872744853</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923000431</data>
  <data key="d3">YOLOWeeds: a novel benchmark of YOLO object detectors for multi-class weed detection in cotton production systems</data>
  <data key="d4">F Dang, D Chen, Y Lu, Z Li</data>
  <data key="d5">2023</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17127958062872744853&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="10256791187069118291">
  <data key="d0">Performance evaluation of deep learning object detectors for weed detection for cotton</data>
  <data key="d1">10256791187069118291</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2772375522000910</data>
  <data key="d3">Performance evaluation of deep learning object detectors for weed detection for cotton</data>
  <data key="d4">A Rahman, Y Lu, H Wang</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10256791187069118291&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="15552447685803913675">
  <data key="d0">More eyes on the prize: open-source data, software and hardware for advancing plant science through collaboration</data>
  <data key="d1">15552447685803913675</data>
  <data key="d2">https://academic.oup.com/aobpla/advance-article/doi/10.1093/aobpla/plad010/7073680</data>
  <data key="d3">More eyes on the prize: open-source data, software and hardware for advancing plant science through collaboration</data>
  <data key="d4">GRY Coleman, WT Salter</data>
  <data key="d5">2023</data>
  <data key="d8">8</data>
</node>
<node id="5222861253143202947">
  <data key="d0">Evaluation of YOLO Object Detectors for Weed Detection in Different Turfgrass Scenarios</data>
  <data key="d1">5222861253143202947</data>
  <data key="d2">https://www.mdpi.com/2076-3417/13/14/8502</data>
  <data key="d3">Evaluation of YOLO Object Detectors for Weed Detection in Different Turfgrass Scenarios</data>
  <data key="d4">M Sportelli, OE Apolo-Apolo, M Fontanelli, C Frasconi…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5222861253143202947&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="2712659024700086363">
  <data key="d0">Integration and preliminary evaluation of a robotic cotton harvester prototype</data>
  <data key="d1">2712659024700086363</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923003319</data>
  <data key="d3">Integration and preliminary evaluation of a robotic cotton harvester prototype</data>
  <data key="d4">H Gharakhani, JA Thomasson, Y Lu</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2712659024700086363&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="11270960296740375515">
  <data key="d0">Label-Efficient Learning in Agriculture: A Comprehensive Review</data>
  <data key="d1">11270960296740375515</data>
  <data key="d2">https://arxiv.org/abs/2305.14691</data>
  <data key="d3">Label-Efficient Learning in Agriculture: A Comprehensive Review</data>
  <data key="d4">J Li, D Chen, X Qi, Z Li, Y Huang, D Morris…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11270960296740375515&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="7774766038187589626">
  <data key="d0">High-Precision Tomato Disease Detection Using NanoSegmenter Based on Transformer and Lightweighting</data>
  <data key="d1">7774766038187589626</data>
  <data key="d2">https://www.mdpi.com/2223-7747/12/13/2559</data>
  <data key="d3">High-Precision Tomato Disease Detection Using NanoSegmenter Based on Transformer and Lightweighting</data>
  <data key="d4">Y Liu, Y Song, R Ye, S Zhu, Y Huang, T Chen, J Zhou…</data>
  <data key="d5">2023</data>
  <data key="d8">8</data>
</node>
<node id="2114425240534661867">
  <data key="d0">Automatic Detection of Pedestrian Crosswalk with Faster R-CNN and YOLOv7</data>
  <data key="d1">2114425240534661867</data>
  <data key="d2">https://www.mdpi.com/2075-5309/13/4/1070</data>
  <data key="d3">Automatic Detection of Pedestrian Crosswalk with Faster R-CNN and YOLOv7</data>
  <data key="d4">Ö Kaya, MY Çodur, E Mustafaraj</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2114425240534661867&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="6877695309876199058">
  <data key="d0">Deep Neural Networks for Weed Detections Towards Precision Weeding</data>
  <data key="d1">6877695309876199058</data>
  <data key="d2">https://elibrary.asabe.org/abstract.asp?aid=53573</data>
  <data key="d3">Deep Neural Networks for Weed Detections Towards Precision Weeding</data>
  <data key="d4">A Rahman, Y Lu, H Wang</data>
  <data key="d5">2022</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6877695309876199058&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="6699633140763047923">
  <data key="d0">A W-shaped convolutional network for robust crop and weed classification in agriculture</data>
  <data key="d1">6699633140763047923</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11119-023-10027-7</data>
  <data key="d3">A W-shaped convolutional network for robust crop and weed classification in agriculture</data>
  <data key="d4">SI Moazzam, T Nawaz, WS Qureshi, US Khan…</data>
  <data key="d5">2023</data>
  <data key="d8">8</data>
</node>
<node id="13776149024359305191">
  <data key="d0">Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges</data>
  <data key="d1">13776149024359305191</data>
  <data key="d2">https://arxiv.org/abs/2308.06668</data>
  <data key="d3">Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges</data>
  <data key="d4">J Li, M Xu, L Xiang, D Chen, W Zhuang, X Yin…</data>
  <data key="d5">2023</data>
  <data key="d8">8</data>
</node>
<node id="13222462680960111198">
  <data key="d0">An attention mechanism-improved YOLOv7 object detection algorithm for hemp duck count estimation</data>
  <data key="d1">13222462680960111198</data>
  <data key="d2">https://www.mdpi.com/2077-0472/12/10/1659</data>
  <data key="d3">An attention mechanism-improved YOLOv7 object detection algorithm for hemp duck count estimation</data>
  <data key="d4">K Jiang, T Xie, R Yan, X Wen, D Li, H Jiang, N Jiang…</data>
  <data key="d5">2022</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13222462680960111198&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="6544902242854208675">
  <data key="d0">Automatic detection of brown hens in cage-free houses with deep learning methods</data>
  <data key="d1">6544902242854208675</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0032579123003036</data>
  <data key="d3">Automatic detection of brown hens in cage-free houses with deep learning methods</data>
  <data key="d4">Y Guo, P Regmi, Y Ding, RB Bist, L Chai</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6544902242854208675&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="12643412623559921352">
  <data key="d0">Automatic Fabric Defect Detection Method Using AC-YOLOv5</data>
  <data key="d1">12643412623559921352</data>
  <data key="d2">https://www.mdpi.com/2079-9292/12/13/2950</data>
  <data key="d3">Automatic Fabric Defect Detection Method Using AC-YOLOv5</data>
  <data key="d4">Y Guo, X Kang, J Li, Y Yang</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12643412623559921352&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="3110755508632090464">
  <data key="d0">Data-driven model SSD-BSP for multi-target coal-gangue detection</data>
  <data key="d1">3110755508632090464</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0263224123008084</data>
  <data key="d3">Data-driven model SSD-BSP for multi-target coal-gangue detection</data>
  <data key="d4">L Wang, X Wang, B Li</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3110755508632090464&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="14940203506181718397">
  <data key="d0">Deep Learning for Highly Accurate Hand Recognition Based on Yolov7 Model</data>
  <data key="d1">14940203506181718397</data>
  <data key="d2">https://www.mdpi.com/2504-2289/7/1/53</data>
  <data key="d3">Deep Learning for Highly Accurate Hand Recognition Based on Yolov7 Model</data>
  <data key="d4">C Dewi, APS Chen, HJ Christanto</data>
  <data key="d5">2023</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14940203506181718397&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="7108335566961034347">
  <data key="d0">Tea leaf disease detection and identification based on YOLOv7 (YOLO-T)</data>
  <data key="d1">7108335566961034347</data>
  <data key="d2">https://www.nature.com/articles/s41598-023-33270-4</data>
  <data key="d3">Tea leaf disease detection and identification based on YOLOv7 (YOLO-T)</data>
  <data key="d4">MJA Soeb, MF Jubayer, TA Tarin, MR Al Mamun…</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7108335566961034347&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="118648389925321836">
  <data key="d0">A Trunk Detection Method for Camellia oleifera Fruit Harvesting Robot Based on Improved YOLOv7</data>
  <data key="d1">118648389925321836</data>
  <data key="d2">https://www.mdpi.com/1999-4907/14/7/1453</data>
  <data key="d3">A Trunk Detection Method for Camellia oleifera Fruit Harvesting Robot Based on Improved YOLOv7</data>
  <data key="d4">Y Liu, H Wang, Y Liu, Y Luo, H Li, H Chen, K Liao, L Li</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=118648389925321836&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="ApU0apKFnLoJ">
  <data key="d0">GlandSegNet: Semantic segmentation model and area detection method for cotton leaf pigment glands</data>
  <data key="d1">ApU0apKFnLoJ</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923005185</data>
  <data key="d3">GlandSegNet: Semantic segmentation model and area detection method for cotton leaf pigment glands</data>
  <data key="d4">Y Xu, G Wang, L Shao, N Wang, L She, Y Liu…</data>
  <data key="d5">2023</data>
  <data key="d8">9</data>
</node>
<node id="10376535638618684847">
  <data key="d0">Cotton Seedling Detection and Counting Based on UAV Multispectral Images and Deep Learning Methods</data>
  <data key="d1">10376535638618684847</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/10/2680</data>
  <data key="d3">Cotton Seedling Detection and Counting Based on UAV Multispectral Images and Deep Learning Methods</data>
  <data key="d4">Y Feng, W Chen, Y Ma, Z Zhang, P Gao, X Lv</data>
  <data key="d5">2023</data>
  <data key="d8">9</data>
</node>
<node id="sdblonX9vEQJ">
  <data key="d0">Fusion of udder temperature and size features for the automatic detection of dairy cow mastitis using deep learning</data>
  <data key="d1">sdblonX9vEQJ</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923005197</data>
  <data key="d3">Fusion of udder temperature and size features for the automatic detection of dairy cow mastitis using deep learning</data>
  <data key="d4">M Chu, Q Li, Y Wang, X Zeng, Y Si, G Liu</data>
  <data key="d5">2023</data>
  <data key="d8">9</data>
</node>
<node id="9083521644969713050">
  <data key="d0">YOLO-DCTI: Small Object Detection in Remote Sensing Base on Contextual Transformer Enhancement</data>
  <data key="d1">9083521644969713050</data>
  <data key="d2">https://www.mdpi.com/2072-4292/15/16/3970</data>
  <data key="d3">YOLO-DCTI: Small Object Detection in Remote Sensing Base on Contextual Transformer Enhancement</data>
  <data key="d4">L Min, Z Fan, Q Lv, M Reda, L Shen, B Wang</data>
  <data key="d5">2023</data>
  <data key="d8">9</data>
</node>
<node id="16827683962441657873">
  <data key="d0">Sf-yolov5: A lightweight small object detection algorithm based on improved feature fusion mode</data>
  <data key="d1">16827683962441657873</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/15/5817</data>
  <data key="d3">Sf-yolov5: A lightweight small object detection algorithm based on improved feature fusion mode</data>
  <data key="d4">H Liu, F Sun, J Gu, L Deng</data>
  <data key="d5">2022</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16827683962441657873&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="14052692408086479292">
  <data key="d0">Development of YOLOv5-based real-time smart monitoring system for increasing lab safety awareness in educational institutions</data>
  <data key="d1">14052692408086479292</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/22/8820</data>
  <data key="d3">Development of YOLOv5-based real-time smart monitoring system for increasing lab safety awareness in educational institutions</data>
  <data key="d4">L Ali, F Alnajjar, MMA Parambil, MI Younes…</data>
  <data key="d5">2022</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14052692408086479292&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="17085777096678660112">
  <data key="d0">An efficient and intelligent detection method for fabric defects based on improved YOLOv5</data>
  <data key="d1">17085777096678660112</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/1/97</data>
  <data key="d3">An efficient and intelligent detection method for fabric defects based on improved YOLOv5</data>
  <data key="d4">G Lin, K Liu, X Xia, R Yan</data>
  <data key="d5">2022</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17085777096678660112&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="915175193869878529">
  <data key="d0">Detection of river floating garbage based on improved YOLOv5</data>
  <data key="d1">915175193869878529</data>
  <data key="d2">https://www.mdpi.com/2227-7390/10/22/4366</data>
  <data key="d3">Detection of river floating garbage based on improved YOLOv5</data>
  <data key="d4">X Yang, J Zhao, L Zhao, H Zhang, L Li, Z Ji, I Ganchev</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=915175193869878529&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="18125073647564822800">
  <data key="d0">High-resolution processing and sigmoid fusion modules for efficient detection of small objects in an embedded system</data>
  <data key="d1">18125073647564822800</data>
  <data key="d2">https://www.nature.com/articles/s41598-022-27189-5</data>
  <data key="d3">High-resolution processing and sigmoid fusion modules for efficient detection of small objects in an embedded system</data>
  <data key="d4">M Kim, H Kim, J Sung, C Park, J Paik</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18125073647564822800&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="11317692785014151150">
  <data key="d0">Wildlife Object Detection Method Applying Segmentation Gradient Flow and Feature Dimensionality Reduction</data>
  <data key="d1">11317692785014151150</data>
  <data key="d2">https://www.mdpi.com/2079-9292/12/2/377</data>
  <data key="d3">Wildlife Object Detection Method Applying Segmentation Gradient Flow and Feature Dimensionality Reduction</data>
  <data key="d4">M Zhang, F Gao, W Yang, H Zhang</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11317692785014151150&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="17823167432429599810">
  <data key="d0">DC-YOLOv8: Small-Size Object Detection Algorithm Based on Camera Sensor</data>
  <data key="d1">17823167432429599810</data>
  <data key="d2">https://www.mdpi.com/2079-9292/12/10/2323</data>
  <data key="d3">DC-YOLOv8: Small-Size Object Detection Algorithm Based on Camera Sensor</data>
  <data key="d4">H Lou, X Duan, J Guo, H Liu, J Gu, L Bi, H Chen</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17823167432429599810&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="851720305858029725">
  <data key="d0">Litchi Detection in a Complex Natural Environment Using the YOLOv5-Litchi Model</data>
  <data key="d1">851720305858029725</data>
  <data key="d2">https://www.mdpi.com/2073-4395/12/12/3054</data>
  <data key="d3">Litchi Detection in a Complex Natural Environment Using the YOLOv5-Litchi Model</data>
  <data key="d4">J Xie, J Peng, J Wang, B Chen, T Jing, D Sun, P Gao…</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=851720305858029725&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="13349023610638525354">
  <data key="d0">Long-Strip Target Detection and Tracking with Autonomous Surface Vehicle</data>
  <data key="d1">13349023610638525354</data>
  <data key="d2">https://www.mdpi.com/2077-1312/11/1/106</data>
  <data key="d3">Long-Strip Target Detection and Tracking with Autonomous Surface Vehicle</data>
  <data key="d4">M Zhang, D Zhao, C Sheng, Z Liu, W Cai</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13349023610638525354&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="10228711962693123964">
  <data key="d0">A lightweight vehicle-pedestrian detection algorithm based on attention mechanism in traffic scenarios</data>
  <data key="d1">10228711962693123964</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/21/8480</data>
  <data key="d3">A lightweight vehicle-pedestrian detection algorithm based on attention mechanism in traffic scenarios</data>
  <data key="d4">Y Zhang, A Zhou, F Zhao, H Wu</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10228711962693123964&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="6497775393041290067">
  <data key="d0">An Improved YOLOv5 Method for Small Object Detection in UAV Capture Scenes</data>
  <data key="d1">6497775393041290067</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10032549/</data>
  <data key="d3">An Improved YOLOv5 Method for Small Object Detection in UAV Capture Scenes</data>
  <data key="d4">Z Liu, X Gao, Y Wan, J Wang, H Lyu</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6497775393041290067&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="11112203079270473115">
  <data key="d0">Detection of Camellia oleifera fruit in complex scenes by using YOLOv7 and data augmentation</data>
  <data key="d1">11112203079270473115</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/22/11318</data>
  <data key="d3">Detection of Camellia oleifera fruit in complex scenes by using YOLOv7 and data augmentation</data>
  <data key="d4">D Wu, S Jiang, E Zhao, Y Liu, H Zhu, W Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">41</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11112203079270473115&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="18283090867219141273">
  <data key="d0">YOLOv7-Plum: Advancing Plum Fruit Detection in Natural Environments with Deep Learning</data>
  <data key="d1">18283090867219141273</data>
  <data key="d2">https://www.mdpi.com/2223-7747/12/15/2883</data>
  <data key="d3">YOLOv7-Plum: Advancing Plum Fruit Detection in Natural Environments with Deep Learning</data>
  <data key="d4">R Tang, Y Lei, B Luo, J Zhang, J Mu</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18283090867219141273&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="3348080849815981295">
  <data key="d0">A pineapple target detection method in a field environment based on improved YOLOv7</data>
  <data key="d1">3348080849815981295</data>
  <data key="d2">https://www.mdpi.com/2076-3417/13/4/2691</data>
  <data key="d3">A pineapple target detection method in a field environment based on improved YOLOv7</data>
  <data key="d4">Y Lai, R Ma, Y Chen, T Wan, R Jiao, H He</data>
  <data key="d5">2023</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3348080849815981295&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="9008218150110989248">
  <data key="d0">Tea-YOLOv8s: A Tea Bud Detection Model Based on Deep Learning and Computer Vision</data>
  <data key="d1">9008218150110989248</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/14/6576</data>
  <data key="d3">Tea-YOLOv8s: A Tea Bud Detection Model Based on Deep Learning and Computer Vision</data>
  <data key="d4">S Xie, H Sun</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9008218150110989248&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="15890436532115930630">
  <data key="d0">Cooperative Heterogeneous Robots for Autonomous Insects Trap Monitoring System in a Precision Agriculture Scenario</data>
  <data key="d1">15890436532115930630</data>
  <data key="d2">https://www.mdpi.com/2077-0472/13/2/239</data>
  <data key="d3">Cooperative Heterogeneous Robots for Autonomous Insects Trap Monitoring System in a Precision Agriculture Scenario</data>
  <data key="d4">GS Berger, M Teixeira, A Cantieri, J Lima, AI Pereira…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15890436532115930630&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="MXxApydlfuYJ">
  <data key="d0">Detecting endosperm cracks in soaked maize using μCT technology and R-YOLOv7-tiny</data>
  <data key="d1">MXxApydlfuYJ</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923006208</data>
  <data key="d3">Detecting endosperm cracks in soaked maize using μCT technology and R-YOLOv7-tiny</data>
  <data key="d4">Y Jiao, Z Wang, Y Shang, R Li, Z Hua…</data>
  <data key="d5">2023</data>
  <data key="d8">5</data>
</node>
<node id="15254646771058273507">
  <data key="d0">Intelligent detection of Multi-Class pitaya fruits in target picking row based on WGB-YOLO network</data>
  <data key="d1">15254646771058273507</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923001680</data>
  <data key="d3">Intelligent detection of Multi-Class pitaya fruits in target picking row based on WGB-YOLO network</data>
  <data key="d4">Y Nan, H Zhang, Y Zeng, J Zheng, Y Ge</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15254646771058273507&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="18262125184766942934">
  <data key="d0">Traffic Sign Detection Based on the Improved YOLOv5</data>
  <data key="d1">18262125184766942934</data>
  <data key="d2">https://www.mdpi.com/2076-3417/13/17/9748</data>
  <data key="d3">Traffic Sign Detection Based on the Improved YOLOv5</data>
  <data key="d4">R Zhang, K Zheng, P Shi, Y Mei, H Li, T Qiu</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18262125184766942934&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="12977604275271393235">
  <data key="d0">TBC-YOLOv7: a refined YOLOv7-based algorithm for tea bud grading detection</data>
  <data key="d1">12977604275271393235</data>
  <data key="d2">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10469839/</data>
  <data key="d3">TBC-YOLOv7: a refined YOLOv7-based algorithm for tea bud grading detection</data>
  <data key="d4">S Wang, D Wu, X Zheng</data>
  <data key="d5">2023</data>
  <data key="d8">5</data>
</node>
<node id="17690753104303819861">
  <data key="d0">Adaptive active positioning of Camellia oleifera fruit picking points: Classical image processing...</data>
  <data key="d1">17690753104303819861</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/24/12959</data>
  <data key="d3">Adaptive active positioning of Camellia oleifera fruit picking points: Classical image processing...</data>
  <data key="d4">Y Zhou, Y Tang, X Zou, M Wu, W Tang, F Meng…</data>
  <data key="d5">2022</data>
  <data key="d6">23</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17690753104303819861&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="11735473174983847098">
  <data key="d0">A Review of Target Recognition Technology for Fruit Picking Robots: From Digital Image Processing to Deep Learning</data>
  <data key="d1">11735473174983847098</data>
  <data key="d2">https://www.mdpi.com/2076-3417/13/7/4160</data>
  <data key="d3">A Review of Target Recognition Technology for Fruit Picking Robots: From Digital Image Processing to Deep Learning</data>
  <data key="d4">X Hua, H Li, J Zeng, C Han, T Chen, L Tang, Y Luo</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11735473174983847098&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="1443894251537732005">
  <data key="d0">Detection and counting of banana bunches by integrating deep learning and classic image-processing algorithms</data>
  <data key="d1">1443894251537732005</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923002156</data>
  <data key="d3">Detection and counting of banana bunches by integrating deep learning and classic image-processing algorithms</data>
  <data key="d4">F Wu, Z Yang, X Mo, Z Wu, W Tang, J Duan…</data>
  <data key="d5">2023</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1443894251537732005&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="1791593962748758598">
  <data key="d0">Branch interference sensing and handling by tactile enabled robotic apple harvesting</data>
  <data key="d1">1791593962748758598</data>
  <data key="d2">https://www.mdpi.com/2073-4395/13/2/503</data>
  <data key="d3">Branch interference sensing and handling by tactile enabled robotic apple harvesting</data>
  <data key="d4">H Zhou, H Kang, X Wang, W Au, MY Wang, C Chen</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1791593962748758598&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="14879519839963588983">
  <data key="d0">Multidirectional Dynamic Response and Swing Shedding of Grapes: An Experimental and Simulation Investigation under Vibration Excitation</data>
  <data key="d1">14879519839963588983</data>
  <data key="d2">https://www.mdpi.com/2073-4395/13/3/869</data>
  <data key="d3">Multidirectional Dynamic Response and Swing Shedding of Grapes: An Experimental and Simulation Investigation under Vibration Excitation</data>
  <data key="d4">P Zhang, D Yan, X Cai, Y Chen, L Luo, Y Pan, X Zou</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14879519839963588983&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">5</data>
</node>
<node id="10264464849626080964">
  <data key="d0">Domain feature mapping with YOLOv7 for automated edge-based pallet racking inspections</data>
  <data key="d1">10264464849626080964</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/18/6927</data>
  <data key="d3">Domain feature mapping with YOLOv7 for automated edge-based pallet racking inspections</data>
  <data key="d4">M Hussain, H Al-Aqrabi, M Munawar, R Hill, T Alsboui</data>
  <data key="d5">2022</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10264464849626080964&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="2392814171009909808">
  <data key="d0">Statistical Analysis of Design Aspects of Various YOLO-Based Deep Learning Models for Object Detection</data>
  <data key="d1">2392814171009909808</data>
  <data key="d2">https://link.springer.com/article/10.1007/s44196-023-00302-w</data>
  <data key="d3">Statistical Analysis of Design Aspects of Various YOLO-Based Deep Learning Models for Object Detection</data>
  <data key="d4">U Sirisha, SP Praveen, PN Srinivasu…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2392814171009909808&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="6327480080772807927">
  <data key="d0">YOLOv7-RAR for Urban Vehicle Detection</data>
  <data key="d1">6327480080772807927</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/4/1801</data>
  <data key="d3">YOLOv7-RAR for Urban Vehicle Detection</data>
  <data key="d4">Y Zhang, Y Sun, Z Wang, Y Jiang</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6327480080772807927&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="12337878440731894992">
  <data key="d0">Fire detection and notification method in ship areas using deep learning and computer vision approaches</data>
  <data key="d1">12337878440731894992</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/16/7078</data>
  <data key="d3">Fire detection and notification method in ship areas using deep learning and computer vision approaches</data>
  <data key="d4">K Avazov, MK Jamil, B Muminov, AB Abdusalomov…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12337878440731894992&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="17386293721154871155">
  <data key="d0">IDOD-YOLOV7: Image-Dehazing YOLOV7 for Object Detection in Low-Light Foggy Traffic Environments</data>
  <data key="d1">17386293721154871155</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/3/1347</data>
  <data key="d3">IDOD-YOLOV7: Image-Dehazing YOLOV7 for Object Detection in Low-Light Foggy Traffic Environments</data>
  <data key="d4">Y Qiu, Y Lu, Y Wang, H Jiang</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17386293721154871155&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="12836955438277091904">
  <data key="d0">PV-CrackNet Architecture for Filter Induced Augmentation and Micro-Cracks Detection within a Photovoltaic Manufacturing Facility</data>
  <data key="d1">12836955438277091904</data>
  <data key="d2">https://www.mdpi.com/1996-1073/15/22/8667</data>
  <data key="d3">PV-CrackNet Architecture for Filter Induced Augmentation and Micro-Cracks Detection within a Photovoltaic Manufacturing Facility</data>
  <data key="d4">M Hussain, H Al-Aqrabi, R Hill</data>
  <data key="d5">2022</data>
  <data key="d6">9</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12836955438277091904&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="18049535498421460297">
  <data key="d0">A Review on Defect Detection of Electroluminescence-Based Photovoltaic Cell Surface Images Using Computer Vision</data>
  <data key="d1">18049535498421460297</data>
  <data key="d2">https://www.mdpi.com/1996-1073/16/10/4012</data>
  <data key="d3">A Review on Defect Detection of Electroluminescence-Based Photovoltaic Cell Surface Images Using Computer Vision</data>
  <data key="d4">T Hussain, M Hussain, H Al-Aqrabi, T Alsboui, R Hill</data>
  <data key="d5">2023</data>
  <data key="d8">9</data>
</node>
<node id="12328613189873235415">
  <data key="d0">Deep Learning-Based Intelligent Forklift Cargo Accurate Transfer System</data>
  <data key="d1">12328613189873235415</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/21/8437</data>
  <data key="d3">Deep Learning-Based Intelligent Forklift Cargo Accurate Transfer System</data>
  <data key="d4">J Ren, Y Pan, P Yao, Y Hu, W Gao, Z Xue</data>
  <data key="d5">2022</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12328613189873235415&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">9</data>
</node>
<node id="5371618923568523141">
  <data key="d0">EFC-YOLO: An Efficient Surface-Defect-Detection Algorithm for Steel Strips</data>
  <data key="d1">5371618923568523141</data>
  <data key="d2">https://www.mdpi.com/1424-8220/23/17/7619</data>
  <data key="d3">EFC-YOLO: An Efficient Surface-Defect-Detection Algorithm for Steel Strips</data>
  <data key="d4">Y Li, S Xu, Z Zhu, P Wang, K Li, Q He, Q Zheng</data>
  <data key="d5">2023</data>
  <data key="d8">9</data>
</node>
<node id="18210588638302847093">
  <data key="d0">TPH-YOLOv5: Improved YOLOv5 based on transformer prediction head for object detection on...</data>
  <data key="d1">18210588638302847093</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Zhu_TPH-YOLOv5_Improved_YOLOv5_Based_on_Transformer_Prediction_Head_for_Object_ICCVW_2021_paper.html</data>
  <data key="d3">TPH-YOLOv5: Improved YOLOv5 based on transformer prediction head for object detection on...</data>
  <data key="d4">X Zhu, S Lyu, X Wang, Q Zhao</data>
  <data key="d5">2021</data>
  <data key="d6">662</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18210588638302847093&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="15045952046135698930">
  <data key="d0">A comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities</data>
  <data key="d1">15045952046135698930</data>
  <data key="d2">https://dl.acm.org/doi/abs/10.1145/3582688</data>
  <data key="d3">A comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities</data>
  <data key="d4">Y Song, T Wang, P Cai, SK Mondal…</data>
  <data key="d5">2023</data>
  <data key="d6">46</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15045952046135698930&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="1223563817431199857">
  <data key="d0">A lightweight vehicles detection network model based on YOLOv5</data>
  <data key="d1">1223563817431199857</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0952197622001415</data>
  <data key="d3">A lightweight vehicles detection network model based on YOLOv5</data>
  <data key="d4">X Dong, S Yan, C Duan</data>
  <data key="d5">2022</data>
  <data key="d6">77</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1223563817431199857&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="3743433833463586545">
  <data key="d0">Improved yolov5: Efficient object detection using drone images under various conditions</data>
  <data key="d1">3743433833463586545</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/14/7255</data>
  <data key="d3">Improved yolov5: Efficient object detection using drone images under various conditions</data>
  <data key="d4">HK Jung, GS Choi</data>
  <data key="d5">2022</data>
  <data key="d6">59</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3743433833463586545&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="3333668025330401477">
  <data key="d0">A hybrid explainable ensemble transformer encoder for pneumonia identification from chest X-ray images</data>
  <data key="d1">3333668025330401477</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2090123222002028</data>
  <data key="d3">A hybrid explainable ensemble transformer encoder for pneumonia identification from chest X-ray images</data>
  <data key="d4">CC Ukwuoma, Z Qin, MBB Heyat, F Akhtar…</data>
  <data key="d5">2023</data>
  <data key="d6">25</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3333668025330401477&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="2805320302601413924">
  <data key="d0">Underwater target detection algorithm based on improved YOLOv5</data>
  <data key="d1">2805320302601413924</data>
  <data key="d2">https://www.mdpi.com/2077-1312/10/3/310</data>
  <data key="d3">Underwater target detection algorithm based on improved YOLOv5</data>
  <data key="d4">F Lei, F Tang, S Li</data>
  <data key="d5">2022</data>
  <data key="d6">47</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2805320302601413924&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="12502648671025662060">
  <data key="d0">Deep learning-based object detection techniques for remote sensing images: A survey</data>
  <data key="d1">12502648671025662060</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/10/2385</data>
  <data key="d3">Deep learning-based object detection techniques for remote sensing images: A survey</data>
  <data key="d4">Z Li, Y Wang, N Zhang, Y Zhang, Z Zhao, D Xu, G Ben…</data>
  <data key="d5">2022</data>
  <data key="d6">34</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12502648671025662060&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">7</data>
</node>
<node id="7039269942427062691">
  <data key="d0">Dino: Detr with improved denoising anchor boxes for end-to-end object detection</data>
  <data key="d1">7039269942427062691</data>
  <data key="d2">https://arxiv.org/abs/2203.03605</data>
  <data key="d3">Dino: Detr with improved denoising anchor boxes for end-to-end object detection</data>
  <data key="d4">H Zhang, F Li, S Liu, L Zhang, H Su, J Zhu…</data>
  <data key="d5">2022</data>
  <data key="d6">337</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7039269942427062691&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="6118595289890500680">
  <data key="d0">Internimage: Exploring large-scale vision foundation models with deformable convolutions</data>
  <data key="d1">6118595289890500680</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Wang_InternImage_Exploring_Large-Scale_Vision_Foundation_Models_With_Deformable_Convolutions_CVPR_2023_paper.html</data>
  <data key="d3">Internimage: Exploring large-scale vision foundation models with deformable convolutions</data>
  <data key="d4">W Wang, J Dai, Z Chen, Z Huang, Z Li…</data>
  <data key="d5">2023</data>
  <data key="d6">120</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6118595289890500680&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3484260075890953852">
  <data key="d0">Context autoencoder for self-supervised representation learning</data>
  <data key="d1">3484260075890953852</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11263-023-01852-4</data>
  <data key="d3">Context autoencoder for self-supervised representation learning</data>
  <data key="d4">X Chen, M Ding, X Wang, Y Xin, S Mo, Y Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">169</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3484260075890953852&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="6077963581280858299">
  <data key="d0">Deep learning for SAR ship detection: Past, present and future</data>
  <data key="d1">6077963581280858299</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/11/2712</data>
  <data key="d3">Deep learning for SAR ship detection: Past, present and future</data>
  <data key="d4">J Li, C Xu, H Su, L Gao, T Wang</data>
  <data key="d5">2022</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6077963581280858299&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="17545941908498686611">
  <data key="d0">Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe</data>
  <data key="d1">17545941908498686611</data>
  <data key="d2">https://arxiv.org/abs/2209.05324</data>
  <data key="d3">Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe</data>
  <data key="d4">H Li, C Sima, J Dai, W Wang, L Lu, H Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17545941908498686611&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="17229720160752682638">
  <data key="d0">Mask dino: Towards a unified transformer-based framework for object detection and segmentation</data>
  <data key="d1">17229720160752682638</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Li_Mask_DINO_Towards_a_Unified_Transformer-Based_Framework_for_Object_Detection_CVPR_2023_paper.html</data>
  <data key="d3">Mask dino: Towards a unified transformer-based framework for object detection and segmentation</data>
  <data key="d4">F Li, H Zhang, H Xu, S Liu, L Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">94</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17229720160752682638&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="2723001857482086032">
  <data key="d0">Point transformer v2: Grouped vector attention and partition-based pooling</data>
  <data key="d1">2723001857482086032</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/d78ece6613953f46501b958b7bb4582f-Abstract-Conference.html</data>
  <data key="d3">Point transformer v2: Grouped vector attention and partition-based pooling</data>
  <data key="d4">X Wu, Y Lao, L Jiang, X Liu…</data>
  <data key="d5">2022</data>
  <data key="d6">37</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2723001857482086032&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12867511582517934835">
  <data key="d0">Focal modulation networks</data>
  <data key="d1">12867511582517934835</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/1b08f585b0171b74d1401a5195e986f1-Abstract-Conference.html</data>
  <data key="d3">Focal modulation networks</data>
  <data key="d4">J Yang, C Li, X Dai, J Gao</data>
  <data key="d5">2022</data>
  <data key="d6">67</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12867511582517934835&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="4488471448867589825">
  <data key="d0">Comparing YOLOv3, YOLOv4 and YOLOv5 for autonomous landing spot detection in faulty UAVs</data>
  <data key="d1">4488471448867589825</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/2/464</data>
  <data key="d3">Comparing YOLOv3, YOLOv4 and YOLOv5 for autonomous landing spot detection in faulty UAVs</data>
  <data key="d4">U Nepal, H Eslamiat</data>
  <data key="d5">2022</data>
  <data key="d6">265</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4488471448867589825&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="8808925328206804280">
  <data key="d0">Tools, techniques, datasets and application areas for object detection in an image: a review</data>
  <data key="d1">8808925328206804280</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11042-022-13153-y</data>
  <data key="d3">Tools, techniques, datasets and application areas for object detection in an image: a review</data>
  <data key="d4">J Kaur, W Singh</data>
  <data key="d5">2022</data>
  <data key="d6">33</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8808925328206804280&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="9110810375335606993">
  <data key="d0">Deep learning based detector yolov5 for identifying insect pests</data>
  <data key="d1">9110810375335606993</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/19/10167</data>
  <data key="d3">Deep learning based detector yolov5 for identifying insect pests</data>
  <data key="d4">I Ahmad, Y Yang, Y Yue, C Ye, M Hassan, X Cheng…</data>
  <data key="d5">2022</data>
  <data key="d6">38</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9110810375335606993&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="708526825456642350">
  <data key="d0">Vision-based autonomous landing for the uav: A review</data>
  <data key="d1">708526825456642350</data>
  <data key="d2">https://www.mdpi.com/2226-4310/9/11/634</data>
  <data key="d3">Vision-based autonomous landing for the uav: A review</data>
  <data key="d4">L Xin, Z Tang, W Gai, H Liu</data>
  <data key="d5">2022</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=708526825456642350&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="515129946496787543">
  <data key="d0">Multi-class detection of kiwifruit flower and its distribution identification in orchard based on YOLOv5l and Euclidean distance</data>
  <data key="d1">515129946496787543</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169922006512</data>
  <data key="d3">Multi-class detection of kiwifruit flower and its distribution identification in orchard based on YOLOv5l and Euclidean distance</data>
  <data key="d4">G Li, L Fu, C Gao, W Fang, G Zhao, F Shi…</data>
  <data key="d5">2022</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=515129946496787543&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="3749665078422249041">
  <data key="d0">Multi-scale ship target detection using SAR images based on improved Yolov5</data>
  <data key="d1">3749665078422249041</data>
  <data key="d2">https://www.frontiersin.org/articles/10.3389/fmars.2022.1086140/full</data>
  <data key="d3">Multi-scale ship target detection using SAR images based on improved Yolov5</data>
  <data key="d4">M Yasir, L Shanwei, X Mingming, S Hui…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3749665078422249041&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="13329597315293984783">
  <data key="d0">Real-time vehicle detection algorithm based on a lightweight You-Only-Look-Once (YOLOv5n-L) approach</data>
  <data key="d1">13329597315293984783</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417422021261</data>
  <data key="d3">Real-time vehicle detection algorithm based on a lightweight You-Only-Look-Once (YOLOv5n-L) approach</data>
  <data key="d4">M Bie, Y Liu, G Li, J Hong, J Li</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13329597315293984783&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="9358192350302074225">
  <data key="d0">Detection of river plastic using UAV sensor data and deep learning</data>
  <data key="d1">9358192350302074225</data>
  <data key="d2">https://www.mdpi.com/2072-4292/14/13/3049</data>
  <data key="d3">Detection of river plastic using UAV sensor data and deep learning</data>
  <data key="d4">N Maharjan, H Miyazaki, BM Pati, MN Dailey…</data>
  <data key="d5">2022</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9358192350302074225&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="9302345027896383303">
  <data key="d0">Design and implementation of uavs for bird's nest inspection on transmission lines based on Deep Learning</data>
  <data key="d1">9302345027896383303</data>
  <data key="d2">https://www.mdpi.com/2504-446X/6/9/252</data>
  <data key="d3">Design and implementation of uavs for bird's nest inspection on transmission lines based on Deep Learning</data>
  <data key="d4">H Li, Y Dong, Y Liu, J Ai</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9302345027896383303&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">3</data>
</node>
<node id="5295462520886771746">
  <data key="d0">Key technologies of machine vision for weeding robots: A review and benchmark</data>
  <data key="d1">5295462520886771746</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169922001971</data>
  <data key="d3">Key technologies of machine vision for weeding robots: A review and benchmark</data>
  <data key="d4">Y Li, Z Guo, F Shuang, M Zhang, X Li</data>
  <data key="d5">2022</data>
  <data key="d6">31</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5295462520886771746&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="6482216091275039752">
  <data key="d0">Vision-based navigation and guidance for agricultural autonomous vehicles and robots: A review</data>
  <data key="d1">6482216091275039752</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169922008924</data>
  <data key="d3">Vision-based navigation and guidance for agricultural autonomous vehicles and robots: A review</data>
  <data key="d4">Y Bai, B Zhang, N Xu, J Zhou, J Shi, Z Diao</data>
  <data key="d5">2023</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6482216091275039752&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="8940062898534376834">
  <data key="d0">A deep learning approach incorporating YOLO v5 and attention mechanisms for field real-time detection of the invasive weed Solanum rostratum Dunal seedlings</data>
  <data key="d1">8940062898534376834</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169922005117</data>
  <data key="d3">A deep learning approach incorporating YOLO v5 and attention mechanisms for field real-time detection of the invasive weed Solanum rostratum Dunal seedlings</data>
  <data key="d4">Q Wang, M Cheng, S Huang, Z Cai, J Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">32</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8940062898534376834&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="15245828836153963354">
  <data key="d0">Digital innovations for sustainable and resilient agricultural systems</data>
  <data key="d1">15245828836153963354</data>
  <data key="d2">https://academic.oup.com/erae/article/50/4/1277/7208892?login=true</data>
  <data key="d3">Digital innovations for sustainable and resilient agricultural systems</data>
  <data key="d4">R Finger</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15245828836153963354&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="3527582028524441499">
  <data key="d0">RGB-D datasets for robotic perception in site-specific agricultural operations—A survey</data>
  <data key="d1">3527582028524441499</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0168169923004234</data>
  <data key="d3">RGB-D datasets for robotic perception in site-specific agricultural operations—A survey</data>
  <data key="d4">P Kurtser, S Lowry</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3527582028524441499&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="1893894085208736639">
  <data key="d0">Boosting precision crop protection towards agriculture 5.0 via machine learning and emerging technologies: A contextual review</data>
  <data key="d1">1893894085208736639</data>
  <data key="d2">https://www.frontiersin.org/articles/10.3389/fpls.2023.1143326/full</data>
  <data key="d3">Boosting precision crop protection towards agriculture 5.0 via machine learning and emerging technologies: A contextual review</data>
  <data key="d4">GA Mesías-Ruiz, M Pérez-Ortiz, J Dorado…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1893894085208736639&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="11761116720729991146">
  <data key="d0">A conceptual evaluation of a weed control method with post-damage application of herbicides: A composite intelligent intra-row weeding robot</data>
  <data key="d1">11761116720729991146</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0167198723002040</data>
  <data key="d3">A conceptual evaluation of a weed control method with post-damage application of herbicides: A composite intelligent intra-row weeding robot</data>
  <data key="d4">W Jiang, L Quan, G Wei, C Chang, T Geng</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11761116720729991146&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="9934965656490088917">
  <data key="d0">Handling severity levels of multiple co-occurring cotton plant diseases using improved YOLOX model</data>
  <data key="d1">9934965656490088917</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9999667/</data>
  <data key="d3">Handling severity levels of multiple co-occurring cotton plant diseases using improved YOLOX model</data>
  <data key="d4">SK Noon, M Amjad, MA Qureshi, A Mannan</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9934965656490088917&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="2188695656210318579">
  <data key="d0">Design and experimental verification of the YOLOV5 model implanted with a transformer module for target-oriented spraying in cabbage farming</data>
  <data key="d1">2188695656210318579</data>
  <data key="d2">https://www.mdpi.com/2073-4395/12/10/2551</data>
  <data key="d3">Design and experimental verification of the YOLOV5 model implanted with a transformer module for target-oriented spraying in cabbage farming</data>
  <data key="d4">H Fu, X Zhao, H Wu, S Zheng, K Zheng, C Zhai</data>
  <data key="d5">2022</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2188695656210318579&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="12517578013806719466">
  <data key="d0">DeepCottonWeeds (DCW): a novel benchmark of YOLO object detectors for weed detection in cotton production systems</data>
  <data key="d1">12517578013806719466</data>
  <data key="d2">https://elibrary.asabe.org/abstract.asp?aid=53312</data>
  <data key="d3">DeepCottonWeeds (DCW): a novel benchmark of YOLO object detectors for weed detection in cotton production systems</data>
  <data key="d4">F Dang, D Chen, Y Lu, Z Li…</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12517578013806719466&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">8</data>
</node>
<node id="14575314968965851624">
  <data key="d0">Motrv2: Bootstrapping end-to-end multi-object tracking by pretrained object detectors</data>
  <data key="d1">14575314968965851624</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_MOTRv2_Bootstrapping_End-to-End_Multi-Object_Tracking_by_Pretrained_Object_Detectors_CVPR_2023_paper.html</data>
  <data key="d3">Motrv2: Bootstrapping end-to-end multi-object tracking by pretrained object detectors</data>
  <data key="d4">Y Zhang, T Wang, X Zhang</data>
  <data key="d5">2023</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14575314968965851624&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="14118575121889987430">
  <data key="d0">A Review of Deep Learning-Based Visual Multi-Object Tracking Algorithms for Autonomous Driving</data>
  <data key="d1">14118575121889987430</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/21/10741</data>
  <data key="d3">A Review of Deep Learning-Based Visual Multi-Object Tracking Algorithms for Autonomous Driving</data>
  <data key="d4">S Guo, S Wang, Z Yang, L Wang, H Zhang, P Guo…</data>
  <data key="d5">2022</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14118575121889987430&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="3318911338829069796">
  <data key="d0">BoT-SORT: Robust associations multi-pedestrian tracking</data>
  <data key="d1">3318911338829069796</data>
  <data key="d2">https://arxiv.org/abs/2206.14651</data>
  <data key="d3">BoT-SORT: Robust associations multi-pedestrian tracking</data>
  <data key="d4">N Aharon, R Orfaig, BZ Bobrovsky</data>
  <data key="d5">2022</data>
  <data key="d6">84</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3318911338829069796&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="9808171637715574951">
  <data key="d0">Qdtrack: Quasi-dense similarity learning for appearance-only multiple object tracking</data>
  <data key="d1">9808171637715574951</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10209207/</data>
  <data key="d3">Qdtrack: Quasi-dense similarity learning for appearance-only multiple object tracking</data>
  <data key="d4">T Fischer, TE Huang, J Pang, L Qiu…</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9808171637715574951&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="8763231865317300809">
  <data key="d0">Jrdb-pose: A large-scale dataset for multi-person pose estimation and tracking</data>
  <data key="d1">8763231865317300809</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Vendrow_JRDB-Pose_A_Large-Scale_Dataset_for_Multi-Person_Pose_Estimation_and_Tracking_CVPR_2023_paper.html</data>
  <data key="d3">Jrdb-pose: A large-scale dataset for multi-person pose estimation and tracking</data>
  <data key="d4">E Vendrow, DT Le, J Cai…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8763231865317300809&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="14978456818526690373">
  <data key="d0">Multiple Object Tracking in Robotic Applications: Trends and Challenges</data>
  <data key="d1">14978456818526690373</data>
  <data key="d2">https://www.mdpi.com/2076-3417/12/19/9408</data>
  <data key="d3">Multiple Object Tracking in Robotic Applications: Trends and Challenges</data>
  <data key="d4">A Gad, T Basmaji, M Yaghi, H Alheeh, M Alkhedher…</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14978456818526690373&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="17206319611652371856">
  <data key="d0">Hard to track objects with irregular motions and similar appearances? make it easier by buffering the matching space</data>
  <data key="d1">17206319611652371856</data>
  <data key="d2">https://openaccess.thecvf.com/content/WACV2023/html/Yang_Hard_To_Track_Objects_With_Irregular_Motions_and_Similar_Appearances_WACV_2023_paper.html</data>
  <data key="d3">Hard to track objects with irregular motions and similar appearances? make it easier by buffering the matching space</data>
  <data key="d4">F Yang, S Odashima, S Masui…</data>
  <data key="d5">2023</data>
  <data key="d6">16</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17206319611652371856&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="13723544815291086075">
  <data key="d0">1st workshop on maritime computer vision (macvi) 2023: Challenge results</data>
  <data key="d1">13723544815291086075</data>
  <data key="d2">https://openaccess.thecvf.com/content/WACV2023W/MaCVi/html/Kiefer_1st_Workshop_on_Maritime_Computer_Vision_MaCVi_2023_Challenge_Results_WACVW_2023_paper.html</data>
  <data key="d3">1st workshop on maritime computer vision (macvi) 2023: Challenge results</data>
  <data key="d4">B Kiefer, M Kristan, J Perš, L Žust…</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=13723544815291086075&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">6</data>
</node>
<node id="4246517974439469322">
  <data key="d0">Intelligent approach for the industrialization of deep learning solutions applied to fault detection</data>
  <data key="d1">4246517974439469322</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0957417423014616</data>
  <data key="d3">Intelligent approach for the industrialization of deep learning solutions applied to fault detection</data>
  <data key="d4">IP Colo, CS Sueldo, M De Paula, GG Acosta</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4246517974439469322&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="137519887286825601">
  <data key="d0">Fast and Non-Destructive Quail Egg Freshness Assessment Using a Thermal Camera and Deep Learning-Based Air Cell Detection Algorithms for the Revalidation of …</data>
  <data key="d1">137519887286825601</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/20/7703</data>
  <data key="d3">Fast and Non-Destructive Quail Egg Freshness Assessment Using a Thermal Camera and Deep Learning-Based Air Cell Detection Algorithms for the Revalidation of …</data>
  <data key="d4">VM Nakaguchi, T Ahamed</data>
  <data key="d5">2022</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=137519887286825601&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="E02bi-l64ZwJ">
  <data key="d0">An efficient DNN splitting scheme for edge-AI enabled smart manufacturing</data>
  <data key="d1">E02bi-l64ZwJ</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2452414X23000547</data>
  <data key="d3">An efficient DNN splitting scheme for edge-AI enabled smart manufacturing</data>
  <data key="d4">H Gauttam, KK Pattanaik, S Bhadauria, G Nain…</data>
  <data key="d5">2023</data>
  <data key="d8">15</data>
</node>
<node id="4896734917901521680">
  <data key="d0">Improved YOLOv3 Model for Workpiece Stud Leakage Detection</data>
  <data key="d1">4896734917901521680</data>
  <data key="d2">https://www.mdpi.com/2079-9292/11/21/3430</data>
  <data key="d3">Improved YOLOv3 Model for Workpiece Stud Leakage Detection</data>
  <data key="d4">P Cong, K Lv, H Feng, J Zhou</data>
  <data key="d5">2022</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4896734917901521680&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="QwXRfW59944J">
  <data key="d0">Knowledge distillation-optimized two-stage anomaly detection for liquid rocket engine with missing multimodal data</data>
  <data key="d1">QwXRfW59944J</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0951832023005902</data>
  <data key="d3">Knowledge distillation-optimized two-stage anomaly detection for liquid rocket engine with missing multimodal data</data>
  <data key="d4">X Zhang, Y Feng, J Chen, Z Liu, J Wang…</data>
  <data key="d5">2023</data>
  <data key="d8">15</data>
</node>
<node id="4747360019864620532">
  <data key="d0">Region‐based fully convolutional networks with deformable convolution and attention fusion for steel surface defect detection in industrial Internet of Things</data>
  <data key="d1">4747360019864620532</data>
  <data key="d2">https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/sil2.12208</data>
  <data key="d3">Region‐based fully convolutional networks with deformable convolution and attention fusion for steel surface defect detection in industrial Internet of Things</data>
  <data key="d4">M Fu, J Wu, Q Wang, L Sun, Z Ma, C Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4747360019864620532&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="15130464980890007333">
  <data key="d0">Bubble detection in photoresist with small samples based on GAN augmentations and modified YOLO</data>
  <data key="d1">15130464980890007333</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0952197623004086</data>
  <data key="d3">Bubble detection in photoresist with small samples based on GAN augmentations and modified YOLO</data>
  <data key="d4">G Yang, C Song, Z Yang, S Cui</data>
  <data key="d5">2023</data>
  <data key="d8">15</data>
</node>
<node id="1126142290960013428">
  <data key="d0">Development and analysis of a holistic function-driven adaptive assembly strategy applied to micro gears</data>
  <data key="d1">1126142290960013428</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0278612523001061</data>
  <data key="d3">Development and analysis of a holistic function-driven adaptive assembly strategy applied to micro gears</data>
  <data key="d4">A Khezri, V Schiller, L Homri, A Etienne…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1126142290960013428&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">15</data>
</node>
<node id="4Fr4jN5uC8oJ">
  <data key="d0">Segmentation of backscattered electron images of cement-based materials using lightweight U-Net with attention mechanism (LWAU-Net)</data>
  <data key="d1">4Fr4jN5uC8oJ</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S2352710223017278</data>
  <data key="d3">Segmentation of backscattered electron images of cement-based materials using lightweight U-Net with attention mechanism (LWAU-Net)</data>
  <data key="d4">P Li, W Zhao, C Fu, T Pan, X Ji</data>
  <data key="d5">2023</data>
  <data key="d8">15</data>
</node>
<node id="uHNtXrDhdSwJ">
  <data key="d0">YOLOv5-SFE: An algorithm fusing spatio-temporal features for detecting and recognizing workers' operating behaviors</data>
  <data key="d1">uHNtXrDhdSwJ</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1474034623001167</data>
  <data key="d3">YOLOv5-SFE: An algorithm fusing spatio-temporal features for detecting and recognizing workers' operating behaviors</data>
  <data key="d4">L Li, P Zhang, S Yang, W Jiao</data>
  <data key="d5">2023</data>
  <data key="d8">15</data>
</node>
<node id="11838073149065061192">
  <data key="d0">Dab-detr: Dynamic anchor boxes are better queries for detr</data>
  <data key="d1">11838073149065061192</data>
  <data key="d2">https://arxiv.org/abs/2201.12329</data>
  <data key="d3">Dab-detr: Dynamic anchor boxes are better queries for detr</data>
  <data key="d4">S Liu, F Li, H Zhang, X Yang, X Qi, H Su, J Zhu…</data>
  <data key="d5">2022</data>
  <data key="d6">251</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11838073149065061192&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3799744009906269739">
  <data key="d0">Petr: Position embedding transformation for multi-view 3d object detection</data>
  <data key="d1">3799744009906269739</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19812-0_31</data>
  <data key="d3">Petr: Position embedding transformation for multi-view 3d object detection</data>
  <data key="d4">Y Liu, T Wang, X Zhang, J Sun</data>
  <data key="d5">2022</data>
  <data key="d6">168</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3799744009906269739&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="9646853108847192407">
  <data key="d0">Dn-detr: Accelerate detr training by introducing query denoising</data>
  <data key="d1">9646853108847192407</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Li_DN-DETR_Accelerate_DETR_Training_by_Introducing_Query_DeNoising_CVPR_2022_paper.html</data>
  <data key="d3">Dn-detr: Accelerate detr training by introducing query denoising</data>
  <data key="d4">F Li, H Zhang, S Liu, J Guo, LM Ni…</data>
  <data key="d5">2022</data>
  <data key="d6">216</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9646853108847192407&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3777854110099563090">
  <data key="d0">Petrv2: A unified framework for 3d perception from multi-camera images</data>
  <data key="d1">3777854110099563090</data>
  <data key="d2">https://arxiv.org/abs/2206.01256</data>
  <data key="d3">Petrv2: A unified framework for 3d perception from multi-camera images</data>
  <data key="d4">Y Liu, J Yan, F Jia, S Li, A Gao, T Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">97</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3777854110099563090&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="18294495860499043250">
  <data key="d0">Oneformer: One transformer to rule universal image segmentation</data>
  <data key="d1">18294495860499043250</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Jain_OneFormer_One_Transformer_To_Rule_Universal_Image_Segmentation_CVPR_2023_paper.html</data>
  <data key="d3">Oneformer: One transformer to rule universal image segmentation</data>
  <data key="d4">J Jain, J Li, MT Chiu, A Hassani…</data>
  <data key="d5">2023</data>
  <data key="d6">45</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18294495860499043250&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12550135891789567985">
  <data key="d0">Detrs with hybrid matching</data>
  <data key="d1">12550135891789567985</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Jia_DETRs_With_Hybrid_Matching_CVPR_2023_paper.html</data>
  <data key="d3">Detrs with hybrid matching</data>
  <data key="d4">D Jia, Y Yuan, H He, X Wu, H Yu…</data>
  <data key="d5">2023</data>
  <data key="d6">50</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12550135891789567985&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">19</data>
</node>
<node id="1672665553767281734">
  <data key="d0">End-to-end object detection with transformers</data>
  <data key="d1">1672665553767281734</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-030-58452-8_13</data>
  <data key="d3">End-to-end object detection with transformers</data>
  <data key="d4">N Carion, F Massa, G Synnaeve, N Usunier…</data>
  <data key="d5">2020</data>
  <data key="d6">7808</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1672665553767281734&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="16898296257676733828">
  <data key="d0">Emergent abilities of large language models</data>
  <data key="d1">16898296257676733828</data>
  <data key="d2">https://arxiv.org/abs/2206.07682</data>
  <data key="d3">Emergent abilities of large language models</data>
  <data key="d4">J Wei, Y Tay, R Bommasani, C Raffel, B Zoph…</data>
  <data key="d5">2022</data>
  <data key="d6">631</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16898296257676733828&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="17636062869988382916">
  <data key="d0">Instructpix2pix: Learning to follow image editing instructions</data>
  <data key="d1">17636062869988382916</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.html</data>
  <data key="d3">Instructpix2pix: Learning to follow image editing instructions</data>
  <data key="d4">T Brooks, A Holynski, AA Efros</data>
  <data key="d5">2023</data>
  <data key="d6">177</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17636062869988382916&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="17712252571307454824">
  <data key="d0">Clip-adapter: Better vision-language models with feature adapters</data>
  <data key="d1">17712252571307454824</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11263-023-01891-x</data>
  <data key="d3">Clip-adapter: Better vision-language models with feature adapters</data>
  <data key="d4">P Gao, S Geng, R Zhang, T Ma, R Fang…</data>
  <data key="d5">2023</data>
  <data key="d6">253</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17712252571307454824&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="8649528985577473031">
  <data key="d0">Scaling language-image pre-training via masking</data>
  <data key="d1">8649528985577473031</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Li_Scaling_Language-Image_Pre-Training_via_Masking_CVPR_2023_paper.html</data>
  <data key="d3">Scaling language-image pre-training via masking</data>
  <data key="d4">Y Li, H Fan, R Hu…</data>
  <data key="d5">2023</data>
  <data key="d6">49</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8649528985577473031&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">2</data>
</node>
<node id="9439766841533136382">
  <data key="d0">Vitpose: Simple vision transformer baselines for human pose estimation</data>
  <data key="d1">9439766841533136382</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/fbb10d319d44f8c3b4720873e4177c65-Abstract-Conference.html</data>
  <data key="d3">Vitpose: Simple vision transformer baselines for human pose estimation</data>
  <data key="d4">Y Xu, J Zhang, Q Zhang, D Tao</data>
  <data key="d5">2022</data>
  <data key="d6">130</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9439766841533136382&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="11165298458048562314">
  <data key="d0">SegFormer: Simple and efficient design for semantic segmentation with transformers</data>
  <data key="d1">11165298458048562314</data>
  <data key="d2">https://proceedings.neurips.cc/paper/2021/hash/64f1f27bf1b4ec22924fd0acb550c235-Abstract.html</data>
  <data key="d3">SegFormer: Simple and efficient design for semantic segmentation with transformers</data>
  <data key="d4">E Xie, W Wang, Z Yu, A Anandkumar…</data>
  <data key="d5">2021</data>
  <data key="d6">1725</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11165298458048562314&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="12500443856801763727">
  <data key="d0">Scaling up your kernels to 31x31: Revisiting large kernel design in cnns</data>
  <data key="d1">12500443856801763727</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.html</data>
  <data key="d3">Scaling up your kernels to 31x31: Revisiting large kernel design in cnns</data>
  <data key="d4">X Ding, X Zhang, J Han, G Ding</data>
  <data key="d5">2022</data>
  <data key="d6">345</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12500443856801763727&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="8140812159859442226">
  <data key="d0">Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training</data>
  <data key="d1">8140812159859442226</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/416f9cb3276121c42eebb86352a4354a-Abstract-Conference.html</data>
  <data key="d3">Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training</data>
  <data key="d4">Z Tong, Y Song, J Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">303</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8140812159859442226&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">22</data>
</node>
<node id="15816136068893942524">
  <data key="d0">Swinir: Image restoration using swin transformer</data>
  <data key="d1">15816136068893942524</data>
  <data key="d2">https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html</data>
  <data key="d3">Swinir: Image restoration using swin transformer</data>
  <data key="d4">J Liang, J Cao, G Sun, K Zhang…</data>
  <data key="d5">2021</data>
  <data key="d6">1245</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15816136068893942524&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="9192461276991269473">
  <data key="d0">SwinFusion: Cross-domain long-range learning for general image fusion via swin transformer</data>
  <data key="d1">9192461276991269473</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9812535/</data>
  <data key="d3">SwinFusion: Cross-domain long-range learning for general image fusion via swin transformer</data>
  <data key="d4">J Ma, L Tang, F Fan, J Huang, X Mei…</data>
  <data key="d5">2022</data>
  <data key="d6">199</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9192461276991269473&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="17810188272131878569">
  <data key="d0">Self-supervised pre-training of swin transformers for 3d medical image analysis</data>
  <data key="d1">17810188272131878569</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.html</data>
  <data key="d3">Self-supervised pre-training of swin transformers for 3d medical image analysis</data>
  <data key="d4">Y Tang, D Yang, W Li, HR Roth…</data>
  <data key="d5">2022</data>
  <data key="d6">198</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17810188272131878569&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="17752815312316743733">
  <data key="d0">Adaptformer: Adapting vision transformers for scalable visual recognition</data>
  <data key="d1">17752815312316743733</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/69e2f49ab0837b71b0e0cb7c555990f8-Abstract-Conference.html</data>
  <data key="d3">Adaptformer: Adapting vision transformers for scalable visual recognition</data>
  <data key="d4">S Chen, C Ge, Z Tong, J Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">103</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17752815312316743733&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="4802045854930781683">
  <data key="d0">Beit: Bert pre-training of image transformers</data>
  <data key="d1">4802045854930781683</data>
  <data key="d2">https://arxiv.org/abs/2106.08254</data>
  <data key="d3">Beit: Bert pre-training of image transformers</data>
  <data key="d4">H Bao, L Dong, S Piao, F Wei</data>
  <data key="d5">2021</data>
  <data key="d6">1458</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4802045854930781683&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="16837829726140559426">
  <data key="d0">Masked autoencoders are scalable vision learners</data>
  <data key="d1">16837829726140559426</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper</data>
  <data key="d3">Masked autoencoders are scalable vision learners</data>
  <data key="d4">K He, X Chen, S Xie, Y Li, P Dollár…</data>
  <data key="d5">2022</data>
  <data key="d6">2930</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16837829726140559426&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="5833041667751260373">
  <data key="d0">Video swin transformer</data>
  <data key="d1">5833041667751260373</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Liu_Video_Swin_Transformer_CVPR_2022_paper.html</data>
  <data key="d3">Video swin transformer</data>
  <data key="d4">Z Liu, J Ning, Y Cao, Y Wei, Z Zhang…</data>
  <data key="d5">2022</data>
  <data key="d6">936</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5833041667751260373&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="1018521690946850362">
  <data key="d0">Do vision transformers see like convolutional neural networks?</data>
  <data key="d1">1018521690946850362</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2021/hash/652cf38361a209088302ba2b8b7f51e0-Abstract.html</data>
  <data key="d3">Do vision transformers see like convolutional neural networks?</data>
  <data key="d4">M Raghu, T Unterthiner, S Kornblith…</data>
  <data key="d5">2021</data>
  <data key="d6">521</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1018521690946850362&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">0</data>
</node>
<node id="6243645967630982889">
  <data key="d0">Transforming medical imaging with Transformers? A comparative review of key properties, current...</data>
  <data key="d1">6243645967630982889</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841523000233</data>
  <data key="d3">Transforming medical imaging with Transformers? A comparative review of key properties, current...</data>
  <data key="d4">J Li, J Chen, Y Tang, C Wang, BA Landman…</data>
  <data key="d5">2023</data>
  <data key="d6">56</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6243645967630982889&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="3936750546972103836">
  <data key="d0">Artificial intelligence for multimodal data integration in oncology</data>
  <data key="d1">3936750546972103836</data>
  <data key="d2">https://www.cell.com/cancer-cell/pdf/S1535-6108(22)00441-X.pdf</data>
  <data key="d3">Artificial intelligence for multimodal data integration in oncology</data>
  <data key="d4">J Lipkova, RJ Chen, B Chen, MY Lu, M Barbieri…</data>
  <data key="d5">2022</data>
  <data key="d6">66</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3936750546972103836&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="11927510402360104647">
  <data key="d0">Vision Transformers for Computational Histopathology</data>
  <data key="d1">11927510402360104647</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10190115/</data>
  <data key="d3">Vision Transformers for Computational Histopathology</data>
  <data key="d4">H Xu, Q Xu, F Cong, J Kang, C Han…</data>
  <data key="d5">2023</data>
  <data key="d6">3</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11927510402360104647&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="6187038158217452627">
  <data key="d0">Delving into masked autoencoders for multi-label thorax disease classification</data>
  <data key="d1">6187038158217452627</data>
  <data key="d2">https://openaccess.thecvf.com/content/WACV2023/html/Xiao_Delving_Into_Masked_Autoencoders_for_Multi-Label_Thorax_Disease_Classification_WACV_2023_paper.html</data>
  <data key="d3">Delving into masked autoencoders for multi-label thorax disease classification</data>
  <data key="d4">J Xiao, Y Bai, A Yuille, Z Zhou</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6187038158217452627&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="16029033897139067670">
  <data key="d0">UNesT: local spatial representation learning with hierarchical transformer for efficient medical segmentation</data>
  <data key="d1">16029033897139067670</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841523001998</data>
  <data key="d3">UNesT: local spatial representation learning with hierarchical transformer for efficient medical segmentation</data>
  <data key="d4">X Yu, Q Yang, Y Zhou, LY Cai, R Gao, HH Lee, T Li…</data>
  <data key="d5">2023</data>
  <data key="d6">7</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16029033897139067670&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="381090561678004553">
  <data key="d0">ReconFormer: Accelerated MRI reconstruction using recurrent transformer</data>
  <data key="d1">381090561678004553</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10251064/</data>
  <data key="d3">ReconFormer: Accelerated MRI reconstruction using recurrent transformer</data>
  <data key="d4">P Guo, Y Mei, J Zhou, S Jiang…</data>
  <data key="d5">2023</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=381090561678004553&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="12607038868340347612">
  <data key="d0">AIROGS: Artificial Intelligence for robust glaucoma screening challenge</data>
  <data key="d1">12607038868340347612</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10253652/</data>
  <data key="d3">AIROGS: Artificial Intelligence for robust glaucoma screening challenge</data>
  <data key="d4">C De Vente, KA Vermeer, N Jaccard…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12607038868340347612&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="4239598804369546383">
  <data key="d0">A review of machine learning applications for the proton MR spectroscopy workflow</data>
  <data key="d1">4239598804369546383</data>
  <data key="d2">https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.29793</data>
  <data key="d3">A review of machine learning applications for the proton MR spectroscopy workflow</data>
  <data key="d4">DMJ van de Sande, JP Merkofer…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4239598804369546383&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="681965397596551869">
  <data key="d0">Recent progress in transformer-based medical image analysis</data>
  <data key="d1">681965397596551869</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0010482523007333</data>
  <data key="d3">Recent progress in transformer-based medical image analysis</data>
  <data key="d4">Z Liu, Q Lv, Z Yang, Y Li, CH Lee, L Shen</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=681965397596551869&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="8582598101542675430">
  <data key="d0">CTUNet: automatic pancreas segmentation using a channel-wise transformer and 3D U-Net</data>
  <data key="d1">8582598101542675430</data>
  <data key="d2">https://link.springer.com/article/10.1007/s00371-022-02656-2</data>
  <data key="d3">CTUNet: automatic pancreas segmentation using a channel-wise transformer and 3D U-Net</data>
  <data key="d4">L Chen, L Wan</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8582598101542675430&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="10060803768210082111">
  <data key="d0">Vision transformers in medical imaging: A review</data>
  <data key="d1">10060803768210082111</data>
  <data key="d2">https://arxiv.org/abs/2211.10043</data>
  <data key="d3">Vision transformers in medical imaging: A review</data>
  <data key="d4">EU Henry, O Emebob, CA Omonhinmin</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10060803768210082111&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">17</data>
</node>
<node id="15741444728855576863">
  <data key="d0">Segment anything</data>
  <data key="d1">15741444728855576863</data>
  <data key="d2">https://arxiv.org/abs/2304.02643</data>
  <data key="d3">Segment anything</data>
  <data key="d4">A Kirillov, E Mintun, N Ravi, H Mao, C Rolland…</data>
  <data key="d5">2023</data>
  <data key="d6">650</data>
  <data key="d7">https://scholar.google.com/scholar?cites=15741444728855576863&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="8437346401990005173">
  <data key="d0">On the opportunities and challenges of foundation models for geospatial artificial intelligence</data>
  <data key="d1">8437346401990005173</data>
  <data key="d2">https://arxiv.org/abs/2304.06798</data>
  <data key="d3">On the opportunities and challenges of foundation models for geospatial artificial intelligence</data>
  <data key="d4">G Mai, W Huang, J Sun, S Song, D Mishra…</data>
  <data key="d5">2023</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8437346401990005173&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="2958442793928445860">
  <data key="d0">Large ai models in health informatics: Applications, challenges, and the future</data>
  <data key="d1">2958442793928445860</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/10261199/</data>
  <data key="d3">Large ai models in health informatics: Applications, challenges, and the future</data>
  <data key="d4">J Qiu, L Li, J Sun, J Peng, P Shi…</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2958442793928445860&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="14444698797987128655">
  <data key="d0">Combined scaling for zero-shot transfer learning</data>
  <data key="d1">14444698797987128655</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S0925231223007816</data>
  <data key="d3">Combined scaling for zero-shot transfer learning</data>
  <data key="d4">H Pham, Z Dai, G Ghiasi, K Kawaguchi, H Liu, AW Yu…</data>
  <data key="d5">2023</data>
  <data key="d6">82</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14444698797987128655&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="9083483030705185424">
  <data key="d0">Visual instruction tuning</data>
  <data key="d1">9083483030705185424</data>
  <data key="d2">https://arxiv.org/abs/2304.08485</data>
  <data key="d3">Visual instruction tuning</data>
  <data key="d4">H Liu, C Li, Q Wu, YJ Lee</data>
  <data key="d5">2023</data>
  <data key="d6">167</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9083483030705185424&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="640071900436442539">
  <data key="d0">Segment anything model for medical image analysis: an experimental study</data>
  <data key="d1">640071900436442539</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1361841523001780</data>
  <data key="d3">Segment anything model for medical image analysis: an experimental study</data>
  <data key="d4">MA Mazurowski, H Dong, H Gu, J Yang, N Konz…</data>
  <data key="d5">2023</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=640071900436442539&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="1608743531206453804">
  <data key="d0">Segment anything in medical images</data>
  <data key="d1">1608743531206453804</data>
  <data key="d2">https://arxiv.org/abs/2304.12306</data>
  <data key="d3">Segment anything in medical images</data>
  <data key="d4">J Ma, B Wang</data>
  <data key="d5">2023</data>
  <data key="d6">98</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1608743531206453804&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="10542319093857998430">
  <data key="d0">Can sam segment anything? when sam meets camouflaged object detection</data>
  <data key="d1">10542319093857998430</data>
  <data key="d2">https://arxiv.org/abs/2304.04709</data>
  <data key="d3">Can sam segment anything? when sam meets camouflaged object detection</data>
  <data key="d4">L Tang, H Xiao, B Li</data>
  <data key="d5">2023</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10542319093857998430&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="11173768905758826676">
  <data key="d0">Accuracy of segment-anything model (sam) in medical image segmentation tasks</data>
  <data key="d1">11173768905758826676</data>
  <data key="d2">https://arxiv.org/abs/2304.09324</data>
  <data key="d3">Accuracy of segment-anything model (sam) in medical image segmentation tasks</data>
  <data key="d4">S He, R Bao, J Li, PE Grant, Y Ou</data>
  <data key="d5">2023</data>
  <data key="d6">44</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11173768905758826676&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="4360518173578415769">
  <data key="d0">Rt-2: Vision-language-action models transfer web knowledge to robotic control</data>
  <data key="d1">4360518173578415769</data>
  <data key="d2">https://arxiv.org/abs/2307.15818</data>
  <data key="d3">Rt-2: Vision-language-action models transfer web knowledge to robotic control</data>
  <data key="d4">A Brohan, N Brown, J Carbajal, Y Chebotar…</data>
  <data key="d5">2023</data>
  <data key="d6">18</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4360518173578415769&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="4884243151249440145">
  <data key="d0">Segment anything model (sam) for digital pathology: Assess zero-shot segmentation on whole slide imaging</data>
  <data key="d1">4884243151249440145</data>
  <data key="d2">https://arxiv.org/abs/2304.04155</data>
  <data key="d3">Segment anything model (sam) for digital pathology: Assess zero-shot segmentation on whole slide imaging</data>
  <data key="d4">R Deng, C Cui, Q Liu, T Yao, LW Remedios…</data>
  <data key="d5">2023</data>
  <data key="d6">55</data>
  <data key="d7">https://scholar.google.com/scholar?cites=4884243151249440145&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">18</data>
</node>
<node id="12973370542610650225">
  <data key="d0">Reproducible scaling laws for contrastive language-image learning</data>
  <data key="d1">12973370542610650225</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper.html</data>
  <data key="d3">Reproducible scaling laws for contrastive language-image learning</data>
  <data key="d4">M Cherti, R Beaumont, R Wightman…</data>
  <data key="d5">2023</data>
  <data key="d6">68</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12973370542610650225&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="1879282532294332322">
  <data key="d0">Minigpt-4: Enhancing vision-language understanding with advanced large language models</data>
  <data key="d1">1879282532294332322</data>
  <data key="d2">https://arxiv.org/abs/2304.10592</data>
  <data key="d3">Minigpt-4: Enhancing vision-language understanding with advanced large language models</data>
  <data key="d4">D Zhu, J Chen, X Shen, X Li, M Elhoseiny</data>
  <data key="d5">2023</data>
  <data key="d6">155</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1879282532294332322&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3768131676399480172">
  <data key="d0">Are aligned neural networks adversarially aligned?</data>
  <data key="d1">3768131676399480172</data>
  <data key="d2">https://arxiv.org/abs/2306.15447</data>
  <data key="d3">Are aligned neural networks adversarially aligned?</data>
  <data key="d4">N Carlini, M Nasr, CA Choquette-Choo…</data>
  <data key="d5">2023</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3768131676399480172&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="2978288424037265447">
  <data key="d0">Eva-clip: Improved training techniques for clip at scale</data>
  <data key="d1">2978288424037265447</data>
  <data key="d2">https://arxiv.org/abs/2303.15389</data>
  <data key="d3">Eva-clip: Improved training techniques for clip at scale</data>
  <data key="d4">Q Sun, Y Fang, L Wu, X Wang, Y Cao</data>
  <data key="d5">2023</data>
  <data key="d6">26</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2978288424037265447&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="9021031822777383648">
  <data key="d0">A cookbook of self-supervised learning</data>
  <data key="d1">9021031822777383648</data>
  <data key="d2">https://arxiv.org/abs/2304.12210</data>
  <data key="d3">A cookbook of self-supervised learning</data>
  <data key="d4">R Balestriero, M Ibrahim, V Sobal, A Morcos…</data>
  <data key="d5">2023</data>
  <data key="d6">40</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9021031822777383648&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="7929876691840755441">
  <data key="d0">Visionllm: Large language model is also an open-ended decoder for vision-centric tasks</data>
  <data key="d1">7929876691840755441</data>
  <data key="d2">https://arxiv.org/abs/2305.11175</data>
  <data key="d3">Visionllm: Large language model is also an open-ended decoder for vision-centric tasks</data>
  <data key="d4">W Wang, Z Chen, X Chen, J Wu, X Zhu, G Zeng…</data>
  <data key="d5">2023</data>
  <data key="d6">24</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7929876691840755441&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3373629156419038558">
  <data key="d0">End-to-end autonomous driving: Challenges and frontiers</data>
  <data key="d1">3373629156419038558</data>
  <data key="d2">https://arxiv.org/abs/2306.16927</data>
  <data key="d3">End-to-end autonomous driving: Challenges and frontiers</data>
  <data key="d4">L Chen, P Wu, K Chitta, B Jaeger, A Geiger…</data>
  <data key="d5">2023</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3373629156419038558&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="9284111923828977279">
  <data key="d0">Foundational models defining a new era in vision: A survey and outlook</data>
  <data key="d1">9284111923828977279</data>
  <data key="d2">https://arxiv.org/abs/2307.13721</data>
  <data key="d3">Foundational models defining a new era in vision: A survey and outlook</data>
  <data key="d4">M Awais, M Naseer, S Khan, RM Anwer…</data>
  <data key="d5">2023</data>
  <data key="d6">2</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9284111923828977279&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="8586493250682863612">
  <data key="d0">BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision</data>
  <data key="d1">8586493250682863612</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Yang_BEVFormer_v2_Adapting_Modern_Image_Backbones_to_Birds-Eye-View_Recognition_via_CVPR_2023_paper.html</data>
  <data key="d3">BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision</data>
  <data key="d4">C Yang, Y Chen, H Tian, C Tao, X Zhu…</data>
  <data key="d5">2023</data>
  <data key="d6">29</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8586493250682863612&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="5337226914153811562">
  <data key="d0">Towards all-in-one pre-training via maximizing multi-modal mutual information</data>
  <data key="d1">5337226914153811562</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Su_Towards_All-in-One_Pre-Training_via_Maximizing_Multi-Modal_Mutual_Information_CVPR_2023_paper.html</data>
  <data key="d3">Towards all-in-one pre-training via maximizing multi-modal mutual information</data>
  <data key="d4">W Su, X Zhu, C Tao, L Lu, B Li…</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5337226914153811562&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="5343811397150749551">
  <data key="d0">Videochat: Chat-centric video understanding</data>
  <data key="d1">5343811397150749551</data>
  <data key="d2">https://arxiv.org/abs/2305.06355</data>
  <data key="d3">Videochat: Chat-centric video understanding</data>
  <data key="d4">KC Li, Y He, Y Wang, Y Li, W Wang, P Luo…</data>
  <data key="d5">2023</data>
  <data key="d6">30</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5343811397150749551&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3683089856994200319">
  <data key="d0">Dinov2: Learning robust visual features without supervision</data>
  <data key="d1">3683089856994200319</data>
  <data key="d2">https://arxiv.org/abs/2304.07193</data>
  <data key="d3">Dinov2: Learning robust visual features without supervision</data>
  <data key="d4">M Oquab, T Darcet, T Moutakanni, H Vo…</data>
  <data key="d5">2023</data>
  <data key="d6">52</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3683089856994200319&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="8072796476168839940">
  <data key="d0">Designing bert for convolutional networks: Sparse and hierarchical masked modeling</data>
  <data key="d1">8072796476168839940</data>
  <data key="d2">https://arxiv.org/abs/2301.03580</data>
  <data key="d3">Designing bert for convolutional networks: Sparse and hierarchical masked modeling</data>
  <data key="d4">K Tian, Y Jiang, Q Diao, C Lin, L Wang…</data>
  <data key="d5">2023</data>
  <data key="d6">20</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8072796476168839940&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="11224588046887428191">
  <data key="d0">Internchat: Solving vision-centric tasks by interacting with chatbots beyond language</data>
  <data key="d1">11224588046887428191</data>
  <data key="d2">https://arxiv.org/abs/2305.05662</data>
  <data key="d3">Internchat: Solving vision-centric tasks by interacting with chatbots beyond language</data>
  <data key="d4">Z Liu, Y He, W Wang, W Wang, Y Wang, S Chen…</data>
  <data key="d5">2023</data>
  <data key="d6">12</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11224588046887428191&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="17997891498068622933">
  <data key="d0">Mixformer: End-to-end tracking with iterative mixed attention</data>
  <data key="d1">17997891498068622933</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2022/html/Cui_MixFormer_End-to-End_Tracking_With_Iterative_Mixed_Attention_CVPR_2022_paper.html</data>
  <data key="d3">Mixformer: End-to-end tracking with iterative mixed attention</data>
  <data key="d4">Y Cui, C Jiang, L Wang, G Wu</data>
  <data key="d5">2022</data>
  <data key="d6">158</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17997891498068622933&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">22</data>
</node>
<node id="5167657309745527366">
  <data key="d0">Swintrack: A simple and strong baseline for transformer tracking</data>
  <data key="d1">5167657309745527366</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/6a5c23219f401f3efd322579002dbb80-Abstract-Conference.html</data>
  <data key="d3">Swintrack: A simple and strong baseline for transformer tracking</data>
  <data key="d4">L Lin, H Fan, Z Zhang, Y Xu…</data>
  <data key="d5">2022</data>
  <data key="d6">120</data>
  <data key="d7">https://scholar.google.com/scholar?cites=5167657309745527366&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">22</data>
</node>
<node id="10341392145675802868">
  <data key="d0">Revealing the dark secrets of masked image modeling</data>
  <data key="d1">10341392145675802868</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Xie_Revealing_the_Dark_Secrets_of_Masked_Image_Modeling_CVPR_2023_paper.html</data>
  <data key="d3">Revealing the dark secrets of masked image modeling</data>
  <data key="d4">Z Xie, Z Geng, J Hu, Z Zhang…</data>
  <data key="d5">2023</data>
  <data key="d6">42</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10341392145675802868&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">22</data>
</node>
<node id="17928092202816562244">
  <data key="d0">Universal instance perception as object discovery and retrieval</data>
  <data key="d1">17928092202816562244</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Yan_Universal_Instance_Perception_As_Object_Discovery_and_Retrieval_CVPR_2023_paper.html</data>
  <data key="d3">Universal instance perception as object discovery and retrieval</data>
  <data key="d4">B Yan, Y Jiang, J Wu, D Wang, P Luo…</data>
  <data key="d5">2023</data>
  <data key="d6">13</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17928092202816562244&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">22</data>
</node>
<node id="9093499936003644917">
  <data key="d0">Decoupling features in hierarchical propagation for video object segmentation</data>
  <data key="d1">9093499936003644917</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/eb890c36af87e4ca82e8ef7bcba6a284-Abstract-Conference.html</data>
  <data key="d3">Decoupling features in hierarchical propagation for video object segmentation</data>
  <data key="d4">Z Yang, Y Yang</data>
  <data key="d5">2022</data>
  <data key="d6">28</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9093499936003644917&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">22</data>
</node>
<node id="6209180784126725956">
  <data key="d0">Divert more attention to vision-language tracking</data>
  <data key="d1">6209180784126725956</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/1c8c87c36dc1e49e63555f95fa56b153-Abstract-Conference.html</data>
  <data key="d3">Divert more attention to vision-language tracking</data>
  <data key="d4">M Guo, Z Zhang, H Fan, L Jing</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6209180784126725956&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">22</data>
</node>
<node id="14246245970329145306">
  <data key="d0">Correlational Image Modeling for Self-Supervised Visual Pre-Training</data>
  <data key="d1">14246245970329145306</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Li_Correlational_Image_Modeling_for_Self-Supervised_Visual_Pre-Training_CVPR_2023_paper.html</data>
  <data key="d3">Correlational Image Modeling for Self-Supervised Visual Pre-Training</data>
  <data key="d4">W Li, J Xie, CC Loy</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14246245970329145306&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">22</data>
</node>
<node id="17211572998346535120">
  <data key="d0">Fully convolutional online tracking</data>
  <data key="d1">17211572998346535120</data>
  <data key="d2">https://www.sciencedirect.com/science/article/pii/S1077314222001254</data>
  <data key="d3">Fully convolutional online tracking</data>
  <data key="d4">Y Cui, C Jiang, L Wang, G Wu</data>
  <data key="d5">2022</data>
  <data key="d6">41</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17211572998346535120&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">22</data>
</node>
<node id="9686322700146945720">
  <data key="d0">A Unified Transformer Based Tracker for Anti-UAV Tracking</data>
  <data key="d1">9686322700146945720</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/html/Yu_A_Unified_Transformer_Based_Tracker_for_Anti-UAV_Tracking_CVPRW_2023_paper.html</data>
  <data key="d3">A Unified Transformer Based Tracker for Anti-UAV Tracking</data>
  <data key="d4">Q Yu, Y Ma, J He, D Yang…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9686322700146945720&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">22</data>
</node>
<node id="2022354001457069046">
  <data key="d0">Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond</data>
  <data key="d1">2022354001457069046</data>
  <data key="d2">https://link.springer.com/article/10.1007/s11263-022-01739-w</data>
  <data key="d3">Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond</data>
  <data key="d4">Q Zhang, Y Xu, J Zhang, D Tao</data>
  <data key="d5">2023</data>
  <data key="d6">116</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2022354001457069046&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="14613261815949086918">
  <data key="d0">Advancing plain vision transformer toward remote sensing foundation model</data>
  <data key="d1">14613261815949086918</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9956816/</data>
  <data key="d3">Advancing plain vision transformer toward remote sensing foundation model</data>
  <data key="d4">D Wang, Q Zhang, Y Xu, J Zhang, B Du…</data>
  <data key="d5">2022</data>
  <data key="d6">46</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14613261815949086918&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="6979440938086217563">
  <data key="d0">Decoupling human and camera motion from videos in the wild</data>
  <data key="d1">6979440938086217563</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Ye_Decoupling_Human_and_Camera_Motion_From_Videos_in_the_Wild_CVPR_2023_paper.html</data>
  <data key="d3">Decoupling human and camera motion from videos in the wild</data>
  <data key="d4">V Ye, G Pavlakos, J Malik…</data>
  <data key="d5">2023</data>
  <data key="d6">14</data>
  <data key="d7">https://scholar.google.com/scholar?cites=6979440938086217563&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="7134900495559356797">
  <data key="d0">Vsa: Learning varied-size window attention in vision transformers</data>
  <data key="d1">7134900495559356797</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19806-9_27</data>
  <data key="d3">Vsa: Learning varied-size window attention in vision transformers</data>
  <data key="d4">Q Zhang, Y Xu, J Zhang, D Tao</data>
  <data key="d5">2022</data>
  <data key="d6">36</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7134900495559356797&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="8918294861982063782">
  <data key="d0">TRACE: 5D temporal regression of avatars with dynamic cameras in 3D environments</data>
  <data key="d1">8918294861982063782</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Sun_TRACE_5D_Temporal_Regression_of_Avatars_With_Dynamic_Cameras_in_CVPR_2023_paper.html</data>
  <data key="d3">TRACE: 5D temporal regression of avatars with dynamic cameras in 3D environments</data>
  <data key="d4">Y Sun, Q Bao, W Liu, T Mei…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=8918294861982063782&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="9002295544688722648">
  <data key="d0">Regioncl: exploring contrastive region pairs for self-supervised representation learning</data>
  <data key="d1">9002295544688722648</data>
  <data key="d2">https://link.springer.com/chapter/10.1007/978-3-031-19827-4_28</data>
  <data key="d3">Regioncl: exploring contrastive region pairs for self-supervised representation learning</data>
  <data key="d4">Y Xu, Q Zhang, J Zhang, D Tao</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9002295544688722648&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="530860336847622267">
  <data key="d0">Unsupervised volumetric animation</data>
  <data key="d1">530860336847622267</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Siarohin_Unsupervised_Volumetric_Animation_CVPR_2023_paper.html</data>
  <data key="d3">Unsupervised volumetric animation</data>
  <data key="d4">A Siarohin, W Menapace…</data>
  <data key="d5">2023</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=530860336847622267&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="10626359895994634728">
  <data key="d0">Vision-based human pose estimation via deep learning: A survey</data>
  <data key="d1">10626359895994634728</data>
  <data key="d2">https://ieeexplore.ieee.org/abstract/document/9955393/</data>
  <data key="d3">Vision-based human pose estimation via deep learning: A survey</data>
  <data key="d4">G Lan, Y Wu, F Hu, Q Hao</data>
  <data key="d5">2022</data>
  <data key="d6">8</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10626359895994634728&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="3429308917664707403">
  <data key="d0">Expediting large-scale vision transformer for dense prediction without fine-tuning</data>
  <data key="d1">3429308917664707403</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/e6c2e85db1f1039177c4495ccd399ac4-Abstract-Conference.html</data>
  <data key="d3">Expediting large-scale vision transformer for dense prediction without fine-tuning</data>
  <data key="d4">W Liang, Y Yuan, H Ding, X Luo…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3429308917664707403&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="16223923155940056199">
  <data key="d0">HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining</data>
  <data key="d1">16223923155940056199</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Tang_HumanBench_Towards_General_Human-Centric_Perception_With_Projector_Assisted_Pretraining_CVPR_2023_paper.html</data>
  <data key="d3">HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining</data>
  <data key="d4">S Tang, C Chen, Q Xie, M Chen…</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16223923155940056199&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">16</data>
</node>
<node id="14316138733173377966">
  <data key="d0">Vision transformer adapter for dense predictions</data>
  <data key="d1">14316138733173377966</data>
  <data key="d2">https://arxiv.org/abs/2205.08534</data>
  <data key="d3">Vision transformer adapter for dense predictions</data>
  <data key="d4">Z Chen, Y Duan, W Wang, J He, T Lu, J Dai…</data>
  <data key="d5">2022</data>
  <data key="d6">161</data>
  <data key="d7">https://scholar.google.com/scholar?cites=14316138733173377966&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="3584347529323049886">
  <data key="d0">Fast vision transformers with hilo attention</data>
  <data key="d1">3584347529323049886</data>
  <data key="d2">https://proceedings.neurips.cc/paper_files/paper/2022/hash/5d5f703ee1dedbfe324b1872f44db939-Abstract-Conference.html</data>
  <data key="d3">Fast vision transformers with hilo attention</data>
  <data key="d4">Z Pan, J Cai, B Zhuang</data>
  <data key="d5">2022</data>
  <data key="d6">48</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3584347529323049886&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="17264780080323807782">
  <data key="d0">Metaformer baselines for vision</data>
  <data key="d1">17264780080323807782</data>
  <data key="d2">https://arxiv.org/abs/2210.13452</data>
  <data key="d3">Metaformer baselines for vision</data>
  <data key="d4">W Yu, C Si, P Zhou, M Luo, Y Zhou, J Feng…</data>
  <data key="d5">2022</data>
  <data key="d6">22</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17264780080323807782&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="16657130336390881372">
  <data key="d0">Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?</data>
  <data key="d1">16657130336390881372</data>
  <data key="d2">https://arxiv.org/abs/2212.08320</data>
  <data key="d3">Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?</data>
  <data key="d4">R Dong, Z Qi, L Zhang, J Zhang, J Sun, Z Ge…</data>
  <data key="d5">2022</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16657130336390881372&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="16345516296958117809">
  <data key="d0">Conv2former: A simple transformer-style convnet for visual recognition</data>
  <data key="d1">16345516296958117809</data>
  <data key="d2">https://arxiv.org/abs/2211.11943</data>
  <data key="d3">Conv2former: A simple transformer-style convnet for visual recognition</data>
  <data key="d4">Q Hou, CZ Lu, MM Cheng, J Feng</data>
  <data key="d5">2022</data>
  <data key="d6">21</data>
  <data key="d7">https://scholar.google.com/scholar?cites=16345516296958117809&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12345969996209231071">
  <data key="d0">Insulator-defect detection algorithm based on improved YOLOv7</data>
  <data key="d1">12345969996209231071</data>
  <data key="d2">https://www.mdpi.com/1424-8220/22/22/8801</data>
  <data key="d3">Insulator-defect detection algorithm based on improved YOLOv7</data>
  <data key="d4">J Zheng, H Wu, H Zhang, Z Wang, W Xu</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12345969996209231071&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="11904702717564235743">
  <data key="d0">Demystify transformers &amp; convolutions in modern image deep networks</data>
  <data key="d1">11904702717564235743</data>
  <data key="d2">https://arxiv.org/abs/2211.05781</data>
  <data key="d3">Demystify transformers &amp; convolutions in modern image deep networks</data>
  <data key="d4">J Dai, M Shi, W Wang, S Wu, L Xing, W Wang…</data>
  <data key="d5">2022</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=11904702717564235743&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="9556534593478071448">
  <data key="d0">Generalized decoding for pixel, image, and language</data>
  <data key="d1">9556534593478071448</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.html</data>
  <data key="d3">Generalized decoding for pixel, image, and language</data>
  <data key="d4">X Zou, ZY Dou, J Yang, Z Gan, L Li…</data>
  <data key="d5">2023</data>
  <data key="d6">35</data>
  <data key="d7">https://scholar.google.com/scholar?cites=9556534593478071448&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="17595429764835777254">
  <data key="d0">Open vocabulary semantic segmentation with patch aligned contrastive learning</data>
  <data key="d1">17595429764835777254</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Mukhoti_Open_Vocabulary_Semantic_Segmentation_With_Patch_Aligned_Contrastive_Learning_CVPR_2023_paper.html</data>
  <data key="d3">Open vocabulary semantic segmentation with patch aligned contrastive learning</data>
  <data key="d4">J Mukhoti, TY Lin, O Poursaeed…</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17595429764835777254&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="2341964736485396655">
  <data key="d0">Balancing Logit Variation for Long-tailed Semantic Segmentation</data>
  <data key="d1">2341964736485396655</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Wang_Balancing_Logit_Variation_for_Long-Tailed_Semantic_Segmentation_CVPR_2023_paper.html</data>
  <data key="d3">Balancing Logit Variation for Long-tailed Semantic Segmentation</data>
  <data key="d4">Y Wang, J Fei, H Wang, W Li, T Bao…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=2341964736485396655&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<node id="12388611019049432202">
  <data key="d0">Deepsolo: Let transformer decoder with explicit points solo for text spotting</data>
  <data key="d1">12388611019049432202</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Ye_DeepSolo_Let_Transformer_Decoder_With_Explicit_Points_Solo_for_Text_CVPR_2023_paper.html</data>
  <data key="d3">Deepsolo: Let transformer decoder with explicit points solo for text spotting</data>
  <data key="d4">M Ye, J Zhang, S Zhao, J Liu, T Liu…</data>
  <data key="d5">2023</data>
  <data key="d6">11</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12388611019049432202&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">19</data>
</node>
<node id="1436259968817456407">
  <data key="d0">Dense Distinct Query for End-to-End Object Detection</data>
  <data key="d1">1436259968817456407</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Dense_Distinct_Query_for_End-to-End_Object_Detection_CVPR_2023_paper.html</data>
  <data key="d3">Dense Distinct Query for End-to-End Object Detection</data>
  <data key="d4">S Zhang, X Wang, J Wang, J Pang…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=1436259968817456407&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">19</data>
</node>
<node id="10221363205500346797">
  <data key="d0">Lite DETR: An interleaved multi-scale encoder for efficient detr</data>
  <data key="d1">10221363205500346797</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Li_Lite_DETR_An_Interleaved_Multi-Scale_Encoder_for_Efficient_DETR_CVPR_2023_paper.html</data>
  <data key="d3">Lite DETR: An interleaved multi-scale encoder for efficient detr</data>
  <data key="d4">F Li, A Zeng, S Liu, H Zhang, H Li…</data>
  <data key="d5">2023</data>
  <data key="d6">10</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10221363205500346797&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">19</data>
</node>
<node id="18387677115510147109">
  <data key="d0">Detrs with collaborative hybrid assignments training</data>
  <data key="d1">18387677115510147109</data>
  <data key="d2">https://arxiv.org/abs/2211.12860</data>
  <data key="d3">Detrs with collaborative hybrid assignments training</data>
  <data key="d4">Z Zong, G Song, Y Liu</data>
  <data key="d5">2022</data>
  <data key="d6">19</data>
  <data key="d7">https://scholar.google.com/scholar?cites=18387677115510147109&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">19</data>
</node>
<node id="10401914588824198986">
  <data key="d0">Transformer-based visual segmentation: A survey</data>
  <data key="d1">10401914588824198986</data>
  <data key="d2">https://arxiv.org/abs/2304.09854</data>
  <data key="d3">Transformer-based visual segmentation: A survey</data>
  <data key="d4">X Li, H Ding, W Zhang, H Yuan, J Pang…</data>
  <data key="d5">2023</data>
  <data key="d6">17</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10401914588824198986&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">19</data>
</node>
<node id="10714034727295496106">
  <data key="d0">Enhanced Training of Query-Based Object Detection via Selective Query Recollection</data>
  <data key="d1">10714034727295496106</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Chen_Enhanced_Training_of_Query-Based_Object_Detection_via_Selective_Query_Recollection_CVPR_2023_paper.html</data>
  <data key="d3">Enhanced Training of Query-Based Object Detection via Selective Query Recollection</data>
  <data key="d4">F Chen, H Zhang, K Hu, YK Huang…</data>
  <data key="d5">2023</data>
  <data key="d6">4</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10714034727295496106&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">19</data>
</node>
<node id="7090138214741207408">
  <data key="d0">One-to-Few Label Assignment for End-to-End Dense Detection</data>
  <data key="d1">7090138214741207408</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Li_One-to-Few_Label_Assignment_for_End-to-End_Dense_Detection_CVPR_2023_paper.html</data>
  <data key="d3">One-to-Few Label Assignment for End-to-End Dense Detection</data>
  <data key="d4">S Li, M Li, R Li, C He, L Zhang</data>
  <data key="d5">2023</data>
  <data key="d6">1</data>
  <data key="d7">https://scholar.google.com/scholar?cites=7090138214741207408&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">19</data>
</node>
<node id="10928542084451319450">
  <data key="d0">Nms strikes back</data>
  <data key="d1">10928542084451319450</data>
  <data key="d2">https://arxiv.org/abs/2212.06137</data>
  <data key="d3">Nms strikes back</data>
  <data key="d4">J Ouyang-Zhang, JH Cho, X Zhou…</data>
  <data key="d5">2022</data>
  <data key="d6">15</data>
  <data key="d7">https://scholar.google.com/scholar?cites=10928542084451319450&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">19</data>
</node>
<node id="17976854884426501420">
  <data key="d0">End-to-end 3d dense captioning with vote2cap-detr</data>
  <data key="d1">17976854884426501420</data>
  <data key="d2">http://openaccess.thecvf.com/content/CVPR2023/html/Chen_End-to-End_3D_Dense_Captioning_With_Vote2Cap-DETR_CVPR_2023_paper.html</data>
  <data key="d3">End-to-end 3d dense captioning with vote2cap-detr</data>
  <data key="d4">S Chen, H Zhu, X Chen, Y Lei…</data>
  <data key="d5">2023</data>
  <data key="d6">5</data>
  <data key="d7">https://scholar.google.com/scholar?cites=17976854884426501420&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">19</data>
</node>
<node id="3308763779023912371">
  <data key="d0">D
ETR: Decoder Distillation for Detection Transformer</data>
  <data key="d1">3308763779023912371</data>
  <data key="d2">https://arxiv.org/abs/2211.09768</data>
  <data key="d3">D
ETR: Decoder Distillation for Detection Transformer</data>
  <data key="d4">X Chen, J Chen, Y Liu, G Zeng</data>
  <data key="d5">2022</data>
  <data key="d6">6</data>
  <data key="d7">https://scholar.google.com/scholar?cites=3308763779023912371&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">19</data>
</node>
<node id="12563424232401784595">
  <data key="d0">Scaled-yolov4: Scaling cross stage partial network</data>
  <data key="d1">12563424232401784595</data>
  <data key="d2">https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Scaled-YOLOv4_Scaling_Cross_Stage_Partial_Network_CVPR_2021_paper.html?ref=</data>
  <data key="d3">Scaled-yolov4: Scaling cross stage partial network</data>
  <data key="d4">CY Wang, A Bochkovskiy…</data>
  <data key="d5">2021</data>
  <data key="d6">1143</data>
  <data key="d7">https://scholar.google.com/scholar?cites=12563424232401784595&amp;as_sdt=40005&amp;sciodt=0,10&amp;hl=en</data>
  <data key="d8">1</data>
</node>
<edge source="7522504961268153944" target="6382612685700818764"/>
<edge source="7522504961268153944" target="16431204865977056518"/>
<edge source="7522504961268153944" target="10110104334022835757"/>
<edge source="7522504961268153944" target="15635397108812213817"/>
<edge source="7522504961268153944" target="15172072370662904150"/>
<edge source="7522504961268153944" target="8793029896395507010"/>
<edge source="7522504961268153944" target="1672665553767281734"/>
<edge source="7522504961268153944" target="11165298458048562314"/>
<edge source="7522504961268153944" target="15816136068893942524"/>
<edge source="7522504961268153944" target="3458396398389387877"/>
<edge source="7522504961268153944" target="7329647594369932315"/>
<edge source="15456065911372617945" target="7522504961268153944"/>
<edge source="15456065911372617945" target="10110104334022835757"/>
<edge source="15456065911372617945" target="8793029896395507010"/>
<edge source="15456065911372617945" target="11838073149065061192"/>
<edge source="15456065911372617945" target="1672665553767281734"/>
<edge source="15456065911372617945" target="11165298458048562314"/>
<edge source="15456065911372617945" target="3458396398389387877"/>
<edge source="15456065911372617945" target="7329647594369932315"/>
<edge source="982391967541643955" target="15456065911372617945"/>
<edge source="982391967541643955" target="7749897961068121501"/>
<edge source="982391967541643955" target="15172072370662904150"/>
<edge source="2452866517197292093" target="15456065911372617945"/>
<edge source="4773463079530656035" target="15456065911372617945"/>
<edge source="4773463079530656035" target="10110104334022835757"/>
<edge source="4773463079530656035" target="11838073149065061192"/>
<edge source="4773463079530656035" target="11165298458048562314"/>
<edge source="761718241536208511" target="15456065911372617945"/>
<edge source="761718241536208511" target="11165298458048562314"/>
<edge source="7104781172538541114" target="15456065911372617945"/>
<edge source="7104781172538541114" target="18210588638302847093"/>
<edge source="6491078858607146383" target="15456065911372617945"/>
<edge source="11978445553624214380" target="15456065911372617945"/>
<edge source="5228146784334715443" target="15456065911372617945"/>
<edge source="9099615620722636165" target="15456065911372617945"/>
<edge source="9099615620722636165" target="18210588638302847093"/>
<edge source="10884589459641707712" target="15456065911372617945"/>
<edge source="7749897961068121501" target="7522504961268153944"/>
<edge source="1539076789580815483" target="7749897961068121501"/>
<edge source="966567457136989804" target="7749897961068121501"/>
<edge source="10761248177036470713" target="7749897961068121501"/>
<edge source="10761248177036470713" target="15635397108812213817"/>
<edge source="14136709172791920331" target="7749897961068121501"/>
<edge source="14136709172791920331" target="17327663970405370182"/>
<edge source="13597902966753793310" target="7749897961068121501"/>
<edge source="4431578198915484435" target="7749897961068121501"/>
<edge source="2600515932282922845" target="7749897961068121501"/>
<edge source="8053588590478703627" target="7749897961068121501"/>
<edge source="15393921212791157727" target="7749897961068121501"/>
<edge source="16431204865977056518" target="7522504961268153944"/>
<edge source="16431204865977056518" target="14988305211504802629"/>
<edge source="16431204865977056518" target="8793029896395507010"/>
<edge source="16431204865977056518" target="11165298458048562314"/>
<edge source="16431204865977056518" target="15816136068893942524"/>
<edge source="16431204865977056518" target="3458396398389387877"/>
<edge source="16932998913230259066" target="16431204865977056518"/>
<edge source="16932998913230259066" target="15816136068893942524"/>
<edge source="498268664873674535" target="16431204865977056518"/>
<edge source="498268664873674535" target="15816136068893942524"/>
<edge source="9110410135628850117" target="16431204865977056518"/>
<edge source="7710701073211724386" target="16431204865977056518"/>
<edge source="13272942409666364496" target="16431204865977056518"/>
<edge source="12297468854729392143" target="16431204865977056518"/>
<edge source="5364344454304770774" target="16431204865977056518"/>
<edge source="3464402829187061665" target="16431204865977056518"/>
<edge source="15226748689642581218" target="16431204865977056518"/>
<edge source="10110104334022835757" target="7522504961268153944"/>
<edge source="10110104334022835757" target="8793029896395507010"/>
<edge source="14443907969977981621" target="10110104334022835757"/>
<edge source="14443907969977981621" target="7329647594369932315"/>
<edge source="13501013621324561884" target="10110104334022835757"/>
<edge source="13629397900287809862" target="10110104334022835757"/>
<edge source="13629397900287809862" target="7770442917120891581"/>
<edge source="13629397900287809862" target="9595110325981705564"/>
<edge source="1273811038957334386" target="10110104334022835757"/>
<edge source="1273811038957334386" target="2321980635951558135"/>
<edge source="610621467807251926" target="10110104334022835757"/>
<edge source="610621467807251926" target="6784655767122395745"/>
<edge source="610621467807251926" target="11165298458048562314"/>
<edge source="18380770342348927722" target="10110104334022835757"/>
<edge source="9618435703828650575" target="10110104334022835757"/>
<edge source="14988305211504802629" target="7522504961268153944"/>
<edge source="14031000766044293652" target="14988305211504802629"/>
<edge source="14031000766044293652" target="15816136068893942524"/>
<edge source="16381083981417715623" target="14988305211504802629"/>
<edge source="6999508588420552953" target="14988305211504802629"/>
<edge source="18275282813589182456" target="14988305211504802629"/>
<edge source="18275282813589182456" target="15816136068893942524"/>
<edge source="11948207786179445379" target="14988305211504802629"/>
<edge source="2731814227384441305" target="14988305211504802629"/>
<edge source="563900006853828372" target="14988305211504802629"/>
<edge source="2892557376887066282" target="14988305211504802629"/>
<edge source="10807347079650485654" target="14988305211504802629"/>
<edge source="15635397108812213817" target="7522504961268153944"/>
<edge source="7329647594369932315" target="15635397108812213817"/>
<edge source="7329647594369932315" target="3458396398389387877"/>
<edge source="7329647594369932315" target="1672665553767281734"/>
<edge source="4431453089685809340" target="15635397108812213817"/>
<edge source="4278610892084589339" target="15635397108812213817"/>
<edge source="4278610892084589339" target="11165298458048562314"/>
<edge source="601129416962130879" target="15635397108812213817"/>
<edge source="4326704403467340422" target="15635397108812213817"/>
<edge source="18177167198432349205" target="15635397108812213817"/>
<edge source="342346550438939327" target="15635397108812213817"/>
<edge source="9632085609200326616" target="15635397108812213817"/>
<edge source="15172072370662904150" target="7522504961268153944"/>
<edge source="6668235945473015803" target="15172072370662904150"/>
<edge source="2316302132679082774" target="15172072370662904150"/>
<edge source="4388759310460601633" target="15172072370662904150"/>
<edge source="3041067607452518927" target="15172072370662904150"/>
<edge source="875131557547078483" target="15172072370662904150"/>
<edge source="17891879498080154736" target="15172072370662904150"/>
<edge source="2028336304446280911" target="15172072370662904150"/>
<edge source="5601871542106060008" target="15172072370662904150"/>
<edge source="14311400318178337111" target="7522504961268153944"/>
<edge source="14311400318178337111" target="9850512646184180167"/>
<edge source="14311400318178337111" target="18210588638302847093"/>
<edge source="11789051068432887660" target="14311400318178337111"/>
<edge source="11789051068432887660" target="11531242419091815801"/>
<edge source="15295068953900215294" target="14311400318178337111"/>
<edge source="15549291371117213871" target="14311400318178337111"/>
<edge source="9884544045603971658" target="14311400318178337111"/>
<edge source="9591435289724766439" target="14311400318178337111"/>
<edge source="10628215061802232868" target="14311400318178337111"/>
<edge source="9532821550175512200" target="14311400318178337111"/>
<edge source="12985976504909847163" target="14311400318178337111"/>
<edge source="12985976504909847163" target="4488471448867589825"/>
<edge source="2812153438552156646" target="14311400318178337111"/>
<edge source="14349697475909471320" target="14311400318178337111"/>
<edge source="17327663970405370182" target="7522504961268153944"/>
<edge source="1930827783125608869" target="17327663970405370182"/>
<edge source="14072888861532659606" target="17327663970405370182"/>
<edge source="1118274744677998915" target="17327663970405370182"/>
<edge source="8230127879015912569" target="17327663970405370182"/>
<edge source="12182702384297284737" target="17327663970405370182"/>
<edge source="17896705208502572006" target="17327663970405370182"/>
<edge source="477874232529254013" target="17327663970405370182"/>
<edge source="16387925596110304701" target="17327663970405370182"/>
<edge source="16387925596110304701" target="12938213222665733645"/>
<edge source="7193625896865391995" target="17327663970405370182"/>
<edge source="6784655767122395745" target="7522504961268153944"/>
<edge source="10588342779298269046" target="6784655767122395745"/>
<edge source="10588342779298269046" target="7039269942427062691"/>
<edge source="10588342779298269046" target="4490918786976048296"/>
<edge source="10588342779298269046" target="12938213222665733645"/>
<edge source="10588342779298269046" target="14316138733173377966"/>
<edge source="12938213222665733645" target="6784655767122395745"/>
<edge source="12938213222665733645" target="7039269942427062691"/>
<edge source="12938213222665733645" target="17997891498068622933"/>
<edge source="12938213222665733645" target="4490918786976048296"/>
<edge source="12938213222665733645" target="14316138733173377966"/>
<edge source="1388490151733704334" target="6784655767122395745"/>
<edge source="15088728781552938978" target="6784655767122395745"/>
<edge source="12692106295877813680" target="6784655767122395745"/>
<edge source="10239175441885037567" target="6784655767122395745"/>
<edge source="10239175441885037567" target="14638466021176544465"/>
<edge source="2000389979125404276" target="6784655767122395745"/>
<edge source="14170076594522259195" target="6784655767122395745"/>
<edge source="5425241495538385765" target="6784655767122395745"/>
<edge source="4041041901496425203" target="6382612685700818764"/>
<edge source="8136644755673586417" target="4041041901496425203"/>
<edge source="7358070883597795569" target="8136644755673586417"/>
<edge source="2188347889974787509" target="8136644755673586417"/>
<edge source="17352312443164396046" target="8136644755673586417"/>
<edge source="17997900354107908438" target="8136644755673586417"/>
<edge source="9888405418392239400" target="8136644755673586417"/>
<edge source="14217745962583788184" target="8136644755673586417"/>
<edge source="3134740227865659264" target="8136644755673586417"/>
<edge source="1681999640612811907" target="8136644755673586417"/>
<edge source="7025715449086074571" target="8136644755673586417"/>
<edge source="4162777562069804924" target="8136644755673586417"/>
<edge source="6141939658324331542" target="4041041901496425203"/>
<edge source="563969953758433323" target="6141939658324331542"/>
<edge source="2331420292369119754" target="6141939658324331542"/>
<edge source="13012757373810467293" target="6141939658324331542"/>
<edge source="1751297388377231685" target="6141939658324331542"/>
<edge source="5520588759774186774" target="6141939658324331542"/>
<edge source="11053439912877888530" target="6141939658324331542"/>
<edge source="4856010013404526375" target="6141939658324331542"/>
<edge source="3951350311895905492" target="6141939658324331542"/>
<edge source="9663211362112210725" target="6141939658324331542"/>
<edge source="727216592699071177" target="6141939658324331542"/>
<edge source="7770442917120891581" target="4041041901496425203"/>
<edge source="7770442917120891581" target="12987945369444025427"/>
<edge source="11430520233869250304" target="7770442917120891581"/>
<edge source="11430520233869250304" target="2325917221075842848"/>
<edge source="5562955281835677624" target="7770442917120891581"/>
<edge source="5562955281835677624" target="12987945369444025427"/>
<edge source="5562955281835677624" target="2325917221075842848"/>
<edge source="2325917221075842848" target="7770442917120891581"/>
<edge source="2325917221075842848" target="1672665553767281734"/>
<edge source="2325917221075842848" target="3458396398389387877"/>
<edge source="8018158103125985189" target="7770442917120891581"/>
<edge source="8018158103125985189" target="2325917221075842848"/>
<edge source="11255276306904968426" target="7770442917120891581"/>
<edge source="11255276306904968426" target="2325917221075842848"/>
<edge source="11255276306904968426" target="10588342779298269046"/>
<edge source="5385256443321262916" target="7770442917120891581"/>
<edge source="17485588102904105060" target="7770442917120891581"/>
<edge source="5354355722706795291" target="7770442917120891581"/>
<edge source="972963521258961554" target="7770442917120891581"/>
<edge source="8793029896395507010" target="4041041901496425203"/>
<edge source="8793029896395507010" target="3458396398389387877"/>
<edge source="8793029896395507010" target="7329647594369932315"/>
<edge source="8793029896395507010" target="1672665553767281734"/>
<edge source="3458396398389387877" target="8793029896395507010"/>
<edge source="3458396398389387877" target="2321980635951558135"/>
<edge source="3458396398389387877" target="1672665553767281734"/>
<edge source="10553738615668616847" target="8793029896395507010"/>
<edge source="11517447940529951525" target="8793029896395507010"/>
<edge source="4461602603986165987" target="8793029896395507010"/>
<edge source="1788827408361087894" target="8793029896395507010"/>
<edge source="15154755818357511167" target="8793029896395507010"/>
<edge source="9595110325981705564" target="4041041901496425203"/>
<edge source="9595110325981705564" target="12987945369444025427"/>
<edge source="3497017024792502078" target="9595110325981705564"/>
<edge source="12550806676654513546" target="9595110325981705564"/>
<edge source="12979976309017799162" target="9595110325981705564"/>
<edge source="10258451456074526049" target="9595110325981705564"/>
<edge source="11775513497668487533" target="9595110325981705564"/>
<edge source="16117828918544644907" target="9595110325981705564"/>
<edge source="14421942083121350206" target="9595110325981705564"/>
<edge source="737352693355482724" target="9595110325981705564"/>
<edge source="2829133918492221101" target="9595110325981705564"/>
<edge source="2829133918492221101" target="2325917221075842848"/>
<edge source="12987945369444025427" target="4041041901496425203"/>
<edge source="12604090720681450553" target="12987945369444025427"/>
<edge source="1440121271646678581" target="12987945369444025427"/>
<edge source="9718581961231347788" target="12987945369444025427"/>
<edge source="4791913395909773486" target="12987945369444025427"/>
<edge source="15973126860844704292" target="12987945369444025427"/>
<edge source="3641665820178422490" target="12987945369444025427"/>
<edge source="3085868247096884162" target="12987945369444025427"/>
<edge source="16770513324417061228" target="4041041901496425203"/>
<edge source="7086855047075759373" target="16770513324417061228"/>
<edge source="669840749336753525" target="16770513324417061228"/>
<edge source="11718687852373920511" target="16770513324417061228"/>
<edge source="2894691460777216219" target="16770513324417061228"/>
<edge source="10327828086531941187" target="16770513324417061228"/>
<edge source="10956894200939947900" target="16770513324417061228"/>
<edge source="6038180113926801086" target="16770513324417061228"/>
<edge source="14636569081444625016" target="16770513324417061228"/>
<edge source="9643084522903856394" target="16770513324417061228"/>
<edge source="17266336177445157331" target="16770513324417061228"/>
<edge source="9063880872255850171" target="4041041901496425203"/>
<edge source="13909256571563279522" target="9063880872255850171"/>
<edge source="6594876225658804365" target="9063880872255850171"/>
<edge source="17368705487922251039" target="9063880872255850171"/>
<edge source="5258718823597512255" target="9063880872255850171"/>
<edge source="7080475698203068086" target="9063880872255850171"/>
<edge source="622631041436591387" target="9063880872255850171"/>
<edge source="12650581806598225058" target="9063880872255850171"/>
<edge source="13161498921981862309" target="9063880872255850171"/>
<edge source="14592788616550656262" target="9063880872255850171"/>
<edge source="4031590341915143464" target="9063880872255850171"/>
<edge source="2321980635951558135" target="4041041901496425203"/>
<edge source="15580172266410736369" target="2321980635951558135"/>
<edge source="1177161480824869016" target="2321980635951558135"/>
<edge source="11531242419091815801" target="2321980635951558135"/>
<edge source="11531242419091815801" target="6382612685700818764"/>
<edge source="11531242419091815801" target="3458396398389387877"/>
<edge source="11531242419091815801" target="1672665553767281734"/>
<edge source="15329990143169836178" target="2321980635951558135"/>
<edge source="17018195497378444438" target="2321980635951558135"/>
<edge source="17018195497378444438" target="7329647594369932315"/>
<edge source="10375739191012965737" target="2321980635951558135"/>
<edge source="10375739191012965737" target="11165298458048562314"/>
<edge source="4490918786976048296" target="2321980635951558135"/>
<edge source="4490918786976048296" target="7329647594369932315"/>
<edge source="4490918786976048296" target="6382612685700818764"/>
<edge source="6004268348151288098" target="2321980635951558135"/>
<edge source="862169174991977666" target="4041041901496425203"/>
<edge source="11748945572016694012" target="862169174991977666"/>
<edge source="3667217905602891315" target="862169174991977666"/>
<edge source="2501577020384058220" target="862169174991977666"/>
<edge source="8511009105279544089" target="862169174991977666"/>
<edge source="13145127891619293871" target="862169174991977666"/>
<edge source="6584420285295374856" target="862169174991977666"/>
<edge source="11074972199329338101" target="862169174991977666"/>
<edge source="15873591660981920322" target="862169174991977666"/>
<edge source="13102079179802671182" target="862169174991977666"/>
<edge source="3720496172950573884" target="862169174991977666"/>
<edge source="2026726248513085794" target="6382612685700818764"/>
<edge source="2026726248513085794" target="14638466021176544465"/>
<edge source="2026726248513085794" target="11531242419091815801"/>
<edge source="2026726248513085794" target="7039269942427062691"/>
<edge source="2026726248513085794" target="1672665553767281734"/>
<edge source="2026726248513085794" target="3458396398389387877"/>
<edge source="2026726248513085794" target="4490918786976048296"/>
<edge source="2026726248513085794" target="14316138733173377966"/>
<edge source="9850512646184180167" target="2026726248513085794"/>
<edge source="13405798017275933263" target="9850512646184180167"/>
<edge source="5669364687087032958" target="9850512646184180167"/>
<edge source="1319826694668830613" target="9850512646184180167"/>
<edge source="8384042348747306199" target="9850512646184180167"/>
<edge source="17642928594905005472" target="9850512646184180167"/>
<edge source="8762454778937977659" target="9850512646184180167"/>
<edge source="10949414717109961618" target="9850512646184180167"/>
<edge source="4438492435191836535" target="9850512646184180167"/>
<edge source="2241015688519496377" target="9850512646184180167"/>
<edge source="14638466021176544465" target="2026726248513085794"/>
<edge source="14638466021176544465" target="11531242419091815801"/>
<edge source="15866259573388647937" target="14638466021176544465"/>
<edge source="2093389731615411975" target="14638466021176544465"/>
<edge source="2093389731615411975" target="11531242419091815801"/>
<edge source="6724748843400977919" target="14638466021176544465"/>
<edge source="16069829188377130053" target="14638466021176544465"/>
<edge source="14300935760162828522" target="14638466021176544465"/>
<edge source="14167857935364818481" target="14638466021176544465"/>
<edge source="14167857935364818481" target="2093389731615411975"/>
<edge source="17632693958287984620" target="14638466021176544465"/>
<edge source="9529319158101525799" target="14638466021176544465"/>
<edge source="9529319158101525799" target="2093389731615411975"/>
<edge source="13702720529764835843" target="2026726248513085794"/>
<edge source="13702720529764835843" target="11531242419091815801"/>
<edge source="17646032300901507453" target="13702720529764835843"/>
<edge source="17646032300901507453" target="2026726248513085794"/>
<edge source="17646032300901507453" target="17690753104303819861"/>
<edge source="8819196546794680544" target="13702720529764835843"/>
<edge source="17785166320928157068" target="13702720529764835843"/>
<edge source="5875186414038374408" target="13702720529764835843"/>
<edge source="958587697277086390" target="13702720529764835843"/>
<edge source="17898472391686934449" target="13702720529764835843"/>
<edge source="15900202697333596864" target="13702720529764835843"/>
<edge source="16133934620382970642" target="13702720529764835843"/>
<edge source="15271212594475018431" target="13702720529764835843"/>
<edge source="10116304010765610972" target="13702720529764835843"/>
<edge source="18022128353994651475" target="17646032300901507453"/>
<edge source="2DdH1BLUQaIJ" target="17646032300901507453"/>
<edge source="4749208375417131840" target="17646032300901507453"/>
<edge source="4749208375417131840" target="17690753104303819861"/>
<edge source="7487952808605114076" target="17646032300901507453"/>
<edge source="7487952808605114076" target="17690753104303819861"/>
<edge source="10897720152285769060" target="17646032300901507453"/>
<edge source="pCHwc0wrutcJ" target="17646032300901507453"/>
<edge source="AtRUthsSjPcJ" target="17646032300901507453"/>
<edge source="1291063805327774306" target="17646032300901507453"/>
<edge source="17567167024662762007" target="17646032300901507453"/>
<edge source="10347681032777129532" target="17646032300901507453"/>
<edge source="17127958062872744853" target="2026726248513085794"/>
<edge source="17127958062872744853" target="4488471448867589825"/>
<edge source="17127958062872744853" target="5295462520886771746"/>
<edge source="10256791187069118291" target="17127958062872744853"/>
<edge source="15552447685803913675" target="17127958062872744853"/>
<edge source="5222861253143202947" target="17127958062872744853"/>
<edge source="2712659024700086363" target="17127958062872744853"/>
<edge source="11270960296740375515" target="17127958062872744853"/>
<edge source="7774766038187589626" target="17127958062872744853"/>
<edge source="2114425240534661867" target="17127958062872744853"/>
<edge source="2114425240534661867" target="11112203079270473115"/>
<edge source="6877695309876199058" target="17127958062872744853"/>
<edge source="6699633140763047923" target="17127958062872744853"/>
<edge source="13776149024359305191" target="17127958062872744853"/>
<edge source="13222462680960111198" target="2026726248513085794"/>
<edge source="6544902242854208675" target="13222462680960111198"/>
<edge source="12643412623559921352" target="13222462680960111198"/>
<edge source="3110755508632090464" target="13222462680960111198"/>
<edge source="14940203506181718397" target="13222462680960111198"/>
<edge source="7108335566961034347" target="13222462680960111198"/>
<edge source="7108335566961034347" target="17690753104303819861"/>
<edge source="118648389925321836" target="13222462680960111198"/>
<edge source="118648389925321836" target="11112203079270473115"/>
<edge source="118648389925321836" target="17690753104303819861"/>
<edge source="ApU0apKFnLoJ" target="13222462680960111198"/>
<edge source="10376535638618684847" target="13222462680960111198"/>
<edge source="sdblonX9vEQJ" target="13222462680960111198"/>
<edge source="sdblonX9vEQJ" target="10264464849626080964"/>
<edge source="9083521644969713050" target="13222462680960111198"/>
<edge source="9083521644969713050" target="10264464849626080964"/>
<edge source="16827683962441657873" target="2026726248513085794"/>
<edge source="16827683962441657873" target="18210588638302847093"/>
<edge source="14052692408086479292" target="16827683962441657873"/>
<edge source="17085777096678660112" target="16827683962441657873"/>
<edge source="915175193869878529" target="16827683962441657873"/>
<edge source="18125073647564822800" target="16827683962441657873"/>
<edge source="11317692785014151150" target="16827683962441657873"/>
<edge source="17823167432429599810" target="16827683962441657873"/>
<edge source="851720305858029725" target="16827683962441657873"/>
<edge source="13349023610638525354" target="16827683962441657873"/>
<edge source="10228711962693123964" target="16827683962441657873"/>
<edge source="6497775393041290067" target="16827683962441657873"/>
<edge source="11112203079270473115" target="2026726248513085794"/>
<edge source="18283090867219141273" target="11112203079270473115"/>
<edge source="3348080849815981295" target="11112203079270473115"/>
<edge source="9008218150110989248" target="11112203079270473115"/>
<edge source="9008218150110989248" target="17690753104303819861"/>
<edge source="15890436532115930630" target="11112203079270473115"/>
<edge source="MXxApydlfuYJ" target="11112203079270473115"/>
<edge source="15254646771058273507" target="11112203079270473115"/>
<edge source="18262125184766942934" target="11112203079270473115"/>
<edge source="12977604275271393235" target="11112203079270473115"/>
<edge source="17690753104303819861" target="2026726248513085794"/>
<edge source="11735473174983847098" target="17690753104303819861"/>
<edge source="1443894251537732005" target="17690753104303819861"/>
<edge source="1791593962748758598" target="17690753104303819861"/>
<edge source="14879519839963588983" target="17690753104303819861"/>
<edge source="10264464849626080964" target="2026726248513085794"/>
<edge source="2392814171009909808" target="10264464849626080964"/>
<edge source="6327480080772807927" target="10264464849626080964"/>
<edge source="12337878440731894992" target="10264464849626080964"/>
<edge source="17386293721154871155" target="10264464849626080964"/>
<edge source="12836955438277091904" target="10264464849626080964"/>
<edge source="18049535498421460297" target="10264464849626080964"/>
<edge source="12328613189873235415" target="10264464849626080964"/>
<edge source="5371618923568523141" target="10264464849626080964"/>
<edge source="18210588638302847093" target="11531242419091815801"/>
<edge source="15045952046135698930" target="18210588638302847093"/>
<edge source="1223563817431199857" target="18210588638302847093"/>
<edge source="3743433833463586545" target="18210588638302847093"/>
<edge source="3333668025330401477" target="18210588638302847093"/>
<edge source="2805320302601413924" target="18210588638302847093"/>
<edge source="12502648671025662060" target="18210588638302847093"/>
<edge source="7039269942427062691" target="11531242419091815801"/>
<edge source="7039269942427062691" target="11838073149065061192"/>
<edge source="6118595289890500680" target="7039269942427062691"/>
<edge source="6118595289890500680" target="4490918786976048296"/>
<edge source="6118595289890500680" target="12938213222665733645"/>
<edge source="6118595289890500680" target="14316138733173377966"/>
<edge source="3484260075890953852" target="7039269942427062691"/>
<edge source="6077963581280858299" target="7039269942427062691"/>
<edge source="17545941908498686611" target="7039269942427062691"/>
<edge source="17545941908498686611" target="11838073149065061192"/>
<edge source="17229720160752682638" target="7039269942427062691"/>
<edge source="17229720160752682638" target="11838073149065061192"/>
<edge source="2723001857482086032" target="7039269942427062691"/>
<edge source="12867511582517934835" target="7039269942427062691"/>
<edge source="12867511582517934835" target="14316138733173377966"/>
<edge source="4488471448867589825" target="11531242419091815801"/>
<edge source="8808925328206804280" target="4488471448867589825"/>
<edge source="9110810375335606993" target="4488471448867589825"/>
<edge source="708526825456642350" target="4488471448867589825"/>
<edge source="515129946496787543" target="4488471448867589825"/>
<edge source="3749665078422249041" target="4488471448867589825"/>
<edge source="13329597315293984783" target="4488471448867589825"/>
<edge source="9358192350302074225" target="4488471448867589825"/>
<edge source="9302345027896383303" target="4488471448867589825"/>
<edge source="5295462520886771746" target="11531242419091815801"/>
<edge source="6482216091275039752" target="5295462520886771746"/>
<edge source="8940062898534376834" target="5295462520886771746"/>
<edge source="15245828836153963354" target="5295462520886771746"/>
<edge source="3527582028524441499" target="5295462520886771746"/>
<edge source="1893894085208736639" target="5295462520886771746"/>
<edge source="11761116720729991146" target="5295462520886771746"/>
<edge source="9934965656490088917" target="5295462520886771746"/>
<edge source="2188695656210318579" target="5295462520886771746"/>
<edge source="12517578013806719466" target="5295462520886771746"/>
<edge source="14575314968965851624" target="2093389731615411975"/>
<edge source="14118575121889987430" target="2093389731615411975"/>
<edge source="3318911338829069796" target="2093389731615411975"/>
<edge source="9808171637715574951" target="2093389731615411975"/>
<edge source="8763231865317300809" target="2093389731615411975"/>
<edge source="14978456818526690373" target="2093389731615411975"/>
<edge source="17206319611652371856" target="2093389731615411975"/>
<edge source="13723544815291086075" target="2093389731615411975"/>
<edge source="4246517974439469322" target="11789051068432887660"/>
<edge source="137519887286825601" target="11789051068432887660"/>
<edge source="E02bi-l64ZwJ" target="11789051068432887660"/>
<edge source="4896734917901521680" target="11789051068432887660"/>
<edge source="QwXRfW59944J" target="11789051068432887660"/>
<edge source="4747360019864620532" target="11789051068432887660"/>
<edge source="15130464980890007333" target="11789051068432887660"/>
<edge source="1126142290960013428" target="11789051068432887660"/>
<edge source="4Fr4jN5uC8oJ" target="11789051068432887660"/>
<edge source="uHNtXrDhdSwJ" target="11789051068432887660"/>
<edge source="11838073149065061192" target="11531242419091815801"/>
<edge source="3799744009906269739" target="11838073149065061192"/>
<edge source="9646853108847192407" target="11838073149065061192"/>
<edge source="3777854110099563090" target="11838073149065061192"/>
<edge source="18294495860499043250" target="11838073149065061192"/>
<edge source="18294495860499043250" target="14316138733173377966"/>
<edge source="12550135891789567985" target="11838073149065061192"/>
<edge source="12550135891789567985" target="4490918786976048296"/>
<edge source="1672665553767281734" target="6382612685700818764"/>
<edge source="16898296257676733828" target="2325917221075842848"/>
<edge source="17636062869988382916" target="2325917221075842848"/>
<edge source="17712252571307454824" target="2325917221075842848"/>
<edge source="8649528985577473031" target="2325917221075842848"/>
<edge source="9439766841533136382" target="2325917221075842848"/>
<edge source="9439766841533136382" target="4490918786976048296"/>
<edge source="11165298458048562314" target="1672665553767281734"/>
<edge source="12500443856801763727" target="11165298458048562314"/>
<edge source="12500443856801763727" target="15816136068893942524"/>
<edge source="8140812159859442226" target="11165298458048562314"/>
<edge source="8140812159859442226" target="17997891498068622933"/>
<edge source="15816136068893942524" target="1672665553767281734"/>
<edge source="15816136068893942524" target="3458396398389387877"/>
<edge source="9192461276991269473" target="15816136068893942524"/>
<edge source="17810188272131878569" target="15816136068893942524"/>
<edge source="17752815312316743733" target="15816136068893942524"/>
<edge source="4802045854930781683" target="3458396398389387877"/>
<edge source="4802045854930781683" target="7329647594369932315"/>
<edge source="16837829726140559426" target="7329647594369932315"/>
<edge source="5833041667751260373" target="7329647594369932315"/>
<edge source="1018521690946850362" target="7329647594369932315"/>
<edge source="6243645967630982889" target="4490918786976048296"/>
<edge source="3936750546972103836" target="6243645967630982889"/>
<edge source="11927510402360104647" target="6243645967630982889"/>
<edge source="6187038158217452627" target="6243645967630982889"/>
<edge source="16029033897139067670" target="6243645967630982889"/>
<edge source="381090561678004553" target="6243645967630982889"/>
<edge source="12607038868340347612" target="6243645967630982889"/>
<edge source="4239598804369546383" target="6243645967630982889"/>
<edge source="681965397596551869" target="6243645967630982889"/>
<edge source="8582598101542675430" target="6243645967630982889"/>
<edge source="10060803768210082111" target="6243645967630982889"/>
<edge source="15741444728855576863" target="4490918786976048296"/>
<edge source="8437346401990005173" target="15741444728855576863"/>
<edge source="2958442793928445860" target="15741444728855576863"/>
<edge source="2958442793928445860" target="6118595289890500680"/>
<edge source="14444698797987128655" target="15741444728855576863"/>
<edge source="9083483030705185424" target="15741444728855576863"/>
<edge source="9083483030705185424" target="10588342779298269046"/>
<edge source="640071900436442539" target="15741444728855576863"/>
<edge source="1608743531206453804" target="15741444728855576863"/>
<edge source="10542319093857998430" target="15741444728855576863"/>
<edge source="11173768905758826676" target="15741444728855576863"/>
<edge source="4360518173578415769" target="15741444728855576863"/>
<edge source="4884243151249440145" target="15741444728855576863"/>
<edge source="12973370542610650225" target="10588342779298269046"/>
<edge source="12973370542610650225" target="6118595289890500680"/>
<edge source="1879282532294332322" target="10588342779298269046"/>
<edge source="3768131676399480172" target="10588342779298269046"/>
<edge source="2978288424037265447" target="10588342779298269046"/>
<edge source="9021031822777383648" target="10588342779298269046"/>
<edge source="7929876691840755441" target="10588342779298269046"/>
<edge source="7929876691840755441" target="6118595289890500680"/>
<edge source="3373629156419038558" target="10588342779298269046"/>
<edge source="3373629156419038558" target="6118595289890500680"/>
<edge source="9284111923828977279" target="10588342779298269046"/>
<edge source="8586493250682863612" target="6118595289890500680"/>
<edge source="5337226914153811562" target="6118595289890500680"/>
<edge source="5337226914153811562" target="14316138733173377966"/>
<edge source="5343811397150749551" target="6118595289890500680"/>
<edge source="3683089856994200319" target="6118595289890500680"/>
<edge source="8072796476168839940" target="6118595289890500680"/>
<edge source="11224588046887428191" target="6118595289890500680"/>
<edge source="17997891498068622933" target="4490918786976048296"/>
<edge source="5167657309745527366" target="17997891498068622933"/>
<edge source="10341392145675802868" target="17997891498068622933"/>
<edge source="17928092202816562244" target="17997891498068622933"/>
<edge source="9093499936003644917" target="17997891498068622933"/>
<edge source="6209180784126725956" target="17997891498068622933"/>
<edge source="14246245970329145306" target="17997891498068622933"/>
<edge source="17211572998346535120" target="17997891498068622933"/>
<edge source="9686322700146945720" target="17997891498068622933"/>
<edge source="2022354001457069046" target="9439766841533136382"/>
<edge source="14613261815949086918" target="9439766841533136382"/>
<edge source="6979440938086217563" target="9439766841533136382"/>
<edge source="7134900495559356797" target="9439766841533136382"/>
<edge source="8918294861982063782" target="9439766841533136382"/>
<edge source="9002295544688722648" target="9439766841533136382"/>
<edge source="530860336847622267" target="9439766841533136382"/>
<edge source="10626359895994634728" target="9439766841533136382"/>
<edge source="3429308917664707403" target="9439766841533136382"/>
<edge source="16223923155940056199" target="9439766841533136382"/>
<edge source="14316138733173377966" target="12938213222665733645"/>
<edge source="14316138733173377966" target="4490918786976048296"/>
<edge source="3584347529323049886" target="12938213222665733645"/>
<edge source="17264780080323807782" target="12938213222665733645"/>
<edge source="16657130336390881372" target="12938213222665733645"/>
<edge source="16345516296958117809" target="12938213222665733645"/>
<edge source="12345969996209231071" target="12938213222665733645"/>
<edge source="11904702717564235743" target="12938213222665733645"/>
<edge source="9556534593478071448" target="14316138733173377966"/>
<edge source="17595429764835777254" target="14316138733173377966"/>
<edge source="2341964736485396655" target="14316138733173377966"/>
<edge source="12388611019049432202" target="12550135891789567985"/>
<edge source="1436259968817456407" target="12550135891789567985"/>
<edge source="10221363205500346797" target="12550135891789567985"/>
<edge source="18387677115510147109" target="12550135891789567985"/>
<edge source="10401914588824198986" target="12550135891789567985"/>
<edge source="10714034727295496106" target="12550135891789567985"/>
<edge source="7090138214741207408" target="12550135891789567985"/>
<edge source="10928542084451319450" target="12550135891789567985"/>
<edge source="17976854884426501420" target="12550135891789567985"/>
<edge source="3308763779023912371" target="12550135891789567985"/>
<edge source="12563424232401784595" target="6382612685700818764"/>
</graph></graphml>