<!DOCTYPE html>

<!--

This Google Scholar network visualization was generated with
https://github.com/edsu/etudier using the following command:

% etudier --pages 2 --depth 2 https://scholar.google.com/scholar?cites=6382612685700818764&as_sdt=40005&sciodt=0,10&hl=en

--> 

<html>
  <head>
    <meta charset="utf-8" />
    <style>
      body {
        overflow: hidden;
        margin: 0;
      }

      text {
        font-family: sans-serif;
        pointer-events: none;
      }
    </style>
  </head>

  <body>
    <script src="https://d3js.org/d3.v3.min.js"></script>
    <script>
      var graph = {
  "nodes": [
    {
      "label": "Transformers in vision: A survey",
      "id": "7522504961268153944",
      "url": "https://dl.acm.org/doi/abs/10.1145/3505244",
      "title": "Transformers in vision: A survey",
      "authors": "S Khan, M Naseer, M Hayat, SW Zamir\u2026",
      "year": "2022",
      "cited_by": 1277,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7522504961268153944&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "You only look once: Unified, real-time object detection",
      "id": "6382612685700818764",
      "title": "You only look once: Unified, real-time object detection",
      "cited_by": 38962,
      "modularity": 7
    },
    {
      "label": "Attention mechanisms in computer vision: A survey",
      "id": "15456065911372617945",
      "url": "https://link.springer.com/article/10.1007/s41095-022-0271-y",
      "title": "Attention mechanisms in computer vision: A survey",
      "authors": "MH Guo, TX Xu, JJ Liu, ZN Liu, PT Jiang, TJ Mu\u2026",
      "year": "2022",
      "cited_by": 606,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15456065911372617945&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Transformers in medical imaging: A survey",
      "id": "982391967541643955",
      "url": "https://www.sciencedirect.com/science/article/pii/S1361841523000634",
      "title": "Transformers in medical imaging: A survey",
      "authors": "F Shamshad, S Khan, SW Zamir, MH Khan\u2026",
      "year": "2023",
      "cited_by": 179,
      "cited_by_url": "https://scholar.google.com/scholar?cites=982391967541643955&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "A comprehensive survey on pretrained foundation models: A history from bert to chatgpt",
      "id": "2452866517197292093",
      "url": "https://arxiv.org/abs/2302.09419",
      "title": "A comprehensive survey on pretrained foundation models: A history from bert to chatgpt",
      "authors": "C Zhou, Q Li, C Li, J Yu, Y Liu, G Wang\u2026",
      "year": "2023",
      "cited_by": 109,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2452866517197292093&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Visual attention network",
      "id": "4773463079530656035",
      "url": "https://link.springer.com/article/10.1007/s41095-023-0364-2",
      "title": "Visual attention network",
      "authors": "MH Guo, CZ Lu, ZN Liu, MM Cheng, SM Hu",
      "year": "2023",
      "cited_by": 235,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4773463079530656035&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Segnext: Rethinking convolutional attention design for semantic segmentation",
      "id": "761718241536208511",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/08050f40fff41616ccfc3080e60a301a-Abstract-Conference.html",
      "title": "Segnext: Rethinking convolutional attention design for semantic segmentation",
      "authors": "MH Guo, CZ Lu, Q Hou, Z Liu\u2026",
      "year": "2022",
      "cited_by": 113,
      "cited_by_url": "https://scholar.google.com/scholar?cites=761718241536208511&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "YOLOv5-Tassel: Detecting tassels in RGB UAV imagery with improved YOLOv5 based on transfer learning",
      "id": "7104781172538541114",
      "url": "https://ieeexplore.ieee.org/abstract/document/9889182/",
      "title": "YOLOv5-Tassel: Detecting tassels in RGB UAV imagery with improved YOLOv5 based on transfer learning",
      "authors": "W Liu, K Quijano, MM Crawford",
      "year": "2022",
      "cited_by": 115,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7104781172538541114&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Towards an end-to-end framework for flow-guided video inpainting",
      "id": "6491078858607146383",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Li_Towards_an_End-to-End_Framework_for_Flow-Guided_Video_Inpainting_CVPR_2022_paper.html",
      "title": "Towards an end-to-end framework for flow-guided video inpainting",
      "authors": "Z Li, CZ Lu, J Qin, CL Guo\u2026",
      "year": "2022",
      "cited_by": 43,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6491078858607146383&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "ISNet: Towards improving separability for remote sensing image change detection",
      "id": "11978445553624214380",
      "url": "https://ieeexplore.ieee.org/abstract/document/9772654/",
      "title": "ISNet: Towards improving separability for remote sensing image change detection",
      "authors": "G Cheng, G Wang, J Han",
      "year": "2022",
      "cited_by": 37,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11978445553624214380&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Diagnosis of brain diseases in fusion of neuroimaging modalities using deep learning: A review",
      "id": "5228146784334715443",
      "url": "https://www.sciencedirect.com/science/article/pii/S1566253522002573",
      "title": "Diagnosis of brain diseases in fusion of neuroimaging modalities using deep learning: A review",
      "authors": "A Shoeibi, M Khodatars, M Jafari, N Ghassemi\u2026",
      "year": "2022",
      "cited_by": 21,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5228146784334715443&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Swin-transformer-enabled YOLOv5 with attention mechanism for small object detection on satellite images",
      "id": "9099615620722636165",
      "url": "https://www.mdpi.com/2072-4292/14/12/2861",
      "title": "Swin-transformer-enabled YOLOv5 with attention mechanism for small object detection on satellite images",
      "authors": "H Gong, T Mu, Q Li, H Dai, C Li, Z He, W Wang, F Han\u2026",
      "year": "2022",
      "cited_by": 51,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9099615620722636165&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Beyond self-attention: External attention using two linear layers for visual tasks",
      "id": "10884589459641707712",
      "url": "https://ieeexplore.ieee.org/abstract/document/9912362/",
      "title": "Beyond self-attention: External attention using two linear layers for visual tasks",
      "authors": "MH Guo, ZN Liu, TJ Mu, SM Hu",
      "year": "2022",
      "cited_by": 264,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10884589459641707712&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Deep learning for reconstructing protein structures from cryo-EM density maps: Recent advances and future directions",
      "id": "16262241741810610288",
      "url": "https://www.sciencedirect.com/science/article/pii/S0959440X23000106",
      "title": "Deep learning for reconstructing protein structures from cryo-EM density maps: Recent advances and future directions",
      "authors": "N Giri, RS Roy, J Cheng",
      "year": "2023",
      "cited_by": 12,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16262241741810610288&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "A unified multiscale learning framework for hyperspectral image classification",
      "id": "17062002127391260256",
      "url": "https://ieeexplore.ieee.org/abstract/document/9701344/",
      "title": "A unified multiscale learning framework for hyperspectral image classification",
      "authors": "X Wang, K Tan, P Du, C Pan\u2026",
      "year": "2022",
      "cited_by": 26,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17062002127391260256&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Braingb: A benchmark for brain network analysis with graph neural networks",
      "id": "2492869366552030070",
      "url": "https://ieeexplore.ieee.org/abstract/document/9933896/",
      "title": "Braingb: A benchmark for brain network analysis with graph neural networks",
      "authors": "H Cui, W Dai, Y Zhu, X Kan, AAC Gu\u2026",
      "year": "2022",
      "cited_by": 29,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2492869366552030070&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Attention-based deep meta-transfer learning for few-shot fine-grained fault diagnosis",
      "id": "2308320307881605474",
      "url": "https://www.sciencedirect.com/science/article/pii/S0950705123000953",
      "title": "Attention-based deep meta-transfer learning for few-shot fine-grained fault diagnosis",
      "authors": "C Li, S Li, H Wang, F Gu, AD Ball",
      "year": "2023",
      "cited_by": 15,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2308320307881605474&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Study of the automatic recognition of landslides by using InSAR images and the improved mask R-CNN model in the Eastern Tibet Plateau",
      "id": "28694845113345021",
      "url": "https://www.mdpi.com/2072-4292/14/14/3362",
      "title": "Study of the automatic recognition of landslides by using InSAR images and the improved mask R-CNN model in the Eastern Tibet Plateau",
      "authors": "Y Liu, X Yao, Z Gu, Z Zhou, X Liu, X Chen, S Wei",
      "year": "2022",
      "cited_by": 16,
      "cited_by_url": "https://scholar.google.com/scholar?cites=28694845113345021&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "An improved apple object detection method based on lightweight YOLOv4 in complex backgrounds",
      "id": "9917302194767651380",
      "url": "https://www.mdpi.com/2072-4292/14/17/4150",
      "title": "An improved apple object detection method based on lightweight YOLOv4 in complex backgrounds",
      "authors": "C Zhang, F Kang, Y Wang",
      "year": "2022",
      "cited_by": 19,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9917302194767651380&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Large-scale multi-modal pre-trained models: A comprehensive survey",
      "id": "8402450993508627791",
      "url": "https://link.springer.com/article/10.1007/s11633-022-1410-8",
      "title": "Large-scale multi-modal pre-trained models: A comprehensive survey",
      "authors": "X Wang, G Chen, G Qian, P Gao, XY Wei\u2026",
      "year": "2023",
      "cited_by": 18,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8402450993508627791&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "YOLOv5-Fog: A multiobjective visual detection algorithm for fog driving scenes based on improved YOLOv5",
      "id": "17059844040400317816",
      "url": "https://ieeexplore.ieee.org/abstract/document/9851677/",
      "title": "YOLOv5-Fog: A multiobjective visual detection algorithm for fog driving scenes based on improved YOLOv5",
      "authors": "H Wang, Y Xu, Y He, Y Cai, L Chen, Y Li\u2026",
      "year": "2022",
      "cited_by": 20,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17059844040400317816&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "AGs-Unet: Building Extraction Model for High Resolution Remote Sensing Images Based on Attention Gates U Network",
      "id": "1609101033275109223",
      "url": "https://www.mdpi.com/1424-8220/22/8/2932",
      "title": "AGs-Unet: Building Extraction Model for High Resolution Remote Sensing Images Based on Attention Gates U Network",
      "authors": "M Yu, X Chen, W Zhang, Y Liu",
      "year": "2022",
      "cited_by": 17,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1609101033275109223&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Generating transferable adversarial examples against vision transformers",
      "id": "9226375279866402413",
      "url": "https://dl.acm.org/doi/abs/10.1145/3503161.3547989",
      "title": "Generating transferable adversarial examples against vision transformers",
      "authors": "Y Wang, J Wang, Z Yin, R Gong, J Wang\u2026",
      "year": "2022",
      "cited_by": 10,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9226375279866402413&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "A survey of transformers",
      "id": "7749897961068121501",
      "url": "https://www.sciencedirect.com/science/article/pii/S2666651022000146",
      "title": "A survey of transformers",
      "authors": "T Lin, Y Wang, X Liu, X Qiu",
      "year": "2022",
      "cited_by": 477,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7749897961068121501&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Pre-trained models for natural language processing: A survey",
      "id": "1539076789580815483",
      "url": "https://link.springer.com/article/10.1007/s11431-020-1647-3",
      "title": "Pre-trained models for natural language processing: A survey",
      "authors": "X Qiu, T Sun, Y Xu, Y Shao, N Dai, X Huang",
      "year": "2020",
      "cited_by": 1177,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1539076789580815483&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Pre-trained models: Past, present and future",
      "id": "966567457136989804",
      "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000231",
      "title": "Pre-trained models: Past, present and future",
      "authors": "X Han, Z Zhang, N Ding, Y Gu, X Liu, Y Huo, J Qiu\u2026",
      "year": "2021",
      "cited_by": 360,
      "cited_by_url": "https://scholar.google.com/scholar?cites=966567457136989804&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Multimodal learning with transformers: A survey",
      "id": "10761248177036470713",
      "url": "https://ieeexplore.ieee.org/abstract/document/10123038/",
      "title": "Multimodal learning with transformers: A survey",
      "authors": "P Xu, X Zhu, DA Clifton",
      "year": "2023",
      "cited_by": 107,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10761248177036470713&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "A survey of visual transformers",
      "id": "14136709172791920331",
      "url": "https://ieeexplore.ieee.org/abstract/document/10088164/",
      "title": "A survey of visual transformers",
      "authors": "Y Liu, Y Zhang, Y Wang, F Hou, J Yuan\u2026",
      "year": "2023",
      "cited_by": 104,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14136709172791920331&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Recent advances in deep learning based dialogue systems: A systematic survey",
      "id": "13597902966753793310",
      "url": "https://link.springer.com/article/10.1007/s10462-022-10248-8",
      "title": "Recent advances in deep learning based dialogue systems: A systematic survey",
      "authors": "J Ni, T Young, V Pandelea, F Xue\u2026",
      "year": "2023",
      "cited_by": 121,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13597902966753793310&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Ammus: A survey of transformer-based pretrained models in natural language processing",
      "id": "4431578198915484435",
      "url": "https://arxiv.org/abs/2108.05542",
      "title": "Ammus: A survey of transformer-based pretrained models in natural language processing",
      "authors": "KS Kalyan, A Rajasekharan, S Sangeetha",
      "year": "2021",
      "cited_by": 141,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4431578198915484435&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "ChatGPT: Jack of all trades, master of none",
      "id": "2600515932282922845",
      "url": "https://www.sciencedirect.com/science/article/pii/S156625352300177X",
      "title": "ChatGPT: Jack of all trades, master of none",
      "authors": "J Koco\u0144, I Cichecki, O Kaszyca, M Kochanek, D Szyd\u0142o\u2026",
      "year": "2023",
      "cited_by": 63,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2600515932282922845&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Physformer: Facial video-based physiological measurement with temporal difference transformer",
      "id": "8053588590478703627",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Yu_PhysFormer_Facial_Video-Based_Physiological_Measurement_With_Temporal_Difference_Transformer_CVPR_2022_paper.html",
      "title": "Physformer: Facial video-based physiological measurement with temporal difference transformer",
      "authors": "Z Yu, Y Shen, J Shi, H Zhao\u2026",
      "year": "2022",
      "cited_by": 54,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8053588590478703627&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "ChatGPT is not all you need. A State of the Art Review of large Generative AI models",
      "id": "15393921212791157727",
      "url": "https://arxiv.org/abs/2301.04655",
      "title": "ChatGPT is not all you need. A State of the Art Review of large Generative AI models",
      "authors": "R Gozalo-Brizuela, EC Garrido-Merchan",
      "year": "2023",
      "cited_by": 80,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15393921212791157727&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Glycoinformatics in the artificial intelligence era",
      "id": "7420567692305480707",
      "url": "https://pubs.acs.org/doi/abs/10.1021/acs.chemrev.2c00110",
      "title": "Glycoinformatics in the artificial intelligence era",
      "authors": "D Bojar, F Lisacek",
      "year": "2022",
      "cited_by": 9,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7420567692305480707&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Deep learning for depression recognition with audiovisual cues: A review",
      "id": "3878971468388928610",
      "url": "https://www.sciencedirect.com/science/article/pii/S1566253521002207",
      "title": "Deep learning for depression recognition with audiovisual cues: A review",
      "authors": "L He, M Niu, P Tiwari, P Marttinen, R Su, J Jiang\u2026",
      "year": "2022",
      "cited_by": 62,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3878971468388928610&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Museformer: Transformer with fine-and coarse-grained attention for music generation",
      "id": "9919738130893761480",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/092c2d45005ea2db40fc24c470663416-Abstract-Conference.html",
      "title": "Museformer: Transformer with fine-and coarse-grained attention for music generation",
      "authors": "B Yu, P Lu, R Wang, W Hu, X Tan\u2026",
      "year": "2022",
      "cited_by": 12,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9919738130893761480&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Video transformers: A survey",
      "id": "15926311935982020340",
      "url": "https://ieeexplore.ieee.org/abstract/document/10041724/",
      "title": "Video transformers: A survey",
      "authors": "J Selva, AS Johansen, S Escalera\u2026",
      "year": "2023",
      "cited_by": 41,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15926311935982020340&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "KNN-contrastive learning for out-of-domain intent classification",
      "id": "836183377042488989",
      "url": "https://aclanthology.org/2022.acl-long.352/",
      "title": "KNN-contrastive learning for out-of-domain intent classification",
      "authors": "Y Zhou, P Liu, X Qiu",
      "year": "2022",
      "cited_by": 31,
      "cited_by_url": "https://scholar.google.com/scholar?cites=836183377042488989&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Contrast and generation make bart a good dialogue emotion recognizer",
      "id": "17269928898680478649",
      "url": "https://ojs.aaai.org/index.php/AAAI/article/view/21348",
      "title": "Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": "S Li, H Yan, X Qiu",
      "year": "2022",
      "cited_by": 34,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17269928898680478649&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Dual-aspect self-attention based on transformer for remaining useful life prediction",
      "id": "6376642340831106711",
      "url": "https://ieeexplore.ieee.org/abstract/document/9737516/",
      "title": "Dual-aspect self-attention based on transformer for remaining useful life prediction",
      "authors": "Z Zhang, W Song, Q Li",
      "year": "2022",
      "cited_by": 53,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6376642340831106711&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Deltar: Depth estimation from a light-weight tof sensor and rgb image",
      "id": "12806372482493631350",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-19769-7_36",
      "title": "Deltar: Depth estimation from a light-weight tof sensor and rgb image",
      "authors": "Y Li, X Liu, W Dong, H Zhou, H Bao, G Zhang\u2026",
      "year": "2022",
      "cited_by": 7,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12806372482493631350&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "A Transformer-based deep neural network model for SSVEP classification",
      "id": "2160563741249466913",
      "url": "https://www.sciencedirect.com/science/article/pii/S0893608023002319",
      "title": "A Transformer-based deep neural network model for SSVEP classification",
      "authors": "J Chen, Y Zhang, Y Pan, P Xu, C Guan",
      "year": "2023",
      "cited_by": 7,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2160563741249466913&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Restormer: Efficient transformer for high-resolution image restoration",
      "id": "16431204865977056518",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zamir_Restormer_Efficient_Transformer_for_High-Resolution_Image_Restoration_CVPR_2022_paper.html",
      "title": "Restormer: Efficient transformer for high-resolution image restoration",
      "authors": "SW Zamir, A Arora, S Khan, M Hayat\u2026",
      "year": "2022",
      "cited_by": 683,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16431204865977056518&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "NTIRE 2023 challenge on efficient super-resolution: Methods and results",
      "id": "16932998913230259066",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Li_NTIRE_2023_Challenge_on_Efficient_Super-Resolution_Methods_and_Results_CVPRW_2023_paper.html",
      "title": "NTIRE 2023 challenge on efficient super-resolution: Methods and results",
      "authors": "Y Li, Y Zhang, R Timofte, L Van Gool\u2026",
      "year": "2023",
      "cited_by": 73,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16932998913230259066&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Simple baselines for image restoration",
      "id": "498268664873674535",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-20071-7_2",
      "title": "Simple baselines for image restoration",
      "authors": "L Chen, X Chu, X Zhang, J Sun",
      "year": "2022",
      "cited_by": 234,
      "cited_by_url": "https://scholar.google.com/scholar?cites=498268664873674535&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Lens-to-lens bokeh effect transformation. NTIRE 2023 challenge report",
      "id": "9110410135628850117",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Conde_Lens-to-Lens_Bokeh_Effect_Transformation._NTIRE_2023_Challenge_Report_CVPRW_2023_paper.html",
      "title": "Lens-to-lens bokeh effect transformation. NTIRE 2023 challenge report",
      "authors": "MV Conde, M Kolmet, T Seizinger\u2026",
      "year": "2023",
      "cited_by": 17,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9110410135628850117&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "NTIRE 2023 video colorization challenge",
      "id": "7710701073211724386",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Kang_NTIRE_2023_Video_Colorization_Challenge_CVPRW_2023_paper.html",
      "title": "NTIRE 2023 video colorization challenge",
      "authors": "X Kang, X Lin, K Zhang, Z Hui\u2026",
      "year": "2023",
      "cited_by": 14,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7710701073211724386&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "NTIRE 2023 Challenge on 360deg Omnidirectional Image and Video Super-Resolution: Datasets, Methods and Results",
      "id": "13272942409666364496",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Cao_NTIRE_2023_Challenge_on_360deg_Omnidirectional_Image_and_Video_Super-Resolution_CVPRW_2023_paper.html",
      "title": "NTIRE 2023 Challenge on 360deg Omnidirectional Image and Video Super-Resolution: Datasets, Methods and Results",
      "authors": "M Cao, C Mou, F Yu, X Wang\u2026",
      "year": "2023",
      "cited_by": 15,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13272942409666364496&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "NTIRE 2023 challenge on image denoising: Methods and results",
      "id": "12297468854729392143",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Li_NTIRE_2023_Challenge_on_Image_Denoising_Methods_and_Results_CVPRW_2023_paper.html",
      "title": "NTIRE 2023 challenge on image denoising: Methods and results",
      "authors": "Y Li, Y Zhang, R Timofte, L Van Gool\u2026",
      "year": "2023",
      "cited_by": 12,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12297468854729392143&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "NTIRE 2023 challenge on night photography rendering",
      "id": "5364344454304770774",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Shutova_NTIRE_2023_Challenge_on_Night_Photography_Rendering_CVPRW_2023_paper.html",
      "title": "NTIRE 2023 challenge on night photography rendering",
      "authors": "A Shutova, E Ershov\u2026",
      "year": "2023",
      "cited_by": 13,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5364344454304770774&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Efficient long-range attention network for image super-resolution",
      "id": "3464402829187061665",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-19790-1_39",
      "title": "Efficient long-range attention network for image super-resolution",
      "authors": "X Zhang, H Zeng, S Guo, L Zhang",
      "year": "2022",
      "cited_by": 70,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3464402829187061665&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion",
      "id": "15226748689642581218",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.html",
      "title": "Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion",
      "authors": "Z Zhao, H Bai, J Zhang, Y Zhang, S Xu\u2026",
      "year": "2023",
      "cited_by": 23,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15226748689642581218&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Mst++: Multi-stage spectral-wise transformer for efficient spectral reconstruction",
      "id": "4776651674359042439",
      "url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Cai_MST_Multi-Stage_Spectral-Wise_Transformer_for_Efficient_Spectral_Reconstruction_CVPRW_2022_paper.html",
      "title": "Mst++: Multi-stage spectral-wise transformer for efficient spectral reconstruction",
      "authors": "Y Cai, J Lin, Z Lin, H Wang, Y Zhang\u2026",
      "year": "2022",
      "cited_by": 40,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4776651674359042439&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Learning enriched features for fast image restoration and enhancement",
      "id": "278490733091580759",
      "url": "https://ieeexplore.ieee.org/abstract/document/9756908/",
      "title": "Learning enriched features for fast image restoration and enhancement",
      "authors": "SW Zamir, A Arora, S Khan, M Hayat\u2026",
      "year": "2022",
      "cited_by": 55,
      "cited_by_url": "https://scholar.google.com/scholar?cites=278490733091580759&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Aim 2022 challenge on super-resolution of compressed image and video: Dataset, methods and results",
      "id": "1598910218006932443",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-25066-8_8",
      "title": "Aim 2022 challenge on super-resolution of compressed image and video: Dataset, methods and results",
      "authors": "R Yang, R Timofte, X Li, Q Zhang, L Zhang\u2026",
      "year": "2022",
      "cited_by": 22,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1598910218006932443&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Rethinking alignment in video super-resolution transformers",
      "id": "13813872909195716054",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/ea4d65c59073e8faf79222654d25fbe2-Abstract-Conference.html",
      "title": "Rethinking alignment in video super-resolution transformers",
      "authors": "S Shi, J Gu, L Xie, X Wang, Y Yang\u2026",
      "year": "2022",
      "cited_by": 14,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13813872909195716054&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Maniqa: Multi-dimension attention network for no-reference image quality assessment",
      "id": "3715909162805048662",
      "url": "https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Yang_MANIQA_Multi-Dimension_Attention_Network_for_No-Reference_Image_Quality_Assessment_CVPRW_2022_paper.html",
      "title": "Maniqa: Multi-dimension attention network for no-reference image quality assessment",
      "authors": "S Yang, T Wu, S Shi, S Lao, Y Gong\u2026",
      "year": "2022",
      "cited_by": 50,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3715909162805048662&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "NTIRE 2023 challenge on stereo image super-resolution: Methods and results",
      "id": "8627981696317437595",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Wang_NTIRE_2023_Challenge_on_Stereo_Image_Super-Resolution_Methods_and_Results_CVPRW_2023_paper.html",
      "title": "NTIRE 2023 challenge on stereo image super-resolution: Methods and results",
      "authors": "L Wang, Y Guo, Y Wang, J Li, S Gu\u2026",
      "year": "2023",
      "cited_by": 23,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8627981696317437595&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Self-supervised video transformer",
      "id": "11308628305789992240",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Ranasinghe_Self-Supervised_Video_Transformer_CVPR_2022_paper.html",
      "title": "Self-supervised video transformer",
      "authors": "K Ranasinghe, M Naseer, S Khan\u2026",
      "year": "2022",
      "cited_by": 48,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11308628305789992240&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Coarse-to-fine sparse transformer for hyperspectral image reconstruction",
      "id": "2900405794489939688",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-19790-1_41",
      "title": "Coarse-to-fine sparse transformer for hyperspectral image reconstruction",
      "authors": "Y Cai, J Lin, X Hu, H Wang, X Yuan, Y Zhang\u2026",
      "year": "2022",
      "cited_by": 41,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2900405794489939688&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Pyramid Attention Network for Image Restoration",
      "id": "1292725169895823488",
      "url": "https://link.springer.com/article/10.1007/s11263-023-01843-5",
      "title": "Pyramid Attention Network for Image Restoration",
      "authors": "Y Mei, Y Fan, Y Zhang, J Yu, Y Zhou, D Liu\u2026",
      "year": "2023",
      "cited_by": 123,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1292725169895823488&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "NTIRE 2022 challenge on perceptual image quality assessment",
      "id": "14640072730760349377",
      "url": "http://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Gu_NTIRE_2022_Challenge_on_Perceptual_Image_Quality_Assessment_CVPRW_2022_paper.html",
      "title": "NTIRE 2022 challenge on perceptual image quality assessment",
      "authors": "J Gu, H Cai, C Dong, JS Ren\u2026",
      "year": "2022",
      "cited_by": 68,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14640072730760349377&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Coatnet: Marrying convolution and attention for all data sizes",
      "id": "10110104334022835757",
      "url": "https://proceedings.neurips.cc/paper/2021/hash/20568692db622456cc42a2e853ca21f8-Abstract.html",
      "title": "Coatnet: Marrying convolution and attention for all data sizes",
      "authors": "Z Dai, H Liu, QV Le, M Tan",
      "year": "2021",
      "cited_by": 704,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10110104334022835757&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "A convnet for the 2020s",
      "id": "14443907969977981621",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html",
      "title": "A convnet for the 2020s",
      "authors": "Z Liu, H Mao, CY Wu, C Feichtenhofer\u2026",
      "year": "2022",
      "cited_by": 2126,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14443907969977981621&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Scaling vision transformers",
      "id": "13501013621324561884",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhai_Scaling_Vision_Transformers_CVPR_2022_paper.html",
      "title": "Scaling vision transformers",
      "authors": "X Zhai, A Kolesnikov, N Houlsby\u2026",
      "year": "2022",
      "cited_by": 561,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13501013621324561884&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Coca: Contrastive captioners are image-text foundation models",
      "id": "13629397900287809862",
      "url": "https://arxiv.org/abs/2205.01917",
      "title": "Coca: Contrastive captioners are image-text foundation models",
      "authors": "J Yu, Z Wang, V Vasudevan, L Yeung\u2026",
      "year": "2022",
      "cited_by": 470,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13629397900287809862&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Mvitv2: Improved multiscale vision transformers for classification and detection",
      "id": "1273811038957334386",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.html",
      "title": "Mvitv2: Improved multiscale vision transformers for classification and detection",
      "authors": "Y Li, CY Wu, H Fan, K Mangalam\u2026",
      "year": "2022",
      "cited_by": 278,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1273811038957334386&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Inception transformer",
      "id": "610621467807251926",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/94e85561a342de88b559b72c9b29f638-Abstract-Conference.html",
      "title": "Inception transformer",
      "authors": "C Si, W Yu, P Zhou, Y Zhou\u2026",
      "year": "2022",
      "cited_by": 145,
      "cited_by_url": "https://scholar.google.com/scholar?cites=610621467807251926&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Lit: Zero-shot transfer with locked-image text tuning",
      "id": "18380770342348927722",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhai_LiT_Zero-Shot_Transfer_With_Locked-Image_Text_Tuning_CVPR_2022_paper.html",
      "title": "Lit: Zero-shot transfer with locked-image text tuning",
      "authors": "X Zhai, X Wang, B Mustafa, A Steiner\u2026",
      "year": "2022",
      "cited_by": 227,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18380770342348927722&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Simvlm: Simple visual language model pretraining with weak supervision",
      "id": "9618435703828650575",
      "url": "https://arxiv.org/abs/2108.10904",
      "title": "Simvlm: Simple visual language model pretraining with weak supervision",
      "authors": "Z Wang, J Yu, AW Yu, Z Dai, Y Tsvetkov\u2026",
      "year": "2021",
      "cited_by": 448,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9618435703828650575&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Conditional positional encodings for vision transformers",
      "id": "17870066505440679476",
      "url": "https://arxiv.org/abs/2102.10882",
      "title": "Conditional positional encodings for vision transformers",
      "authors": "X Chu, Z Tian, B Zhang, X Wang, X Wei, H Xia\u2026",
      "year": "2021",
      "cited_by": 368,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17870066505440679476&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Eva: Exploring the limits of masked visual representation learning at scale",
      "id": "10588342779298269046",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.html",
      "title": "Eva: Exploring the limits of masked visual representation learning at scale",
      "authors": "Y Fang, W Wang, B Xie, Q Sun, L Wu\u2026",
      "year": "2023",
      "cited_by": 101,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10588342779298269046&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Maxvit: Multi-axis vision transformer",
      "id": "6784655767122395745",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_27",
      "title": "Maxvit: Multi-axis vision transformer",
      "authors": "Z Tu, H Talebi, H Zhang, F Yang, P Milanfar\u2026",
      "year": "2022",
      "cited_by": 158,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6784655767122395745&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Internimage: Exploring large-scale vision foundation models with deformable convolutions",
      "id": "6118595289890500680",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wang_InternImage_Exploring_Large-Scale_Vision_Foundation_Models_With_Deformable_Convolutions_CVPR_2023_paper.html",
      "title": "Internimage: Exploring large-scale vision foundation models with deformable convolutions",
      "authors": "W Wang, J Dai, Z Chen, Z Huang, Z Li\u2026",
      "year": "2023",
      "cited_by": 113,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6118595289890500680&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Hornet: Efficient high-order spatial interactions with recursive gated convolutions",
      "id": "12938213222665733645",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/436d042b2dd81214d23ae43eb196b146-Abstract-Conference.html",
      "title": "Hornet: Efficient high-order spatial interactions with recursive gated convolutions",
      "authors": "Y Rao, W Zhao, Y Tang, J Zhou\u2026",
      "year": "2022",
      "cited_by": 99,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12938213222665733645&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Convnext v2: Co-designing and scaling convnets with masked autoencoders",
      "id": "1388490151733704334",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Woo_ConvNeXt_V2_Co-Designing_and_Scaling_ConvNets_With_Masked_Autoencoders_CVPR_2023_paper.html",
      "title": "Convnext v2: Co-designing and scaling convnets with masked autoencoders",
      "authors": "S Woo, S Debnath, R Hu, X Chen\u2026",
      "year": "2023",
      "cited_by": 61,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1388490151733704334&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Davit: Dual attention vision transformers",
      "id": "18356109755771918503",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_5",
      "title": "Davit: Dual attention vision transformers",
      "authors": "M Ding, B Xiao, N Codella, P Luo, J Wang\u2026",
      "year": "2022",
      "cited_by": 93,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18356109755771918503&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Patches are all you need?",
      "id": "15188717593606933557",
      "url": "https://arxiv.org/abs/2201.09792",
      "title": "Patches are all you need?",
      "authors": "A Trockman, JZ Kolter",
      "year": "2022",
      "cited_by": 221,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15188717593606933557&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Efficientformer: Vision transformers at mobilenet speed",
      "id": "12692106295877813680",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/5452ad8ee6ea6e7dc41db1cbd31ba0b8-Abstract-Conference.html",
      "title": "Efficientformer: Vision transformers at mobilenet speed",
      "authors": "Y Li, G Yuan, Y Wen, J Hu\u2026",
      "year": "2022",
      "cited_by": 86,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12692106295877813680&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Pure transformers are powerful graph learners",
      "id": "1854387804616571098",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/5d84236751fe6d25dc06db055a3180b0-Abstract-Conference.html",
      "title": "Pure transformers are powerful graph learners",
      "authors": "J Kim, D Nguyen, S Min, S Cho\u2026",
      "year": "2022",
      "cited_by": 54,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1854387804616571098&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Multi-stage progressive image restoration",
      "id": "14988305211504802629",
      "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Zamir_Multi-Stage_Progressive_Image_Restoration_CVPR_2021_paper.html",
      "title": "Multi-stage progressive image restoration",
      "authors": "SW Zamir, A Arora, S Khan, M Hayat\u2026",
      "year": "2021",
      "cited_by": 860,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14988305211504802629&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Uformer: A general u-shaped transformer for image restoration",
      "id": "14031000766044293652",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Wang_Uformer_A_General_U-Shaped_Transformer_for_Image_Restoration_CVPR_2022_paper.html",
      "title": "Uformer: A general u-shaped transformer for image restoration",
      "authors": "Z Wang, X Cun, J Bao, W Zhou\u2026",
      "year": "2022",
      "cited_by": 599,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14031000766044293652&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Ntire 2022 spectral recovery challenge and data set",
      "id": "16381083981417715623",
      "url": "https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Arad_NTIRE_2022_Spectral_Recovery_Challenge_and_Data_Set_CVPRW_2022_paper.html",
      "title": "Ntire 2022 spectral recovery challenge and data set",
      "authors": "B Arad, R Timofte, R Yahel, N Morag\u2026",
      "year": "2022",
      "cited_by": 39,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16381083981417715623&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "NTIRE 2021 challenge on image deblurring",
      "id": "6999508588420552953",
      "url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Nah_NTIRE_2021_Challenge_on_Image_Deblurring_CVPRW_2021_paper.html",
      "title": "NTIRE 2021 challenge on image deblurring",
      "authors": "S Nah, S Son, S Lee, R Timofte\u2026",
      "year": "2021",
      "cited_by": 60,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6999508588420552953&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Maxim: Multi-axis mlp for image processing",
      "id": "18275282813589182456",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Tu_MAXIM_Multi-Axis_MLP_for_Image_Processing_CVPR_2022_paper.html",
      "title": "Maxim: Multi-axis mlp for image processing",
      "authors": "Z Tu, H Talebi, H Zhang, F Yang\u2026",
      "year": "2022",
      "cited_by": 188,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18275282813589182456&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Rethinking coarse-to-fine approach in single image deblurring",
      "id": "11948207786179445379",
      "url": "https://openaccess.thecvf.com/content/ICCV2021/html/Cho_Rethinking_Coarse-To-Fine_Approach_in_Single_Image_Deblurring_ICCV_2021_paper.html?ref=https://githubhelp.com",
      "title": "Rethinking coarse-to-fine approach in single image deblurring",
      "authors": "SJ Cho, SW Ji, JP Hong, SW Jung\u2026",
      "year": "2021",
      "cited_by": 271,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11948207786179445379&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Hinet: Half instance normalization network for image restoration",
      "id": "2731814227384441305",
      "url": "https://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Chen_HINet_Half_Instance_Normalization_Network_for_Image_Restoration_CVPRW_2021_paper.html",
      "title": "Hinet: Half instance normalization network for image restoration",
      "authors": "L Chen, X Lu, J Zhang, X Chu\u2026",
      "year": "2021",
      "cited_by": 240,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2731814227384441305&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Transweather: Transformer-based restoration of images degraded by adverse weather conditions",
      "id": "563900006853828372",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Valanarasu_TransWeather_Transformer-Based_Restoration_of_Images_Degraded_by_Adverse_Weather_Conditions_CVPR_2022_paper.html",
      "title": "Transweather: Transformer-based restoration of images degraded by adverse weather conditions",
      "authors": "JMJ Valanarasu, R Yasarla\u2026",
      "year": "2022",
      "cited_by": 90,
      "cited_by_url": "https://scholar.google.com/scholar?cites=563900006853828372&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Deblurring via stochastic refinement",
      "id": "2892557376887066282",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Whang_Deblurring_via_Stochastic_Refinement_CVPR_2022_paper.html",
      "title": "Deblurring via stochastic refinement",
      "authors": "J Whang, M Delbracio, H Talebi\u2026",
      "year": "2022",
      "cited_by": 88,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2892557376887066282&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "All-in-one image restoration for unknown corruption",
      "id": "10807347079650485654",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Li_All-in-One_Image_Restoration_for_Unknown_Corruption_CVPR_2022_paper.html",
      "title": "All-in-one image restoration for unknown corruption",
      "authors": "B Li, X Liu, P Hu, Z Wu, J Lv\u2026",
      "year": "2022",
      "cited_by": 72,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10807347079650485654&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Images speak in images: A generalist painter for in-context visual learning",
      "id": "8278982060432150337",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wang_Images_Speak_in_Images_A_Generalist_Painter_for_In-Context_Visual_CVPR_2023_paper.html",
      "title": "Images speak in images: A generalist painter for in-context visual learning",
      "authors": "X Wang, W Wang, Y Cao, C Shen\u2026",
      "year": "2023",
      "cited_by": 31,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8278982060432150337&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Vrt: A video restoration transformer",
      "id": "9248261167097334152",
      "url": "https://arxiv.org/abs/2201.12288",
      "title": "Vrt: A video restoration transformer",
      "authors": "J Liang, J Cao, Y Fan, K Zhang, R Ranjan, Y Li\u2026",
      "year": "2022",
      "cited_by": 99,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9248261167097334152&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Deep generalized unfolding networks for image restoration",
      "id": "7494856167361955847",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Mou_Deep_Generalized_Unfolding_Networks_for_Image_Restoration_CVPR_2022_paper.html",
      "title": "Deep generalized unfolding networks for image restoration",
      "authors": "C Mou, Q Wang, J Zhang",
      "year": "2022",
      "cited_by": 68,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7494856167361955847&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Learning multiple adverse weather removal via two-stage knowledge learning and multi-contrastive regularization: Toward a unified model",
      "id": "6389162819004784674",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Chen_Learning_Multiple_Adverse_Weather_Removal_via_Two-Stage_Knowledge_Learning_and_CVPR_2022_paper.html",
      "title": "Learning multiple adverse weather removal via two-stage knowledge learning and multi-contrastive regularization: Toward a unified model",
      "authors": "WT Chen, ZK Huang, CC Tsai\u2026",
      "year": "2022",
      "cited_by": 39,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6389162819004784674&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Cross Aggregation Transformer for Image Restoration",
      "id": "10294262589674995487",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/a37fea8e67f907311826bc1ba2654d97-Abstract-Conference.html",
      "title": "Cross Aggregation Transformer for Image Restoration",
      "authors": "Z Chen, Y Zhang, J Gu, L Kong\u2026",
      "year": "2022",
      "cited_by": 19,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10294262589674995487&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Improving image restoration by revisiting global information aggregation",
      "id": "7561375020407203939",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-20071-7_4",
      "title": "Improving image restoration by revisiting global information aggregation",
      "authors": "X Chu, L Chen, C Chen, X Lu",
      "year": "2022",
      "cited_by": 39,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7561375020407203939&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Transreid: Transformer-based object re-identification",
      "id": "15635397108812213817",
      "url": "http://openaccess.thecvf.com/content/ICCV2021/html/He_TransReID_Transformer-Based_Object_Re-Identification_ICCV_2021_paper.html",
      "title": "Transreid: Transformer-based object re-identification",
      "authors": "S He, H Luo, P Wang, F Wang, H Li\u2026",
      "year": "2021",
      "cited_by": 495,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15635397108812213817&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Multiscale vision transformers",
      "id": "7329647594369932315",
      "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Fan_Multiscale_Vision_Transformers_ICCV_2021_paper.html",
      "title": "Multiscale vision transformers",
      "authors": "H Fan, B Xiong, K Mangalam, Y Li\u2026",
      "year": "2021",
      "cited_by": 1596,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7329647594369932315&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Cswin transformer: A general vision transformer backbone with cross-shaped windows",
      "id": "4431453089685809340",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Dong_CSWin_Transformer_A_General_Vision_Transformer_Backbone_With_Cross-Shaped_Windows_CVPR_2022_paper.html",
      "title": "Cswin transformer: A general vision transformer backbone with cross-shaped windows",
      "authors": "X Dong, J Bao, D Chen, W Zhang\u2026",
      "year": "2022",
      "cited_by": 479,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4431453089685809340&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "A survey on vision transformer",
      "id": "4278610892084589339",
      "url": "https://ieeexplore.ieee.org/abstract/document/9716741/",
      "title": "A survey on vision transformer",
      "authors": "K Han, Y Wang, H Chen, X Chen, J Guo\u2026",
      "year": "2022",
      "cited_by": 684,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4278610892084589339&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Transfg: A transformer architecture for fine-grained recognition",
      "id": "601129416962130879",
      "url": "https://ojs.aaai.org/index.php/AAAI/article/view/19967",
      "title": "Transfg: A transformer architecture for fine-grained recognition",
      "authors": "J He, JN Chen, S Liu, A Kortylewski, C Yang\u2026",
      "year": "2022",
      "cited_by": 212,
      "cited_by_url": "https://scholar.google.com/scholar?cites=601129416962130879&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Multi-animal pose estimation, identification and tracking with DeepLabCut",
      "id": "4326704403467340422",
      "url": "https://www.nature.com/articles/s41592-022-01443-0",
      "title": "Multi-animal pose estimation, identification and tracking with DeepLabCut",
      "authors": "J Lauer, M Zhou, S Ye, W Menegas, S Schneider\u2026",
      "year": "2022",
      "cited_by": 167,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4326704403467340422&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Mhformer: Multi-hypothesis transformer for 3d human pose estimation",
      "id": "18177167198432349205",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Li_MHFormer_Multi-Hypothesis_Transformer_for_3D_Human_Pose_Estimation_CVPR_2022_paper.html",
      "title": "Mhformer: Multi-hypothesis transformer for 3d human pose estimation",
      "authors": "W Li, H Liu, H Tang, P Wang\u2026",
      "year": "2022",
      "cited_by": 120,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18177167198432349205&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Uniformer: Unifying convolution and self-attention for visual recognition",
      "id": "342346550438939327",
      "url": "https://ieeexplore.ieee.org/abstract/document/10143709/",
      "title": "Uniformer: Unifying convolution and self-attention for visual recognition",
      "authors": "K Li, Y Wang, J Zhang, P Gao, G Song\u2026",
      "year": "2023",
      "cited_by": 100,
      "cited_by_url": "https://scholar.google.com/scholar?cites=342346550438939327&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition",
      "id": "9632085609200326616",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/64517d8435994992e682b3e4aa0a0661-Abstract.html",
      "title": "Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition",
      "authors": "Y Wang, R Huang, S Song\u2026",
      "year": "2021",
      "cited_by": 99,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9632085609200326616&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Adavit: Adaptive vision transformers for efficient image recognition",
      "id": "14716201437512299394",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Meng_AdaViT_Adaptive_Vision_Transformers_for_Efficient_Image_Recognition_CVPR_2022_paper.html",
      "title": "Adavit: Adaptive vision transformers for efficient image recognition",
      "authors": "L Meng, H Li, BC Chen, S Lan, Z Wu\u2026",
      "year": "2022",
      "cited_by": 69,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14716201437512299394&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Accelerating DETR convergence via semantic-aligned matching",
      "id": "6658129475220097194",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhang_Accelerating_DETR_Convergence_via_Semantic-Aligned_Matching_CVPR_2022_paper.html",
      "title": "Accelerating DETR convergence via semantic-aligned matching",
      "authors": "G Zhang, Z Luo, Y Yu, K Cui\u2026",
      "year": "2022",
      "cited_by": 51,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6658129475220097194&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Cdtrans: Cross-domain transformer for unsupervised domain adaptation",
      "id": "9897783945226246229",
      "url": "https://arxiv.org/abs/2109.06165",
      "title": "Cdtrans: Cross-domain transformer for unsupervised domain adaptation",
      "authors": "T Xu, W Chen, P Wang, F Wang, H Li, R Jin",
      "year": "2021",
      "cited_by": 114,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9897783945226246229&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Dual cross-attention learning for fine-grained visual categorization and object re-identification",
      "id": "5598507831422168925",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhu_Dual_Cross-Attention_Learning_for_Fine-Grained_Visual_Categorization_and_Object_Re-Identification_CVPR_2022_paper.html",
      "title": "Dual cross-attention learning for fine-grained visual categorization and object re-identification",
      "authors": "H Zhu, W Ke, D Li, J Liu, L Tian\u2026",
      "year": "2022",
      "cited_by": 48,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5598507831422168925&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Towards discriminative representation learning for unsupervised person re-identification",
      "id": "3707506564686588016",
      "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Isobe_Towards_Discriminative_Representation_Learning_for_Unsupervised_Person_Re-Identification_ICCV_2021_paper.html",
      "title": "Towards discriminative representation learning for unsupervised person re-identification",
      "authors": "T Isobe, D Li, L Tian, W Chen\u2026",
      "year": "2021",
      "cited_by": 54,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3707506564686588016&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search",
      "id": "4988594281116353083",
      "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Li_BossNAS_Exploring_Hybrid_CNN-Transformers_With_Block-Wisely_Self-Supervised_Neural_Architecture_Search_ICCV_2021_paper.html",
      "title": "Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search",
      "authors": "C Li, T Tang, G Wang, J Peng\u2026",
      "year": "2021",
      "cited_by": 81,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4988594281116353083&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Exploiting temporal contexts with strided transformer for 3d human pose estimation",
      "id": "16448186114991458904",
      "url": "https://ieeexplore.ieee.org/abstract/document/9674785/",
      "title": "Exploiting temporal contexts with strided transformer for 3d human pose estimation",
      "authors": "W Li, H Liu, R Ding, M Liu, P Wang\u2026",
      "year": "2022",
      "cited_by": 92,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16448186114991458904&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Recent advances in vision transformer: A survey and outlook of recent work",
      "id": "10189561822678692220",
      "url": "https://arxiv.org/abs/2203.01536",
      "title": "Recent advances in vision transformer: A survey and outlook of recent work",
      "authors": "K Islam",
      "year": "2022",
      "cited_by": 17,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10189561822678692220&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Structure-aware positional transformer for visible-infrared person re-identification",
      "id": "12518043648515496043",
      "url": "https://ieeexplore.ieee.org/abstract/document/9725265/",
      "title": "Structure-aware positional transformer for visible-infrared person re-identification",
      "authors": "C Chen, M Ye, M Qi, J Wu, J Jiang\u2026",
      "year": "2022",
      "cited_by": 58,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12518043648515496043&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Intriguing properties of vision transformers",
      "id": "15172072370662904150",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html",
      "title": "Intriguing properties of vision transformers",
      "authors": "MM Naseer, K Ranasinghe, SH Khan\u2026",
      "year": "2021",
      "cited_by": 359,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15172072370662904150&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "ibot: Image bert pre-training with online tokenizer",
      "id": "6668235945473015803",
      "url": "https://arxiv.org/abs/2111.07832",
      "title": "ibot: Image bert pre-training with online tokenizer",
      "authors": "J Zhou, C Wei, H Wang, W Shen, C Xie, A Yuille\u2026",
      "year": "2021",
      "cited_by": 372,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6668235945473015803&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Are transformers more robust than cnns?",
      "id": "2316302132679082774",
      "url": "https://proceedings.neurips.cc/paper/2021/hash/e19347e1c3ca0c0b97de5fb3b690855a-Abstract.html",
      "title": "Are transformers more robust than cnns?",
      "authors": "Y Bai, J Mei, AL Yuille, C Xie",
      "year": "2021",
      "cited_by": 167,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2316302132679082774&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation",
      "id": "4388759310460601633",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Hoyer_DAFormer_Improving_Network_Architectures_and_Training_Strategies_for_Domain-Adaptive_Semantic_CVPR_2022_paper.html",
      "title": "Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation",
      "authors": "L Hoyer, D Dai, L Van Gool",
      "year": "2022",
      "cited_by": 177,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4388759310460601633&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Understanding the robustness in vision transformers",
      "id": "3041067607452518927",
      "url": "https://proceedings.mlr.press/v162/zhou22m.html",
      "title": "Understanding the robustness in vision transformers",
      "authors": "D Zhou, Z Yu, E Xie, C Xiao\u2026",
      "year": "2022",
      "cited_by": 82,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3041067607452518927&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Partial success in closing the gap between human and machine vision",
      "id": "875131557547078483",
      "url": "https://proceedings.neurips.cc/paper/2021/hash/c8877cff22082a16395a57e97232bb6f-Abstract.html",
      "title": "Partial success in closing the gap between human and machine vision",
      "authors": "R Geirhos, K Narayanappa, B Mitzkus\u2026",
      "year": "2021",
      "cited_by": 110,
      "cited_by_url": "https://scholar.google.com/scholar?cites=875131557547078483&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Efficient training of visual transformers with small datasets",
      "id": "17891879498080154736",
      "url": "https://proceedings.neurips.cc/paper/2021/hash/c81e155d85dae5430a8cee6f2242e82c-Abstract.html",
      "title": "Efficient training of visual transformers with small datasets",
      "authors": "Y Liu, E Sangineto, W Bi, N Sebe\u2026",
      "year": "2021",
      "cited_by": 106,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17891879498080154736&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Assaying out-of-distribution generalization in transfer learning",
      "id": "2028336304446280911",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/2f5acc925919209370a3af4eac5cad4a-Abstract-Conference.html",
      "title": "Assaying out-of-distribution generalization in transfer learning",
      "authors": "F Wenzel, A Dittadi, P Gehler\u2026",
      "year": "2022",
      "cited_by": 29,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2028336304446280911&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Ow-detr: Open-world detection transformer",
      "id": "5601871542106060008",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Gupta_OW-DETR_Open-World_Detection_Transformer_CVPR_2022_paper.html",
      "title": "Ow-detr: Open-world detection transformer",
      "authors": "A Gupta, S Narayan, KJ Joseph\u2026",
      "year": "2022",
      "cited_by": 67,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5601871542106060008&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "ViT-YOLO: Transformer-based YOLO for object detection",
      "id": "14672720829595281606",
      "url": "http://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Zhang_ViT-YOLOTransformer-Based_YOLO_for_Object_Detection_ICCVW_2021_paper.html",
      "title": "ViT-YOLO: Transformer-based YOLO for object detection",
      "authors": "Z Zhang, X Lu, G Cao, Y Yang\u2026",
      "year": "2021",
      "cited_by": 82,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14672720829595281606&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Not all patches are what you need: Expediting vision transformers via token reorganizations",
      "id": "13367059770507522630",
      "url": "https://arxiv.org/abs/2202.07800",
      "title": "Not all patches are what you need: Expediting vision transformers via token reorganizations",
      "authors": "Y Liang, C Ge, Z Tong, Y Song, J Wang\u2026",
      "year": "2022",
      "cited_by": 111,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13367059770507522630&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Msft-yolo: Improved yolov5 based on transformer for detecting defects of steel surface",
      "id": "4241401890946035983",
      "url": "https://www.mdpi.com/1424-8220/22/9/3467",
      "title": "Msft-yolo: Improved yolov5 based on transformer for detecting defects of steel surface",
      "authors": "Z Guo, C Wang, G Yang, Z Huang, G Li",
      "year": "2022",
      "cited_by": 68,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4241401890946035983&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives",
      "id": "6243645967630982889",
      "url": "https://www.sciencedirect.com/science/article/pii/S1361841523000233",
      "title": "Transforming medical imaging with Transformers? A comparative review of key properties, current progresses, and future perspectives",
      "authors": "J Li, J Chen, Y Tang, C Wang, BA Landman\u2026",
      "year": "2023",
      "cited_by": 56,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6243645967630982889&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Viewfool: Evaluating the robustness of visual recognition to adversarial viewpoints",
      "id": "4486454263174539234",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/eee7ae5cf0c4356c2aeca400771791aa-Abstract-Conference.html",
      "title": "Viewfool: Evaluating the robustness of visual recognition to adversarial viewpoints",
      "authors": "Y Dong, S Ruan, H Su, C Kang\u2026",
      "year": "2022",
      "cited_by": 18,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4486454263174539234&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Efficient training of audio transformers with patchout",
      "id": "1157620601786084092",
      "url": "https://arxiv.org/abs/2110.05069",
      "title": "Efficient training of audio transformers with patchout",
      "authors": "K Koutini, J Schl\u00fcter, H Eghbal-Zadeh\u2026",
      "year": "2021",
      "cited_by": 106,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1157620601786084092&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Panoptic segformer: Delving deeper into panoptic segmentation with transformers",
      "id": "11135517644142739642",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Li_Panoptic_SegFormer_Delving_Deeper_Into_Panoptic_Segmentation_With_Transformers_CVPR_2022_paper.html",
      "title": "Panoptic segformer: Delving deeper into panoptic segmentation with transformers",
      "authors": "Z Li, W Wang, E Xie, Z Yu\u2026",
      "year": "2022",
      "cited_by": 47,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11135517644142739642&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Localizing objects with self-supervised transformers and no labels",
      "id": "13429345883781912849",
      "url": "https://arxiv.org/abs/2109.14279",
      "title": "Localizing objects with self-supervised transformers and no labels",
      "authors": "O Sim\u00e9oni, G Puy, HV Vo, S Roburin, S Gidaris\u2026",
      "year": "2021",
      "cited_by": 92,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13429345883781912849&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "A survey of modern deep learning based object detection models",
      "id": "14311400318178337111",
      "url": "https://www.sciencedirect.com/science/article/pii/S1051200422001312",
      "title": "A survey of modern deep learning based object detection models",
      "authors": "SSA Zaidi, MS Ansari, A Aslam, N Kanwal\u2026",
      "year": "2022",
      "cited_by": 425,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14311400318178337111&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Deep learning methods for object detection in smart manufacturing: A survey",
      "id": "11789051068432887660",
      "url": "https://www.sciencedirect.com/science/article/pii/S0278612522001066",
      "title": "Deep learning methods for object detection in smart manufacturing: A survey",
      "authors": "HM Ahmad, A Rahimi",
      "year": "2022",
      "cited_by": 20,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11789051068432887660&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Mammogram breast cancer CAD systems for mass detection and classification: a review",
      "id": "15295068953900215294",
      "url": "https://link.springer.com/article/10.1007/s11042-022-12332-1",
      "title": "Mammogram breast cancer CAD systems for mass detection and classification: a review",
      "authors": "NM Hassan, S Hamad, K Mahar",
      "year": "2022",
      "cited_by": 32,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15295068953900215294&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Precise single-stage detector",
      "id": "15549291371117213871",
      "url": "https://arxiv.org/abs/2210.04252",
      "title": "Precise single-stage detector",
      "authors": "A Chandio, G Gui, T Kumar, I Ullah\u2026",
      "year": "2022",
      "cited_by": 41,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15549291371117213871&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "CE-FPN: Enhancing channel information for object detection",
      "id": "9884544045603971658",
      "url": "https://link.springer.com/article/10.1007/s11042-022-11940-1",
      "title": "CE-FPN: Enhancing channel information for object detection",
      "authors": "Y Luo, X Cao, J Zhang, J Guo, H Shen, T Wang\u2026",
      "year": "2022",
      "cited_by": 68,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9884544045603971658&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "A survey of self-supervised and few-shot object detection",
      "id": "9591435289724766439",
      "url": "https://ieeexplore.ieee.org/abstract/document/9860087/",
      "title": "A survey of self-supervised and few-shot object detection",
      "authors": "G Huang, I Laradji, D Vazquez\u2026",
      "year": "2022",
      "cited_by": 42,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9591435289724766439&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Feature split\u2013merge\u2013enhancement network for remote sensing object detection",
      "id": "10628215061802232868",
      "url": "https://ieeexplore.ieee.org/abstract/document/9673713/",
      "title": "Feature split\u2013merge\u2013enhancement network for remote sensing object detection",
      "authors": "W Ma, N Li, H Zhu, L Jiao, X Tang\u2026",
      "year": "2022",
      "cited_by": 42,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10628215061802232868&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Guiding pretraining in reinforcement learning with large language models",
      "id": "9532821550175512200",
      "url": "https://arxiv.org/abs/2302.06692",
      "title": "Guiding pretraining in reinforcement learning with large language models",
      "authors": "Y Du, O Watkins, Z Wang, C Colas, T Darrell\u2026",
      "year": "2023",
      "cited_by": 19,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9532821550175512200&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Integrating deep learning-based iot and fog computing with software-defined networking for detecting weapons in video surveillance systems",
      "id": "12985976504909847163",
      "url": "https://www.mdpi.com/1424-8220/22/14/5075",
      "title": "Integrating deep learning-based iot and fog computing with software-defined networking for detecting weapons in video surveillance systems",
      "authors": "C Fathy, SN Saleh",
      "year": "2022",
      "cited_by": 18,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12985976504909847163&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Backbones-review: Feature extraction networks for deep learning and deep reinforcement learning approaches",
      "id": "2812153438552156646",
      "url": "https://arxiv.org/abs/2206.08016",
      "title": "Backbones-review: Feature extraction networks for deep learning and deep reinforcement learning approaches",
      "authors": "O Elharrouss, Y Akbari, N Almaadeed\u2026",
      "year": "2022",
      "cited_by": 28,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2812153438552156646&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Small-object detection based on YOLOv5 in autonomous driving systems",
      "id": "14349697475909471320",
      "url": "https://www.sciencedirect.com/science/article/pii/S0167865523000727",
      "title": "Small-object detection based on YOLOv5 in autonomous driving systems",
      "authors": "B Mahaur, KK Mishra",
      "year": "2023",
      "cited_by": 14,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14349697475909471320&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Single-stage uav detection and classification with yolov5: Mosaic data augmentation and panet",
      "id": "18351357225093508985",
      "url": "https://ieeexplore.ieee.org/abstract/document/9663841/",
      "title": "Single-stage uav detection and classification with yolov5: Mosaic data augmentation and panet",
      "authors": "F Dadboud, V Patel, V Mehta, M Bolic\u2026",
      "year": "2021",
      "cited_by": 25,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18351357225093508985&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "FPGA-based accelerator for object detection: A comprehensive survey",
      "id": "11021564807628327569",
      "url": "https://link.springer.com/article/10.1007/s11227-022-04415-5",
      "title": "FPGA-based accelerator for object detection: A comprehensive survey",
      "authors": "K Zeng, Q Ma, JW Wu, Z Chen, T Shen\u2026",
      "year": "2022",
      "cited_by": 14,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11021564807628327569&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Rail wheel tread defect detection using improved YOLOv3",
      "id": "4655434626952320958",
      "url": "https://www.sciencedirect.com/science/article/pii/S0263224122011551",
      "title": "Rail wheel tread defect detection using improved YOLOv3",
      "authors": "Z Xing, Z Zhang, X Yao, Y Qin, L Jia",
      "year": "2022",
      "cited_by": 11,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4655434626952320958&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Development of a Low-Power IoMT Portable Pillbox for Medication Adherence Improvement and Remote Treatment Adjustment",
      "id": "16079747206913991174",
      "url": "https://www.mdpi.com/1424-8220/22/15/5818",
      "title": "Development of a Low-Power IoMT Portable Pillbox for Medication Adherence Improvement and Remote Treatment Adjustment",
      "authors": "D Karagiannis, K Mitsis, KS Nikita",
      "year": "2022",
      "cited_by": 8,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16079747206913991174&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "A comprehensive review of object detection with deep learning",
      "id": "16432688928939217038",
      "url": "https://www.sciencedirect.com/science/article/pii/S1051200422004298",
      "title": "A comprehensive review of object detection with deep learning",
      "authors": "R Kaur, S Singh",
      "year": "2022",
      "cited_by": 11,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16432688928939217038&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications",
      "id": "6456248456066525600",
      "url": "https://link.springer.com/article/10.1186/s40537-023-00727-2",
      "title": "A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications",
      "authors": "L Alzubaidi, J Bai, A Al-Sabaawi, J Santamar\u00eda\u2026",
      "year": "2023",
      "cited_by": 25,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6456248456066525600&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Video surveillance using deep transfer learning and deep domain adaptation: Towards better generalization",
      "id": "8123745338600640152",
      "url": "https://www.sciencedirect.com/science/article/pii/S0952197622006881",
      "title": "Video surveillance using deep transfer learning and deep domain adaptation: Towards better generalization",
      "authors": "Y Himeur, S Al-Maadeed, H Kheddar\u2026",
      "year": "2023",
      "cited_by": 18,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8123745338600640152&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Review of recent automated pothole-detection methods",
      "id": "10334732198267289555",
      "url": "https://www.mdpi.com/2076-3417/12/11/5320",
      "title": "Review of recent automated pothole-detection methods",
      "authors": "YM Kim, YG Kim, SY Son, SY Lim, BY Choi, DH Choi",
      "year": "2022",
      "cited_by": 15,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10334732198267289555&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Towards domain generalization in object detection",
      "id": "5740479134802475221",
      "url": "https://arxiv.org/abs/2203.14387",
      "title": "Towards domain generalization in object detection",
      "authors": "X Zhang, Z Xu, R Xu, J Liu, P Cui, W Wan\u2026",
      "year": "2022",
      "cited_by": 15,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5740479134802475221&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Allergen30: detecting food items with possible allergens using deep learning-based computer vision",
      "id": "6447041552955227244",
      "url": "https://link.springer.com/article/10.1007/s12161-022-02353-9",
      "title": "Allergen30: detecting food items with possible allergens using deep learning-based computer vision",
      "authors": "M Mishra, T Sarkar, T Choudhury, N Bansal\u2026",
      "year": "2022",
      "cited_by": 7,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6447041552955227244&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Point-bert: Pre-training 3d point cloud transformers with masked point modeling",
      "id": "17327663970405370182",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Yu_Point-BERT_Pre-Training_3D_Point_Cloud_Transformers_With_Masked_Point_Modeling_CVPR_2022_paper.html",
      "title": "Point-bert: Pre-training 3d point cloud transformers with masked point modeling",
      "authors": "X Yu, L Tang, Y Rao, T Huang\u2026",
      "year": "2022",
      "cited_by": 215,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17327663970405370182&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    }
  ],
  "links": [
    {
      "source": 0,
      "target": 1
    },
    {
      "source": 0,
      "target": 42
    },
    {
      "source": 0,
      "target": 62
    },
    {
      "source": 0,
      "target": 96
    },
    {
      "source": 0,
      "target": 114
    },
    {
      "source": 2,
      "target": 0
    },
    {
      "source": 2,
      "target": 62
    },
    {
      "source": 3,
      "target": 2
    },
    {
      "source": 3,
      "target": 23
    },
    {
      "source": 3,
      "target": 114
    },
    {
      "source": 4,
      "target": 2
    },
    {
      "source": 5,
      "target": 2
    },
    {
      "source": 5,
      "target": 62
    },
    {
      "source": 6,
      "target": 2
    },
    {
      "source": 7,
      "target": 2
    },
    {
      "source": 8,
      "target": 2
    },
    {
      "source": 9,
      "target": 2
    },
    {
      "source": 10,
      "target": 2
    },
    {
      "source": 10,
      "target": 23
    },
    {
      "source": 11,
      "target": 2
    },
    {
      "source": 12,
      "target": 2
    },
    {
      "source": 13,
      "target": 2
    },
    {
      "source": 14,
      "target": 2
    },
    {
      "source": 15,
      "target": 2
    },
    {
      "source": 16,
      "target": 2
    },
    {
      "source": 17,
      "target": 2
    },
    {
      "source": 18,
      "target": 2
    },
    {
      "source": 19,
      "target": 2
    },
    {
      "source": 20,
      "target": 2
    },
    {
      "source": 21,
      "target": 2
    },
    {
      "source": 22,
      "target": 2
    },
    {
      "source": 23,
      "target": 0
    },
    {
      "source": 24,
      "target": 23
    },
    {
      "source": 25,
      "target": 23
    },
    {
      "source": 26,
      "target": 23
    },
    {
      "source": 26,
      "target": 96
    },
    {
      "source": 27,
      "target": 23
    },
    {
      "source": 28,
      "target": 23
    },
    {
      "source": 29,
      "target": 23
    },
    {
      "source": 30,
      "target": 23
    },
    {
      "source": 31,
      "target": 23
    },
    {
      "source": 31,
      "target": 96
    },
    {
      "source": 32,
      "target": 23
    },
    {
      "source": 33,
      "target": 23
    },
    {
      "source": 34,
      "target": 23
    },
    {
      "source": 35,
      "target": 23
    },
    {
      "source": 36,
      "target": 23
    },
    {
      "source": 37,
      "target": 23
    },
    {
      "source": 38,
      "target": 23
    },
    {
      "source": 39,
      "target": 23
    },
    {
      "source": 40,
      "target": 23
    },
    {
      "source": 41,
      "target": 23
    },
    {
      "source": 42,
      "target": 0
    },
    {
      "source": 42,
      "target": 80
    },
    {
      "source": 43,
      "target": 42
    },
    {
      "source": 44,
      "target": 42
    },
    {
      "source": 45,
      "target": 42
    },
    {
      "source": 46,
      "target": 42
    },
    {
      "source": 47,
      "target": 42
    },
    {
      "source": 48,
      "target": 42
    },
    {
      "source": 49,
      "target": 42
    },
    {
      "source": 50,
      "target": 42
    },
    {
      "source": 51,
      "target": 42
    },
    {
      "source": 52,
      "target": 42
    },
    {
      "source": 52,
      "target": 80
    },
    {
      "source": 53,
      "target": 42
    },
    {
      "source": 53,
      "target": 80
    },
    {
      "source": 54,
      "target": 42
    },
    {
      "source": 55,
      "target": 42
    },
    {
      "source": 56,
      "target": 42
    },
    {
      "source": 57,
      "target": 42
    },
    {
      "source": 58,
      "target": 42
    },
    {
      "source": 58,
      "target": 114
    },
    {
      "source": 59,
      "target": 42
    },
    {
      "source": 59,
      "target": 80
    },
    {
      "source": 60,
      "target": 42
    },
    {
      "source": 60,
      "target": 80
    },
    {
      "source": 61,
      "target": 42
    },
    {
      "source": 62,
      "target": 0
    },
    {
      "source": 63,
      "target": 62
    },
    {
      "source": 64,
      "target": 62
    },
    {
      "source": 65,
      "target": 62
    },
    {
      "source": 66,
      "target": 62
    },
    {
      "source": 67,
      "target": 62
    },
    {
      "source": 68,
      "target": 62
    },
    {
      "source": 69,
      "target": 62
    },
    {
      "source": 70,
      "target": 62
    },
    {
      "source": 71,
      "target": 62
    },
    {
      "source": 72,
      "target": 62
    },
    {
      "source": 73,
      "target": 62
    },
    {
      "source": 74,
      "target": 62
    },
    {
      "source": 75,
      "target": 62
    },
    {
      "source": 76,
      "target": 62
    },
    {
      "source": 77,
      "target": 62
    },
    {
      "source": 78,
      "target": 62
    },
    {
      "source": 79,
      "target": 62
    },
    {
      "source": 80,
      "target": 0
    },
    {
      "source": 81,
      "target": 80
    },
    {
      "source": 82,
      "target": 80
    },
    {
      "source": 83,
      "target": 80
    },
    {
      "source": 84,
      "target": 80
    },
    {
      "source": 85,
      "target": 80
    },
    {
      "source": 86,
      "target": 80
    },
    {
      "source": 87,
      "target": 80
    },
    {
      "source": 88,
      "target": 80
    },
    {
      "source": 89,
      "target": 80
    },
    {
      "source": 90,
      "target": 80
    },
    {
      "source": 91,
      "target": 80
    },
    {
      "source": 92,
      "target": 80
    },
    {
      "source": 93,
      "target": 80
    },
    {
      "source": 94,
      "target": 80
    },
    {
      "source": 95,
      "target": 80
    },
    {
      "source": 96,
      "target": 0
    },
    {
      "source": 97,
      "target": 96
    },
    {
      "source": 98,
      "target": 96
    },
    {
      "source": 99,
      "target": 96
    },
    {
      "source": 100,
      "target": 96
    },
    {
      "source": 101,
      "target": 96
    },
    {
      "source": 102,
      "target": 96
    },
    {
      "source": 103,
      "target": 96
    },
    {
      "source": 104,
      "target": 96
    },
    {
      "source": 105,
      "target": 96
    },
    {
      "source": 106,
      "target": 96
    },
    {
      "source": 107,
      "target": 96
    },
    {
      "source": 107,
      "target": 114
    },
    {
      "source": 108,
      "target": 96
    },
    {
      "source": 109,
      "target": 96
    },
    {
      "source": 110,
      "target": 96
    },
    {
      "source": 111,
      "target": 96
    },
    {
      "source": 112,
      "target": 96
    },
    {
      "source": 113,
      "target": 96
    },
    {
      "source": 114,
      "target": 0
    },
    {
      "source": 115,
      "target": 114
    },
    {
      "source": 116,
      "target": 114
    },
    {
      "source": 117,
      "target": 114
    },
    {
      "source": 118,
      "target": 114
    },
    {
      "source": 119,
      "target": 114
    },
    {
      "source": 120,
      "target": 114
    },
    {
      "source": 121,
      "target": 114
    },
    {
      "source": 122,
      "target": 114
    },
    {
      "source": 123,
      "target": 114
    },
    {
      "source": 124,
      "target": 114
    },
    {
      "source": 125,
      "target": 114
    },
    {
      "source": 126,
      "target": 114
    },
    {
      "source": 127,
      "target": 114
    },
    {
      "source": 128,
      "target": 114
    },
    {
      "source": 129,
      "target": 114
    },
    {
      "source": 130,
      "target": 114
    },
    {
      "source": 131,
      "target": 0
    },
    {
      "source": 132,
      "target": 131
    },
    {
      "source": 133,
      "target": 131
    },
    {
      "source": 134,
      "target": 131
    },
    {
      "source": 135,
      "target": 131
    },
    {
      "source": 136,
      "target": 131
    },
    {
      "source": 137,
      "target": 131
    },
    {
      "source": 138,
      "target": 131
    },
    {
      "source": 139,
      "target": 131
    },
    {
      "source": 140,
      "target": 131
    },
    {
      "source": 141,
      "target": 131
    },
    {
      "source": 142,
      "target": 131
    },
    {
      "source": 143,
      "target": 131
    },
    {
      "source": 144,
      "target": 131
    },
    {
      "source": 145,
      "target": 131
    },
    {
      "source": 146,
      "target": 131
    },
    {
      "source": 147,
      "target": 131
    },
    {
      "source": 148,
      "target": 131
    },
    {
      "source": 149,
      "target": 131
    },
    {
      "source": 150,
      "target": 131
    },
    {
      "source": 151,
      "target": 131
    },
    {
      "source": 152,
      "target": 0
    }
  ]
};
      var w = window.innerWidth;
      var h = window.innerHeight;

      var focusNode = null;
      var highlightNode = null;

      var textCenter = false;
      var outline = false;

      var minScore = Math.min(...graph.nodes.map(n => n.modularity));
      var maxScore = Math.max(...graph.nodes.map(n => n.modularity));

      var color = d3.scale
        .linear()
        .domain([
          minScore,
          (minScore + maxScore) / 4,
          (minScore + maxScore) / 2,
          ((minScore + maxScore) * 3) / 4,
          maxScore,
        ])
        .range(["lime", "yellow", "red", "deepskyblue"]);

      var highlightColor = "blue";
      var highlightTrans = 0.1;

      const citedBy = graph.nodes
        .map(n => n.cited_by)
        .filter(n => n != null)

      const maxCitedBy = Math.max(...citedBy)
      const minCitedBy = Math.min(...citedBy)

      var size = d3.scale
        .pow()
        .exponent(1)
        .domain([minCitedBy, maxCitedBy])
        .range([8, 24]);

      var force = d3.layout
        .force()
        .linkDistance(h / (graph.nodes.length / 10))
        .charge(-300)
        .size([w, h]);

      var defaultNodeColor = "#ccc";
      var defaultLinkColor = "#888";
      var nominalBaseNodeSize = 8;
      var nominalTextSize = 10;
      var maxTextSize = 24;
      var nominalStroke = 1.5;
      var maxStroke = 4.5;
      var maxBaseNodeSize = 36;
      var minZoom = 0.1;
      var maxZoom = 7;
      var svg = d3.select("body").append("svg");
      var zoom = d3.behavior.zoom().scaleExtent([minZoom, maxZoom]);
      var g = svg.append("g");
      svg.style("cursor", "move");

      var linkedByIndex = {};
      graph.links.forEach(function (d) {
        linkedByIndex[d.source + "," + d.target] = true;
      });

      function isConnected(a, b) {
        return (
          linkedByIndex[a.index + "," + b.index] ||
          linkedByIndex[b.index + "," + a.index] ||
          a.index == b.index
        );
      }

      force.size([w, h]);

      force
        .nodes(graph.nodes)
        .links(graph.links)
        .start();

      function getLine(data) {

        const x1 = data.source.x;
        const y1= data.source.y;
        const x2 = data.target.x;
        const y2 = data.target.y;

        const r = size(data.target.cited_by) + 1;

        const m = (y2 - y1) / (x2 - x1);
        const b = y1 - m * x1;

        const c = Math.sqrt(Math.pow((y2 - y1), 2) + Math.pow((x2 - x1), 2))
        const a = y2 - y1
        const cos = a / c

        const a2 = cos * r
        const b2 = Math.sqrt(Math.pow(r, 2) - Math.pow(a2, 2))

        const x = x2 > x1 ? x2 - b2 : x2 + b2;
        const y = y2 - a2;

        const path = 'M ' + data.source.x + ',' + data.source.y + ' L ' + x + ',' + y;
        return path;
      }

      var link = g
        .selectAll(".link")
        .data(graph.links)
        .enter()
        .append("svg:path")
        .attr("d", getLine) 
        .attr("stroke", defaultLinkColor)
        .attr("fill", "red")
        .style("stroke-width", nominalStroke)
        .style("marker-end", "url(#end)")

      var node = g
        .selectAll(".node")
        .data(graph.nodes)
        .enter()
        .append("g")
        .attr("class", "node")
        .call(force.drag);

      var timeout = null;

      node.on("dblclick", function (d) {
        clearTimeout(timeout);

        timeout = setTimeout(function () {
          window.open(d.url, "_blank");
          d3.event.stopPropagation();
        }, 300);
      });

      var tocolor = "fill";
      var towhite = "stroke";
      if (outline) {
        tocolor = "stroke";
        towhite = "fill";
      }

      var circle = node
        .append("path")
        .attr(
          "d",
          d3.svg
            .symbol()
            .size(function (d) {
              return (
                Math.PI * Math.pow(size(d.cited_by) || nominalBaseNodeSize, 2)
              );
            })
            .type(function (d) {
              return d.type;
            })
        )
        .style(tocolor, function (d) {
          if (isNumber(d.modularity) && d.modularity >= 0) return color(d.modularity);
          else return defaultNodeColor;
        })
        .style("stroke-width", nominalStroke)
        .style(towhite, "white");

      svg.append("svg:defs").selectAll("marker")
	  .data(["end"])
	.enter().append("svg:marker")
	  .attr("id", String)
	  .attr("viewBox", "0 -5 10 10")
	  .attr("refX", 10)
	  .attr("refY", 0)
	  .attr("markerWidth", 6)
	  .attr("markerHeight", 6)
	  .attr("orient", "auto")
          .style("fill", defaultLinkColor)
	.append("svg:path")
	  .attr("d", "M 0,-5 L 10,0 L 0,5")
          .style("stroke", defaultLinkColor);

      var text = g
        .selectAll(".text")
        .data(graph.nodes)
        .enter()
        .append("text")
        .attr("dy", ".35em")
        .style("font-size", nominalTextSize + "px");

      node
        .on("mouseover", function (d) {
          setHighlight(d);
        })
        .on("mousedown", function (d) {
          d3.event.stopPropagation();
          focusNode = d;
          setFocus(d);
          if (highlightNode === null) setHighlight(d);
        })
        .on("mouseout", function (d) {
          exitHighlight();
        });

      d3.select(window).on("mouseup", function () {
        if (focusNode !== null) {
          focusNode = null;
          if (highlightTrans < 1) {
            circle.style("opacity", 1);
            text.style("opacity", 1);
            link.style("opacity", 1);
          }
        }

        if (highlightNode === null) exitHighlight();
      });

      function exitHighlight() {
        highlightNode = null;
        if (focusNode === null) {
          svg.style("cursor", "move");
          if (highlightColor != "white") {
            circle.style(towhite, "white");
            text.text('')
            link.style("stroke", function (o) {
              return isNumber(o.score) && o.score >= 0
                ? color(o.score)
                : defaultLinkColor;
            });
          }
        }
      }

      function setFocus(d) {
        if (highlightTrans < 1) {
          circle.style("opacity", function (o) {
            return isConnected(d, o) ? 1 : highlightTrans;
          });

          text.style("opacity", function (o) {
            return isConnected(d, o) ? 1 : highlightTrans;
          });

          link.style("opacity", function (o) {
            return o.source.index == d.index || o.target.index == d.index
              ? 1
              : highlightTrans;
          });
        }
      }

      function setHighlight(d) {
        svg.style("cursor", "pointer");
        if (focusNode !== null) d = focusNode;
        highlightNode = d;

        if (highlightColor != "white") {

          circle.style(towhite, function (o) {
            return isConnected(d, o) ? highlightColor : "white";
          });
          
          text.attr("dx", function (d) {
            return size(d.cited_by)
          });

          text.text(function (o) {
            if (isConnected(d, o)) {
              let title = o.title;
              if (o.year) title = title + " (" + o.year + ")";
              if (o.authors) title = title + " - " + o.authors;
              return title
            } else {
              return ""
            }
          });

        }
      }

      zoom.on("zoom", function () {
        var stroke = nominalStroke;
        if (nominalStroke * zoom.scale() > maxStroke)
          stroke = maxStroke / zoom.scale();
        link.style("stroke-width", stroke);
        circle.style("stroke-width", stroke);

        var baseRadius = nominalBaseNodeSize;
        if (nominalBaseNodeSize * zoom.scale() > maxBaseNodeSize)
          baseRadius = maxBaseNodeSize / zoom.scale();
        circle.attr(
          "d",
          d3.svg
            .symbol()
            .size(function (d) {
              return (
                Math.PI *
                Math.pow(
                  (size(d.cited_by) * baseRadius) / nominalBaseNodeSize ||
                    baseRadius,
                  2
                )
              );
            })
        );

        if (!textCenter)
          text.attr("dx", function (d) {
            return (
              (size(d.cited_by) * baseRadius) / nominalBaseNodeSize ||
              baseRadius
            );
          });

        var textSize = nominalTextSize;
        if (nominalTextSize * zoom.scale() > maxTextSize)
          textSize = maxTextSize / zoom.scale();
        text.style("font-size", textSize + "px");

        g.attr(
          "transform",
          "translate(" + d3.event.translate + ")scale(" + d3.event.scale + ")"
        );
      });

      svg.call(zoom);

      resize();
      d3.select(window).on("resize", resize);

      force.on("tick", function () {
        node.attr("transform", function (d) {
          return "translate(" + d.x + "," + d.y + ")";
        });
        text.attr("transform", function (d) {
          return "translate(" + d.x + "," + d.y + ")";
        });

        link.attr("d", getLine)

        node
          .attr("cx", function (d) {
            return d.x;
          })
          .attr("cy", function (d) {
            return d.y;
          });
      });

      function resize() {
        var width = window.innerWidth,
          height = window.innerHeight;
        svg.attr("width", width).attr("height", height);

        force
          .size([
            force.size()[0] + (width - w) / zoom.scale(),
            force.size()[1] + (height - h) / zoom.scale(),
          ])
          .resume();
        w = width;
        h = height;
      }

      function isNumber(n) {
        return !isNaN(parseFloat(n)) && isFinite(n);
      }

    </script>
  </body>
</html>
