<!DOCTYPE html>

<!--

This Google Scholar network visualization was generated with
https://github.com/edsu/etudier using the following command:

% etudier --pages 1000 --depth 2 --max-citations 50 https://scholar.google.com/scholar?cites=6382612685700818764&as_sdt=40005&sciodt=0,10&hl=en

--> 

<html>
  <head>
    <meta charset="utf-8" />
    <style>
      body {
        overflow: hidden;
        margin: 0;
      }

      text {
        font-family: sans-serif;
        pointer-events: none;
      }
    </style>
  </head>

  <body>
    <script src="https://d3js.org/d3.v3.min.js"></script>
    <script>
      var graph = {
  "nodes": [
    {
      "label": "Transformers in vision: A survey",
      "id": "7522504961268153944",
      "url": "https://dl.acm.org/doi/abs/10.1145/3505244",
      "title": "Transformers in vision: A survey",
      "authors": "S Khan, M Naseer, M Hayat, SW Zamir\u2026",
      "year": "2022",
      "cited_by": 1298,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7522504961268153944&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "You only look once: Unified, real-time object detection",
      "id": "6382612685700818764",
      "title": "You only look once: Unified, real-time object detection",
      "cited_by": 39258,
      "modularity": 1
    },
    {
      "label": "Attention mechanisms in computer vision: A survey",
      "id": "15456065911372617945",
      "url": "https://link.springer.com/article/10.1007/s41095-022-0271-y",
      "title": "Attention mechanisms in computer vision: A survey",
      "authors": "MH Guo, TX Xu, JJ Liu, ZN Liu, PT Jiang, TJ Mu\u2026",
      "year": "2022",
      "cited_by": 620,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15456065911372617945&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Transformers in medical imaging: A survey",
      "id": "982391967541643955",
      "url": "https://www.sciencedirect.com/science/article/pii/S1361841523000634",
      "title": "Transformers in medical imaging: A survey",
      "authors": "F Shamshad, S Khan, SW Zamir, MH Khan\u2026",
      "year": "2023",
      "cited_by": 183,
      "cited_by_url": "https://scholar.google.com/scholar?cites=982391967541643955&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 20
    },
    {
      "label": "A comprehensive survey on pretrained foundation models: A history from bert to chatgpt",
      "id": "2452866517197292093",
      "url": "https://arxiv.org/abs/2302.09419",
      "title": "A comprehensive survey on pretrained foundation models: A history from bert to chatgpt",
      "authors": "C Zhou, Q Li, C Li, J Yu, Y Liu, G Wang\u2026",
      "year": "2023",
      "cited_by": 112,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2452866517197292093&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Visual attention network",
      "id": "4773463079530656035",
      "url": "https://link.springer.com/article/10.1007/s41095-023-0364-2",
      "title": "Visual attention network",
      "authors": "MH Guo, CZ Lu, ZN Liu, MM Cheng, SM Hu",
      "year": "2023",
      "cited_by": 244,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4773463079530656035&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Segnext: Rethinking convolutional attention design for semantic segmentation",
      "id": "761718241536208511",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/08050f40fff41616ccfc3080e60a301a-Abstract-Conference.html",
      "title": "Segnext: Rethinking convolutional attention design for semantic segmentation",
      "authors": "MH Guo, CZ Lu, Q Hou, Z Liu\u2026",
      "year": "2022",
      "cited_by": 116,
      "cited_by_url": "https://scholar.google.com/scholar?cites=761718241536208511&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "YOLOv5-Tassel: Detecting tassels in RGB UAV imagery with improved YOLOv5 based on transfer learning",
      "id": "7104781172538541114",
      "url": "https://ieeexplore.ieee.org/abstract/document/9889182/",
      "title": "YOLOv5-Tassel: Detecting tassels in RGB UAV imagery with improved YOLOv5 based on transfer learning",
      "authors": "W Liu, K Quijano, MM Crawford",
      "year": "2022",
      "cited_by": 119,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7104781172538541114&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "Towards an end-to-end framework for flow-guided video inpainting",
      "id": "6491078858607146383",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Li_Towards_an_End-to-End_Framework_for_Flow-Guided_Video_Inpainting_CVPR_2022_paper.html",
      "title": "Towards an end-to-end framework for flow-guided video inpainting",
      "authors": "Z Li, CZ Lu, J Qin, CL Guo\u2026",
      "year": "2022",
      "cited_by": 43,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6491078858607146383&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "ISNet: Towards improving separability for remote sensing image change detection",
      "id": "11978445553624214380",
      "url": "https://ieeexplore.ieee.org/abstract/document/9772654/",
      "title": "ISNet: Towards improving separability for remote sensing image change detection",
      "authors": "G Cheng, G Wang, J Han",
      "year": "2022",
      "cited_by": 38,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11978445553624214380&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Diagnosis of brain diseases in fusion of neuroimaging modalities using deep learning: A review",
      "id": "5228146784334715443",
      "url": "https://www.sciencedirect.com/science/article/pii/S1566253522002573",
      "title": "Diagnosis of brain diseases in fusion of neuroimaging modalities using deep learning: A review",
      "authors": "A Shoeibi, M Khodatars, M Jafari, N Ghassemi\u2026",
      "year": "2022",
      "cited_by": 22,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5228146784334715443&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Swin-transformer-enabled YOLOv5 with attention mechanism for small object detection on satellite images",
      "id": "9099615620722636165",
      "url": "https://www.mdpi.com/2072-4292/14/12/2861",
      "title": "Swin-transformer-enabled YOLOv5 with attention mechanism for small object detection on satellite images",
      "authors": "H Gong, T Mu, Q Li, H Dai, C Li, Z He, W Wang, F Han\u2026",
      "year": "2022",
      "cited_by": 52,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9099615620722636165&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "Beyond self-attention: External attention using two linear layers for visual tasks",
      "id": "10884589459641707712",
      "url": "https://ieeexplore.ieee.org/abstract/document/9912362/",
      "title": "Beyond self-attention: External attention using two linear layers for visual tasks",
      "authors": "MH Guo, ZN Liu, TJ Mu, SM Hu",
      "year": "2022",
      "cited_by": 272,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10884589459641707712&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "A survey of transformers",
      "id": "7749897961068121501",
      "url": "https://www.sciencedirect.com/science/article/pii/S2666651022000146",
      "title": "A survey of transformers",
      "authors": "T Lin, Y Wang, X Liu, X Qiu",
      "year": "2022",
      "cited_by": 486,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7749897961068121501&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Pre-trained models for natural language processing: A survey",
      "id": "1539076789580815483",
      "url": "https://link.springer.com/article/10.1007/s11431-020-1647-3",
      "title": "Pre-trained models for natural language processing: A survey",
      "authors": "X Qiu, T Sun, Y Xu, Y Shao, N Dai, X Huang",
      "year": "2020",
      "cited_by": 1184,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1539076789580815483&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Pre-trained models: Past, present and future",
      "id": "966567457136989804",
      "url": "https://www.sciencedirect.com/science/article/pii/S2666651021000231",
      "title": "Pre-trained models: Past, present and future",
      "authors": "X Han, Z Zhang, N Ding, Y Gu, X Liu, Y Huo, J Qiu\u2026",
      "year": "2021",
      "cited_by": 366,
      "cited_by_url": "https://scholar.google.com/scholar?cites=966567457136989804&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Multimodal learning with transformers: A survey",
      "id": "10761248177036470713",
      "url": "https://ieeexplore.ieee.org/abstract/document/10123038/",
      "title": "Multimodal learning with transformers: A survey",
      "authors": "P Xu, X Zhu, DA Clifton",
      "year": "2023",
      "cited_by": 110,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10761248177036470713&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "A survey of visual transformers",
      "id": "14136709172791920331",
      "url": "https://ieeexplore.ieee.org/abstract/document/10088164/",
      "title": "A survey of visual transformers",
      "authors": "Y Liu, Y Zhang, Y Wang, F Hou, J Yuan\u2026",
      "year": "2023",
      "cited_by": 108,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14136709172791920331&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Recent advances in deep learning based dialogue systems: A systematic survey",
      "id": "13597902966753793310",
      "url": "https://link.springer.com/article/10.1007/s10462-022-10248-8",
      "title": "Recent advances in deep learning based dialogue systems: A systematic survey",
      "authors": "J Ni, T Young, V Pandelea, F Xue\u2026",
      "year": "2023",
      "cited_by": 122,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13597902966753793310&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Ammus: A survey of transformer-based pretrained models in natural language processing",
      "id": "4431578198915484435",
      "url": "https://arxiv.org/abs/2108.05542",
      "title": "Ammus: A survey of transformer-based pretrained models in natural language processing",
      "authors": "KS Kalyan, A Rajasekharan, S Sangeetha",
      "year": "2021",
      "cited_by": 145,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4431578198915484435&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "ChatGPT: Jack of all trades, master of none",
      "id": "2600515932282922845",
      "url": "https://www.sciencedirect.com/science/article/pii/S156625352300177X",
      "title": "ChatGPT: Jack of all trades, master of none",
      "authors": "J Koco\u0144, I Cichecki, O Kaszyca, M Kochanek, D Szyd\u0142o\u2026",
      "year": "2023",
      "cited_by": 66,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2600515932282922845&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Physformer: Facial video-based physiological measurement with temporal difference transformer",
      "id": "8053588590478703627",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Yu_PhysFormer_Facial_Video-Based_Physiological_Measurement_With_Temporal_Difference_Transformer_CVPR_2022_paper.html",
      "title": "Physformer: Facial video-based physiological measurement with temporal difference transformer",
      "authors": "Z Yu, Y Shen, J Shi, H Zhao\u2026",
      "year": "2022",
      "cited_by": 54,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8053588590478703627&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "ChatGPT is not all you need. A State of the Art Review of large Generative AI models",
      "id": "15393921212791157727",
      "url": "https://arxiv.org/abs/2301.04655",
      "title": "ChatGPT is not all you need. A State of the Art Review of large Generative AI models",
      "authors": "R Gozalo-Brizuela, EC Garrido-Merchan",
      "year": "2023",
      "cited_by": 83,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15393921212791157727&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Restormer: Efficient transformer for high-resolution image restoration",
      "id": "16431204865977056518",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zamir_Restormer_Efficient_Transformer_for_High-Resolution_Image_Restoration_CVPR_2022_paper.html",
      "title": "Restormer: Efficient transformer for high-resolution image restoration",
      "authors": "SW Zamir, A Arora, S Khan, M Hayat\u2026",
      "year": "2022",
      "cited_by": 700,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16431204865977056518&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "NTIRE 2023 challenge on efficient super-resolution: Methods and results",
      "id": "16932998913230259066",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Li_NTIRE_2023_Challenge_on_Efficient_Super-Resolution_Methods_and_Results_CVPRW_2023_paper.html",
      "title": "NTIRE 2023 challenge on efficient super-resolution: Methods and results",
      "authors": "Y Li, Y Zhang, R Timofte, L Van Gool\u2026",
      "year": "2023",
      "cited_by": 73,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16932998913230259066&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Simple baselines for image restoration",
      "id": "498268664873674535",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-20071-7_2",
      "title": "Simple baselines for image restoration",
      "authors": "L Chen, X Chu, X Zhang, J Sun",
      "year": "2022",
      "cited_by": 242,
      "cited_by_url": "https://scholar.google.com/scholar?cites=498268664873674535&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Lens-to-lens bokeh effect transformation. NTIRE 2023 challenge report",
      "id": "9110410135628850117",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Conde_Lens-to-Lens_Bokeh_Effect_Transformation._NTIRE_2023_Challenge_Report_CVPRW_2023_paper.html",
      "title": "Lens-to-lens bokeh effect transformation. NTIRE 2023 challenge report",
      "authors": "MV Conde, M Kolmet, T Seizinger\u2026",
      "year": "2023",
      "cited_by": 17,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9110410135628850117&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "NTIRE 2023 video colorization challenge",
      "id": "7710701073211724386",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Kang_NTIRE_2023_Video_Colorization_Challenge_CVPRW_2023_paper.html",
      "title": "NTIRE 2023 video colorization challenge",
      "authors": "X Kang, X Lin, K Zhang, Z Hui\u2026",
      "year": "2023",
      "cited_by": 14,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7710701073211724386&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "NTIRE 2023 Challenge on 360deg Omnidirectional Image and Video Super-Resolution: Datasets, Methods and Results",
      "id": "13272942409666364496",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Cao_NTIRE_2023_Challenge_on_360deg_Omnidirectional_Image_and_Video_Super-Resolution_CVPRW_2023_paper.html",
      "title": "NTIRE 2023 Challenge on 360deg Omnidirectional Image and Video Super-Resolution: Datasets, Methods and Results",
      "authors": "M Cao, C Mou, F Yu, X Wang\u2026",
      "year": "2023",
      "cited_by": 15,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13272942409666364496&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "NTIRE 2023 challenge on image denoising: Methods and results",
      "id": "12297468854729392143",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Li_NTIRE_2023_Challenge_on_Image_Denoising_Methods_and_Results_CVPRW_2023_paper.html",
      "title": "NTIRE 2023 challenge on image denoising: Methods and results",
      "authors": "Y Li, Y Zhang, R Timofte, L Van Gool\u2026",
      "year": "2023",
      "cited_by": 12,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12297468854729392143&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "NTIRE 2023 challenge on night photography rendering",
      "id": "5364344454304770774",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/html/Shutova_NTIRE_2023_Challenge_on_Night_Photography_Rendering_CVPRW_2023_paper.html",
      "title": "NTIRE 2023 challenge on night photography rendering",
      "authors": "A Shutova, E Ershov\u2026",
      "year": "2023",
      "cited_by": 13,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5364344454304770774&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Efficient long-range attention network for image super-resolution",
      "id": "3464402829187061665",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-19790-1_39",
      "title": "Efficient long-range attention network for image super-resolution",
      "authors": "X Zhang, H Zeng, S Guo, L Zhang",
      "year": "2022",
      "cited_by": 74,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3464402829187061665&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion",
      "id": "15226748689642581218",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhao_CDDFuse_Correlation-Driven_Dual-Branch_Feature_Decomposition_for_Multi-Modality_Image_Fusion_CVPR_2023_paper.html",
      "title": "Cddfuse: Correlation-driven dual-branch feature decomposition for multi-modality image fusion",
      "authors": "Z Zhao, H Bai, J Zhang, Y Zhang, S Xu\u2026",
      "year": "2023",
      "cited_by": 23,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15226748689642581218&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Coatnet: Marrying convolution and attention for all data sizes",
      "id": "10110104334022835757",
      "url": "https://proceedings.neurips.cc/paper/2021/hash/20568692db622456cc42a2e853ca21f8-Abstract.html",
      "title": "Coatnet: Marrying convolution and attention for all data sizes",
      "authors": "Z Dai, H Liu, QV Le, M Tan",
      "year": "2021",
      "cited_by": 711,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10110104334022835757&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "A convnet for the 2020s",
      "id": "14443907969977981621",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.html",
      "title": "A convnet for the 2020s",
      "authors": "Z Liu, H Mao, CY Wu, C Feichtenhofer\u2026",
      "year": "2022",
      "cited_by": 2184,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14443907969977981621&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Scaling vision transformers",
      "id": "13501013621324561884",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhai_Scaling_Vision_Transformers_CVPR_2022_paper.html",
      "title": "Scaling vision transformers",
      "authors": "X Zhai, A Kolesnikov, N Houlsby\u2026",
      "year": "2022",
      "cited_by": 570,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13501013621324561884&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Coca: Contrastive captioners are image-text foundation models",
      "id": "13629397900287809862",
      "url": "https://arxiv.org/abs/2205.01917",
      "title": "Coca: Contrastive captioners are image-text foundation models",
      "authors": "J Yu, Z Wang, V Vasudevan, L Yeung\u2026",
      "year": "2022",
      "cited_by": 483,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13629397900287809862&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Mvitv2: Improved multiscale vision transformers for classification and detection",
      "id": "1273811038957334386",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Li_MViTv2_Improved_Multiscale_Vision_Transformers_for_Classification_and_Detection_CVPR_2022_paper.html",
      "title": "Mvitv2: Improved multiscale vision transformers for classification and detection",
      "authors": "Y Li, CY Wu, H Fan, K Mangalam\u2026",
      "year": "2022",
      "cited_by": 293,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1273811038957334386&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Inception transformer",
      "id": "610621467807251926",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/94e85561a342de88b559b72c9b29f638-Abstract-Conference.html",
      "title": "Inception transformer",
      "authors": "C Si, W Yu, P Zhou, Y Zhou\u2026",
      "year": "2022",
      "cited_by": 152,
      "cited_by_url": "https://scholar.google.com/scholar?cites=610621467807251926&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Lit: Zero-shot transfer with locked-image text tuning",
      "id": "18380770342348927722",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhai_LiT_Zero-Shot_Transfer_With_Locked-Image_Text_Tuning_CVPR_2022_paper.html",
      "title": "Lit: Zero-shot transfer with locked-image text tuning",
      "authors": "X Zhai, X Wang, B Mustafa, A Steiner\u2026",
      "year": "2022",
      "cited_by": 229,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18380770342348927722&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Simvlm: Simple visual language model pretraining with weak supervision",
      "id": "9618435703828650575",
      "url": "https://arxiv.org/abs/2108.10904",
      "title": "Simvlm: Simple visual language model pretraining with weak supervision",
      "authors": "Z Wang, J Yu, AW Yu, Z Dai, Y Tsvetkov\u2026",
      "year": "2021",
      "cited_by": 456,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9618435703828650575&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Multi-stage progressive image restoration",
      "id": "14988305211504802629",
      "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Zamir_Multi-Stage_Progressive_Image_Restoration_CVPR_2021_paper.html",
      "title": "Multi-stage progressive image restoration",
      "authors": "SW Zamir, A Arora, S Khan, M Hayat\u2026",
      "year": "2021",
      "cited_by": 873,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14988305211504802629&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Uformer: A general u-shaped transformer for image restoration",
      "id": "14031000766044293652",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Wang_Uformer_A_General_U-Shaped_Transformer_for_Image_Restoration_CVPR_2022_paper.html",
      "title": "Uformer: A general u-shaped transformer for image restoration",
      "authors": "Z Wang, X Cun, J Bao, W Zhou\u2026",
      "year": "2022",
      "cited_by": 613,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14031000766044293652&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Ntire 2022 spectral recovery challenge and data set",
      "id": "16381083981417715623",
      "url": "https://openaccess.thecvf.com/content/CVPR2022W/NTIRE/html/Arad_NTIRE_2022_Spectral_Recovery_Challenge_and_Data_Set_CVPRW_2022_paper.html",
      "title": "Ntire 2022 spectral recovery challenge and data set",
      "authors": "B Arad, R Timofte, R Yahel, N Morag\u2026",
      "year": "2022",
      "cited_by": 41,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16381083981417715623&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "NTIRE 2021 challenge on image deblurring",
      "id": "6999508588420552953",
      "url": "http://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Nah_NTIRE_2021_Challenge_on_Image_Deblurring_CVPRW_2021_paper.html",
      "title": "NTIRE 2021 challenge on image deblurring",
      "authors": "S Nah, S Son, S Lee, R Timofte\u2026",
      "year": "2021",
      "cited_by": 60,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6999508588420552953&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Maxim: Multi-axis mlp for image processing",
      "id": "18275282813589182456",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Tu_MAXIM_Multi-Axis_MLP_for_Image_Processing_CVPR_2022_paper.html",
      "title": "Maxim: Multi-axis mlp for image processing",
      "authors": "Z Tu, H Talebi, H Zhang, F Yang\u2026",
      "year": "2022",
      "cited_by": 192,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18275282813589182456&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Rethinking coarse-to-fine approach in single image deblurring",
      "id": "11948207786179445379",
      "url": "https://openaccess.thecvf.com/content/ICCV2021/html/Cho_Rethinking_Coarse-To-Fine_Approach_in_Single_Image_Deblurring_ICCV_2021_paper.html?ref=https://githubhelp.com",
      "title": "Rethinking coarse-to-fine approach in single image deblurring",
      "authors": "SJ Cho, SW Ji, JP Hong, SW Jung\u2026",
      "year": "2021",
      "cited_by": 276,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11948207786179445379&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Hinet: Half instance normalization network for image restoration",
      "id": "2731814227384441305",
      "url": "https://openaccess.thecvf.com/content/CVPR2021W/NTIRE/html/Chen_HINet_Half_Instance_Normalization_Network_for_Image_Restoration_CVPRW_2021_paper.html",
      "title": "Hinet: Half instance normalization network for image restoration",
      "authors": "L Chen, X Lu, J Zhang, X Chu\u2026",
      "year": "2021",
      "cited_by": 247,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2731814227384441305&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Transweather: Transformer-based restoration of images degraded by adverse weather conditions",
      "id": "563900006853828372",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Valanarasu_TransWeather_Transformer-Based_Restoration_of_Images_Degraded_by_Adverse_Weather_Conditions_CVPR_2022_paper.html",
      "title": "Transweather: Transformer-based restoration of images degraded by adverse weather conditions",
      "authors": "JMJ Valanarasu, R Yasarla\u2026",
      "year": "2022",
      "cited_by": 93,
      "cited_by_url": "https://scholar.google.com/scholar?cites=563900006853828372&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Deblurring via stochastic refinement",
      "id": "2892557376887066282",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Whang_Deblurring_via_Stochastic_Refinement_CVPR_2022_paper.html",
      "title": "Deblurring via stochastic refinement",
      "authors": "J Whang, M Delbracio, H Talebi\u2026",
      "year": "2022",
      "cited_by": 89,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2892557376887066282&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "All-in-one image restoration for unknown corruption",
      "id": "10807347079650485654",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Li_All-in-One_Image_Restoration_for_Unknown_Corruption_CVPR_2022_paper.html",
      "title": "All-in-one image restoration for unknown corruption",
      "authors": "B Li, X Liu, P Hu, Z Wu, J Lv\u2026",
      "year": "2022",
      "cited_by": 73,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10807347079650485654&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Transreid: Transformer-based object re-identification",
      "id": "15635397108812213817",
      "url": "http://openaccess.thecvf.com/content/ICCV2021/html/He_TransReID_Transformer-Based_Object_Re-Identification_ICCV_2021_paper.html",
      "title": "Transreid: Transformer-based object re-identification",
      "authors": "S He, H Luo, P Wang, F Wang, H Li\u2026",
      "year": "2021",
      "cited_by": 499,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15635397108812213817&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Multiscale vision transformers",
      "id": "7329647594369932315",
      "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Fan_Multiscale_Vision_Transformers_ICCV_2021_paper.html",
      "title": "Multiscale vision transformers",
      "authors": "H Fan, B Xiong, K Mangalam, Y Li\u2026",
      "year": "2021",
      "cited_by": 1621,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7329647594369932315&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Cswin transformer: A general vision transformer backbone with cross-shaped windows",
      "id": "4431453089685809340",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Dong_CSWin_Transformer_A_General_Vision_Transformer_Backbone_With_Cross-Shaped_Windows_CVPR_2022_paper.html",
      "title": "Cswin transformer: A general vision transformer backbone with cross-shaped windows",
      "authors": "X Dong, J Bao, D Chen, W Zhang\u2026",
      "year": "2022",
      "cited_by": 488,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4431453089685809340&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "A survey on vision transformer",
      "id": "4278610892084589339",
      "url": "https://ieeexplore.ieee.org/abstract/document/9716741/",
      "title": "A survey on vision transformer",
      "authors": "K Han, Y Wang, H Chen, X Chen, J Guo\u2026",
      "year": "2022",
      "cited_by": 698,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4278610892084589339&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Transfg: A transformer architecture for fine-grained recognition",
      "id": "601129416962130879",
      "url": "https://ojs.aaai.org/index.php/AAAI/article/view/19967",
      "title": "Transfg: A transformer architecture for fine-grained recognition",
      "authors": "J He, JN Chen, S Liu, A Kortylewski, C Yang\u2026",
      "year": "2022",
      "cited_by": 220,
      "cited_by_url": "https://scholar.google.com/scholar?cites=601129416962130879&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Multi-animal pose estimation, identification and tracking with DeepLabCut",
      "id": "4326704403467340422",
      "url": "https://www.nature.com/articles/s41592-022-01443-0",
      "title": "Multi-animal pose estimation, identification and tracking with DeepLabCut",
      "authors": "J Lauer, M Zhou, S Ye, W Menegas, S Schneider\u2026",
      "year": "2022",
      "cited_by": 169,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4326704403467340422&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Mhformer: Multi-hypothesis transformer for 3d human pose estimation",
      "id": "18177167198432349205",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Li_MHFormer_Multi-Hypothesis_Transformer_for_3D_Human_Pose_Estimation_CVPR_2022_paper.html",
      "title": "Mhformer: Multi-hypothesis transformer for 3d human pose estimation",
      "authors": "W Li, H Liu, H Tang, P Wang\u2026",
      "year": "2022",
      "cited_by": 121,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18177167198432349205&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Uniformer: Unifying convolution and self-attention for visual recognition",
      "id": "342346550438939327",
      "url": "https://ieeexplore.ieee.org/abstract/document/10143709/",
      "title": "Uniformer: Unifying convolution and self-attention for visual recognition",
      "authors": "K Li, Y Wang, J Zhang, P Gao, G Song\u2026",
      "year": "2023",
      "cited_by": 102,
      "cited_by_url": "https://scholar.google.com/scholar?cites=342346550438939327&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition",
      "id": "9632085609200326616",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/64517d8435994992e682b3e4aa0a0661-Abstract.html",
      "title": "Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition",
      "authors": "Y Wang, R Huang, S Song\u2026",
      "year": "2021",
      "cited_by": 101,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9632085609200326616&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Intriguing properties of vision transformers",
      "id": "15172072370662904150",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html",
      "title": "Intriguing properties of vision transformers",
      "authors": "MM Naseer, K Ranasinghe, SH Khan\u2026",
      "year": "2021",
      "cited_by": 366,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15172072370662904150&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 20
    },
    {
      "label": "ibot: Image bert pre-training with online tokenizer",
      "id": "6668235945473015803",
      "url": "https://arxiv.org/abs/2111.07832",
      "title": "ibot: Image bert pre-training with online tokenizer",
      "authors": "J Zhou, C Wei, H Wang, W Shen, C Xie, A Yuille\u2026",
      "year": "2021",
      "cited_by": 377,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6668235945473015803&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 20
    },
    {
      "label": "Are transformers more robust than cnns?",
      "id": "2316302132679082774",
      "url": "https://proceedings.neurips.cc/paper/2021/hash/e19347e1c3ca0c0b97de5fb3b690855a-Abstract.html",
      "title": "Are transformers more robust than cnns?",
      "authors": "Y Bai, J Mei, AL Yuille, C Xie",
      "year": "2021",
      "cited_by": 168,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2316302132679082774&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 20
    },
    {
      "label": "Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation",
      "id": "4388759310460601633",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Hoyer_DAFormer_Improving_Network_Architectures_and_Training_Strategies_for_Domain-Adaptive_Semantic_CVPR_2022_paper.html",
      "title": "Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation",
      "authors": "L Hoyer, D Dai, L Van Gool",
      "year": "2022",
      "cited_by": 182,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4388759310460601633&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 20
    },
    {
      "label": "Understanding the robustness in vision transformers",
      "id": "3041067607452518927",
      "url": "https://proceedings.mlr.press/v162/zhou22m.html",
      "title": "Understanding the robustness in vision transformers",
      "authors": "D Zhou, Z Yu, E Xie, C Xiao\u2026",
      "year": "2022",
      "cited_by": 82,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3041067607452518927&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 20
    },
    {
      "label": "Partial success in closing the gap between human and machine vision",
      "id": "875131557547078483",
      "url": "https://proceedings.neurips.cc/paper/2021/hash/c8877cff22082a16395a57e97232bb6f-Abstract.html",
      "title": "Partial success in closing the gap between human and machine vision",
      "authors": "R Geirhos, K Narayanappa, B Mitzkus\u2026",
      "year": "2021",
      "cited_by": 110,
      "cited_by_url": "https://scholar.google.com/scholar?cites=875131557547078483&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 20
    },
    {
      "label": "Efficient training of visual transformers with small datasets",
      "id": "17891879498080154736",
      "url": "https://proceedings.neurips.cc/paper/2021/hash/c81e155d85dae5430a8cee6f2242e82c-Abstract.html",
      "title": "Efficient training of visual transformers with small datasets",
      "authors": "Y Liu, E Sangineto, W Bi, N Sebe\u2026",
      "year": "2021",
      "cited_by": 108,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17891879498080154736&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 20
    },
    {
      "label": "Assaying out-of-distribution generalization in transfer learning",
      "id": "2028336304446280911",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/2f5acc925919209370a3af4eac5cad4a-Abstract-Conference.html",
      "title": "Assaying out-of-distribution generalization in transfer learning",
      "authors": "F Wenzel, A Dittadi, P Gehler\u2026",
      "year": "2022",
      "cited_by": 29,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2028336304446280911&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 20
    },
    {
      "label": "Ow-detr: Open-world detection transformer",
      "id": "5601871542106060008",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Gupta_OW-DETR_Open-World_Detection_Transformer_CVPR_2022_paper.html",
      "title": "Ow-detr: Open-world detection transformer",
      "authors": "A Gupta, S Narayan, KJ Joseph\u2026",
      "year": "2022",
      "cited_by": 68,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5601871542106060008&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 20
    },
    {
      "label": "A survey of modern deep learning based object detection models",
      "id": "14311400318178337111",
      "url": "https://www.sciencedirect.com/science/article/pii/S1051200422001312",
      "title": "A survey of modern deep learning based object detection models",
      "authors": "SSA Zaidi, MS Ansari, A Aslam, N Kanwal\u2026",
      "year": "2022",
      "cited_by": 435,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14311400318178337111&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Deep learning methods for object detection in smart manufacturing: A survey",
      "id": "11789051068432887660",
      "url": "https://www.sciencedirect.com/science/article/pii/S0278612522001066",
      "title": "Deep learning methods for object detection in smart manufacturing: A survey",
      "authors": "HM Ahmad, A Rahimi",
      "year": "2022",
      "cited_by": 21,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11789051068432887660&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 15
    },
    {
      "label": "Mammogram breast cancer CAD systems for mass detection and classification: a review",
      "id": "15295068953900215294",
      "url": "https://link.springer.com/article/10.1007/s11042-022-12332-1",
      "title": "Mammogram breast cancer CAD systems for mass detection and classification: a review",
      "authors": "NM Hassan, S Hamad, K Mahar",
      "year": "2022",
      "cited_by": 33,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15295068953900215294&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Precise single-stage detector",
      "id": "15549291371117213871",
      "url": "https://arxiv.org/abs/2210.04252",
      "title": "Precise single-stage detector",
      "authors": "A Chandio, G Gui, T Kumar, I Ullah\u2026",
      "year": "2022",
      "cited_by": 41,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15549291371117213871&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "CE-FPN: Enhancing channel information for object detection",
      "id": "9884544045603971658",
      "url": "https://link.springer.com/article/10.1007/s11042-022-11940-1",
      "title": "CE-FPN: Enhancing channel information for object detection",
      "authors": "Y Luo, X Cao, J Zhang, J Guo, H Shen, T Wang\u2026",
      "year": "2022",
      "cited_by": 68,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9884544045603971658&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "A survey of self-supervised and few-shot object detection",
      "id": "9591435289724766439",
      "url": "https://ieeexplore.ieee.org/abstract/document/9860087/",
      "title": "A survey of self-supervised and few-shot object detection",
      "authors": "G Huang, I Laradji, D Vazquez\u2026",
      "year": "2022",
      "cited_by": 43,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9591435289724766439&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Feature split\u2013merge\u2013enhancement network for remote sensing object detection",
      "id": "10628215061802232868",
      "url": "https://ieeexplore.ieee.org/abstract/document/9673713/",
      "title": "Feature split\u2013merge\u2013enhancement network for remote sensing object detection",
      "authors": "W Ma, N Li, H Zhu, L Jiao, X Tang\u2026",
      "year": "2022",
      "cited_by": 42,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10628215061802232868&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Guiding pretraining in reinforcement learning with large language models",
      "id": "9532821550175512200",
      "url": "https://arxiv.org/abs/2302.06692",
      "title": "Guiding pretraining in reinforcement learning with large language models",
      "authors": "Y Du, O Watkins, Z Wang, C Colas, T Darrell\u2026",
      "year": "2023",
      "cited_by": 19,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9532821550175512200&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Integrating deep learning-based iot and fog computing with software-defined networking for detecting weapons in video surveillance systems",
      "id": "12985976504909847163",
      "url": "https://www.mdpi.com/1424-8220/22/14/5075",
      "title": "Integrating deep learning-based iot and fog computing with software-defined networking for detecting weapons in video surveillance systems",
      "authors": "C Fathy, SN Saleh",
      "year": "2022",
      "cited_by": 18,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12985976504909847163&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Backbones-review: Feature extraction networks for deep learning and deep reinforcement learning approaches",
      "id": "2812153438552156646",
      "url": "https://arxiv.org/abs/2206.08016",
      "title": "Backbones-review: Feature extraction networks for deep learning and deep reinforcement learning approaches",
      "authors": "O Elharrouss, Y Akbari, N Almaadeed\u2026",
      "year": "2022",
      "cited_by": 28,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2812153438552156646&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Small-object detection based on YOLOv5 in autonomous driving systems",
      "id": "14349697475909471320",
      "url": "https://www.sciencedirect.com/science/article/pii/S0167865523000727",
      "title": "Small-object detection based on YOLOv5 in autonomous driving systems",
      "authors": "B Mahaur, KK Mishra",
      "year": "2023",
      "cited_by": 15,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14349697475909471320&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Point-bert: Pre-training 3d point cloud transformers with masked point modeling",
      "id": "17327663970405370182",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Yu_Point-BERT_Pre-Training_3D_Point_Cloud_Transformers_With_Masked_Point_Modeling_CVPR_2022_paper.html",
      "title": "Point-bert: Pre-training 3d point cloud transformers with masked point modeling",
      "authors": "X Yu, L Tang, Y Rao, T Huang\u2026",
      "year": "2022",
      "cited_by": 219,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17327663970405370182&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Masked autoencoders for point cloud self-supervised learning",
      "id": "1930827783125608869",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-20086-1_35",
      "title": "Masked autoencoders for point cloud self-supervised learning",
      "authors": "Y Pang, W Wang, FEH Tay, W Liu, Y Tian\u2026",
      "year": "2022",
      "cited_by": 143,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1930827783125608869&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Pointnext: Revisiting pointnet++ with improved training and scaling strategies",
      "id": "14072888861532659606",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/9318763d049edf9a1f2779b2a59911d3-Abstract-Conference.html",
      "title": "Pointnext: Revisiting pointnet++ with improved training and scaling strategies",
      "authors": "G Qian, Y Li, H Peng, J Mai\u2026",
      "year": "2022",
      "cited_by": 152,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14072888861532659606&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Unsupervised point cloud representation learning with deep neural networks: A survey",
      "id": "1118274744677998915",
      "url": "https://ieeexplore.ieee.org/abstract/document/10086697/",
      "title": "Unsupervised point cloud representation learning with deep neural networks: A survey",
      "authors": "A Xiao, J Huang, D Guan, X Zhang\u2026",
      "year": "2023",
      "cited_by": 31,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1118274744677998915&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training",
      "id": "8230127879015912569",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/ad1d7a4df30a9c0c46b387815a774a84-Abstract-Conference.html",
      "title": "Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training",
      "authors": "R Zhang, Z Guo, P Gao, R Fang\u2026",
      "year": "2022",
      "cited_by": 60,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8230127879015912569&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Masked discrimination for self-supervised learning on point clouds",
      "id": "12182702384297284737",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-20086-1_38",
      "title": "Masked discrimination for self-supervised learning on point clouds",
      "authors": "H Liu, M Cai, YJ Lee",
      "year": "2022",
      "cited_by": 57,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12182702384297284737&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders",
      "id": "17896705208502572006",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Learning_3D_Representations_From_2D_Pre-Trained_Models_via_Image-to-Point_Masked_CVPR_2023_paper.html",
      "title": "Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders",
      "authors": "R Zhang, L Wang, Y Qiao, P Gao\u2026",
      "year": "2023",
      "cited_by": 32,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17896705208502572006&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "ULIP: Learning a unified representation of language, images, and point clouds for 3D understanding",
      "id": "477874232529254013",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Gao_ULIP_Learning_a_Unified_Representation_of_Language_Images_and_Point_CVPR_2023_paper.html",
      "title": "ULIP: Learning a unified representation of language, images, and point clouds for 3D understanding",
      "authors": "L Xue, M Gao, C Xing, R Mart\u00edn-Mart\u00edn\u2026",
      "year": "2023",
      "cited_by": 37,
      "cited_by_url": "https://scholar.google.com/scholar?cites=477874232529254013&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "P2p: Tuning pre-trained image models for point cloud analysis with point-to-pixel prompting",
      "id": "16387925596110304701",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/5cd6dc946ccc37ae6c9f4fc6b6181e1d-Abstract-Conference.html",
      "title": "P2p: Tuning pre-trained image models for point cloud analysis with point-to-pixel prompting",
      "authors": "Z Wang, X Yu, Y Rao, J Zhou\u2026",
      "year": "2022",
      "cited_by": 31,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16387925596110304701&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Shapeformer: Transformer-based shape completion via sparse representation",
      "id": "7193625896865391995",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Yan_ShapeFormer_Transformer-Based_Shape_Completion_via_Sparse_Representation_CVPR_2022_paper.html",
      "title": "Shapeformer: Transformer-based shape completion via sparse representation",
      "authors": "X Yan, L Lin, NJ Mitra, D Lischinski\u2026",
      "year": "2022",
      "cited_by": 57,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7193625896865391995&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 4
    },
    {
      "label": "Maxvit: Multi-axis vision transformer",
      "id": "6784655767122395745",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-20053-3_27",
      "title": "Maxvit: Multi-axis vision transformer",
      "authors": "Z Tu, H Talebi, H Zhang, F Yang, P Milanfar\u2026",
      "year": "2022",
      "cited_by": 166,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6784655767122395745&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Eva: Exploring the limits of masked visual representation learning at scale",
      "id": "10588342779298269046",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Fang_EVA_Exploring_the_Limits_of_Masked_Visual_Representation_Learning_at_CVPR_2023_paper.html",
      "title": "Eva: Exploring the limits of masked visual representation learning at scale",
      "authors": "Y Fang, W Wang, B Xie, Q Sun, L Wu\u2026",
      "year": "2023",
      "cited_by": 102,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10588342779298269046&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Hornet: Efficient high-order spatial interactions with recursive gated convolutions",
      "id": "12938213222665733645",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/436d042b2dd81214d23ae43eb196b146-Abstract-Conference.html",
      "title": "Hornet: Efficient high-order spatial interactions with recursive gated convolutions",
      "authors": "Y Rao, W Zhao, Y Tang, J Zhou\u2026",
      "year": "2022",
      "cited_by": 101,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12938213222665733645&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Convnext v2: Co-designing and scaling convnets with masked autoencoders",
      "id": "1388490151733704334",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Woo_ConvNeXt_V2_Co-Designing_and_Scaling_ConvNets_With_Masked_Autoencoders_CVPR_2023_paper.html",
      "title": "Convnext v2: Co-designing and scaling convnets with masked autoencoders",
      "authors": "S Woo, S Debnath, R Hu, X Chen\u2026",
      "year": "2023",
      "cited_by": 63,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1388490151733704334&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "V2x-vit: Vehicle-to-everything cooperative perception with vision transformer",
      "id": "15088728781552938978",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-19842-7_7",
      "title": "V2x-vit: Vehicle-to-everything cooperative perception with vision transformer",
      "authors": "R Xu, H Xiang, Z Tu, X Xia, MH Yang, J Ma",
      "year": "2022",
      "cited_by": 111,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15088728781552938978&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Efficientformer: Vision transformers at mobilenet speed",
      "id": "12692106295877813680",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/5452ad8ee6ea6e7dc41db1cbd31ba0b8-Abstract-Conference.html",
      "title": "Efficientformer: Vision transformers at mobilenet speed",
      "authors": "Y Li, G Yuan, Y Wen, J Hu\u2026",
      "year": "2022",
      "cited_by": 87,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12692106295877813680&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Tracking objects as pixel-wise distributions",
      "id": "10239175441885037567",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-20047-2_5",
      "title": "Tracking objects as pixel-wise distributions",
      "authors": "Z Zhao, Z Wu, Y Zhuang, B Li, J Jia",
      "year": "2022",
      "cited_by": 25,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10239175441885037567&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "CoBEVT: Cooperative bird's eye view semantic segmentation with sparse transformers",
      "id": "2000389979125404276",
      "url": "https://arxiv.org/abs/2207.02202",
      "title": "CoBEVT: Cooperative bird's eye view semantic segmentation with sparse transformers",
      "authors": "R Xu, Z Tu, H Xiang, W Shao, B Zhou, J Ma",
      "year": "2022",
      "cited_by": 61,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2000389979125404276&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Multi-agent reinforcement learning is a sequence modeling problem",
      "id": "14170076594522259195",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/69413f87e5a34897cd010ca698097d0a-Abstract-Conference.html",
      "title": "Multi-agent reinforcement learning is a sequence modeling problem",
      "authors": "M Wen, J Kuba, R Lin, W Zhang\u2026",
      "year": "2022",
      "cited_by": 39,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14170076594522259195&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Dual vision transformer",
      "id": "5425241495538385765",
      "url": "https://ieeexplore.ieee.org/abstract/document/10105499/",
      "title": "Dual vision transformer",
      "authors": "T Yao, Y Li, Y Pan, Y Wang\u2026",
      "year": "2023",
      "cited_by": 31,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5425241495538385765&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "A survey on image data augmentation for deep learning",
      "id": "4041041901496425203",
      "url": "https://journalofbigdata.springeropen.com/track/pdf/10.1186/s40537-019-0197-0.pdf",
      "title": "A survey on image data augmentation for deep learning",
      "authors": "C Shorten, TM Khoshgoftaar",
      "year": "2019",
      "cited_by": 7696,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4041041901496425203&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions",
      "id": "8136644755673586417",
      "url": "https://link.springer.com/article/10.1186/s40537-021-00444-8",
      "title": "Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions",
      "authors": "L Alzubaidi, J Zhang, AJ Humaidi, A Al-Dujaili\u2026",
      "year": "2021",
      "cited_by": 2391,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8136644755673586417&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 10
    },
    {
      "label": "Multi-sensor information fusion based on machine learning for real applications in human activity recognition: State-of-the-art and research challenges",
      "id": "7358070883597795569",
      "url": "https://www.sciencedirect.com/science/article/pii/S1566253521002311",
      "title": "Multi-sensor information fusion based on machine learning for real applications in human activity recognition: State-of-the-art and research challenges",
      "authors": "S Qiu, H Zhao, N Jiang, Z Wang, L Liu, Y An, H Zhao\u2026",
      "year": "2022",
      "cited_by": 215,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7358070883597795569&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 10
    },
    {
      "label": "Deep learning in computer vision: A critical review of emerging techniques and application scenarios",
      "id": "2188347889974787509",
      "url": "https://www.sciencedirect.com/science/article/pii/S2666827021000670",
      "title": "Deep learning in computer vision: A critical review of emerging techniques and application scenarios",
      "authors": "J Chai, H Zeng, A Li, EWT Ngai",
      "year": "2021",
      "cited_by": 252,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2188347889974787509&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 10
    },
    {
      "label": "Machine learning for structural engineering: A state-of-the-art review",
      "id": "17352312443164396046",
      "url": "https://www.sciencedirect.com/science/article/pii/S2352012422000947",
      "title": "Machine learning for structural engineering: A state-of-the-art review",
      "authors": "HT Thai",
      "year": "2022",
      "cited_by": 136,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17352312443164396046&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 10
    },
    {
      "label": "Measuring biological age using omics data",
      "id": "17997900354107908438",
      "url": "https://www.nature.com/articles/s41576-022-00511-7",
      "title": "Measuring biological age using omics data",
      "authors": "J Rutledge, H Oh, T Wyss-Coray",
      "year": "2022",
      "cited_by": 83,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17997900354107908438&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 10
    },
    {
      "label": "Transmed: Transformers advance multi-modal medical image classification",
      "id": "9888405418392239400",
      "url": "https://www.mdpi.com/2075-4418/11/8/1384",
      "title": "Transmed: Transformers advance multi-modal medical image classification",
      "authors": "Y Dai, Y Gao, F Liu",
      "year": "2021",
      "cited_by": 155,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9888405418392239400&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 10
    },
    {
      "label": "Machine learning in aerodynamic shape optimization",
      "id": "14217745962583788184",
      "url": "https://www.sciencedirect.com/science/article/pii/S0376042122000410",
      "title": "Machine learning in aerodynamic shape optimization",
      "authors": "J Li, X Du, JRRA Martins",
      "year": "2022",
      "cited_by": 59,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14217745962583788184&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 10
    },
    {
      "label": "Part of speech tagging: a systematic review of deep learning and machine learning approaches",
      "id": "3134740227865659264",
      "url": "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00561-y",
      "title": "Part of speech tagging: a systematic review of deep learning and machine learning approaches",
      "authors": "A Chiche, B Yitagesu",
      "year": "2022",
      "cited_by": 61,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3134740227865659264&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 10
    },
    {
      "label": "Deep learning in analytical chemistry",
      "id": "1681999640612811907",
      "url": "https://www.sciencedirect.com/science/article/pii/S016599362100282X",
      "title": "Deep learning in analytical chemistry",
      "authors": "B Debus, H Parastar, P Harrington\u2026",
      "year": "2021",
      "cited_by": 66,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1681999640612811907&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 10
    },
    {
      "label": "Making radiomics more reproducible across scanner and imaging protocol variations: a review of harmonization methods",
      "id": "7025715449086074571",
      "url": "https://www.mdpi.com/2075-4426/11/9/842",
      "title": "Making radiomics more reproducible across scanner and imaging protocol variations: a review of harmonization methods",
      "authors": "SA Mali, A Ibrahim, HC Woodruff\u2026",
      "year": "2021",
      "cited_by": 76,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7025715449086074571&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 10
    },
    {
      "label": "An overview of deep learning methods for multimodal medical data mining",
      "id": "4162777562069804924",
      "url": "https://www.sciencedirect.com/science/article/pii/S0957417422004249",
      "title": "An overview of deep learning methods for multimodal medical data mining",
      "authors": "F Behrad, MS Abadeh",
      "year": "2022",
      "cited_by": 43,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4162777562069804924&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 10
    },
    {
      "label": "Review on Convolutional Neural Networks (CNN) in vegetation remote sensing",
      "id": "6141939658324331542",
      "url": "https://www.sciencedirect.com/science/article/pii/S0924271620303488",
      "title": "Review on Convolutional Neural Networks (CNN) in vegetation remote sensing",
      "authors": "T Kattenborn, J Leitloff, F Schiefer, S Hinz",
      "year": "2021",
      "cited_by": 601,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6141939658324331542&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 11
    },
    {
      "label": "UAV-based forest health monitoring: A systematic review",
      "id": "563969953758433323",
      "url": "https://www.mdpi.com/2072-4292/14/13/3205",
      "title": "UAV-based forest health monitoring: A systematic review",
      "authors": "S Ecke, J Dempewolf, J Frey, A Schwaller, E Endres\u2026",
      "year": "2022",
      "cited_by": 46,
      "cited_by_url": "https://scholar.google.com/scholar?cites=563969953758433323&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 11
    },
    {
      "label": "Effect of attention mechanism in deep learning-based remote sensing image processing: A systematic literature review",
      "id": "2331420292369119754",
      "url": "https://www.mdpi.com/2072-4292/13/15/2965",
      "title": "Effect of attention mechanism in deep learning-based remote sensing image processing: A systematic literature review",
      "authors": "S Ghaffarian, J Valente, M Van Der Voort\u2026",
      "year": "2021",
      "cited_by": 71,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2331420292369119754&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 11
    },
    {
      "label": "Automated tree-crown and height detection in a young forest plantation using mask region-based convolutional neural network (Mask R-CNN)",
      "id": "13012757373810467293",
      "url": "https://www.sciencedirect.com/science/article/pii/S0924271621001611",
      "title": "Automated tree-crown and height detection in a young forest plantation using mask region-based convolutional neural network (Mask R-CNN)",
      "authors": "Z Hao, L Lin, CJ Post, EA Mikhailova, M Li\u2026",
      "year": "2021",
      "cited_by": 74,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13012757373810467293&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 11
    },
    {
      "label": "Global wheat head detection 2021: An improved dataset for benchmarking wheat head detection methods",
      "id": "1751297388377231685",
      "url": "https://spj.science.org/doi/full/10.34133/2021/9846158",
      "title": "Global wheat head detection 2021: An improved dataset for benchmarking wheat head detection methods",
      "authors": "E David, M Serouart, D Smith, S Madec\u2026",
      "year": "2021",
      "cited_by": 54,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1751297388377231685&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 11
    },
    {
      "label": "Deep learning in forestry using uav-acquired rgb data: A practical review",
      "id": "5520588759774186774",
      "url": "https://www.mdpi.com/2072-4292/13/14/2837",
      "title": "Deep learning in forestry using uav-acquired rgb data: A practical review",
      "authors": "Y Diez, S Kentsch, M Fukuda, MLL Caceres\u2026",
      "year": "2021",
      "cited_by": 59,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5520588759774186774&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 11
    },
    {
      "label": "Land use land cover classification with U-net: Advantages of combining sentinel-1 and sentinel-2 imagery",
      "id": "11053439912877888530",
      "url": "https://www.mdpi.com/2072-4292/13/18/3600",
      "title": "Land use land cover classification with U-net: Advantages of combining sentinel-1 and sentinel-2 imagery",
      "authors": "JV Sol\u00f3rzano, JF Mas, Y Gao, JA Gallardo-Cruz",
      "year": "2021",
      "cited_by": 51,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11053439912877888530&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 11
    },
    {
      "label": "A high-resolution canopy height model of the Earth",
      "id": "4856010013404526375",
      "url": "https://arxiv.org/abs/2204.08322",
      "title": "A high-resolution canopy height model of the Earth",
      "authors": "N Lang, W Jetz, K Schindler, JD Wegner",
      "year": "2022",
      "cited_by": 50,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4856010013404526375&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 11
    },
    {
      "label": "Mowing event detection in permanent grasslands: Systematic evaluation of input features from Sentinel-1, Sentinel-2, and Landsat 8 time series",
      "id": "3951350311895905492",
      "url": "https://www.sciencedirect.com/science/article/pii/S0034425721004715",
      "title": "Mowing event detection in permanent grasslands: Systematic evaluation of input features from Sentinel-1, Sentinel-2, and Landsat 8 time series",
      "authors": "F Lobert, AK Holtgrave, M Schwieder, M Pause\u2026",
      "year": "2021",
      "cited_by": 27,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3951350311895905492&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 11
    },
    {
      "label": "Deep neural networks to detect weeds from crops in agricultural environments in real-time: A review",
      "id": "9663211362112210725",
      "url": "https://www.mdpi.com/2072-4292/13/21/4486",
      "title": "Deep neural networks to detect weeds from crops in agricultural environments in real-time: A review",
      "authors": "I Rakhmatulin, A Kamilaris, C Andreasen",
      "year": "2021",
      "cited_by": 33,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9663211362112210725&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 11
    },
    {
      "label": "A review of landcover classification with very-high resolution remotely sensed optical images\u2014Analysis unit, model scalability and transferability",
      "id": "727216592699071177",
      "url": "https://www.mdpi.com/2072-4292/14/3/646",
      "title": "A review of landcover classification with very-high resolution remotely sensed optical images\u2014Analysis unit, model scalability and transferability",
      "authors": "R Qin, T Liu",
      "year": "2022",
      "cited_by": 32,
      "cited_by_url": "https://scholar.google.com/scholar?cites=727216592699071177&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 11
    },
    {
      "label": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
      "id": "7770442917120891581",
      "url": "https://proceedings.mlr.press/v162/li22n.html",
      "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
      "authors": "J Li, D Li, C Xiong, S Hoi",
      "year": "2022",
      "cited_by": 790,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7770442917120891581&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Survey of hallucination in natural language generation",
      "id": "11430520233869250304",
      "url": "https://dl.acm.org/doi/abs/10.1145/3571730",
      "title": "Survey of hallucination in natural language generation",
      "authors": "Z Ji, N Lee, R Frieske, T Yu, D Su, Y Xu, E Ishii\u2026",
      "year": "2023",
      "cited_by": 472,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11430520233869250304&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Vision-language pre-training: Basics, recent advances, and future trends",
      "id": "5562955281835677624",
      "url": "https://www.nowpublishers.com/article/Details/CGV-105",
      "title": "Vision-language pre-training: Basics, recent advances, and future trends",
      "authors": "Z Gan, L Li, C Li, L Wang, Z Liu\u2026",
      "year": "2022",
      "cited_by": 55,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5562955281835677624&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Flamingo: a visual language model for few-shot learning",
      "id": "2325917221075842848",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": "JB Alayrac, J Donahue, P Luc\u2026",
      "year": "2022",
      "cited_by": 781,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2325917221075842848&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Laion-5b: An open large-scale dataset for training next generation image-text models",
      "id": "8018158103125985189",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/a1859debfb3b59d094f3504d5ebb6c25-Abstract-Datasets_and_Benchmarks.html",
      "title": "Laion-5b: An open large-scale dataset for training next generation image-text models",
      "authors": "C Schuhmann, R Beaumont, R Vencu\u2026",
      "year": "2022",
      "cited_by": 513,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8018158103125985189&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "id": "11255276306904968426",
      "url": "https://arxiv.org/abs/2301.12597",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": "J Li, D Li, S Savarese, S Hoi",
      "year": "2023",
      "cited_by": 376,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11255276306904968426&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Git: A generative image-to-text transformer for vision and language",
      "id": "5385256443321262916",
      "url": "https://arxiv.org/abs/2205.14100",
      "title": "Git: A generative image-to-text transformer for vision and language",
      "authors": "J Wang, Z Yang, X Hu, L Li, K Lin, Z Gan, Z Liu\u2026",
      "year": "2022",
      "cited_by": 151,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5385256443321262916&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Socratic models: Composing zero-shot multimodal reasoning with language",
      "id": "17485588102904105060",
      "url": "https://arxiv.org/abs/2204.00598",
      "title": "Socratic models: Composing zero-shot multimodal reasoning with language",
      "authors": "A Zeng, M Attarian, B Ichter, K Choromanski\u2026",
      "year": "2022",
      "cited_by": 158,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17485588102904105060&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Latent-nerf for shape-guided generation of 3d shapes and textures",
      "id": "5354355722706795291",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Metzer_Latent-NeRF_for_Shape-Guided_Generation_of_3D_Shapes_and_Textures_CVPR_2023_paper.html",
      "title": "Latent-nerf for shape-guided generation of 3d shapes and textures",
      "authors": "G Metzer, E Richardson, O Patashnik\u2026",
      "year": "2023",
      "cited_by": 62,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5354355722706795291&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Omnivl: One foundation model for image-language and video-language tasks",
      "id": "972963521258961554",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/259a5df46308d60f8454bd4adcc3b462-Abstract-Conference.html",
      "title": "Omnivl: One foundation model for image-language and video-language tasks",
      "authors": "J Wang, D Chen, Z Wu, C Luo, L Zhou\u2026",
      "year": "2022",
      "cited_by": 46,
      "cited_by_url": "https://scholar.google.com/scholar?cites=972963521258961554&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
      "id": "8793029896395507010",
      "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Wang_Pyramid_Vision_Transformer_A_Versatile_Backbone_for_Dense_Prediction_Without_ICCV_2021_paper.html",
      "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
      "authors": "W Wang, E Xie, X Li, DP Fan, K Song\u2026",
      "year": "2021",
      "cited_by": 2304,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8793029896395507010&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "id": "3458396398389387877",
      "url": "https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": "Z Liu, Y Lin, Y Cao, H Hu, Y Wei\u2026",
      "year": "2021",
      "cited_by": 10359,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3458396398389387877&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Mlp-mixer: An all-mlp architecture for vision",
      "id": "10553738615668616847",
      "url": "https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html",
      "title": "Mlp-mixer: An all-mlp architecture for vision",
      "authors": "IO Tolstikhin, N Houlsby, A Kolesnikov\u2026",
      "year": "2021",
      "cited_by": 1439,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10553738615668616847&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Cvt: Introducing convolutions to vision transformers",
      "id": "11517447940529951525",
      "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Wu_CvT_Introducing_Convolutions_to_Vision_Transformers_ICCV_2021_paper.html",
      "title": "Cvt: Introducing convolutions to vision transformers",
      "authors": "H Wu, B Xiao, N Codella, M Liu, X Dai\u2026",
      "year": "2021",
      "cited_by": 1208,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11517447940529951525&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Swin-unet: Unet-like pure transformer for medical image segmentation",
      "id": "4461602603986165987",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-25066-8_9",
      "title": "Swin-unet: Unet-like pure transformer for medical image segmentation",
      "authors": "H Cao, Y Wang, J Chen, D Jiang, X Zhang\u2026",
      "year": "2022",
      "cited_by": 1176,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4461602603986165987&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Vivit: A video vision transformer",
      "id": "1788827408361087894",
      "url": "https://openaccess.thecvf.com/content/ICCV2021/html/Arnab_ViViT_A_Video_Vision_Transformer_ICCV_2021_paper.html?ref=https://githubhelp.com",
      "title": "Vivit: A video vision transformer",
      "authors": "A Arnab, M Dehghani, G Heigold\u2026",
      "year": "2021",
      "cited_by": 1184,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1788827408361087894&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Transformer in transformer",
      "id": "15154755818357511167",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/854d9fca60b4bd07f9bb215d59ef5561-Abstract.html",
      "title": "Transformer in transformer",
      "authors": "K Han, A Xiao, E Wu, J Guo, C Xu\u2026",
      "year": "2021",
      "cited_by": 910,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15154755818357511167&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "On the opportunities and risks of foundation models",
      "id": "9595110325981705564",
      "url": "https://arxiv.org/abs/2108.07258",
      "title": "On the opportunities and risks of foundation models",
      "authors": "R Bommasani, DA Hudson, E Adeli, R Altman\u2026",
      "year": "2021",
      "cited_by": 1520,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9595110325981705564&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Large language models in medicine",
      "id": "3497017024792502078",
      "url": "https://www.nature.com/articles/s41591-023-02448-8",
      "title": "Large language models in medicine",
      "authors": "AJ Thirunavukarasu, DSJ Ting, K Elangovan\u2026",
      "year": "2023",
      "cited_by": 26,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3497017024792502078&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Sustainable ai: Environmental implications, challenges and opportunities",
      "id": "12550806676654513546",
      "url": "https://proceedings.mlsys.org/paper_files/paper/2022/hash/462211f67c7d858f663355eff93b745e-Abstract.html",
      "title": "Sustainable ai: Environmental implications, challenges and opportunities",
      "authors": "CJ Wu, R Raghavendra, U Gupta\u2026",
      "year": "2022",
      "cited_by": 132,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12550806676654513546&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Training language models to follow instructions with human feedback",
      "id": "12979976309017799162",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html",
      "title": "Training language models to follow instructions with human feedback",
      "authors": "L Ouyang, J Wu, X Jiang, D Almeida\u2026",
      "year": "2022",
      "cited_by": 2206,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12979976309017799162&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Palm: Scaling language modeling with pathways",
      "id": "10258451456074526049",
      "url": "https://arxiv.org/abs/2204.02311",
      "title": "Palm: Scaling language modeling with pathways",
      "authors": "A Chowdhery, S Narang, J Devlin, M Bosma\u2026",
      "year": "2022",
      "cited_by": 1519,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10258451456074526049&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Scaling autoregressive models for content-rich text-to-image generation",
      "id": "11775513497668487533",
      "url": "https://3dvar.com/Yu2022Scaling.pdf",
      "title": "Scaling autoregressive models for content-rich text-to-image generation",
      "authors": "J Yu, Y Xu, JY Koh, T Luong, G Baid, Z Wang\u2026",
      "year": "2022",
      "cited_by": 340,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11775513497668487533&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Learning to prompt for vision-language models",
      "id": "16117828918544644907",
      "url": "https://link.springer.com/article/10.1007/s11263-022-01653-1",
      "title": "Learning to prompt for vision-language models",
      "authors": "K Zhou, J Yang, CC Loy, Z Liu",
      "year": "2022",
      "cited_by": 644,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16117828918544644907&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Visual prompt tuning",
      "id": "14421942083121350206",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-19827-4_41",
      "title": "Visual prompt tuning",
      "authors": "M Jia, L Tang, BC Chen, C Cardie, S Belongie\u2026",
      "year": "2022",
      "cited_by": 375,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14421942083121350206&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Conditional prompt learning for vision-language models",
      "id": "737352693355482724",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Zhou_Conditional_Prompt_Learning_for_Vision-Language_Models_CVPR_2022_paper.html",
      "title": "Conditional prompt learning for vision-language models",
      "authors": "K Zhou, J Yang, CC Loy, Z Liu",
      "year": "2022",
      "cited_by": 350,
      "cited_by_url": "https://scholar.google.com/scholar?cites=737352693355482724&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "A generalist agent",
      "id": "2829133918492221101",
      "url": "https://arxiv.org/abs/2205.06175",
      "title": "A generalist agent",
      "authors": "S Reed, K Zolna, E Parisotto, SG Colmenarejo\u2026",
      "year": "2022",
      "cited_by": 395,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2829133918492221101&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "id": "12987945369444025427",
      "url": "https://proceedings.mlr.press/v139/kim21k.html",
      "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "authors": "W Kim, B Son, I Kim",
      "year": "2021",
      "cited_by": 829,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12987945369444025427&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt",
      "id": "12604090720681450553",
      "url": "https://arxiv.org/abs/2303.04226",
      "title": "A comprehensive survey of ai-generated content (aigc): A history of generative ai from gan to chatgpt",
      "authors": "Y Cao, S Li, Y Liu, Z Yan, Y Dai, PS Yu\u2026",
      "year": "2023",
      "cited_by": 86,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12604090720681450553&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Flava: A foundational language and vision alignment model",
      "id": "1440121271646678581",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Singh_FLAVA_A_Foundational_Language_and_Vision_Alignment_Model_CVPR_2022_paper.html",
      "title": "Flava: A foundational language and vision alignment model",
      "authors": "A Singh, R Hu, V Goswami\u2026",
      "year": "2022",
      "cited_by": 261,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1440121271646678581&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "An empirical study of training end-to-end vision-and-language transformers",
      "id": "9718581961231347788",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Dou_An_Empirical_Study_of_Training_End-to-End_Vision-and-Language_Transformers_CVPR_2022_paper.html",
      "title": "An empirical study of training end-to-end vision-and-language transformers",
      "authors": "ZY Dou, Y Xu, Z Gan, J Wang, S Wang\u2026",
      "year": "2022",
      "cited_by": 196,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9718581961231347788&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts",
      "id": "4791913395909773486",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/d46662aa53e78a62afd980a29e0c37ed-Abstract-Conference.html",
      "title": "Vlmo: Unified vision-language pre-training with mixture-of-modality-experts",
      "authors": "H Bao, W Wang, L Dong, Q Liu\u2026",
      "year": "2022",
      "cited_by": 174,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4791913395909773486&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Merlot: Multimodal neural script knowledge models",
      "id": "15973126860844704292",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/c6d4eb15f1e84a36eff58eca3627c82e-Abstract.html",
      "title": "Merlot: Multimodal neural script knowledge models",
      "authors": "R Zellers, X Lu, J Hessel, Y Yu\u2026",
      "year": "2021",
      "cited_by": 217,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15973126860844704292&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Winoground: Probing vision and language models for visio-linguistic compositionality",
      "id": "3641665820178422490",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Thrush_Winoground_Probing_Vision_and_Language_Models_for_Visio-Linguistic_Compositionality_CVPR_2022_paper.html",
      "title": "Winoground: Probing vision and language models for visio-linguistic compositionality",
      "authors": "T Thrush, R Jiang, M Bartolo, A Singh\u2026",
      "year": "2022",
      "cited_by": 124,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3641665820178422490&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Scaling up vision-language pre-training for image captioning",
      "id": "3085868247096884162",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Hu_Scaling_Up_Vision-Language_Pre-Training_for_Image_Captioning_CVPR_2022_paper.html",
      "title": "Scaling up vision-language pre-training for image captioning",
      "authors": "X Hu, Z Gan, J Wang, Z Yang, Z Liu\u2026",
      "year": "2022",
      "cited_by": 142,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3085868247096884162&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "A survey of uncertainty in deep neural networks",
      "id": "16770513324417061228",
      "url": "https://link.springer.com/article/10.1007/s10462-023-10562-9",
      "title": "A survey of uncertainty in deep neural networks",
      "authors": "J Gawlikowski, CRN Tassi, M Ali, J Lee, M Humt\u2026",
      "year": "2023",
      "cited_by": 442,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16770513324417061228&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 12
    },
    {
      "label": "Remote patient monitoring using artificial intelligence: Current state, applications, and challenges",
      "id": "7086855047075759373",
      "url": "https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1485",
      "title": "Remote patient monitoring using artificial intelligence: Current state, applications, and challenges",
      "authors": "T Shaik, X Tao, N Higgins, L Li\u2026",
      "year": "2023",
      "cited_by": 26,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7086855047075759373&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 12
    },
    {
      "label": "A comprehensive review of digital twin\u2014part 2: roles of uncertainty quantification and optimization, a battery digital twin, and perspectives",
      "id": "669840749336753525",
      "url": "https://link.springer.com/article/10.1007/s00158-022-03410-x",
      "title": "A comprehensive review of digital twin\u2014part 2: roles of uncertainty quantification and optimization, a battery digital twin, and perspectives",
      "authors": "A Thelen, X Zhang, O Fink, Y Lu, S Ghosh\u2026",
      "year": "2023",
      "cited_by": 21,
      "cited_by_url": "https://scholar.google.com/scholar?cites=669840749336753525&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 12
    },
    {
      "label": "Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons",
      "id": "11718687852373920511",
      "url": "https://www.sciencedirect.com/science/article/pii/S0021999122009652",
      "title": "Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons",
      "authors": "AF Psaros, X Meng, Z Zou, L Guo\u2026",
      "year": "2023",
      "cited_by": 78,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11718687852373920511&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 12
    },
    {
      "label": "Towards trustworthy machine fault diagnosis: A probabilistic Bayesian deep learning framework",
      "id": "2894691460777216219",
      "url": "https://www.sciencedirect.com/science/article/pii/S095183202200179X",
      "title": "Towards trustworthy machine fault diagnosis: A probabilistic Bayesian deep learning framework",
      "authors": "T Zhou, T Han, EL Droguett",
      "year": "2022",
      "cited_by": 68,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2894691460777216219&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 12
    },
    {
      "label": "Out-of-distribution detection-assisted trustworthy machinery fault diagnosis approach with uncertainty-aware deep ensembles",
      "id": "10327828086531941187",
      "url": "https://www.sciencedirect.com/science/article/pii/S0951832022002836",
      "title": "Out-of-distribution detection-assisted trustworthy machinery fault diagnosis approach with uncertainty-aware deep ensembles",
      "authors": "T Han, YF Li",
      "year": "2022",
      "cited_by": 64,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10327828086531941187&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 12
    },
    {
      "label": "Rambo-rl: Robust adversarial model-based offline reinforcement learning",
      "id": "10956894200939947900",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/6691c5e4a199b72dffd9c90acb63bcd6-Abstract-Conference.html",
      "title": "Rambo-rl: Robust adversarial model-based offline reinforcement learning",
      "authors": "M Rigter, B Lacerda, N Hawes",
      "year": "2022",
      "cited_by": 39,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10956894200939947900&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 12
    },
    {
      "label": "Uncertainty-aware multiview deep learning for internet of things applications",
      "id": "6038180113926801086",
      "url": "https://ieeexplore.ieee.org/abstract/document/9906001/",
      "title": "Uncertainty-aware multiview deep learning for internet of things applications",
      "authors": "C Xu, W Zhao, J Zhao, Z Guan\u2026",
      "year": "2022",
      "cited_by": 34,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6038180113926801086&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 12
    },
    {
      "label": "Generative time series forecasting with diffusion, denoise, and disentanglement",
      "id": "14636569081444625016",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/91a85f3fb8f570e6be52b333b5ab017a-Abstract-Conference.html",
      "title": "Generative time series forecasting with diffusion, denoise, and disentanglement",
      "authors": "Y Li, X Lu, Y Wang, D Dou",
      "year": "2022",
      "cited_by": 15,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14636569081444625016&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 12
    },
    {
      "label": "Probabilistic deep learning for real-time large deformation simulations",
      "id": "9643084522903856394",
      "url": "https://www.sciencedirect.com/science/article/pii/S004578252200411X",
      "title": "Probabilistic deep learning for real-time large deformation simulations",
      "authors": "S Deshpande, J Lengiewicz, SPA Bordas",
      "year": "2022",
      "cited_by": 31,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9643084522903856394&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 12
    },
    {
      "label": "Uncertainty quantification in DenseNet model using myocardial infarction ECG signals",
      "id": "17266336177445157331",
      "url": "https://www.sciencedirect.com/science/article/pii/S0169260722006897",
      "title": "Uncertainty quantification in DenseNet model using myocardial infarction ECG signals",
      "authors": "V Jahmunah, EYK Ng, RS Tan, SL Oh\u2026",
      "year": "2023",
      "cited_by": 20,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17266336177445157331&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 12
    },
    {
      "label": "Training generative adversarial networks with limited data",
      "id": "9063880872255850171",
      "url": "https://proceedings.neurips.cc/paper/2020/hash/8d30aa96e72440759f74bd2306c1fa3d-Abstract.html",
      "title": "Training generative adversarial networks with limited data",
      "authors": "T Karras, M Aittala, J Hellsten, S Laine\u2026",
      "year": "2020",
      "cited_by": 1303,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9063880872255850171&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 13
    },
    {
      "label": "Text data augmentation for deep learning",
      "id": "13909256571563279522",
      "url": "https://link.springer.com/article/10.1186/s40537-021-00492-0",
      "title": "Text data augmentation for deep learning",
      "authors": "C Shorten, TM Khoshgoftaar, B Furht",
      "year": "2021",
      "cited_by": 215,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13909256571563279522&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 13
    },
    {
      "label": "Deep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models",
      "id": "6594876225658804365",
      "url": "https://ieeexplore.ieee.org/abstract/document/9555209/",
      "title": "Deep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models",
      "authors": "S Bond-Taylor, A Leach, Y Long\u2026",
      "year": "2021",
      "cited_by": 238,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6594876225658804365&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 13
    },
    {
      "label": "Alias-free generative adversarial networks",
      "id": "17368705487922251039",
      "url": "https://proceedings.neurips.cc/paper/2021/hash/076ccd93ad68be51f23707988e934906-Abstract.html",
      "title": "Alias-free generative adversarial networks",
      "authors": "T Karras, M Aittala, S Laine\u2026",
      "year": "2021",
      "cited_by": 923,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17368705487922251039&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 13
    },
    {
      "label": "Elucidating the design space of diffusion-based generative models",
      "id": "5258718823597512255",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/a98846e9d9cc01cfb87eb694d946ce6b-Abstract-Conference.html",
      "title": "Elucidating the design space of diffusion-based generative models",
      "authors": "T Karras, M Aittala, T Aila\u2026",
      "year": "2022",
      "cited_by": 283,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5258718823597512255&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 13
    },
    {
      "label": "Efficient geometry-aware 3D generative adversarial networks",
      "id": "7080475698203068086",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Chan_Efficient_Geometry-Aware_3D_Generative_Adversarial_Networks_CVPR_2022_paper.html",
      "title": "Efficient geometry-aware 3D generative adversarial networks",
      "authors": "ER Chan, CZ Lin, MA Chan\u2026",
      "year": "2022",
      "cited_by": 451,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7080475698203068086&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 13
    },
    {
      "label": "Denoising diffusion probabilistic models",
      "id": "622631041436591387",
      "url": "https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html",
      "title": "Denoising diffusion probabilistic models",
      "authors": "J Ho, A Jain, P Abbeel",
      "year": "2020",
      "cited_by": 3757,
      "cited_by_url": "https://scholar.google.com/scholar?cites=622631041436591387&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 13
    },
    {
      "label": "Taming transformers for high-resolution image synthesis",
      "id": "12650581806598225058",
      "url": "https://openaccess.thecvf.com/content/CVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.html?ref=https://githubhelp.com",
      "title": "Taming transformers for high-resolution image synthesis",
      "authors": "P Esser, R Rombach, B Ommer",
      "year": "2021",
      "cited_by": 1103,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12650581806598225058&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 13
    },
    {
      "label": "Card: Classification and regression diffusion models",
      "id": "13161498921981862309",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/72dad95a24fae750f8ab1cb3dab5e58d-Abstract-Conference.html",
      "title": "Card: Classification and regression diffusion models",
      "authors": "X Han, H Zheng, M Zhou",
      "year": "2022",
      "cited_by": 372,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13161498921981862309&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 13
    },
    {
      "label": "Score-based generative modeling through stochastic differential equations",
      "id": "14592788616550656262",
      "url": "https://arxiv.org/abs/2011.13456",
      "title": "Score-based generative modeling through stochastic differential equations",
      "authors": "Y Song, J Sohl-Dickstein, DP Kingma, A Kumar\u2026",
      "year": "2020",
      "cited_by": 1619,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14592788616550656262&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 13
    },
    {
      "label": "Styleclip: Text-driven manipulation of stylegan imagery",
      "id": "4031590341915143464",
      "url": "http://openaccess.thecvf.com/content/ICCV2021/html/Patashnik_StyleCLIP_Text-Driven_Manipulation_of_StyleGAN_Imagery_ICCV_2021_paper.html",
      "title": "Styleclip: Text-driven manipulation of stylegan imagery",
      "authors": "O Patashnik, Z Wu, E Shechtman\u2026",
      "year": "2021",
      "cited_by": 699,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4031590341915143464&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 13
    },
    {
      "label": "Simple copy-paste is a strong data augmentation method for instance segmentation",
      "id": "2321980635951558135",
      "url": "https://openaccess.thecvf.com/content/CVPR2021/html/Ghiasi_Simple_Copy-Paste_Is_a_Strong_Data_Augmentation_Method_for_Instance_CVPR_2021_paper.html?ref=https://githubhelp.com",
      "title": "Simple copy-paste is a strong data augmentation method for instance segmentation",
      "authors": "G Ghiasi, Y Cui, A Srinivas, R Qian\u2026",
      "year": "2021",
      "cited_by": 670,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2321980635951558135&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "A comprehensive survey of image augmentation techniques for deep learning",
      "id": "15580172266410736369",
      "url": "https://www.sciencedirect.com/science/article/pii/S0031320323000481",
      "title": "A comprehensive survey of image augmentation techniques for deep learning",
      "authors": "M Xu, S Yoon, A Fuentes, DS Park",
      "year": "2023",
      "cited_by": 71,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15580172266410736369&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Deep learning for neuroimaging-based diagnosis and rehabilitation of autism spectrum disorder: a review",
      "id": "1177161480824869016",
      "url": "https://www.sciencedirect.com/science/article/pii/S0010482521007435",
      "title": "Deep learning for neuroimaging-based diagnosis and rehabilitation of autism spectrum disorder: a review",
      "authors": "M Khodatars, A Shoeibi, D Sadeghi\u2026",
      "year": "2021",
      "cited_by": 130,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1177161480824869016&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Yolox: Exceeding yolo series in 2021",
      "id": "11531242419091815801",
      "url": "https://arxiv.org/abs/2107.08430",
      "title": "Yolox: Exceeding yolo series in 2021",
      "authors": "Z Ge, S Liu, F Wang, Z Li, J Sun",
      "year": "2021",
      "cited_by": 2232,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11531242419091815801&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Swin transformer v2: Scaling up capacity and resolution",
      "id": "15329990143169836178",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Liu_Swin_Transformer_V2_Scaling_Up_Capacity_and_Resolution_CVPR_2022_paper.html",
      "title": "Swin transformer v2: Scaling up capacity and resolution",
      "authors": "Z Liu, H Hu, Y Lin, Z Yao, Z Xie, Y Wei\u2026",
      "year": "2022",
      "cited_by": 669,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15329990143169836178&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Simmim: A simple framework for masked image modeling",
      "id": "17018195497378444438",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Xie_SimMIM_A_Simple_Framework_for_Masked_Image_Modeling_CVPR_2022_paper.html",
      "title": "Simmim: A simple framework for masked image modeling",
      "authors": "Z Xie, Z Zhang, Y Cao, Y Lin, J Bao\u2026",
      "year": "2022",
      "cited_by": 563,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17018195497378444438&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Masked-attention mask transformer for universal image segmentation",
      "id": "10375739191012965737",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Cheng_Masked-Attention_Mask_Transformer_for_Universal_Image_Segmentation_CVPR_2022_paper.html",
      "title": "Masked-attention mask transformer for universal image segmentation",
      "authors": "B Cheng, I Misra, AG Schwing\u2026",
      "year": "2022",
      "cited_by": 603,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10375739191012965737&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Exploring plain vision transformer backbones for object detection",
      "id": "4490918786976048296",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-20077-9_17",
      "title": "Exploring plain vision transformer backbones for object detection",
      "authors": "Y Li, H Mao, R Girshick, K He",
      "year": "2022",
      "cited_by": 285,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4490918786976048296&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Grounded language-image pre-training",
      "id": "6004268348151288098",
      "url": "https://openaccess.thecvf.com/content/CVPR2022/html/Li_Grounded_Language-Image_Pre-Training_CVPR_2022_paper.html?ref=blog.roboflow.com",
      "title": "Grounded language-image pre-training",
      "authors": "LH Li, P Zhang, H Zhang, J Yang, C Li\u2026",
      "year": "2022",
      "cited_by": 318,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6004268348151288098&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "A survey of data augmentation approaches for NLP",
      "id": "862169174991977666",
      "url": "https://arxiv.org/abs/2105.03075",
      "title": "A survey of data augmentation approaches for NLP",
      "authors": "SY Feng, V Gangal, J Wei, S Chandar\u2026",
      "year": "2021",
      "cited_by": 490,
      "cited_by_url": "https://scholar.google.com/scholar?cites=862169174991977666&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 14
    },
    {
      "label": "Data augmentation approaches in natural language processing: A survey",
      "id": "11748945572016694012",
      "url": "https://www.sciencedirect.com/science/article/pii/S2666651022000080",
      "title": "Data augmentation approaches in natural language processing: A survey",
      "authors": "B Li, Y Hou, W Che",
      "year": "2022",
      "cited_by": 113,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11748945572016694012&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 14
    },
    {
      "label": "A survey on data augmentation for text classification",
      "id": "3667217905602891315",
      "url": "https://dl.acm.org/doi/abs/10.1145/3544558",
      "title": "A survey on data augmentation for text classification",
      "authors": "M Bayer, MA Kaufhold, C Reuter",
      "year": "2022",
      "cited_by": 160,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3667217905602891315&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 14
    },
    {
      "label": "Natural language processing applied to mental illness detection: a narrative review",
      "id": "2501577020384058220",
      "url": "https://www.nature.com/articles/s41746-022-00589-7",
      "title": "Natural language processing applied to mental illness detection: a narrative review",
      "authors": "T Zhang, AM Schoene, S Ji, S Ananiadou",
      "year": "2022",
      "cited_by": 68,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2501577020384058220&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 14
    },
    {
      "label": "Data augmentation as feature manipulation",
      "id": "8511009105279544089",
      "url": "https://proceedings.mlr.press/v162/shen22a.html",
      "title": "Data augmentation as feature manipulation",
      "authors": "R Shen, S Bubeck\u2026",
      "year": "2022",
      "cited_by": 28,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8511009105279544089&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 14
    },
    {
      "label": "An empirical survey of data augmentation for limited data learning in NLP",
      "id": "13145127891619293871",
      "url": "https://direct.mit.edu/tacl/article-abstract/doi/10.1162/tacl_a_00542/115238",
      "title": "An empirical survey of data augmentation for limited data learning in NLP",
      "authors": "J Chen, D Tam, C Raffel, M Bansal\u2026",
      "year": "2023",
      "cited_by": 67,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13145127891619293871&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 14
    },
    {
      "label": "Graph data augmentation for graph machine learning: A survey",
      "id": "6584420285295374856",
      "url": "https://arxiv.org/abs/2202.08871",
      "title": "Graph data augmentation for graph machine learning: A survey",
      "authors": "T Zhao, W Jin, Y Liu, Y Wang, G Liu\u2026",
      "year": "2022",
      "cited_by": 54,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6584420285295374856&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 14
    },
    {
      "label": "Measure and improve robustness in nlp models: A survey",
      "id": "11074972199329338101",
      "url": "https://arxiv.org/abs/2112.08313",
      "title": "Measure and improve robustness in nlp models: A survey",
      "authors": "X Wang, H Wang, D Yang",
      "year": "2021",
      "cited_by": 45,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11074972199329338101&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 14
    },
    {
      "label": "Spectral feature augmentation for graph contrastive learning and beyond",
      "id": "15873591660981920322",
      "url": "https://ojs.aaai.org/index.php/AAAI/article/view/26336",
      "title": "Spectral feature augmentation for graph contrastive learning and beyond",
      "authors": "Y Zhang, H Zhu, Z Song, P Koniusz\u2026",
      "year": "2023",
      "cited_by": 14,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15873591660981920322&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 14
    },
    {
      "label": "Chataug: Leveraging chatgpt for text data augmentation",
      "id": "13102079179802671182",
      "url": "https://arxiv.org/abs/2302.13007",
      "title": "Chataug: Leveraging chatgpt for text data augmentation",
      "authors": "H Dai, Z Liu, W Liao, X Huang, Z Wu, L Zhao\u2026",
      "year": "2023",
      "cited_by": 42,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13102079179802671182&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 14
    },
    {
      "label": "Curriculum contrastive context denoising for few-shot conversational dense retrieval",
      "id": "3720496172950573884",
      "url": "https://dl.acm.org/doi/abs/10.1145/3477495.3531961",
      "title": "Curriculum contrastive context denoising for few-shot conversational dense retrieval",
      "authors": "K Mao, Z Dou, H Qian",
      "year": "2022",
      "cited_by": 20,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3720496172950573884&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 14
    },
    {
      "label": "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors",
      "id": "2026726248513085794",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wang_YOLOv7_Trainable_Bag-of-Freebies_Sets_New_State-of-the-Art_for_Real-Time_Object_Detectors_CVPR_2023_paper.html",
      "title": "YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors",
      "authors": "CY Wang, A Bochkovskiy\u2026",
      "year": "2023",
      "cited_by": 1822,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2026726248513085794&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Object detection in 20 years: A survey",
      "id": "9850512646184180167",
      "url": "https://ieeexplore.ieee.org/abstract/document/10028728/",
      "title": "Object detection in 20 years: A survey",
      "authors": "Z Zou, K Chen, Z Shi, Y Guo, J Ye",
      "year": "2023",
      "cited_by": 1601,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9850512646184180167&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "A survey of human-in-the-loop for machine learning",
      "id": "13405798017275933263",
      "url": "https://www.sciencedirect.com/science/article/pii/S0167739X22001790",
      "title": "A survey of human-in-the-loop for machine learning",
      "authors": "X Wu, L Xiao, Y Sun, J Zhang, T Ma, L He",
      "year": "2022",
      "cited_by": 214,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13405798017275933263&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "A survey of deep learning-based object detection",
      "id": "5669364687087032958",
      "url": "https://ieeexplore.ieee.org/abstract/document/8825470/",
      "title": "A survey of deep learning-based object detection",
      "authors": "L Jiao, F Zhang, F Liu, S Yang, L Li, Z Feng\u2026",
      "year": "2019",
      "cited_by": 1054,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5669364687087032958&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Dynamic head: Unifying object detection heads with attentions",
      "id": "1319826694668830613",
      "url": "http://openaccess.thecvf.com/content/CVPR2021/html/Dai_Dynamic_Head_Unifying_Object_Detection_Heads_With_Attentions_CVPR_2021_paper.html",
      "title": "Dynamic head: Unifying object detection heads with attentions",
      "authors": "X Dai, Y Chen, B Xiao, D Chen, M Liu\u2026",
      "year": "2021",
      "cited_by": 274,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1319826694668830613&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "A comparative analysis of object detection metrics with a companion open-source toolkit",
      "id": "8384042348747306199",
      "url": "https://www.mdpi.com/2079-9292/10/3/279",
      "title": "A comparative analysis of object detection metrics with a companion open-source toolkit",
      "authors": "R Padilla, WL Passos, TLB Dias, SL Netto\u2026",
      "year": "2021",
      "cited_by": 343,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8384042348747306199&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Recent advances in small object detection based on deep learning: A review",
      "id": "17642928594905005472",
      "url": "https://www.sciencedirect.com/science/article/pii/S0262885620300421",
      "title": "Recent advances in small object detection based on deep learning: A review",
      "authors": "K Tong, Y Wu, F Zhou",
      "year": "2020",
      "cited_by": 316,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17642928594905005472&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Imbalance problems in object detection: A review",
      "id": "8762454778937977659",
      "url": "https://ieeexplore.ieee.org/abstract/document/9042296/",
      "title": "Imbalance problems in object detection: A review",
      "authors": "K Oksuz, BC Cam, S Kalkan\u2026",
      "year": "2020",
      "cited_by": 369,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8762454778937977659&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "YOLOv4-5D: An effective and efficient object detector for autonomous driving",
      "id": "10949414717109961618",
      "url": "https://ieeexplore.ieee.org/abstract/document/9374990/",
      "title": "YOLOv4-5D: An effective and efficient object detector for autonomous driving",
      "authors": "Y Cai, T Luan, H Gao, H Wang, L Chen\u2026",
      "year": "2021",
      "cited_by": 200,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10949414717109961618&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Real-time detection of uneaten feed pellets in underwater images for aquaculture using an improved YOLO-V4 network",
      "id": "4438492435191836535",
      "url": "https://www.sciencedirect.com/science/article/pii/S0168169921001538",
      "title": "Real-time detection of uneaten feed pellets in underwater images for aquaculture using an improved YOLO-V4 network",
      "authors": "X Hu, Y Liu, Z Zhao, J Liu, X Yang, C Sun\u2026",
      "year": "2021",
      "cited_by": 129,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4438492435191836535&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Deep learning-based object detection in low-altitude UAV datasets: A survey",
      "id": "2241015688519496377",
      "url": "https://www.sciencedirect.com/science/article/pii/S0262885620301785",
      "title": "Deep learning-based object detection in low-altitude UAV datasets: A survey",
      "authors": "P Mittal, R Singh, A Sharma",
      "year": "2020",
      "cited_by": 150,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2241015688519496377&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Bytetrack: Multi-object tracking by associating every detection box",
      "id": "14638466021176544465",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-20047-2_1",
      "title": "Bytetrack: Multi-object tracking by associating every detection box",
      "authors": "Y Zhang, P Sun, Y Jiang, D Yu, F Weng, Z Yuan\u2026",
      "year": "2022",
      "cited_by": 515,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14638466021176544465&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Motr: End-to-end multiple-object tracking with transformer",
      "id": "15866259573388647937",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-19812-0_38",
      "title": "Motr: End-to-end multiple-object tracking with transformer",
      "authors": "F Zeng, B Dong, Y Zhang, T Wang, X Zhang\u2026",
      "year": "2022",
      "cited_by": 231,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15866259573388647937&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Observation-centric sort: Rethinking sort for robust multi-object tracking",
      "id": "2093389731615411975",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Cao_Observation-Centric_SORT_Rethinking_SORT_for_Robust_Multi-Object_Tracking_CVPR_2023_paper.html",
      "title": "Observation-centric sort: Rethinking sort for robust multi-object tracking",
      "authors": "J Cao, J Pang, X Weng\u2026",
      "year": "2023",
      "cited_by": 136,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2093389731615411975&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Aiatrack: Attention in attention for transformer visual tracking",
      "id": "6724748843400977919",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-20047-2_9",
      "title": "Aiatrack: Attention in attention for transformer visual tracking",
      "authors": "S Gao, C Zhou, C Ma, X Wang, J Yuan",
      "year": "2022",
      "cited_by": 66,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6724748843400977919&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "In defense of online models for video instance segmentation",
      "id": "16069829188377130053",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-19815-1_34",
      "title": "In defense of online models for video instance segmentation",
      "authors": "J Wu, Q Liu, Y Jiang, S Bai, A Yuille, X Bai",
      "year": "2022",
      "cited_by": 49,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16069829188377130053&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Towards grand unification of object tracking",
      "id": "14300935760162828522",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-19803-8_43",
      "title": "Towards grand unification of object tracking",
      "authors": "B Yan, Y Jiang, P Sun, D Wang, Z Yuan, P Luo\u2026",
      "year": "2022",
      "cited_by": 68,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14300935760162828522&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Strongsort: Make deepsort great again",
      "id": "14167857935364818481",
      "url": "https://ieeexplore.ieee.org/abstract/document/10032656/",
      "title": "Strongsort: Make deepsort great again",
      "authors": "Y Du, Z Zhao, Y Song, Y Zhao, F Su\u2026",
      "year": "2023",
      "cited_by": 145,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14167857935364818481&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "The 7th AI City Challenge",
      "id": "17632693958287984620",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/AICity/html/Naphade_The_7th_AI_City_Challenge_CVPRW_2023_paper.html",
      "title": "The 7th AI City Challenge",
      "authors": "M Naphade, S Wang, DC Anastasiu\u2026",
      "year": "2023",
      "cited_by": 188,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17632693958287984620&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Dancetrack: Multi-object tracking in uniform appearance and diverse motion",
      "id": "9529319158101525799",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Sun_DanceTrack_Multi-Object_Tracking_in_Uniform_Appearance_and_Diverse_Motion_CVPR_2022_paper.html",
      "title": "Dancetrack: Multi-object tracking in uniform appearance and diverse motion",
      "authors": "P Sun, J Cao, Y Jiang, Z Yuan, S Bai\u2026",
      "year": "2022",
      "cited_by": 70,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9529319158101525799&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "YOLOv6: A single-stage object detection framework for industrial applications",
      "id": "13702720529764835843",
      "url": "https://arxiv.org/abs/2209.02976",
      "title": "YOLOv6: A single-stage object detection framework for industrial applications",
      "authors": "C Li, L Li, H Jiang, K Weng, Y Geng, L Li, Z Ke\u2026",
      "year": "2022",
      "cited_by": 464,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13702720529764835843&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 21
    },
    {
      "label": "Optimization strategies of fruit detection to overcome the challenge of unstructured background in field orchard environment: A review",
      "id": "17646032300901507453",
      "url": "https://link.springer.com/article/10.1007/s11119-023-10009-9",
      "title": "Optimization strategies of fruit detection to overcome the challenge of unstructured background in field orchard environment: A review",
      "authors": "Y Tang, J Qiu, Y Zhang, D Wu, Y Cao, K Zhao\u2026",
      "year": "2023",
      "cited_by": 16,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17646032300901507453&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "A comprehensive review of YOLO: From YOLOv1 to YOLOv8 and beyond",
      "id": "8819196546794680544",
      "url": "https://arxiv.org/abs/2304.00501",
      "title": "A comprehensive review of YOLO: From YOLOv1 to YOLOv8 and beyond",
      "authors": "J Terven, D Cordova-Esparza",
      "year": "2023",
      "cited_by": 84,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8819196546794680544&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 21
    },
    {
      "label": "Rtmdet: An empirical study of designing real-time object detectors",
      "id": "17785166320928157068",
      "url": "https://arxiv.org/abs/2212.07784",
      "title": "Rtmdet: An empirical study of designing real-time object detectors",
      "authors": "C Lyu, W Zhang, H Huang, Y Zhou, Y Wang\u2026",
      "year": "2022",
      "cited_by": 36,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17785166320928157068&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 21
    },
    {
      "label": "Hybrid-YOLO for classification of insulators defects in transmission lines based on UAV",
      "id": "5875186414038374408",
      "url": "https://www.sciencedirect.com/science/article/pii/S014206152300039X",
      "title": "Hybrid-YOLO for classification of insulators defects in transmission lines based on UAV",
      "authors": "BJ Souza, SF Stefenon, G Singh, RZ Freire",
      "year": "2023",
      "cited_by": 25,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5875186414038374408&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 21
    },
    {
      "label": "Yolov6 v3. 0: A full-scale reloading",
      "id": "958587697277086390",
      "url": "https://arxiv.org/abs/2301.05586",
      "title": "Yolov6 v3. 0: A full-scale reloading",
      "authors": "C Li, L Li, Y Geng, H Jiang, M Cheng, B Zhang\u2026",
      "year": "2023",
      "cited_by": 41,
      "cited_by_url": "https://scholar.google.com/scholar?cites=958587697277086390&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 21
    },
    {
      "label": "Deep object detection of crop weeds: Performance of YOLOv7 on a real case dataset from UAV images",
      "id": "17898472391686934449",
      "url": "https://www.mdpi.com/2072-4292/15/2/539?ref=blog.roboflow.com",
      "title": "Deep object detection of crop weeds: Performance of YOLOv7 on a real case dataset from UAV images",
      "authors": "I Gallo, AU Rehman, RH Dehkordi, N Landro\u2026",
      "year": "2023",
      "cited_by": 20,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17898472391686934449&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 21
    },
    {
      "label": "Helmet wearing detection of motorcycle drivers using deep learning network with residual transformer-spatial attention",
      "id": "15900202697333596864",
      "url": "https://www.mdpi.com/2504-446X/6/12/415",
      "title": "Helmet wearing detection of motorcycle drivers using deep learning network with residual transformer-spatial attention",
      "authors": "S Chen, J Lan, H Liu, C Chen, X Wang",
      "year": "2022",
      "cited_by": 10,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15900202697333596864&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 21
    },
    {
      "label": "Stall number detection of cow teats key frames",
      "id": "16133934620382970642",
      "url": "https://arxiv.org/abs/2303.10444",
      "title": "Stall number detection of cow teats key frames",
      "authors": "Y Zhang",
      "year": "2023",
      "cited_by": 19,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16133934620382970642&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 21
    },
    {
      "label": "Adversarial patch attack on multi-scale object detection for uav remote sensing images",
      "id": "15271212594475018431",
      "url": "https://www.mdpi.com/2072-4292/14/21/5298",
      "title": "Adversarial patch attack on multi-scale object detection for uav remote sensing images",
      "authors": "Y Zhang, Y Zhang, J Qi, K Bin, H Wen, X Tong\u2026",
      "year": "2022",
      "cited_by": 14,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15271212594475018431&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 21
    },
    {
      "label": "Gbh-yolov5: Ghost convolution with bottleneckcsp and tiny target prediction head incorporating yolov5 for pv panel defect detection",
      "id": "10116304010765610972",
      "url": "https://www.mdpi.com/2079-9292/12/3/561",
      "title": "Gbh-yolov5: Ghost convolution with bottleneckcsp and tiny target prediction head incorporating yolov5 for pv panel defect detection",
      "authors": "L Li, Z Wang, T Zhang",
      "year": "2023",
      "cited_by": 18,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10116304010765610972&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 21
    },
    {
      "label": "Spatial-Temporal Semantic Perception Network for Remote Sensing Image Semantic Change Detection",
      "id": "18022128353994651475",
      "url": "https://www.mdpi.com/2072-4292/15/16/4095",
      "title": "Spatial-Temporal Semantic Perception Network for Remote Sensing Image Semantic Change Detection",
      "authors": "Y He, H Zhang, X Ning, R Zhang, D Chang, M Hao",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18022128353994651475&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Towards the synthesis of spectral imaging and machine learning-based approaches for non-invasive phenotyping of plants",
      "id": "2DdH1BLUQaIJ",
      "url": "https://link.springer.com/article/10.1007/s12551-023-01125-x",
      "title": "Towards the synthesis of spectral imaging and machine learning-based approaches for non-invasive phenotyping of plants",
      "authors": "A Solovchenko, B Shurygin, DA Nesterov\u2026",
      "year": "2023",
      "modularity": 5
    },
    {
      "label": "An Accurate Forest Fire Recognition Method Based on Improved BPNN and IoT",
      "id": "4749208375417131840",
      "url": "https://www.mdpi.com/2072-4292/15/9/2365",
      "title": "An Accurate Forest Fire Recognition Method Based on Improved BPNN and IoT",
      "authors": "S Zheng, P Gao, Y Zhou, Z Wu, L Wan, F Hu, W Wang\u2026",
      "year": "2023",
      "cited_by": 2,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4749208375417131840&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Unstructured road extraction and roadside fruit recognition in grape orchards based on a synchronous detection algorithm",
      "id": "7487952808605114076",
      "url": "https://www.frontiersin.org/articles/10.3389/fpls.2023.1103276/full",
      "title": "Unstructured road extraction and roadside fruit recognition in grape orchards based on a synchronous detection algorithm",
      "authors": "X Zhou, X Zou, W Tang, Z Yan, H Meng\u2026",
      "year": "2023",
      "modularity": 5
    },
    {
      "label": "Grape-bunch identification and location of picking points on occluded fruit axis based on YOLOv5-GAP",
      "id": "10897720152285769060",
      "url": "https://www.mdpi.com/2311-7524/9/4/498",
      "title": "Grape-bunch identification and location of picking points on occluded fruit axis based on YOLOv5-GAP",
      "authors": "T Zhang, F Wu, M Wang, Z Chen, L Li, X Zou",
      "year": "2023",
      "cited_by": 2,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10897720152285769060&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "SOD head: A network for locating small fruits from top to bottom in layers of feature maps",
      "id": "pCHwc0wrutcJ",
      "url": "https://www.sciencedirect.com/science/article/pii/S0168169923005215",
      "title": "SOD head: A network for locating small fruits from top to bottom in layers of feature maps",
      "authors": "Y Lu, M Sun, Y Guan, J Lian, Z Ji, X Yin\u2026",
      "year": "2023",
      "modularity": 5
    },
    {
      "label": "Nondestructive Detection of Egg Freshness Based on Infrared Thermal Imaging",
      "id": "AtRUthsSjPcJ",
      "url": "https://www.mdpi.com/1424-8220/23/12/5530",
      "title": "Nondestructive Detection of Egg Freshness Based on Infrared Thermal Imaging",
      "authors": "J Zhang, W Lu, X Jian, Q Hu, D Dai",
      "year": "2023",
      "modularity": 5
    },
    {
      "label": "DenseNet-201 and Xception Pre-Trained Deep Learning Models for Fruit Recognition",
      "id": "1291063805327774306",
      "url": "https://www.mdpi.com/2079-9292/12/14/3132",
      "title": "DenseNet-201 and Xception Pre-Trained Deep Learning Models for Fruit Recognition",
      "authors": "F Salim, F Saeed, S Basurra, SN Qasem\u2026",
      "year": "2023",
      "modularity": 5
    },
    {
      "label": "Deep Learning in Precision Agriculture: Artificially Generated VNIR Images Segmentation for Early Postharvest Decay Prediction in Apples",
      "id": "17567167024662762007",
      "url": "https://www.mdpi.com/1099-4300/25/7/987",
      "title": "Deep Learning in Precision Agriculture: Artificially Generated VNIR Images Segmentation for Early Postharvest Decay Prediction in Apples",
      "authors": "N Stasenko, I Shukhratov, M Savinov, D Shadrin\u2026",
      "year": "2023",
      "modularity": 5
    },
    {
      "label": "Rapid detection of Yunnan Xiaomila based on lightweight YOLOv7 algorithm",
      "id": "10347681032777129532",
      "url": "https://www.frontiersin.org/articles/10.3389/fpls.2023.1200144/full",
      "title": "Rapid detection of Yunnan Xiaomila based on lightweight YOLOv7 algorithm",
      "authors": "F Wang, J Jiang, Y Chen, Z Sun, Y Tang\u2026",
      "year": "2023",
      "modularity": 5
    },
    {
      "label": "YOLOWeeds: a novel benchmark of YOLO object detectors for multi-class weed detection in cotton production systems",
      "id": "17127958062872744853",
      "url": "https://www.sciencedirect.com/science/article/pii/S0168169923000431",
      "title": "YOLOWeeds: a novel benchmark of YOLO object detectors for multi-class weed detection in cotton production systems",
      "authors": "F Dang, D Chen, Y Lu, Z Li",
      "year": "2023",
      "cited_by": 17,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17127958062872744853&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "Performance evaluation of deep learning object detectors for weed detection for cotton",
      "id": "10256791187069118291",
      "url": "https://www.sciencedirect.com/science/article/pii/S2772375522000910",
      "title": "Performance evaluation of deep learning object detectors for weed detection for cotton",
      "authors": "A Rahman, Y Lu, H Wang",
      "year": "2023",
      "cited_by": 8,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10256791187069118291&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "More eyes on the prize: open-source data, software and hardware for advancing plant science through collaboration",
      "id": "15552447685803913675",
      "url": "https://academic.oup.com/aobpla/advance-article/doi/10.1093/aobpla/plad010/7073680",
      "title": "More eyes on the prize: open-source data, software and hardware for advancing plant science through collaboration",
      "authors": "GRY Coleman, WT Salter",
      "year": "2023",
      "modularity": 8
    },
    {
      "label": "Evaluation of YOLO Object Detectors for Weed Detection in Different Turfgrass Scenarios",
      "id": "5222861253143202947",
      "url": "https://www.mdpi.com/2076-3417/13/14/8502",
      "title": "Evaluation of YOLO Object Detectors for Weed Detection in Different Turfgrass Scenarios",
      "authors": "M Sportelli, OE Apolo-Apolo, M Fontanelli, C Frasconi\u2026",
      "year": "2023",
      "cited_by": 2,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5222861253143202947&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "Integration and preliminary evaluation of a robotic cotton harvester prototype",
      "id": "2712659024700086363",
      "url": "https://www.sciencedirect.com/science/article/pii/S0168169923003319",
      "title": "Integration and preliminary evaluation of a robotic cotton harvester prototype",
      "authors": "H Gharakhani, JA Thomasson, Y Lu",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2712659024700086363&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "Label-Efficient Learning in Agriculture: A Comprehensive Review",
      "id": "11270960296740375515",
      "url": "https://arxiv.org/abs/2305.14691",
      "title": "Label-Efficient Learning in Agriculture: A Comprehensive Review",
      "authors": "J Li, D Chen, X Qi, Z Li, Y Huang, D Morris\u2026",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11270960296740375515&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "High-Precision Tomato Disease Detection Using NanoSegmenter Based on Transformer and Lightweighting",
      "id": "7774766038187589626",
      "url": "https://www.mdpi.com/2223-7747/12/13/2559",
      "title": "High-Precision Tomato Disease Detection Using NanoSegmenter Based on Transformer and Lightweighting",
      "authors": "Y Liu, Y Song, R Ye, S Zhu, Y Huang, T Chen, J Zhou\u2026",
      "year": "2023",
      "modularity": 8
    },
    {
      "label": "Automatic Detection of Pedestrian Crosswalk with Faster R-CNN and YOLOv7",
      "id": "2114425240534661867",
      "url": "https://www.mdpi.com/2075-5309/13/4/1070",
      "title": "Automatic Detection of Pedestrian Crosswalk with Faster R-CNN and YOLOv7",
      "authors": "\u00d6 Kaya, MY \u00c7odur, E Mustafaraj",
      "year": "2023",
      "cited_by": 5,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2114425240534661867&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Deep Neural Networks for Weed Detections Towards Precision Weeding",
      "id": "6877695309876199058",
      "url": "https://elibrary.asabe.org/abstract.asp?aid=53573",
      "title": "Deep Neural Networks for Weed Detections Towards Precision Weeding",
      "authors": "A Rahman, Y Lu, H Wang",
      "year": "2022",
      "cited_by": 2,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6877695309876199058&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "A W-shaped convolutional network for robust crop and weed classification in agriculture",
      "id": "6699633140763047923",
      "url": "https://link.springer.com/article/10.1007/s11119-023-10027-7",
      "title": "A W-shaped convolutional network for robust crop and weed classification in agriculture",
      "authors": "SI Moazzam, T Nawaz, WS Qureshi, US Khan\u2026",
      "year": "2023",
      "modularity": 8
    },
    {
      "label": "Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges",
      "id": "13776149024359305191",
      "url": "https://arxiv.org/abs/2308.06668",
      "title": "Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges",
      "authors": "J Li, M Xu, L Xiang, D Chen, W Zhuang, X Yin\u2026",
      "year": "2023",
      "modularity": 8
    },
    {
      "label": "An attention mechanism-improved YOLOv7 object detection algorithm for hemp duck count estimation",
      "id": "13222462680960111198",
      "url": "https://www.mdpi.com/2077-0472/12/10/1659",
      "title": "An attention mechanism-improved YOLOv7 object detection algorithm for hemp duck count estimation",
      "authors": "K Jiang, T Xie, R Yan, X Wen, D Li, H Jiang, N Jiang\u2026",
      "year": "2022",
      "cited_by": 38,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13222462680960111198&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 9
    },
    {
      "label": "Automatic detection of brown hens in cage-free houses with deep learning methods",
      "id": "6544902242854208675",
      "url": "https://www.sciencedirect.com/science/article/pii/S0032579123003036",
      "title": "Automatic detection of brown hens in cage-free houses with deep learning methods",
      "authors": "Y Guo, P Regmi, Y Ding, RB Bist, L Chai",
      "year": "2023",
      "cited_by": 3,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6544902242854208675&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 9
    },
    {
      "label": "Automatic Fabric Defect Detection Method Using AC-YOLOv5",
      "id": "12643412623559921352",
      "url": "https://www.mdpi.com/2079-9292/12/13/2950",
      "title": "Automatic Fabric Defect Detection Method Using AC-YOLOv5",
      "authors": "Y Guo, X Kang, J Li, Y Yang",
      "year": "2023",
      "cited_by": 3,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12643412623559921352&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 9
    },
    {
      "label": "Data-driven model SSD-BSP for multi-target coal-gangue detection",
      "id": "3110755508632090464",
      "url": "https://www.sciencedirect.com/science/article/pii/S0263224123008084",
      "title": "Data-driven model SSD-BSP for multi-target coal-gangue detection",
      "authors": "L Wang, X Wang, B Li",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3110755508632090464&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 9
    },
    {
      "label": "Deep Learning for Highly Accurate Hand Recognition Based on Yolov7 Model",
      "id": "14940203506181718397",
      "url": "https://www.mdpi.com/2504-2289/7/1/53",
      "title": "Deep Learning for Highly Accurate Hand Recognition Based on Yolov7 Model",
      "authors": "C Dewi, APS Chen, HJ Christanto",
      "year": "2023",
      "cited_by": 10,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14940203506181718397&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 9
    },
    {
      "label": "Tea leaf disease detection and identification based on YOLOv7 (YOLO-T)",
      "id": "7108335566961034347",
      "url": "https://www.nature.com/articles/s41598-023-33270-4",
      "title": "Tea leaf disease detection and identification based on YOLOv7 (YOLO-T)",
      "authors": "MJA Soeb, MF Jubayer, TA Tarin, MR Al Mamun\u2026",
      "year": "2023",
      "cited_by": 4,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7108335566961034347&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 9
    },
    {
      "label": "A Trunk Detection Method for Camellia oleifera Fruit Harvesting Robot Based on Improved YOLOv7",
      "id": "118648389925321836",
      "url": "https://www.mdpi.com/1999-4907/14/7/1453",
      "title": "A Trunk Detection Method for Camellia oleifera Fruit Harvesting Robot Based on Improved YOLOv7",
      "authors": "Y Liu, H Wang, Y Liu, Y Luo, H Li, H Chen, K Liao, L Li",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=118648389925321836&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "GlandSegNet: Semantic segmentation model and area detection method for cotton leaf pigment glands",
      "id": "ApU0apKFnLoJ",
      "url": "https://www.sciencedirect.com/science/article/pii/S0168169923005185",
      "title": "GlandSegNet: Semantic segmentation model and area detection method for cotton leaf pigment glands",
      "authors": "Y Xu, G Wang, L Shao, N Wang, L She, Y Liu\u2026",
      "year": "2023",
      "modularity": 9
    },
    {
      "label": "Cotton Seedling Detection and Counting Based on UAV Multispectral Images and Deep Learning Methods",
      "id": "10376535638618684847",
      "url": "https://www.mdpi.com/2072-4292/15/10/2680",
      "title": "Cotton Seedling Detection and Counting Based on UAV Multispectral Images and Deep Learning Methods",
      "authors": "Y Feng, W Chen, Y Ma, Z Zhang, P Gao, X Lv",
      "year": "2023",
      "modularity": 9
    },
    {
      "label": "Fusion of udder temperature and size features for the automatic detection of dairy cow mastitis using deep learning",
      "id": "sdblonX9vEQJ",
      "url": "https://www.sciencedirect.com/science/article/pii/S0168169923005197",
      "title": "Fusion of udder temperature and size features for the automatic detection of dairy cow mastitis using deep learning",
      "authors": "M Chu, Q Li, Y Wang, X Zeng, Y Si, G Liu",
      "year": "2023",
      "modularity": 9
    },
    {
      "label": "YOLO-DCTI: Small Object Detection in Remote Sensing Base on Contextual Transformer Enhancement",
      "id": "9083521644969713050",
      "url": "https://www.mdpi.com/2072-4292/15/16/3970",
      "title": "YOLO-DCTI: Small Object Detection in Remote Sensing Base on Contextual Transformer Enhancement",
      "authors": "L Min, Z Fan, Q Lv, M Reda, L Shen, B Wang",
      "year": "2023",
      "modularity": 9
    },
    {
      "label": "Sf-yolov5: A lightweight small object detection algorithm based on improved feature fusion mode",
      "id": "16827683962441657873",
      "url": "https://www.mdpi.com/1424-8220/22/15/5817",
      "title": "Sf-yolov5: A lightweight small object detection algorithm based on improved feature fusion mode",
      "authors": "H Liu, F Sun, J Gu, L Deng",
      "year": "2022",
      "cited_by": 28,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16827683962441657873&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "Development of YOLOv5-based real-time smart monitoring system for increasing lab safety awareness in educational institutions",
      "id": "14052692408086479292",
      "url": "https://www.mdpi.com/1424-8220/22/22/8820",
      "title": "Development of YOLOv5-based real-time smart monitoring system for increasing lab safety awareness in educational institutions",
      "authors": "L Ali, F Alnajjar, MMA Parambil, MI Younes\u2026",
      "year": "2022",
      "cited_by": 11,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14052692408086479292&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "An efficient and intelligent detection method for fabric defects based on improved YOLOv5",
      "id": "17085777096678660112",
      "url": "https://www.mdpi.com/1424-8220/23/1/97",
      "title": "An efficient and intelligent detection method for fabric defects based on improved YOLOv5",
      "authors": "G Lin, K Liu, X Xia, R Yan",
      "year": "2022",
      "cited_by": 12,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17085777096678660112&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "Detection of river floating garbage based on improved YOLOv5",
      "id": "915175193869878529",
      "url": "https://www.mdpi.com/2227-7390/10/22/4366",
      "title": "Detection of river floating garbage based on improved YOLOv5",
      "authors": "X Yang, J Zhao, L Zhao, H Zhang, L Li, Z Ji, I Ganchev",
      "year": "2022",
      "cited_by": 8,
      "cited_by_url": "https://scholar.google.com/scholar?cites=915175193869878529&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "High-resolution processing and sigmoid fusion modules for efficient detection of small objects in an embedded system",
      "id": "18125073647564822800",
      "url": "https://www.nature.com/articles/s41598-022-27189-5",
      "title": "High-resolution processing and sigmoid fusion modules for efficient detection of small objects in an embedded system",
      "authors": "M Kim, H Kim, J Sung, C Park, J Paik",
      "year": "2023",
      "cited_by": 4,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18125073647564822800&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "Wildlife Object Detection Method Applying Segmentation Gradient Flow and Feature Dimensionality Reduction",
      "id": "11317692785014151150",
      "url": "https://www.mdpi.com/2079-9292/12/2/377",
      "title": "Wildlife Object Detection Method Applying Segmentation Gradient Flow and Feature Dimensionality Reduction",
      "authors": "M Zhang, F Gao, W Yang, H Zhang",
      "year": "2023",
      "cited_by": 5,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11317692785014151150&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "DC-YOLOv8: Small-Size Object Detection Algorithm Based on Camera Sensor",
      "id": "17823167432429599810",
      "url": "https://www.mdpi.com/2079-9292/12/10/2323",
      "title": "DC-YOLOv8: Small-Size Object Detection Algorithm Based on Camera Sensor",
      "authors": "H Lou, X Duan, J Guo, H Liu, J Gu, L Bi, H Chen",
      "year": "2023",
      "cited_by": 11,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17823167432429599810&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "Litchi Detection in a Complex Natural Environment Using the YOLOv5-Litchi Model",
      "id": "851720305858029725",
      "url": "https://www.mdpi.com/2073-4395/12/12/3054",
      "title": "Litchi Detection in a Complex Natural Environment Using the YOLOv5-Litchi Model",
      "authors": "J Xie, J Peng, J Wang, B Chen, T Jing, D Sun, P Gao\u2026",
      "year": "2022",
      "cited_by": 6,
      "cited_by_url": "https://scholar.google.com/scholar?cites=851720305858029725&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "Long-Strip Target Detection and Tracking with Autonomous Surface Vehicle",
      "id": "13349023610638525354",
      "url": "https://www.mdpi.com/2077-1312/11/1/106",
      "title": "Long-Strip Target Detection and Tracking with Autonomous Surface Vehicle",
      "authors": "M Zhang, D Zhao, C Sheng, Z Liu, W Cai",
      "year": "2023",
      "cited_by": 3,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13349023610638525354&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "A lightweight vehicle-pedestrian detection algorithm based on attention mechanism in traffic scenarios",
      "id": "10228711962693123964",
      "url": "https://www.mdpi.com/1424-8220/22/21/8480",
      "title": "A lightweight vehicle-pedestrian detection algorithm based on attention mechanism in traffic scenarios",
      "authors": "Y Zhang, A Zhou, F Zhao, H Wu",
      "year": "2022",
      "cited_by": 6,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10228711962693123964&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "An Improved YOLOv5 Method for Small Object Detection in UAV Capture Scenes",
      "id": "6497775393041290067",
      "url": "https://ieeexplore.ieee.org/abstract/document/10032549/",
      "title": "An Improved YOLOv5 Method for Small Object Detection in UAV Capture Scenes",
      "authors": "Z Liu, X Gao, Y Wan, J Wang, H Lyu",
      "year": "2023",
      "cited_by": 7,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6497775393041290067&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "Detection of Camellia oleifera fruit in complex scenes by using YOLOv7 and data augmentation",
      "id": "11112203079270473115",
      "url": "https://www.mdpi.com/2076-3417/12/22/11318",
      "title": "Detection of Camellia oleifera fruit in complex scenes by using YOLOv7 and data augmentation",
      "authors": "D Wu, S Jiang, E Zhao, Y Liu, H Zhu, W Wang\u2026",
      "year": "2022",
      "cited_by": 41,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11112203079270473115&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "YOLOv7-Plum: Advancing Plum Fruit Detection in Natural Environments with Deep Learning",
      "id": "18283090867219141273",
      "url": "https://www.mdpi.com/2223-7747/12/15/2883",
      "title": "YOLOv7-Plum: Advancing Plum Fruit Detection in Natural Environments with Deep Learning",
      "authors": "R Tang, Y Lei, B Luo, J Zhang, J Mu",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18283090867219141273&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "A pineapple target detection method in a field environment based on improved YOLOv7",
      "id": "3348080849815981295",
      "url": "https://www.mdpi.com/2076-3417/13/4/2691",
      "title": "A pineapple target detection method in a field environment based on improved YOLOv7",
      "authors": "Y Lai, R Ma, Y Chen, T Wan, R Jiao, H He",
      "year": "2023",
      "cited_by": 9,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3348080849815981295&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Tea-YOLOv8s: A Tea Bud Detection Model Based on Deep Learning and Computer Vision",
      "id": "9008218150110989248",
      "url": "https://www.mdpi.com/1424-8220/23/14/6576",
      "title": "Tea-YOLOv8s: A Tea Bud Detection Model Based on Deep Learning and Computer Vision",
      "authors": "S Xie, H Sun",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9008218150110989248&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Cooperative Heterogeneous Robots for Autonomous Insects Trap Monitoring System in a Precision Agriculture Scenario",
      "id": "15890436532115930630",
      "url": "https://www.mdpi.com/2077-0472/13/2/239",
      "title": "Cooperative Heterogeneous Robots for Autonomous Insects Trap Monitoring System in a Precision Agriculture Scenario",
      "authors": "GS Berger, M Teixeira, A Cantieri, J Lima, AI Pereira\u2026",
      "year": "2023",
      "cited_by": 7,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15890436532115930630&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Detecting endosperm cracks in soaked maize using \u03bcCT technology and R-YOLOv7-tiny",
      "id": "MXxApydlfuYJ",
      "url": "https://www.sciencedirect.com/science/article/pii/S0168169923006208",
      "title": "Detecting endosperm cracks in soaked maize using \u03bcCT technology and R-YOLOv7-tiny",
      "authors": "Y Jiao, Z Wang, Y Shang, R Li, Z Hua\u2026",
      "year": "2023",
      "modularity": 5
    },
    {
      "label": "Intelligent detection of Multi-Class pitaya fruits in target picking row based on WGB-YOLO network",
      "id": "15254646771058273507",
      "url": "https://www.sciencedirect.com/science/article/pii/S0168169923001680",
      "title": "Intelligent detection of Multi-Class pitaya fruits in target picking row based on WGB-YOLO network",
      "authors": "Y Nan, H Zhang, Y Zeng, J Zheng, Y Ge",
      "year": "2023",
      "cited_by": 2,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15254646771058273507&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Traffic Sign Detection Based on the Improved YOLOv5",
      "id": "18262125184766942934",
      "url": "https://www.mdpi.com/2076-3417/13/17/9748",
      "title": "Traffic Sign Detection Based on the Improved YOLOv5",
      "authors": "R Zhang, K Zheng, P Shi, Y Mei, H Li, T Qiu",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18262125184766942934&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "TBC-YOLOv7: a refined YOLOv7-based algorithm for tea bud grading detection",
      "id": "12977604275271393235",
      "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10469839/",
      "title": "TBC-YOLOv7: a refined YOLOv7-based algorithm for tea bud grading detection",
      "authors": "S Wang, D Wu, X Zheng",
      "year": "2023",
      "modularity": 5
    },
    {
      "label": "Adaptive active positioning of Camellia oleifera fruit picking points: Classical image processing...",
      "id": "17690753104303819861",
      "url": "https://www.mdpi.com/2076-3417/12/24/12959",
      "title": "Adaptive active positioning of Camellia oleifera fruit picking points: Classical image processing...",
      "authors": "Y Zhou, Y Tang, X Zou, M Wu, W Tang, F Meng\u2026",
      "year": "2022",
      "cited_by": 23,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17690753104303819861&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "A Review of Target Recognition Technology for Fruit Picking Robots: From Digital Image Processing to Deep Learning",
      "id": "11735473174983847098",
      "url": "https://www.mdpi.com/2076-3417/13/7/4160",
      "title": "A Review of Target Recognition Technology for Fruit Picking Robots: From Digital Image Processing to Deep Learning",
      "authors": "X Hua, H Li, J Zeng, C Han, T Chen, L Tang, Y Luo",
      "year": "2023",
      "cited_by": 6,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11735473174983847098&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Detection and counting of banana bunches by integrating deep learning and classic image-processing algorithms",
      "id": "1443894251537732005",
      "url": "https://www.sciencedirect.com/science/article/pii/S0168169923002156",
      "title": "Detection and counting of banana bunches by integrating deep learning and classic image-processing algorithms",
      "authors": "F Wu, Z Yang, X Mo, Z Wu, W Tang, J Duan\u2026",
      "year": "2023",
      "cited_by": 13,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1443894251537732005&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Branch interference sensing and handling by tactile enabled robotic apple harvesting",
      "id": "1791593962748758598",
      "url": "https://www.mdpi.com/2073-4395/13/2/503",
      "title": "Branch interference sensing and handling by tactile enabled robotic apple harvesting",
      "authors": "H Zhou, H Kang, X Wang, W Au, MY Wang, C Chen",
      "year": "2023",
      "cited_by": 4,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1791593962748758598&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Multidirectional Dynamic Response and Swing Shedding of Grapes: An Experimental and Simulation Investigation under Vibration Excitation",
      "id": "14879519839963588983",
      "url": "https://www.mdpi.com/2073-4395/13/3/869",
      "title": "Multidirectional Dynamic Response and Swing Shedding of Grapes: An Experimental and Simulation Investigation under Vibration Excitation",
      "authors": "P Zhang, D Yan, X Cai, Y Chen, L Luo, Y Pan, X Zou",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14879519839963588983&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 5
    },
    {
      "label": "Domain feature mapping with YOLOv7 for automated edge-based pallet racking inspections",
      "id": "10264464849626080964",
      "url": "https://www.mdpi.com/1424-8220/22/18/6927",
      "title": "Domain feature mapping with YOLOv7 for automated edge-based pallet racking inspections",
      "authors": "M Hussain, H Al-Aqrabi, M Munawar, R Hill, T Alsboui",
      "year": "2022",
      "cited_by": 30,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10264464849626080964&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 9
    },
    {
      "label": "Statistical Analysis of Design Aspects of Various YOLO-Based Deep Learning Models for Object Detection",
      "id": "2392814171009909808",
      "url": "https://link.springer.com/article/10.1007/s44196-023-00302-w",
      "title": "Statistical Analysis of Design Aspects of Various YOLO-Based Deep Learning Models for Object Detection",
      "authors": "U Sirisha, SP Praveen, PN Srinivasu\u2026",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2392814171009909808&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 9
    },
    {
      "label": "YOLOv7-RAR for Urban Vehicle Detection",
      "id": "6327480080772807927",
      "url": "https://www.mdpi.com/1424-8220/23/4/1801",
      "title": "YOLOv7-RAR for Urban Vehicle Detection",
      "authors": "Y Zhang, Y Sun, Z Wang, Y Jiang",
      "year": "2023",
      "cited_by": 11,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6327480080772807927&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 9
    },
    {
      "label": "Fire detection and notification method in ship areas using deep learning and computer vision approaches",
      "id": "12337878440731894992",
      "url": "https://www.mdpi.com/1424-8220/23/16/7078",
      "title": "Fire detection and notification method in ship areas using deep learning and computer vision approaches",
      "authors": "K Avazov, MK Jamil, B Muminov, AB Abdusalomov\u2026",
      "year": "2023",
      "cited_by": 2,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12337878440731894992&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 9
    },
    {
      "label": "IDOD-YOLOV7: Image-Dehazing YOLOV7 for Object Detection in Low-Light Foggy Traffic Environments",
      "id": "17386293721154871155",
      "url": "https://www.mdpi.com/1424-8220/23/3/1347",
      "title": "IDOD-YOLOV7: Image-Dehazing YOLOV7 for Object Detection in Low-Light Foggy Traffic Environments",
      "authors": "Y Qiu, Y Lu, Y Wang, H Jiang",
      "year": "2023",
      "cited_by": 11,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17386293721154871155&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 9
    },
    {
      "label": "PV-CrackNet Architecture for Filter Induced Augmentation and Micro-Cracks Detection within a Photovoltaic Manufacturing Facility",
      "id": "12836955438277091904",
      "url": "https://www.mdpi.com/1996-1073/15/22/8667",
      "title": "PV-CrackNet Architecture for Filter Induced Augmentation and Micro-Cracks Detection within a Photovoltaic Manufacturing Facility",
      "authors": "M Hussain, H Al-Aqrabi, R Hill",
      "year": "2022",
      "cited_by": 9,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12836955438277091904&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 9
    },
    {
      "label": "A Review on Defect Detection of Electroluminescence-Based Photovoltaic Cell Surface Images Using Computer Vision",
      "id": "18049535498421460297",
      "url": "https://www.mdpi.com/1996-1073/16/10/4012",
      "title": "A Review on Defect Detection of Electroluminescence-Based Photovoltaic Cell Surface Images Using Computer Vision",
      "authors": "T Hussain, M Hussain, H Al-Aqrabi, T Alsboui, R Hill",
      "year": "2023",
      "modularity": 9
    },
    {
      "label": "Deep Learning-Based Intelligent Forklift Cargo Accurate Transfer System",
      "id": "12328613189873235415",
      "url": "https://www.mdpi.com/1424-8220/22/21/8437",
      "title": "Deep Learning-Based Intelligent Forklift Cargo Accurate Transfer System",
      "authors": "J Ren, Y Pan, P Yao, Y Hu, W Gao, Z Xue",
      "year": "2022",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12328613189873235415&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 9
    },
    {
      "label": "EFC-YOLO: An Efficient Surface-Defect-Detection Algorithm for Steel Strips",
      "id": "5371618923568523141",
      "url": "https://www.mdpi.com/1424-8220/23/17/7619",
      "title": "EFC-YOLO: An Efficient Surface-Defect-Detection Algorithm for Steel Strips",
      "authors": "Y Li, S Xu, Z Zhu, P Wang, K Li, Q He, Q Zheng",
      "year": "2023",
      "modularity": 9
    },
    {
      "label": "TPH-YOLOv5: Improved YOLOv5 based on transformer prediction head for object detection on...",
      "id": "18210588638302847093",
      "url": "https://openaccess.thecvf.com/content/ICCV2021W/VisDrone/html/Zhu_TPH-YOLOv5_Improved_YOLOv5_Based_on_Transformer_Prediction_Head_for_Object_ICCVW_2021_paper.html",
      "title": "TPH-YOLOv5: Improved YOLOv5 based on transformer prediction head for object detection on...",
      "authors": "X Zhu, S Lyu, X Wang, Q Zhao",
      "year": "2021",
      "cited_by": 662,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18210588638302847093&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "A comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities",
      "id": "15045952046135698930",
      "url": "https://dl.acm.org/doi/abs/10.1145/3582688",
      "title": "A comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities",
      "authors": "Y Song, T Wang, P Cai, SK Mondal\u2026",
      "year": "2023",
      "cited_by": 46,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15045952046135698930&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "A lightweight vehicles detection network model based on YOLOv5",
      "id": "1223563817431199857",
      "url": "https://www.sciencedirect.com/science/article/pii/S0952197622001415",
      "title": "A lightweight vehicles detection network model based on YOLOv5",
      "authors": "X Dong, S Yan, C Duan",
      "year": "2022",
      "cited_by": 77,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1223563817431199857&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "Improved yolov5: Efficient object detection using drone images under various conditions",
      "id": "3743433833463586545",
      "url": "https://www.mdpi.com/2076-3417/12/14/7255",
      "title": "Improved yolov5: Efficient object detection using drone images under various conditions",
      "authors": "HK Jung, GS Choi",
      "year": "2022",
      "cited_by": 59,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3743433833463586545&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "A hybrid explainable ensemble transformer encoder for pneumonia identification from chest X-ray images",
      "id": "3333668025330401477",
      "url": "https://www.sciencedirect.com/science/article/pii/S2090123222002028",
      "title": "A hybrid explainable ensemble transformer encoder for pneumonia identification from chest X-ray images",
      "authors": "CC Ukwuoma, Z Qin, MBB Heyat, F Akhtar\u2026",
      "year": "2023",
      "cited_by": 25,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3333668025330401477&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "Underwater target detection algorithm based on improved YOLOv5",
      "id": "2805320302601413924",
      "url": "https://www.mdpi.com/2077-1312/10/3/310",
      "title": "Underwater target detection algorithm based on improved YOLOv5",
      "authors": "F Lei, F Tang, S Li",
      "year": "2022",
      "cited_by": 47,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2805320302601413924&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "Deep learning-based object detection techniques for remote sensing images: A survey",
      "id": "12502648671025662060",
      "url": "https://www.mdpi.com/2072-4292/14/10/2385",
      "title": "Deep learning-based object detection techniques for remote sensing images: A survey",
      "authors": "Z Li, Y Wang, N Zhang, Y Zhang, Z Zhao, D Xu, G Ben\u2026",
      "year": "2022",
      "cited_by": 34,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12502648671025662060&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 7
    },
    {
      "label": "Dino: Detr with improved denoising anchor boxes for end-to-end object detection",
      "id": "7039269942427062691",
      "url": "https://arxiv.org/abs/2203.03605",
      "title": "Dino: Detr with improved denoising anchor boxes for end-to-end object detection",
      "authors": "H Zhang, F Li, S Liu, L Zhang, H Su, J Zhu\u2026",
      "year": "2022",
      "cited_by": 337,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7039269942427062691&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Internimage: Exploring large-scale vision foundation models with deformable convolutions",
      "id": "6118595289890500680",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wang_InternImage_Exploring_Large-Scale_Vision_Foundation_Models_With_Deformable_Convolutions_CVPR_2023_paper.html",
      "title": "Internimage: Exploring large-scale vision foundation models with deformable convolutions",
      "authors": "W Wang, J Dai, Z Chen, Z Huang, Z Li\u2026",
      "year": "2023",
      "cited_by": 120,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6118595289890500680&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Context autoencoder for self-supervised representation learning",
      "id": "3484260075890953852",
      "url": "https://link.springer.com/article/10.1007/s11263-023-01852-4",
      "title": "Context autoencoder for self-supervised representation learning",
      "authors": "X Chen, M Ding, X Wang, Y Xin, S Mo, Y Wang\u2026",
      "year": "2023",
      "cited_by": 169,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3484260075890953852&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Deep learning for SAR ship detection: Past, present and future",
      "id": "6077963581280858299",
      "url": "https://www.mdpi.com/2072-4292/14/11/2712",
      "title": "Deep learning for SAR ship detection: Past, present and future",
      "authors": "J Li, C Xu, H Su, L Gao, T Wang",
      "year": "2022",
      "cited_by": 29,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6077963581280858299&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe",
      "id": "17545941908498686611",
      "url": "https://arxiv.org/abs/2209.05324",
      "title": "Delving into the Devils of Bird's-eye-view Perception: A Review, Evaluation and Recipe",
      "authors": "H Li, C Sima, J Dai, W Wang, L Lu, H Wang\u2026",
      "year": "2022",
      "cited_by": 28,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17545941908498686611&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Mask dino: Towards a unified transformer-based framework for object detection and segmentation",
      "id": "17229720160752682638",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Li_Mask_DINO_Towards_a_Unified_Transformer-Based_Framework_for_Object_Detection_CVPR_2023_paper.html",
      "title": "Mask dino: Towards a unified transformer-based framework for object detection and segmentation",
      "authors": "F Li, H Zhang, H Xu, S Liu, L Zhang\u2026",
      "year": "2023",
      "cited_by": 94,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17229720160752682638&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Point transformer v2: Grouped vector attention and partition-based pooling",
      "id": "2723001857482086032",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/d78ece6613953f46501b958b7bb4582f-Abstract-Conference.html",
      "title": "Point transformer v2: Grouped vector attention and partition-based pooling",
      "authors": "X Wu, Y Lao, L Jiang, X Liu\u2026",
      "year": "2022",
      "cited_by": 37,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2723001857482086032&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Focal modulation networks",
      "id": "12867511582517934835",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/1b08f585b0171b74d1401a5195e986f1-Abstract-Conference.html",
      "title": "Focal modulation networks",
      "authors": "J Yang, C Li, X Dai, J Gao",
      "year": "2022",
      "cited_by": 67,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12867511582517934835&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Comparing YOLOv3, YOLOv4 and YOLOv5 for autonomous landing spot detection in faulty UAVs",
      "id": "4488471448867589825",
      "url": "https://www.mdpi.com/1424-8220/22/2/464",
      "title": "Comparing YOLOv3, YOLOv4 and YOLOv5 for autonomous landing spot detection in faulty UAVs",
      "authors": "U Nepal, H Eslamiat",
      "year": "2022",
      "cited_by": 265,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4488471448867589825&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Tools, techniques, datasets and application areas for object detection in an image: a review",
      "id": "8808925328206804280",
      "url": "https://link.springer.com/article/10.1007/s11042-022-13153-y",
      "title": "Tools, techniques, datasets and application areas for object detection in an image: a review",
      "authors": "J Kaur, W Singh",
      "year": "2022",
      "cited_by": 33,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8808925328206804280&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Deep learning based detector yolov5 for identifying insect pests",
      "id": "9110810375335606993",
      "url": "https://www.mdpi.com/2076-3417/12/19/10167",
      "title": "Deep learning based detector yolov5 for identifying insect pests",
      "authors": "I Ahmad, Y Yang, Y Yue, C Ye, M Hassan, X Cheng\u2026",
      "year": "2022",
      "cited_by": 38,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9110810375335606993&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Vision-based autonomous landing for the uav: A review",
      "id": "708526825456642350",
      "url": "https://www.mdpi.com/2226-4310/9/11/634",
      "title": "Vision-based autonomous landing for the uav: A review",
      "authors": "L Xin, Z Tang, W Gai, H Liu",
      "year": "2022",
      "cited_by": 13,
      "cited_by_url": "https://scholar.google.com/scholar?cites=708526825456642350&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Multi-class detection of kiwifruit flower and its distribution identification in orchard based on YOLOv5l and Euclidean distance",
      "id": "515129946496787543",
      "url": "https://www.sciencedirect.com/science/article/pii/S0168169922006512",
      "title": "Multi-class detection of kiwifruit flower and its distribution identification in orchard based on YOLOv5l and Euclidean distance",
      "authors": "G Li, L Fu, C Gao, W Fang, G Zhao, F Shi\u2026",
      "year": "2022",
      "cited_by": 11,
      "cited_by_url": "https://scholar.google.com/scholar?cites=515129946496787543&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Multi-scale ship target detection using SAR images based on improved Yolov5",
      "id": "3749665078422249041",
      "url": "https://www.frontiersin.org/articles/10.3389/fmars.2022.1086140/full",
      "title": "Multi-scale ship target detection using SAR images based on improved Yolov5",
      "authors": "M Yasir, L Shanwei, X Mingming, S Hui\u2026",
      "year": "2023",
      "cited_by": 8,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3749665078422249041&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Real-time vehicle detection algorithm based on a lightweight You-Only-Look-Once (YOLOv5n-L) approach",
      "id": "13329597315293984783",
      "url": "https://www.sciencedirect.com/science/article/pii/S0957417422021261",
      "title": "Real-time vehicle detection algorithm based on a lightweight You-Only-Look-Once (YOLOv5n-L) approach",
      "authors": "M Bie, Y Liu, G Li, J Hong, J Li",
      "year": "2023",
      "cited_by": 11,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13329597315293984783&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Detection of river plastic using UAV sensor data and deep learning",
      "id": "9358192350302074225",
      "url": "https://www.mdpi.com/2072-4292/14/13/3049",
      "title": "Detection of river plastic using UAV sensor data and deep learning",
      "authors": "N Maharjan, H Miyazaki, BM Pati, MN Dailey\u2026",
      "year": "2022",
      "cited_by": 14,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9358192350302074225&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Design and implementation of uavs for bird's nest inspection on transmission lines based on Deep Learning",
      "id": "9302345027896383303",
      "url": "https://www.mdpi.com/2504-446X/6/9/252",
      "title": "Design and implementation of uavs for bird's nest inspection on transmission lines based on Deep Learning",
      "authors": "H Li, Y Dong, Y Liu, J Ai",
      "year": "2022",
      "cited_by": 8,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9302345027896383303&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 3
    },
    {
      "label": "Key technologies of machine vision for weeding robots: A review and benchmark",
      "id": "5295462520886771746",
      "url": "https://www.sciencedirect.com/science/article/pii/S0168169922001971",
      "title": "Key technologies of machine vision for weeding robots: A review and benchmark",
      "authors": "Y Li, Z Guo, F Shuang, M Zhang, X Li",
      "year": "2022",
      "cited_by": 31,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5295462520886771746&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "Vision-based navigation and guidance for agricultural autonomous vehicles and robots: A review",
      "id": "6482216091275039752",
      "url": "https://www.sciencedirect.com/science/article/pii/S0168169922008924",
      "title": "Vision-based navigation and guidance for agricultural autonomous vehicles and robots: A review",
      "authors": "Y Bai, B Zhang, N Xu, J Zhou, J Shi, Z Diao",
      "year": "2023",
      "cited_by": 22,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6482216091275039752&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "A deep learning approach incorporating YOLO v5 and attention mechanisms for field real-time detection of the invasive weed Solanum rostratum Dunal seedlings",
      "id": "8940062898534376834",
      "url": "https://www.sciencedirect.com/science/article/pii/S0168169922005117",
      "title": "A deep learning approach incorporating YOLO v5 and attention mechanisms for field real-time detection of the invasive weed Solanum rostratum Dunal seedlings",
      "authors": "Q Wang, M Cheng, S Huang, Z Cai, J Zhang\u2026",
      "year": "2022",
      "cited_by": 32,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8940062898534376834&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "Digital innovations for sustainable and resilient agricultural systems",
      "id": "15245828836153963354",
      "url": "https://academic.oup.com/erae/article/50/4/1277/7208892?login=true",
      "title": "Digital innovations for sustainable and resilient agricultural systems",
      "authors": "R Finger",
      "year": "2023",
      "cited_by": 2,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15245828836153963354&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "RGB-D datasets for robotic perception in site-specific agricultural operations\u2014A survey",
      "id": "3527582028524441499",
      "url": "https://www.sciencedirect.com/science/article/pii/S0168169923004234",
      "title": "RGB-D datasets for robotic perception in site-specific agricultural operations\u2014A survey",
      "authors": "P Kurtser, S Lowry",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3527582028524441499&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "Boosting precision crop protection towards agriculture 5.0 via machine learning and emerging technologies: A contextual review",
      "id": "1893894085208736639",
      "url": "https://www.frontiersin.org/articles/10.3389/fpls.2023.1143326/full",
      "title": "Boosting precision crop protection towards agriculture 5.0 via machine learning and emerging technologies: A contextual review",
      "authors": "GA Mes\u00edas-Ruiz, M P\u00e9rez-Ortiz, J Dorado\u2026",
      "year": "2023",
      "cited_by": 3,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1893894085208736639&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "A conceptual evaluation of a weed control method with post-damage application of herbicides: A composite intelligent intra-row weeding robot",
      "id": "11761116720729991146",
      "url": "https://www.sciencedirect.com/science/article/pii/S0167198723002040",
      "title": "A conceptual evaluation of a weed control method with post-damage application of herbicides: A composite intelligent intra-row weeding robot",
      "authors": "W Jiang, L Quan, G Wei, C Chang, T Geng",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11761116720729991146&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "Handling severity levels of multiple co-occurring cotton plant diseases using improved YOLOX model",
      "id": "9934965656490088917",
      "url": "https://ieeexplore.ieee.org/abstract/document/9999667/",
      "title": "Handling severity levels of multiple co-occurring cotton plant diseases using improved YOLOX model",
      "authors": "SK Noon, M Amjad, MA Qureshi, A Mannan",
      "year": "2022",
      "cited_by": 5,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9934965656490088917&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "Design and experimental verification of the YOLOV5 model implanted with a transformer module for target-oriented spraying in cabbage farming",
      "id": "2188695656210318579",
      "url": "https://www.mdpi.com/2073-4395/12/10/2551",
      "title": "Design and experimental verification of the YOLOV5 model implanted with a transformer module for target-oriented spraying in cabbage farming",
      "authors": "H Fu, X Zhao, H Wu, S Zheng, K Zheng, C Zhai",
      "year": "2022",
      "cited_by": 4,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2188695656210318579&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "DeepCottonWeeds (DCW): a novel benchmark of YOLO object detectors for weed detection in cotton production systems",
      "id": "12517578013806719466",
      "url": "https://elibrary.asabe.org/abstract.asp?aid=53312",
      "title": "DeepCottonWeeds (DCW): a novel benchmark of YOLO object detectors for weed detection in cotton production systems",
      "authors": "F Dang, D Chen, Y Lu, Z Li\u2026",
      "year": "2022",
      "cited_by": 7,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12517578013806719466&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 8
    },
    {
      "label": "Motrv2: Bootstrapping end-to-end multi-object tracking by pretrained object detectors",
      "id": "14575314968965851624",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_MOTRv2_Bootstrapping_End-to-End_Multi-Object_Tracking_by_Pretrained_Object_Detectors_CVPR_2023_paper.html",
      "title": "Motrv2: Bootstrapping end-to-end multi-object tracking by pretrained object detectors",
      "authors": "Y Zhang, T Wang, X Zhang",
      "year": "2023",
      "cited_by": 13,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14575314968965851624&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "A Review of Deep Learning-Based Visual Multi-Object Tracking Algorithms for Autonomous Driving",
      "id": "14118575121889987430",
      "url": "https://www.mdpi.com/2076-3417/12/21/10741",
      "title": "A Review of Deep Learning-Based Visual Multi-Object Tracking Algorithms for Autonomous Driving",
      "authors": "S Guo, S Wang, Z Yang, L Wang, H Zhang, P Guo\u2026",
      "year": "2022",
      "cited_by": 7,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14118575121889987430&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "BoT-SORT: Robust associations multi-pedestrian tracking",
      "id": "3318911338829069796",
      "url": "https://arxiv.org/abs/2206.14651",
      "title": "BoT-SORT: Robust associations multi-pedestrian tracking",
      "authors": "N Aharon, R Orfaig, BZ Bobrovsky",
      "year": "2022",
      "cited_by": 84,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3318911338829069796&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Qdtrack: Quasi-dense similarity learning for appearance-only multiple object tracking",
      "id": "9808171637715574951",
      "url": "https://ieeexplore.ieee.org/abstract/document/10209207/",
      "title": "Qdtrack: Quasi-dense similarity learning for appearance-only multiple object tracking",
      "authors": "T Fischer, TE Huang, J Pang, L Qiu\u2026",
      "year": "2023",
      "cited_by": 11,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9808171637715574951&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Jrdb-pose: A large-scale dataset for multi-person pose estimation and tracking",
      "id": "8763231865317300809",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Vendrow_JRDB-Pose_A_Large-Scale_Dataset_for_Multi-Person_Pose_Estimation_and_Tracking_CVPR_2023_paper.html",
      "title": "Jrdb-pose: A large-scale dataset for multi-person pose estimation and tracking",
      "authors": "E Vendrow, DT Le, J Cai\u2026",
      "year": "2023",
      "cited_by": 5,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8763231865317300809&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Multiple Object Tracking in Robotic Applications: Trends and Challenges",
      "id": "14978456818526690373",
      "url": "https://www.mdpi.com/2076-3417/12/19/9408",
      "title": "Multiple Object Tracking in Robotic Applications: Trends and Challenges",
      "authors": "A Gad, T Basmaji, M Yaghi, H Alheeh, M Alkhedher\u2026",
      "year": "2022",
      "cited_by": 6,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14978456818526690373&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Hard to track objects with irregular motions and similar appearances? make it easier by buffering the matching space",
      "id": "17206319611652371856",
      "url": "https://openaccess.thecvf.com/content/WACV2023/html/Yang_Hard_To_Track_Objects_With_Irregular_Motions_and_Similar_Appearances_WACV_2023_paper.html",
      "title": "Hard to track objects with irregular motions and similar appearances? make it easier by buffering the matching space",
      "authors": "F Yang, S Odashima, S Masui\u2026",
      "year": "2023",
      "cited_by": 16,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17206319611652371856&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "1st workshop on maritime computer vision (macvi) 2023: Challenge results",
      "id": "13723544815291086075",
      "url": "https://openaccess.thecvf.com/content/WACV2023W/MaCVi/html/Kiefer_1st_Workshop_on_Maritime_Computer_Vision_MaCVi_2023_Challenge_Results_WACVW_2023_paper.html",
      "title": "1st workshop on maritime computer vision (macvi) 2023: Challenge results",
      "authors": "B Kiefer, M Kristan, J Per\u0161, L \u017dust\u2026",
      "year": "2023",
      "cited_by": 6,
      "cited_by_url": "https://scholar.google.com/scholar?cites=13723544815291086075&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 6
    },
    {
      "label": "Intelligent approach for the industrialization of deep learning solutions applied to fault detection",
      "id": "4246517974439469322",
      "url": "https://www.sciencedirect.com/science/article/pii/S0957417423014616",
      "title": "Intelligent approach for the industrialization of deep learning solutions applied to fault detection",
      "authors": "IP Colo, CS Sueldo, M De Paula, GG Acosta",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4246517974439469322&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 15
    },
    {
      "label": "Fast and Non-Destructive Quail Egg Freshness Assessment Using a Thermal Camera and Deep Learning-Based Air Cell Detection Algorithms for the Revalidation of\u00a0\u2026",
      "id": "137519887286825601",
      "url": "https://www.mdpi.com/1424-8220/22/20/7703",
      "title": "Fast and Non-Destructive Quail Egg Freshness Assessment Using a Thermal Camera and Deep Learning-Based Air Cell Detection Algorithms for the Revalidation of\u00a0\u2026",
      "authors": "VM Nakaguchi, T Ahamed",
      "year": "2022",
      "cited_by": 3,
      "cited_by_url": "https://scholar.google.com/scholar?cites=137519887286825601&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 15
    },
    {
      "label": "An efficient DNN splitting scheme for edge-AI enabled smart manufacturing",
      "id": "E02bi-l64ZwJ",
      "url": "https://www.sciencedirect.com/science/article/pii/S2452414X23000547",
      "title": "An efficient DNN splitting scheme for edge-AI enabled smart manufacturing",
      "authors": "H Gauttam, KK Pattanaik, S Bhadauria, G Nain\u2026",
      "year": "2023",
      "modularity": 15
    },
    {
      "label": "Improved YOLOv3 Model for Workpiece Stud Leakage Detection",
      "id": "4896734917901521680",
      "url": "https://www.mdpi.com/2079-9292/11/21/3430",
      "title": "Improved YOLOv3 Model for Workpiece Stud Leakage Detection",
      "authors": "P Cong, K Lv, H Feng, J Zhou",
      "year": "2022",
      "cited_by": 2,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4896734917901521680&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 15
    },
    {
      "label": "Knowledge distillation-optimized two-stage anomaly detection for liquid rocket engine with missing multimodal data",
      "id": "QwXRfW59944J",
      "url": "https://www.sciencedirect.com/science/article/pii/S0951832023005902",
      "title": "Knowledge distillation-optimized two-stage anomaly detection for liquid rocket engine with missing multimodal data",
      "authors": "X Zhang, Y Feng, J Chen, Z Liu, J Wang\u2026",
      "year": "2023",
      "modularity": 15
    },
    {
      "label": "Region\u2010based fully convolutional networks with deformable convolution and attention fusion for steel surface defect detection in industrial Internet of Things",
      "id": "4747360019864620532",
      "url": "https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/sil2.12208",
      "title": "Region\u2010based fully convolutional networks with deformable convolution and attention fusion for steel surface defect detection in industrial Internet of Things",
      "authors": "M Fu, J Wu, Q Wang, L Sun, Z Ma, C Zhang\u2026",
      "year": "2023",
      "cited_by": 2,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4747360019864620532&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 15
    },
    {
      "label": "Bubble detection in photoresist with small samples based on GAN augmentations and modified YOLO",
      "id": "15130464980890007333",
      "url": "https://www.sciencedirect.com/science/article/pii/S0952197623004086",
      "title": "Bubble detection in photoresist with small samples based on GAN augmentations and modified YOLO",
      "authors": "G Yang, C Song, Z Yang, S Cui",
      "year": "2023",
      "modularity": 15
    },
    {
      "label": "Development and analysis of a holistic function-driven adaptive assembly strategy applied to micro gears",
      "id": "1126142290960013428",
      "url": "https://www.sciencedirect.com/science/article/pii/S0278612523001061",
      "title": "Development and analysis of a holistic function-driven adaptive assembly strategy applied to micro gears",
      "authors": "A Khezri, V Schiller, L Homri, A Etienne\u2026",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1126142290960013428&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 15
    },
    {
      "label": "Segmentation of backscattered electron images of cement-based materials using lightweight U-Net with attention mechanism (LWAU-Net)",
      "id": "4Fr4jN5uC8oJ",
      "url": "https://www.sciencedirect.com/science/article/pii/S2352710223017278",
      "title": "Segmentation of backscattered electron images of cement-based materials using lightweight U-Net with attention mechanism (LWAU-Net)",
      "authors": "P Li, W Zhao, C Fu, T Pan, X Ji",
      "year": "2023",
      "modularity": 15
    },
    {
      "label": "YOLOv5-SFE: An algorithm fusing spatio-temporal features for detecting and recognizing workers' operating behaviors",
      "id": "uHNtXrDhdSwJ",
      "url": "https://www.sciencedirect.com/science/article/pii/S1474034623001167",
      "title": "YOLOv5-SFE: An algorithm fusing spatio-temporal features for detecting and recognizing workers' operating behaviors",
      "authors": "L Li, P Zhang, S Yang, W Jiao",
      "year": "2023",
      "modularity": 15
    },
    {
      "label": "Dab-detr: Dynamic anchor boxes are better queries for detr",
      "id": "11838073149065061192",
      "url": "https://arxiv.org/abs/2201.12329",
      "title": "Dab-detr: Dynamic anchor boxes are better queries for detr",
      "authors": "S Liu, F Li, H Zhang, X Yang, X Qi, H Su, J Zhu\u2026",
      "year": "2022",
      "cited_by": 251,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11838073149065061192&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Petr: Position embedding transformation for multi-view 3d object detection",
      "id": "3799744009906269739",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-19812-0_31",
      "title": "Petr: Position embedding transformation for multi-view 3d object detection",
      "authors": "Y Liu, T Wang, X Zhang, J Sun",
      "year": "2022",
      "cited_by": 168,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3799744009906269739&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Dn-detr: Accelerate detr training by introducing query denoising",
      "id": "9646853108847192407",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Li_DN-DETR_Accelerate_DETR_Training_by_Introducing_Query_DeNoising_CVPR_2022_paper.html",
      "title": "Dn-detr: Accelerate detr training by introducing query denoising",
      "authors": "F Li, H Zhang, S Liu, J Guo, LM Ni\u2026",
      "year": "2022",
      "cited_by": 216,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9646853108847192407&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Petrv2: A unified framework for 3d perception from multi-camera images",
      "id": "3777854110099563090",
      "url": "https://arxiv.org/abs/2206.01256",
      "title": "Petrv2: A unified framework for 3d perception from multi-camera images",
      "authors": "Y Liu, J Yan, F Jia, S Li, A Gao, T Wang\u2026",
      "year": "2022",
      "cited_by": 97,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3777854110099563090&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Oneformer: One transformer to rule universal image segmentation",
      "id": "18294495860499043250",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Jain_OneFormer_One_Transformer_To_Rule_Universal_Image_Segmentation_CVPR_2023_paper.html",
      "title": "Oneformer: One transformer to rule universal image segmentation",
      "authors": "J Jain, J Li, MT Chiu, A Hassani\u2026",
      "year": "2023",
      "cited_by": 45,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18294495860499043250&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Detrs with hybrid matching",
      "id": "12550135891789567985",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Jia_DETRs_With_Hybrid_Matching_CVPR_2023_paper.html",
      "title": "Detrs with hybrid matching",
      "authors": "D Jia, Y Yuan, H He, X Wu, H Yu\u2026",
      "year": "2023",
      "cited_by": 50,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12550135891789567985&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 19
    },
    {
      "label": "End-to-end object detection with transformers",
      "id": "1672665553767281734",
      "url": "https://link.springer.com/chapter/10.1007/978-3-030-58452-8_13",
      "title": "End-to-end object detection with transformers",
      "authors": "N Carion, F Massa, G Synnaeve, N Usunier\u2026",
      "year": "2020",
      "cited_by": 7808,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1672665553767281734&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Emergent abilities of large language models",
      "id": "16898296257676733828",
      "url": "https://arxiv.org/abs/2206.07682",
      "title": "Emergent abilities of large language models",
      "authors": "J Wei, Y Tay, R Bommasani, C Raffel, B Zoph\u2026",
      "year": "2022",
      "cited_by": 631,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16898296257676733828&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Instructpix2pix: Learning to follow image editing instructions",
      "id": "17636062869988382916",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Brooks_InstructPix2Pix_Learning_To_Follow_Image_Editing_Instructions_CVPR_2023_paper.html",
      "title": "Instructpix2pix: Learning to follow image editing instructions",
      "authors": "T Brooks, A Holynski, AA Efros",
      "year": "2023",
      "cited_by": 177,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17636062869988382916&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Clip-adapter: Better vision-language models with feature adapters",
      "id": "17712252571307454824",
      "url": "https://link.springer.com/article/10.1007/s11263-023-01891-x",
      "title": "Clip-adapter: Better vision-language models with feature adapters",
      "authors": "P Gao, S Geng, R Zhang, T Ma, R Fang\u2026",
      "year": "2023",
      "cited_by": 253,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17712252571307454824&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Scaling language-image pre-training via masking",
      "id": "8649528985577473031",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Li_Scaling_Language-Image_Pre-Training_via_Masking_CVPR_2023_paper.html",
      "title": "Scaling language-image pre-training via masking",
      "authors": "Y Li, H Fan, R Hu\u2026",
      "year": "2023",
      "cited_by": 49,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8649528985577473031&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 2
    },
    {
      "label": "Vitpose: Simple vision transformer baselines for human pose estimation",
      "id": "9439766841533136382",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/fbb10d319d44f8c3b4720873e4177c65-Abstract-Conference.html",
      "title": "Vitpose: Simple vision transformer baselines for human pose estimation",
      "authors": "Y Xu, J Zhang, Q Zhang, D Tao",
      "year": "2022",
      "cited_by": 130,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9439766841533136382&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 16
    },
    {
      "label": "SegFormer: Simple and efficient design for semantic segmentation with transformers",
      "id": "11165298458048562314",
      "url": "https://proceedings.neurips.cc/paper/2021/hash/64f1f27bf1b4ec22924fd0acb550c235-Abstract.html",
      "title": "SegFormer: Simple and efficient design for semantic segmentation with transformers",
      "authors": "E Xie, W Wang, Z Yu, A Anandkumar\u2026",
      "year": "2021",
      "cited_by": 1725,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11165298458048562314&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Scaling up your kernels to 31x31: Revisiting large kernel design in cnns",
      "id": "12500443856801763727",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Ding_Scaling_Up_Your_Kernels_to_31x31_Revisiting_Large_Kernel_Design_CVPR_2022_paper.html",
      "title": "Scaling up your kernels to 31x31: Revisiting large kernel design in cnns",
      "authors": "X Ding, X Zhang, J Han, G Ding",
      "year": "2022",
      "cited_by": 345,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12500443856801763727&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "id": "8140812159859442226",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/416f9cb3276121c42eebb86352a4354a-Abstract-Conference.html",
      "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "authors": "Z Tong, Y Song, J Wang\u2026",
      "year": "2022",
      "cited_by": 303,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8140812159859442226&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 22
    },
    {
      "label": "Swinir: Image restoration using swin transformer",
      "id": "15816136068893942524",
      "url": "https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Liang_SwinIR_Image_Restoration_Using_Swin_Transformer_ICCVW_2021_paper.html",
      "title": "Swinir: Image restoration using swin transformer",
      "authors": "J Liang, J Cao, G Sun, K Zhang\u2026",
      "year": "2021",
      "cited_by": 1245,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15816136068893942524&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "SwinFusion: Cross-domain long-range learning for general image fusion via swin transformer",
      "id": "9192461276991269473",
      "url": "https://ieeexplore.ieee.org/abstract/document/9812535/",
      "title": "SwinFusion: Cross-domain long-range learning for general image fusion via swin transformer",
      "authors": "J Ma, L Tang, F Fan, J Huang, X Mei\u2026",
      "year": "2022",
      "cited_by": 199,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9192461276991269473&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Self-supervised pre-training of swin transformers for 3d medical image analysis",
      "id": "17810188272131878569",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Tang_Self-Supervised_Pre-Training_of_Swin_Transformers_for_3D_Medical_Image_Analysis_CVPR_2022_paper.html",
      "title": "Self-supervised pre-training of swin transformers for 3d medical image analysis",
      "authors": "Y Tang, D Yang, W Li, HR Roth\u2026",
      "year": "2022",
      "cited_by": 198,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17810188272131878569&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Adaptformer: Adapting vision transformers for scalable visual recognition",
      "id": "17752815312316743733",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/69e2f49ab0837b71b0e0cb7c555990f8-Abstract-Conference.html",
      "title": "Adaptformer: Adapting vision transformers for scalable visual recognition",
      "authors": "S Chen, C Ge, Z Tong, J Wang\u2026",
      "year": "2022",
      "cited_by": 103,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17752815312316743733&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Beit: Bert pre-training of image transformers",
      "id": "4802045854930781683",
      "url": "https://arxiv.org/abs/2106.08254",
      "title": "Beit: Bert pre-training of image transformers",
      "authors": "H Bao, L Dong, S Piao, F Wei",
      "year": "2021",
      "cited_by": 1458,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4802045854930781683&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Masked autoencoders are scalable vision learners",
      "id": "16837829726140559426",
      "url": "https://openaccess.thecvf.com/content/CVPR2022/html/He_Masked_Autoencoders_Are_Scalable_Vision_Learners_CVPR_2022_paper",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": "K He, X Chen, S Xie, Y Li, P Doll\u00e1r\u2026",
      "year": "2022",
      "cited_by": 2930,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16837829726140559426&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Video swin transformer",
      "id": "5833041667751260373",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Liu_Video_Swin_Transformer_CVPR_2022_paper.html",
      "title": "Video swin transformer",
      "authors": "Z Liu, J Ning, Y Cao, Y Wei, Z Zhang\u2026",
      "year": "2022",
      "cited_by": 936,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5833041667751260373&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Do vision transformers see like convolutional neural networks?",
      "id": "1018521690946850362",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2021/hash/652cf38361a209088302ba2b8b7f51e0-Abstract.html",
      "title": "Do vision transformers see like convolutional neural networks?",
      "authors": "M Raghu, T Unterthiner, S Kornblith\u2026",
      "year": "2021",
      "cited_by": 521,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1018521690946850362&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 0
    },
    {
      "label": "Transforming medical imaging with Transformers? A comparative review of key properties, current...",
      "id": "6243645967630982889",
      "url": "https://www.sciencedirect.com/science/article/pii/S1361841523000233",
      "title": "Transforming medical imaging with Transformers? A comparative review of key properties, current...",
      "authors": "J Li, J Chen, Y Tang, C Wang, BA Landman\u2026",
      "year": "2023",
      "cited_by": 56,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6243645967630982889&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 17
    },
    {
      "label": "Artificial intelligence for multimodal data integration in oncology",
      "id": "3936750546972103836",
      "url": "https://www.cell.com/cancer-cell/pdf/S1535-6108(22)00441-X.pdf",
      "title": "Artificial intelligence for multimodal data integration in oncology",
      "authors": "J Lipkova, RJ Chen, B Chen, MY Lu, M Barbieri\u2026",
      "year": "2022",
      "cited_by": 66,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3936750546972103836&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 17
    },
    {
      "label": "Vision Transformers for Computational Histopathology",
      "id": "11927510402360104647",
      "url": "https://ieeexplore.ieee.org/abstract/document/10190115/",
      "title": "Vision Transformers for Computational Histopathology",
      "authors": "H Xu, Q Xu, F Cong, J Kang, C Han\u2026",
      "year": "2023",
      "cited_by": 3,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11927510402360104647&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 17
    },
    {
      "label": "Delving into masked autoencoders for multi-label thorax disease classification",
      "id": "6187038158217452627",
      "url": "https://openaccess.thecvf.com/content/WACV2023/html/Xiao_Delving_Into_Masked_Autoencoders_for_Multi-Label_Thorax_Disease_Classification_WACV_2023_paper.html",
      "title": "Delving into masked autoencoders for multi-label thorax disease classification",
      "authors": "J Xiao, Y Bai, A Yuille, Z Zhou",
      "year": "2023",
      "cited_by": 12,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6187038158217452627&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 17
    },
    {
      "label": "UNesT: local spatial representation learning with hierarchical transformer for efficient medical segmentation",
      "id": "16029033897139067670",
      "url": "https://www.sciencedirect.com/science/article/pii/S1361841523001998",
      "title": "UNesT: local spatial representation learning with hierarchical transformer for efficient medical segmentation",
      "authors": "X Yu, Q Yang, Y Zhou, LY Cai, R Gao, HH Lee, T Li\u2026",
      "year": "2023",
      "cited_by": 7,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16029033897139067670&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 17
    },
    {
      "label": "ReconFormer: Accelerated MRI reconstruction using recurrent transformer",
      "id": "381090561678004553",
      "url": "https://ieeexplore.ieee.org/abstract/document/10251064/",
      "title": "ReconFormer: Accelerated MRI reconstruction using recurrent transformer",
      "authors": "P Guo, Y Mei, J Zhou, S Jiang\u2026",
      "year": "2023",
      "cited_by": 17,
      "cited_by_url": "https://scholar.google.com/scholar?cites=381090561678004553&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 17
    },
    {
      "label": "AIROGS: Artificial Intelligence for robust glaucoma screening challenge",
      "id": "12607038868340347612",
      "url": "https://ieeexplore.ieee.org/abstract/document/10253652/",
      "title": "AIROGS: Artificial Intelligence for robust glaucoma screening challenge",
      "authors": "C De Vente, KA Vermeer, N Jaccard\u2026",
      "year": "2023",
      "cited_by": 5,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12607038868340347612&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 17
    },
    {
      "label": "A review of machine learning applications for the proton MR spectroscopy workflow",
      "id": "4239598804369546383",
      "url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.29793",
      "title": "A review of machine learning applications for the proton MR spectroscopy workflow",
      "authors": "DMJ van de Sande, JP Merkofer\u2026",
      "year": "2023",
      "cited_by": 2,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4239598804369546383&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 17
    },
    {
      "label": "Recent progress in transformer-based medical image analysis",
      "id": "681965397596551869",
      "url": "https://www.sciencedirect.com/science/article/pii/S0010482523007333",
      "title": "Recent progress in transformer-based medical image analysis",
      "authors": "Z Liu, Q Lv, Z Yang, Y Li, CH Lee, L Shen",
      "year": "2023",
      "cited_by": 2,
      "cited_by_url": "https://scholar.google.com/scholar?cites=681965397596551869&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 17
    },
    {
      "label": "CTUNet: automatic pancreas segmentation using a channel-wise transformer and 3D U-Net",
      "id": "8582598101542675430",
      "url": "https://link.springer.com/article/10.1007/s00371-022-02656-2",
      "title": "CTUNet: automatic pancreas segmentation using a channel-wise transformer and 3D U-Net",
      "authors": "L Chen, L Wan",
      "year": "2022",
      "cited_by": 5,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8582598101542675430&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 17
    },
    {
      "label": "Vision transformers in medical imaging: A review",
      "id": "10060803768210082111",
      "url": "https://arxiv.org/abs/2211.10043",
      "title": "Vision transformers in medical imaging: A review",
      "authors": "EU Henry, O Emebob, CA Omonhinmin",
      "year": "2022",
      "cited_by": 8,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10060803768210082111&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 17
    },
    {
      "label": "Segment anything",
      "id": "15741444728855576863",
      "url": "https://arxiv.org/abs/2304.02643",
      "title": "Segment anything",
      "authors": "A Kirillov, E Mintun, N Ravi, H Mao, C Rolland\u2026",
      "year": "2023",
      "cited_by": 650,
      "cited_by_url": "https://scholar.google.com/scholar?cites=15741444728855576863&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 18
    },
    {
      "label": "On the opportunities and challenges of foundation models for geospatial artificial intelligence",
      "id": "8437346401990005173",
      "url": "https://arxiv.org/abs/2304.06798",
      "title": "On the opportunities and challenges of foundation models for geospatial artificial intelligence",
      "authors": "G Mai, W Huang, J Sun, S Song, D Mishra\u2026",
      "year": "2023",
      "cited_by": 30,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8437346401990005173&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 18
    },
    {
      "label": "Large ai models in health informatics: Applications, challenges, and the future",
      "id": "2958442793928445860",
      "url": "https://ieeexplore.ieee.org/abstract/document/10261199/",
      "title": "Large ai models in health informatics: Applications, challenges, and the future",
      "authors": "J Qiu, L Li, J Sun, J Peng, P Shi\u2026",
      "year": "2023",
      "cited_by": 11,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2958442793928445860&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 18
    },
    {
      "label": "Combined scaling for zero-shot transfer learning",
      "id": "14444698797987128655",
      "url": "https://www.sciencedirect.com/science/article/pii/S0925231223007816",
      "title": "Combined scaling for zero-shot transfer learning",
      "authors": "H Pham, Z Dai, G Ghiasi, K Kawaguchi, H Liu, AW Yu\u2026",
      "year": "2023",
      "cited_by": 82,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14444698797987128655&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 18
    },
    {
      "label": "Visual instruction tuning",
      "id": "9083483030705185424",
      "url": "https://arxiv.org/abs/2304.08485",
      "title": "Visual instruction tuning",
      "authors": "H Liu, C Li, Q Wu, YJ Lee",
      "year": "2023",
      "cited_by": 167,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9083483030705185424&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 18
    },
    {
      "label": "Segment anything model for medical image analysis: an experimental study",
      "id": "640071900436442539",
      "url": "https://www.sciencedirect.com/science/article/pii/S1361841523001780",
      "title": "Segment anything model for medical image analysis: an experimental study",
      "authors": "MA Mazurowski, H Dong, H Gu, J Yang, N Konz\u2026",
      "year": "2023",
      "cited_by": 28,
      "cited_by_url": "https://scholar.google.com/scholar?cites=640071900436442539&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 18
    },
    {
      "label": "Segment anything in medical images",
      "id": "1608743531206453804",
      "url": "https://arxiv.org/abs/2304.12306",
      "title": "Segment anything in medical images",
      "authors": "J Ma, B Wang",
      "year": "2023",
      "cited_by": 98,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1608743531206453804&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 18
    },
    {
      "label": "Can sam segment anything? when sam meets camouflaged object detection",
      "id": "10542319093857998430",
      "url": "https://arxiv.org/abs/2304.04709",
      "title": "Can sam segment anything? when sam meets camouflaged object detection",
      "authors": "L Tang, H Xiao, B Li",
      "year": "2023",
      "cited_by": 30,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10542319093857998430&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 18
    },
    {
      "label": "Accuracy of segment-anything model (sam) in medical image segmentation tasks",
      "id": "11173768905758826676",
      "url": "https://arxiv.org/abs/2304.09324",
      "title": "Accuracy of segment-anything model (sam) in medical image segmentation tasks",
      "authors": "S He, R Bao, J Li, PE Grant, Y Ou",
      "year": "2023",
      "cited_by": 44,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11173768905758826676&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 18
    },
    {
      "label": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
      "id": "4360518173578415769",
      "url": "https://arxiv.org/abs/2307.15818",
      "title": "Rt-2: Vision-language-action models transfer web knowledge to robotic control",
      "authors": "A Brohan, N Brown, J Carbajal, Y Chebotar\u2026",
      "year": "2023",
      "cited_by": 18,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4360518173578415769&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 18
    },
    {
      "label": "Segment anything model (sam) for digital pathology: Assess zero-shot segmentation on whole slide imaging",
      "id": "4884243151249440145",
      "url": "https://arxiv.org/abs/2304.04155",
      "title": "Segment anything model (sam) for digital pathology: Assess zero-shot segmentation on whole slide imaging",
      "authors": "R Deng, C Cui, Q Liu, T Yao, LW Remedios\u2026",
      "year": "2023",
      "cited_by": 55,
      "cited_by_url": "https://scholar.google.com/scholar?cites=4884243151249440145&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 18
    },
    {
      "label": "Reproducible scaling laws for contrastive language-image learning",
      "id": "12973370542610650225",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Cherti_Reproducible_Scaling_Laws_for_Contrastive_Language-Image_Learning_CVPR_2023_paper.html",
      "title": "Reproducible scaling laws for contrastive language-image learning",
      "authors": "M Cherti, R Beaumont, R Wightman\u2026",
      "year": "2023",
      "cited_by": 68,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12973370542610650225&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "id": "1879282532294332322",
      "url": "https://arxiv.org/abs/2304.10592",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": "D Zhu, J Chen, X Shen, X Li, M Elhoseiny",
      "year": "2023",
      "cited_by": 155,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1879282532294332322&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Are aligned neural networks adversarially aligned?",
      "id": "3768131676399480172",
      "url": "https://arxiv.org/abs/2306.15447",
      "title": "Are aligned neural networks adversarially aligned?",
      "authors": "N Carlini, M Nasr, CA Choquette-Choo\u2026",
      "year": "2023",
      "cited_by": 17,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3768131676399480172&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Eva-clip: Improved training techniques for clip at scale",
      "id": "2978288424037265447",
      "url": "https://arxiv.org/abs/2303.15389",
      "title": "Eva-clip: Improved training techniques for clip at scale",
      "authors": "Q Sun, Y Fang, L Wu, X Wang, Y Cao",
      "year": "2023",
      "cited_by": 26,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2978288424037265447&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "A cookbook of self-supervised learning",
      "id": "9021031822777383648",
      "url": "https://arxiv.org/abs/2304.12210",
      "title": "A cookbook of self-supervised learning",
      "authors": "R Balestriero, M Ibrahim, V Sobal, A Morcos\u2026",
      "year": "2023",
      "cited_by": 40,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9021031822777383648&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Visionllm: Large language model is also an open-ended decoder for vision-centric tasks",
      "id": "7929876691840755441",
      "url": "https://arxiv.org/abs/2305.11175",
      "title": "Visionllm: Large language model is also an open-ended decoder for vision-centric tasks",
      "authors": "W Wang, Z Chen, X Chen, J Wu, X Zhu, G Zeng\u2026",
      "year": "2023",
      "cited_by": 24,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7929876691840755441&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "End-to-end autonomous driving: Challenges and frontiers",
      "id": "3373629156419038558",
      "url": "https://arxiv.org/abs/2306.16927",
      "title": "End-to-end autonomous driving: Challenges and frontiers",
      "authors": "L Chen, P Wu, K Chitta, B Jaeger, A Geiger\u2026",
      "year": "2023",
      "cited_by": 8,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3373629156419038558&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Foundational models defining a new era in vision: A survey and outlook",
      "id": "9284111923828977279",
      "url": "https://arxiv.org/abs/2307.13721",
      "title": "Foundational models defining a new era in vision: A survey and outlook",
      "authors": "M Awais, M Naseer, S Khan, RM Anwer\u2026",
      "year": "2023",
      "cited_by": 2,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9284111923828977279&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision",
      "id": "8586493250682863612",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Yang_BEVFormer_v2_Adapting_Modern_Image_Backbones_to_Birds-Eye-View_Recognition_via_CVPR_2023_paper.html",
      "title": "BEVFormer v2: Adapting Modern Image Backbones to Bird's-Eye-View Recognition via Perspective Supervision",
      "authors": "C Yang, Y Chen, H Tian, C Tao, X Zhu\u2026",
      "year": "2023",
      "cited_by": 29,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8586493250682863612&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Towards all-in-one pre-training via maximizing multi-modal mutual information",
      "id": "5337226914153811562",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Su_Towards_All-in-One_Pre-Training_via_Maximizing_Multi-Modal_Mutual_Information_CVPR_2023_paper.html",
      "title": "Towards all-in-one pre-training via maximizing multi-modal mutual information",
      "authors": "W Su, X Zhu, C Tao, L Lu, B Li\u2026",
      "year": "2023",
      "cited_by": 12,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5337226914153811562&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Videochat: Chat-centric video understanding",
      "id": "5343811397150749551",
      "url": "https://arxiv.org/abs/2305.06355",
      "title": "Videochat: Chat-centric video understanding",
      "authors": "KC Li, Y He, Y Wang, Y Li, W Wang, P Luo\u2026",
      "year": "2023",
      "cited_by": 30,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5343811397150749551&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Dinov2: Learning robust visual features without supervision",
      "id": "3683089856994200319",
      "url": "https://arxiv.org/abs/2304.07193",
      "title": "Dinov2: Learning robust visual features without supervision",
      "authors": "M Oquab, T Darcet, T Moutakanni, H Vo\u2026",
      "year": "2023",
      "cited_by": 52,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3683089856994200319&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Designing bert for convolutional networks: Sparse and hierarchical masked modeling",
      "id": "8072796476168839940",
      "url": "https://arxiv.org/abs/2301.03580",
      "title": "Designing bert for convolutional networks: Sparse and hierarchical masked modeling",
      "authors": "K Tian, Y Jiang, Q Diao, C Lin, L Wang\u2026",
      "year": "2023",
      "cited_by": 20,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8072796476168839940&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Internchat: Solving vision-centric tasks by interacting with chatbots beyond language",
      "id": "11224588046887428191",
      "url": "https://arxiv.org/abs/2305.05662",
      "title": "Internchat: Solving vision-centric tasks by interacting with chatbots beyond language",
      "authors": "Z Liu, Y He, W Wang, W Wang, Y Wang, S Chen\u2026",
      "year": "2023",
      "cited_by": 12,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11224588046887428191&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Mixformer: End-to-end tracking with iterative mixed attention",
      "id": "17997891498068622933",
      "url": "http://openaccess.thecvf.com/content/CVPR2022/html/Cui_MixFormer_End-to-End_Tracking_With_Iterative_Mixed_Attention_CVPR_2022_paper.html",
      "title": "Mixformer: End-to-end tracking with iterative mixed attention",
      "authors": "Y Cui, C Jiang, L Wang, G Wu",
      "year": "2022",
      "cited_by": 158,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17997891498068622933&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 22
    },
    {
      "label": "Swintrack: A simple and strong baseline for transformer tracking",
      "id": "5167657309745527366",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/6a5c23219f401f3efd322579002dbb80-Abstract-Conference.html",
      "title": "Swintrack: A simple and strong baseline for transformer tracking",
      "authors": "L Lin, H Fan, Z Zhang, Y Xu\u2026",
      "year": "2022",
      "cited_by": 120,
      "cited_by_url": "https://scholar.google.com/scholar?cites=5167657309745527366&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 22
    },
    {
      "label": "Revealing the dark secrets of masked image modeling",
      "id": "10341392145675802868",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Xie_Revealing_the_Dark_Secrets_of_Masked_Image_Modeling_CVPR_2023_paper.html",
      "title": "Revealing the dark secrets of masked image modeling",
      "authors": "Z Xie, Z Geng, J Hu, Z Zhang\u2026",
      "year": "2023",
      "cited_by": 42,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10341392145675802868&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 22
    },
    {
      "label": "Universal instance perception as object discovery and retrieval",
      "id": "17928092202816562244",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Yan_Universal_Instance_Perception_As_Object_Discovery_and_Retrieval_CVPR_2023_paper.html",
      "title": "Universal instance perception as object discovery and retrieval",
      "authors": "B Yan, Y Jiang, J Wu, D Wang, P Luo\u2026",
      "year": "2023",
      "cited_by": 13,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17928092202816562244&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 22
    },
    {
      "label": "Decoupling features in hierarchical propagation for video object segmentation",
      "id": "9093499936003644917",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/eb890c36af87e4ca82e8ef7bcba6a284-Abstract-Conference.html",
      "title": "Decoupling features in hierarchical propagation for video object segmentation",
      "authors": "Z Yang, Y Yang",
      "year": "2022",
      "cited_by": 28,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9093499936003644917&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 22
    },
    {
      "label": "Divert more attention to vision-language tracking",
      "id": "6209180784126725956",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/1c8c87c36dc1e49e63555f95fa56b153-Abstract-Conference.html",
      "title": "Divert more attention to vision-language tracking",
      "authors": "M Guo, Z Zhang, H Fan, L Jing",
      "year": "2022",
      "cited_by": 8,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6209180784126725956&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 22
    },
    {
      "label": "Correlational Image Modeling for Self-Supervised Visual Pre-Training",
      "id": "14246245970329145306",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Li_Correlational_Image_Modeling_for_Self-Supervised_Visual_Pre-Training_CVPR_2023_paper.html",
      "title": "Correlational Image Modeling for Self-Supervised Visual Pre-Training",
      "authors": "W Li, J Xie, CC Loy",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14246245970329145306&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 22
    },
    {
      "label": "Fully convolutional online tracking",
      "id": "17211572998346535120",
      "url": "https://www.sciencedirect.com/science/article/pii/S1077314222001254",
      "title": "Fully convolutional online tracking",
      "authors": "Y Cui, C Jiang, L Wang, G Wu",
      "year": "2022",
      "cited_by": 41,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17211572998346535120&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 22
    },
    {
      "label": "A Unified Transformer Based Tracker for Anti-UAV Tracking",
      "id": "9686322700146945720",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/Anti-UAV/html/Yu_A_Unified_Transformer_Based_Tracker_for_Anti-UAV_Tracking_CVPRW_2023_paper.html",
      "title": "A Unified Transformer Based Tracker for Anti-UAV Tracking",
      "authors": "Q Yu, Y Ma, J He, D Yang\u2026",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9686322700146945720&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 22
    },
    {
      "label": "Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond",
      "id": "2022354001457069046",
      "url": "https://link.springer.com/article/10.1007/s11263-022-01739-w",
      "title": "Vitaev2: Vision transformer advanced by exploring inductive bias for image recognition and beyond",
      "authors": "Q Zhang, Y Xu, J Zhang, D Tao",
      "year": "2023",
      "cited_by": 116,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2022354001457069046&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 16
    },
    {
      "label": "Advancing plain vision transformer toward remote sensing foundation model",
      "id": "14613261815949086918",
      "url": "https://ieeexplore.ieee.org/abstract/document/9956816/",
      "title": "Advancing plain vision transformer toward remote sensing foundation model",
      "authors": "D Wang, Q Zhang, Y Xu, J Zhang, B Du\u2026",
      "year": "2022",
      "cited_by": 46,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14613261815949086918&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 16
    },
    {
      "label": "Decoupling human and camera motion from videos in the wild",
      "id": "6979440938086217563",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Ye_Decoupling_Human_and_Camera_Motion_From_Videos_in_the_Wild_CVPR_2023_paper.html",
      "title": "Decoupling human and camera motion from videos in the wild",
      "authors": "V Ye, G Pavlakos, J Malik\u2026",
      "year": "2023",
      "cited_by": 14,
      "cited_by_url": "https://scholar.google.com/scholar?cites=6979440938086217563&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 16
    },
    {
      "label": "Vsa: Learning varied-size window attention in vision transformers",
      "id": "7134900495559356797",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-19806-9_27",
      "title": "Vsa: Learning varied-size window attention in vision transformers",
      "authors": "Q Zhang, Y Xu, J Zhang, D Tao",
      "year": "2022",
      "cited_by": 36,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7134900495559356797&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 16
    },
    {
      "label": "TRACE: 5D temporal regression of avatars with dynamic cameras in 3D environments",
      "id": "8918294861982063782",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Sun_TRACE_5D_Temporal_Regression_of_Avatars_With_Dynamic_Cameras_in_CVPR_2023_paper.html",
      "title": "TRACE: 5D temporal regression of avatars with dynamic cameras in 3D environments",
      "authors": "Y Sun, Q Bao, W Liu, T Mei\u2026",
      "year": "2023",
      "cited_by": 5,
      "cited_by_url": "https://scholar.google.com/scholar?cites=8918294861982063782&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 16
    },
    {
      "label": "Regioncl: exploring contrastive region pairs for self-supervised representation learning",
      "id": "9002295544688722648",
      "url": "https://link.springer.com/chapter/10.1007/978-3-031-19827-4_28",
      "title": "Regioncl: exploring contrastive region pairs for self-supervised representation learning",
      "authors": "Y Xu, Q Zhang, J Zhang, D Tao",
      "year": "2022",
      "cited_by": 5,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9002295544688722648&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 16
    },
    {
      "label": "Unsupervised volumetric animation",
      "id": "530860336847622267",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Siarohin_Unsupervised_Volumetric_Animation_CVPR_2023_paper.html",
      "title": "Unsupervised volumetric animation",
      "authors": "A Siarohin, W Menapace\u2026",
      "year": "2023",
      "cited_by": 6,
      "cited_by_url": "https://scholar.google.com/scholar?cites=530860336847622267&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 16
    },
    {
      "label": "Vision-based human pose estimation via deep learning: A survey",
      "id": "10626359895994634728",
      "url": "https://ieeexplore.ieee.org/abstract/document/9955393/",
      "title": "Vision-based human pose estimation via deep learning: A survey",
      "authors": "G Lan, Y Wu, F Hu, Q Hao",
      "year": "2022",
      "cited_by": 8,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10626359895994634728&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 16
    },
    {
      "label": "Expediting large-scale vision transformer for dense prediction without fine-tuning",
      "id": "3429308917664707403",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/e6c2e85db1f1039177c4495ccd399ac4-Abstract-Conference.html",
      "title": "Expediting large-scale vision transformer for dense prediction without fine-tuning",
      "authors": "W Liang, Y Yuan, H Ding, X Luo\u2026",
      "year": "2022",
      "cited_by": 5,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3429308917664707403&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 16
    },
    {
      "label": "HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining",
      "id": "16223923155940056199",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Tang_HumanBench_Towards_General_Human-Centric_Perception_With_Projector_Assisted_Pretraining_CVPR_2023_paper.html",
      "title": "HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining",
      "authors": "S Tang, C Chen, Q Xie, M Chen\u2026",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16223923155940056199&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 16
    },
    {
      "label": "Vision transformer adapter for dense predictions",
      "id": "14316138733173377966",
      "url": "https://arxiv.org/abs/2205.08534",
      "title": "Vision transformer adapter for dense predictions",
      "authors": "Z Chen, Y Duan, W Wang, J He, T Lu, J Dai\u2026",
      "year": "2022",
      "cited_by": 161,
      "cited_by_url": "https://scholar.google.com/scholar?cites=14316138733173377966&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Fast vision transformers with hilo attention",
      "id": "3584347529323049886",
      "url": "https://proceedings.neurips.cc/paper_files/paper/2022/hash/5d5f703ee1dedbfe324b1872f44db939-Abstract-Conference.html",
      "title": "Fast vision transformers with hilo attention",
      "authors": "Z Pan, J Cai, B Zhuang",
      "year": "2022",
      "cited_by": 48,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3584347529323049886&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Metaformer baselines for vision",
      "id": "17264780080323807782",
      "url": "https://arxiv.org/abs/2210.13452",
      "title": "Metaformer baselines for vision",
      "authors": "W Yu, C Si, P Zhou, M Luo, Y Zhou, J Feng\u2026",
      "year": "2022",
      "cited_by": 22,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17264780080323807782&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?",
      "id": "16657130336390881372",
      "url": "https://arxiv.org/abs/2212.08320",
      "title": "Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?",
      "authors": "R Dong, Z Qi, L Zhang, J Zhang, J Sun, Z Ge\u2026",
      "year": "2022",
      "cited_by": 19,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16657130336390881372&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Conv2former: A simple transformer-style convnet for visual recognition",
      "id": "16345516296958117809",
      "url": "https://arxiv.org/abs/2211.11943",
      "title": "Conv2former: A simple transformer-style convnet for visual recognition",
      "authors": "Q Hou, CZ Lu, MM Cheng, J Feng",
      "year": "2022",
      "cited_by": 21,
      "cited_by_url": "https://scholar.google.com/scholar?cites=16345516296958117809&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Insulator-defect detection algorithm based on improved YOLOv7",
      "id": "12345969996209231071",
      "url": "https://www.mdpi.com/1424-8220/22/22/8801",
      "title": "Insulator-defect detection algorithm based on improved YOLOv7",
      "authors": "J Zheng, H Wu, H Zhang, Z Wang, W Xu",
      "year": "2022",
      "cited_by": 15,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12345969996209231071&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Demystify transformers & convolutions in modern image deep networks",
      "id": "11904702717564235743",
      "url": "https://arxiv.org/abs/2211.05781",
      "title": "Demystify transformers & convolutions in modern image deep networks",
      "authors": "J Dai, M Shi, W Wang, S Wu, L Xing, W Wang\u2026",
      "year": "2022",
      "cited_by": 5,
      "cited_by_url": "https://scholar.google.com/scholar?cites=11904702717564235743&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Generalized decoding for pixel, image, and language",
      "id": "9556534593478071448",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zou_Generalized_Decoding_for_Pixel_Image_and_Language_CVPR_2023_paper.html",
      "title": "Generalized decoding for pixel, image, and language",
      "authors": "X Zou, ZY Dou, J Yang, Z Gan, L Li\u2026",
      "year": "2023",
      "cited_by": 35,
      "cited_by_url": "https://scholar.google.com/scholar?cites=9556534593478071448&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Open vocabulary semantic segmentation with patch aligned contrastive learning",
      "id": "17595429764835777254",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Mukhoti_Open_Vocabulary_Semantic_Segmentation_With_Patch_Aligned_Contrastive_Learning_CVPR_2023_paper.html",
      "title": "Open vocabulary semantic segmentation with patch aligned contrastive learning",
      "authors": "J Mukhoti, TY Lin, O Poursaeed\u2026",
      "year": "2023",
      "cited_by": 11,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17595429764835777254&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Balancing Logit Variation for Long-tailed Semantic Segmentation",
      "id": "2341964736485396655",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Wang_Balancing_Logit_Variation_for_Long-Tailed_Semantic_Segmentation_CVPR_2023_paper.html",
      "title": "Balancing Logit Variation for Long-tailed Semantic Segmentation",
      "authors": "Y Wang, J Fei, H Wang, W Li, T Bao\u2026",
      "year": "2023",
      "cited_by": 5,
      "cited_by_url": "https://scholar.google.com/scholar?cites=2341964736485396655&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    },
    {
      "label": "Deepsolo: Let transformer decoder with explicit points solo for text spotting",
      "id": "12388611019049432202",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Ye_DeepSolo_Let_Transformer_Decoder_With_Explicit_Points_Solo_for_Text_CVPR_2023_paper.html",
      "title": "Deepsolo: Let transformer decoder with explicit points solo for text spotting",
      "authors": "M Ye, J Zhang, S Zhao, J Liu, T Liu\u2026",
      "year": "2023",
      "cited_by": 11,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12388611019049432202&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 19
    },
    {
      "label": "Dense Distinct Query for End-to-End Object Detection",
      "id": "1436259968817456407",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Zhang_Dense_Distinct_Query_for_End-to-End_Object_Detection_CVPR_2023_paper.html",
      "title": "Dense Distinct Query for End-to-End Object Detection",
      "authors": "S Zhang, X Wang, J Wang, J Pang\u2026",
      "year": "2023",
      "cited_by": 5,
      "cited_by_url": "https://scholar.google.com/scholar?cites=1436259968817456407&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 19
    },
    {
      "label": "Lite DETR: An interleaved multi-scale encoder for efficient detr",
      "id": "10221363205500346797",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Li_Lite_DETR_An_Interleaved_Multi-Scale_Encoder_for_Efficient_DETR_CVPR_2023_paper.html",
      "title": "Lite DETR: An interleaved multi-scale encoder for efficient detr",
      "authors": "F Li, A Zeng, S Liu, H Zhang, H Li\u2026",
      "year": "2023",
      "cited_by": 10,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10221363205500346797&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 19
    },
    {
      "label": "Detrs with collaborative hybrid assignments training",
      "id": "18387677115510147109",
      "url": "https://arxiv.org/abs/2211.12860",
      "title": "Detrs with collaborative hybrid assignments training",
      "authors": "Z Zong, G Song, Y Liu",
      "year": "2022",
      "cited_by": 19,
      "cited_by_url": "https://scholar.google.com/scholar?cites=18387677115510147109&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 19
    },
    {
      "label": "Transformer-based visual segmentation: A survey",
      "id": "10401914588824198986",
      "url": "https://arxiv.org/abs/2304.09854",
      "title": "Transformer-based visual segmentation: A survey",
      "authors": "X Li, H Ding, W Zhang, H Yuan, J Pang\u2026",
      "year": "2023",
      "cited_by": 17,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10401914588824198986&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 19
    },
    {
      "label": "Enhanced Training of Query-Based Object Detection via Selective Query Recollection",
      "id": "10714034727295496106",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_Enhanced_Training_of_Query-Based_Object_Detection_via_Selective_Query_Recollection_CVPR_2023_paper.html",
      "title": "Enhanced Training of Query-Based Object Detection via Selective Query Recollection",
      "authors": "F Chen, H Zhang, K Hu, YK Huang\u2026",
      "year": "2023",
      "cited_by": 4,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10714034727295496106&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 19
    },
    {
      "label": "One-to-Few Label Assignment for End-to-End Dense Detection",
      "id": "7090138214741207408",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Li_One-to-Few_Label_Assignment_for_End-to-End_Dense_Detection_CVPR_2023_paper.html",
      "title": "One-to-Few Label Assignment for End-to-End Dense Detection",
      "authors": "S Li, M Li, R Li, C He, L Zhang",
      "year": "2023",
      "cited_by": 1,
      "cited_by_url": "https://scholar.google.com/scholar?cites=7090138214741207408&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 19
    },
    {
      "label": "Nms strikes back",
      "id": "10928542084451319450",
      "url": "https://arxiv.org/abs/2212.06137",
      "title": "Nms strikes back",
      "authors": "J Ouyang-Zhang, JH Cho, X Zhou\u2026",
      "year": "2022",
      "cited_by": 15,
      "cited_by_url": "https://scholar.google.com/scholar?cites=10928542084451319450&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 19
    },
    {
      "label": "End-to-end 3d dense captioning with vote2cap-detr",
      "id": "17976854884426501420",
      "url": "http://openaccess.thecvf.com/content/CVPR2023/html/Chen_End-to-End_3D_Dense_Captioning_With_Vote2Cap-DETR_CVPR_2023_paper.html",
      "title": "End-to-end 3d dense captioning with vote2cap-detr",
      "authors": "S Chen, H Zhu, X Chen, Y Lei\u2026",
      "year": "2023",
      "cited_by": 5,
      "cited_by_url": "https://scholar.google.com/scholar?cites=17976854884426501420&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 19
    },
    {
      "label": "D\nETR: Decoder Distillation for Detection Transformer",
      "id": "3308763779023912371",
      "url": "https://arxiv.org/abs/2211.09768",
      "title": "D\nETR: Decoder Distillation for Detection Transformer",
      "authors": "X Chen, J Chen, Y Liu, G Zeng",
      "year": "2022",
      "cited_by": 6,
      "cited_by_url": "https://scholar.google.com/scholar?cites=3308763779023912371&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 19
    },
    {
      "label": "Scaled-yolov4: Scaling cross stage partial network",
      "id": "12563424232401784595",
      "url": "https://openaccess.thecvf.com/content/CVPR2021/html/Wang_Scaled-YOLOv4_Scaling_Cross_Stage_Partial_Network_CVPR_2021_paper.html?ref=",
      "title": "Scaled-yolov4: Scaling cross stage partial network",
      "authors": "CY Wang, A Bochkovskiy\u2026",
      "year": "2021",
      "cited_by": 1143,
      "cited_by_url": "https://scholar.google.com/scholar?cites=12563424232401784595&as_sdt=40005&sciodt=0,10&hl=en",
      "modularity": 1
    }
  ],
  "links": [
    {
      "source": 0,
      "target": 1
    },
    {
      "source": 0,
      "target": 23
    },
    {
      "source": 0,
      "target": 33
    },
    {
      "source": 0,
      "target": 51
    },
    {
      "source": 0,
      "target": 60
    },
    {
      "source": 0,
      "target": 133
    },
    {
      "source": 0,
      "target": 355
    },
    {
      "source": 0,
      "target": 361
    },
    {
      "source": 0,
      "target": 364
    },
    {
      "source": 0,
      "target": 134
    },
    {
      "source": 0,
      "target": 52
    },
    {
      "source": 2,
      "target": 0
    },
    {
      "source": 2,
      "target": 33
    },
    {
      "source": 2,
      "target": 133
    },
    {
      "source": 2,
      "target": 349
    },
    {
      "source": 2,
      "target": 355
    },
    {
      "source": 2,
      "target": 361
    },
    {
      "source": 2,
      "target": 134
    },
    {
      "source": 2,
      "target": 52
    },
    {
      "source": 3,
      "target": 2
    },
    {
      "source": 3,
      "target": 13
    },
    {
      "source": 3,
      "target": 60
    },
    {
      "source": 4,
      "target": 2
    },
    {
      "source": 5,
      "target": 2
    },
    {
      "source": 5,
      "target": 33
    },
    {
      "source": 5,
      "target": 349
    },
    {
      "source": 5,
      "target": 361
    },
    {
      "source": 6,
      "target": 2
    },
    {
      "source": 6,
      "target": 361
    },
    {
      "source": 7,
      "target": 2
    },
    {
      "source": 7,
      "target": 297
    },
    {
      "source": 8,
      "target": 2
    },
    {
      "source": 9,
      "target": 2
    },
    {
      "source": 10,
      "target": 2
    },
    {
      "source": 11,
      "target": 2
    },
    {
      "source": 11,
      "target": 297
    },
    {
      "source": 12,
      "target": 2
    },
    {
      "source": 13,
      "target": 0
    },
    {
      "source": 14,
      "target": 13
    },
    {
      "source": 15,
      "target": 13
    },
    {
      "source": 16,
      "target": 13
    },
    {
      "source": 16,
      "target": 51
    },
    {
      "source": 17,
      "target": 13
    },
    {
      "source": 17,
      "target": 80
    },
    {
      "source": 18,
      "target": 13
    },
    {
      "source": 19,
      "target": 13
    },
    {
      "source": 20,
      "target": 13
    },
    {
      "source": 21,
      "target": 13
    },
    {
      "source": 22,
      "target": 13
    },
    {
      "source": 23,
      "target": 0
    },
    {
      "source": 23,
      "target": 41
    },
    {
      "source": 23,
      "target": 133
    },
    {
      "source": 23,
      "target": 361
    },
    {
      "source": 23,
      "target": 364
    },
    {
      "source": 23,
      "target": 134
    },
    {
      "source": 24,
      "target": 23
    },
    {
      "source": 24,
      "target": 364
    },
    {
      "source": 25,
      "target": 23
    },
    {
      "source": 25,
      "target": 364
    },
    {
      "source": 26,
      "target": 23
    },
    {
      "source": 27,
      "target": 23
    },
    {
      "source": 28,
      "target": 23
    },
    {
      "source": 29,
      "target": 23
    },
    {
      "source": 30,
      "target": 23
    },
    {
      "source": 31,
      "target": 23
    },
    {
      "source": 32,
      "target": 23
    },
    {
      "source": 33,
      "target": 0
    },
    {
      "source": 33,
      "target": 133
    },
    {
      "source": 34,
      "target": 33
    },
    {
      "source": 34,
      "target": 52
    },
    {
      "source": 35,
      "target": 33
    },
    {
      "source": 36,
      "target": 33
    },
    {
      "source": 36,
      "target": 123
    },
    {
      "source": 36,
      "target": 140
    },
    {
      "source": 37,
      "target": 33
    },
    {
      "source": 37,
      "target": 180
    },
    {
      "source": 38,
      "target": 33
    },
    {
      "source": 38,
      "target": 90
    },
    {
      "source": 38,
      "target": 361
    },
    {
      "source": 39,
      "target": 33
    },
    {
      "source": 40,
      "target": 33
    },
    {
      "source": 41,
      "target": 0
    },
    {
      "source": 42,
      "target": 41
    },
    {
      "source": 42,
      "target": 364
    },
    {
      "source": 43,
      "target": 41
    },
    {
      "source": 44,
      "target": 41
    },
    {
      "source": 45,
      "target": 41
    },
    {
      "source": 45,
      "target": 364
    },
    {
      "source": 46,
      "target": 41
    },
    {
      "source": 47,
      "target": 41
    },
    {
      "source": 48,
      "target": 41
    },
    {
      "source": 49,
      "target": 41
    },
    {
      "source": 50,
      "target": 41
    },
    {
      "source": 51,
      "target": 0
    },
    {
      "source": 52,
      "target": 51
    },
    {
      "source": 52,
      "target": 134
    },
    {
      "source": 52,
      "target": 355
    },
    {
      "source": 53,
      "target": 51
    },
    {
      "source": 54,
      "target": 51
    },
    {
      "source": 54,
      "target": 361
    },
    {
      "source": 55,
      "target": 51
    },
    {
      "source": 56,
      "target": 51
    },
    {
      "source": 57,
      "target": 51
    },
    {
      "source": 58,
      "target": 51
    },
    {
      "source": 59,
      "target": 51
    },
    {
      "source": 60,
      "target": 0
    },
    {
      "source": 61,
      "target": 60
    },
    {
      "source": 62,
      "target": 60
    },
    {
      "source": 63,
      "target": 60
    },
    {
      "source": 64,
      "target": 60
    },
    {
      "source": 65,
      "target": 60
    },
    {
      "source": 66,
      "target": 60
    },
    {
      "source": 67,
      "target": 60
    },
    {
      "source": 68,
      "target": 60
    },
    {
      "source": 69,
      "target": 0
    },
    {
      "source": 69,
      "target": 201
    },
    {
      "source": 69,
      "target": 297
    },
    {
      "source": 70,
      "target": 69
    },
    {
      "source": 70,
      "target": 183
    },
    {
      "source": 71,
      "target": 69
    },
    {
      "source": 72,
      "target": 69
    },
    {
      "source": 73,
      "target": 69
    },
    {
      "source": 74,
      "target": 69
    },
    {
      "source": 75,
      "target": 69
    },
    {
      "source": 76,
      "target": 69
    },
    {
      "source": 77,
      "target": 69
    },
    {
      "source": 77,
      "target": 312
    },
    {
      "source": 78,
      "target": 69
    },
    {
      "source": 79,
      "target": 69
    },
    {
      "source": 80,
      "target": 0
    },
    {
      "source": 81,
      "target": 80
    },
    {
      "source": 82,
      "target": 80
    },
    {
      "source": 83,
      "target": 80
    },
    {
      "source": 84,
      "target": 80
    },
    {
      "source": 85,
      "target": 80
    },
    {
      "source": 86,
      "target": 80
    },
    {
      "source": 87,
      "target": 80
    },
    {
      "source": 88,
      "target": 80
    },
    {
      "source": 88,
      "target": 92
    },
    {
      "source": 89,
      "target": 80
    },
    {
      "source": 90,
      "target": 0
    },
    {
      "source": 91,
      "target": 90
    },
    {
      "source": 91,
      "target": 304
    },
    {
      "source": 91,
      "target": 187
    },
    {
      "source": 91,
      "target": 92
    },
    {
      "source": 91,
      "target": 427
    },
    {
      "source": 92,
      "target": 90
    },
    {
      "source": 92,
      "target": 304
    },
    {
      "source": 92,
      "target": 408
    },
    {
      "source": 92,
      "target": 187
    },
    {
      "source": 92,
      "target": 427
    },
    {
      "source": 93,
      "target": 90
    },
    {
      "source": 94,
      "target": 90
    },
    {
      "source": 95,
      "target": 90
    },
    {
      "source": 96,
      "target": 90
    },
    {
      "source": 96,
      "target": 211
    },
    {
      "source": 97,
      "target": 90
    },
    {
      "source": 98,
      "target": 90
    },
    {
      "source": 99,
      "target": 90
    },
    {
      "source": 100,
      "target": 1
    },
    {
      "source": 101,
      "target": 100
    },
    {
      "source": 102,
      "target": 101
    },
    {
      "source": 103,
      "target": 101
    },
    {
      "source": 104,
      "target": 101
    },
    {
      "source": 105,
      "target": 101
    },
    {
      "source": 106,
      "target": 101
    },
    {
      "source": 107,
      "target": 101
    },
    {
      "source": 108,
      "target": 101
    },
    {
      "source": 109,
      "target": 101
    },
    {
      "source": 110,
      "target": 101
    },
    {
      "source": 111,
      "target": 101
    },
    {
      "source": 112,
      "target": 100
    },
    {
      "source": 113,
      "target": 112
    },
    {
      "source": 114,
      "target": 112
    },
    {
      "source": 115,
      "target": 112
    },
    {
      "source": 116,
      "target": 112
    },
    {
      "source": 117,
      "target": 112
    },
    {
      "source": 118,
      "target": 112
    },
    {
      "source": 119,
      "target": 112
    },
    {
      "source": 120,
      "target": 112
    },
    {
      "source": 121,
      "target": 112
    },
    {
      "source": 122,
      "target": 112
    },
    {
      "source": 123,
      "target": 100
    },
    {
      "source": 123,
      "target": 150
    },
    {
      "source": 124,
      "target": 123
    },
    {
      "source": 124,
      "target": 126
    },
    {
      "source": 125,
      "target": 123
    },
    {
      "source": 125,
      "target": 150
    },
    {
      "source": 125,
      "target": 126
    },
    {
      "source": 126,
      "target": 123
    },
    {
      "source": 126,
      "target": 355
    },
    {
      "source": 126,
      "target": 134
    },
    {
      "source": 127,
      "target": 123
    },
    {
      "source": 127,
      "target": 126
    },
    {
      "source": 128,
      "target": 123
    },
    {
      "source": 128,
      "target": 126
    },
    {
      "source": 128,
      "target": 91
    },
    {
      "source": 129,
      "target": 123
    },
    {
      "source": 130,
      "target": 123
    },
    {
      "source": 131,
      "target": 123
    },
    {
      "source": 132,
      "target": 123
    },
    {
      "source": 133,
      "target": 100
    },
    {
      "source": 133,
      "target": 134
    },
    {
      "source": 133,
      "target": 52
    },
    {
      "source": 133,
      "target": 355
    },
    {
      "source": 134,
      "target": 133
    },
    {
      "source": 134,
      "target": 180
    },
    {
      "source": 134,
      "target": 355
    },
    {
      "source": 135,
      "target": 133
    },
    {
      "source": 136,
      "target": 133
    },
    {
      "source": 137,
      "target": 133
    },
    {
      "source": 138,
      "target": 133
    },
    {
      "source": 139,
      "target": 133
    },
    {
      "source": 140,
      "target": 100
    },
    {
      "source": 140,
      "target": 150
    },
    {
      "source": 141,
      "target": 140
    },
    {
      "source": 142,
      "target": 140
    },
    {
      "source": 143,
      "target": 140
    },
    {
      "source": 144,
      "target": 140
    },
    {
      "source": 145,
      "target": 140
    },
    {
      "source": 146,
      "target": 140
    },
    {
      "source": 147,
      "target": 140
    },
    {
      "source": 148,
      "target": 140
    },
    {
      "source": 149,
      "target": 140
    },
    {
      "source": 149,
      "target": 126
    },
    {
      "source": 150,
      "target": 100
    },
    {
      "source": 151,
      "target": 150
    },
    {
      "source": 152,
      "target": 150
    },
    {
      "source": 153,
      "target": 150
    },
    {
      "source": 154,
      "target": 150
    },
    {
      "source": 155,
      "target": 150
    },
    {
      "source": 156,
      "target": 150
    },
    {
      "source": 157,
      "target": 150
    },
    {
      "source": 158,
      "target": 100
    },
    {
      "source": 159,
      "target": 158
    },
    {
      "source": 160,
      "target": 158
    },
    {
      "source": 161,
      "target": 158
    },
    {
      "source": 162,
      "target": 158
    },
    {
      "source": 163,
      "target": 158
    },
    {
      "source": 164,
      "target": 158
    },
    {
      "source": 165,
      "target": 158
    },
    {
      "source": 166,
      "target": 158
    },
    {
      "source": 167,
      "target": 158
    },
    {
      "source": 168,
      "target": 158
    },
    {
      "source": 169,
      "target": 100
    },
    {
      "source": 170,
      "target": 169
    },
    {
      "source": 171,
      "target": 169
    },
    {
      "source": 172,
      "target": 169
    },
    {
      "source": 173,
      "target": 169
    },
    {
      "source": 174,
      "target": 169
    },
    {
      "source": 175,
      "target": 169
    },
    {
      "source": 176,
      "target": 169
    },
    {
      "source": 177,
      "target": 169
    },
    {
      "source": 178,
      "target": 169
    },
    {
      "source": 179,
      "target": 169
    },
    {
      "source": 180,
      "target": 100
    },
    {
      "source": 181,
      "target": 180
    },
    {
      "source": 182,
      "target": 180
    },
    {
      "source": 183,
      "target": 180
    },
    {
      "source": 183,
      "target": 1
    },
    {
      "source": 183,
      "target": 134
    },
    {
      "source": 183,
      "target": 355
    },
    {
      "source": 184,
      "target": 180
    },
    {
      "source": 185,
      "target": 180
    },
    {
      "source": 185,
      "target": 52
    },
    {
      "source": 186,
      "target": 180
    },
    {
      "source": 186,
      "target": 361
    },
    {
      "source": 187,
      "target": 180
    },
    {
      "source": 187,
      "target": 52
    },
    {
      "source": 187,
      "target": 1
    },
    {
      "source": 188,
      "target": 180
    },
    {
      "source": 189,
      "target": 100
    },
    {
      "source": 190,
      "target": 189
    },
    {
      "source": 191,
      "target": 189
    },
    {
      "source": 192,
      "target": 189
    },
    {
      "source": 193,
      "target": 189
    },
    {
      "source": 194,
      "target": 189
    },
    {
      "source": 195,
      "target": 189
    },
    {
      "source": 196,
      "target": 189
    },
    {
      "source": 197,
      "target": 189
    },
    {
      "source": 198,
      "target": 189
    },
    {
      "source": 199,
      "target": 189
    },
    {
      "source": 200,
      "target": 1
    },
    {
      "source": 200,
      "target": 211
    },
    {
      "source": 200,
      "target": 183
    },
    {
      "source": 200,
      "target": 304
    },
    {
      "source": 200,
      "target": 355
    },
    {
      "source": 200,
      "target": 134
    },
    {
      "source": 200,
      "target": 187
    },
    {
      "source": 200,
      "target": 427
    },
    {
      "source": 201,
      "target": 200
    },
    {
      "source": 202,
      "target": 201
    },
    {
      "source": 203,
      "target": 201
    },
    {
      "source": 204,
      "target": 201
    },
    {
      "source": 205,
      "target": 201
    },
    {
      "source": 206,
      "target": 201
    },
    {
      "source": 207,
      "target": 201
    },
    {
      "source": 208,
      "target": 201
    },
    {
      "source": 209,
      "target": 201
    },
    {
      "source": 210,
      "target": 201
    },
    {
      "source": 211,
      "target": 200
    },
    {
      "source": 211,
      "target": 183
    },
    {
      "source": 212,
      "target": 211
    },
    {
      "source": 213,
      "target": 211
    },
    {
      "source": 213,
      "target": 183
    },
    {
      "source": 214,
      "target": 211
    },
    {
      "source": 215,
      "target": 211
    },
    {
      "source": 216,
      "target": 211
    },
    {
      "source": 217,
      "target": 211
    },
    {
      "source": 217,
      "target": 213
    },
    {
      "source": 218,
      "target": 211
    },
    {
      "source": 219,
      "target": 211
    },
    {
      "source": 219,
      "target": 213
    },
    {
      "source": 220,
      "target": 200
    },
    {
      "source": 220,
      "target": 183
    },
    {
      "source": 221,
      "target": 220
    },
    {
      "source": 221,
      "target": 200
    },
    {
      "source": 221,
      "target": 283
    },
    {
      "source": 222,
      "target": 220
    },
    {
      "source": 223,
      "target": 220
    },
    {
      "source": 224,
      "target": 220
    },
    {
      "source": 225,
      "target": 220
    },
    {
      "source": 226,
      "target": 220
    },
    {
      "source": 227,
      "target": 220
    },
    {
      "source": 228,
      "target": 220
    },
    {
      "source": 229,
      "target": 220
    },
    {
      "source": 230,
      "target": 220
    },
    {
      "source": 231,
      "target": 221
    },
    {
      "source": 232,
      "target": 221
    },
    {
      "source": 233,
      "target": 221
    },
    {
      "source": 233,
      "target": 283
    },
    {
      "source": 234,
      "target": 221
    },
    {
      "source": 234,
      "target": 283
    },
    {
      "source": 235,
      "target": 221
    },
    {
      "source": 236,
      "target": 221
    },
    {
      "source": 237,
      "target": 221
    },
    {
      "source": 238,
      "target": 221
    },
    {
      "source": 239,
      "target": 221
    },
    {
      "source": 240,
      "target": 221
    },
    {
      "source": 241,
      "target": 200
    },
    {
      "source": 241,
      "target": 312
    },
    {
      "source": 241,
      "target": 321
    },
    {
      "source": 242,
      "target": 241
    },
    {
      "source": 243,
      "target": 241
    },
    {
      "source": 244,
      "target": 241
    },
    {
      "source": 245,
      "target": 241
    },
    {
      "source": 246,
      "target": 241
    },
    {
      "source": 247,
      "target": 241
    },
    {
      "source": 248,
      "target": 241
    },
    {
      "source": 248,
      "target": 274
    },
    {
      "source": 249,
      "target": 241
    },
    {
      "source": 250,
      "target": 241
    },
    {
      "source": 251,
      "target": 241
    },
    {
      "source": 252,
      "target": 200
    },
    {
      "source": 253,
      "target": 252
    },
    {
      "source": 254,
      "target": 252
    },
    {
      "source": 255,
      "target": 252
    },
    {
      "source": 256,
      "target": 252
    },
    {
      "source": 257,
      "target": 252
    },
    {
      "source": 257,
      "target": 283
    },
    {
      "source": 258,
      "target": 252
    },
    {
      "source": 258,
      "target": 274
    },
    {
      "source": 258,
      "target": 283
    },
    {
      "source": 259,
      "target": 252
    },
    {
      "source": 260,
      "target": 252
    },
    {
      "source": 261,
      "target": 252
    },
    {
      "source": 261,
      "target": 288
    },
    {
      "source": 262,
      "target": 252
    },
    {
      "source": 262,
      "target": 288
    },
    {
      "source": 263,
      "target": 200
    },
    {
      "source": 263,
      "target": 297
    },
    {
      "source": 264,
      "target": 263
    },
    {
      "source": 265,
      "target": 263
    },
    {
      "source": 266,
      "target": 263
    },
    {
      "source": 267,
      "target": 263
    },
    {
      "source": 268,
      "target": 263
    },
    {
      "source": 269,
      "target": 263
    },
    {
      "source": 270,
      "target": 263
    },
    {
      "source": 271,
      "target": 263
    },
    {
      "source": 272,
      "target": 263
    },
    {
      "source": 273,
      "target": 263
    },
    {
      "source": 274,
      "target": 200
    },
    {
      "source": 275,
      "target": 274
    },
    {
      "source": 276,
      "target": 274
    },
    {
      "source": 277,
      "target": 274
    },
    {
      "source": 277,
      "target": 283
    },
    {
      "source": 278,
      "target": 274
    },
    {
      "source": 279,
      "target": 274
    },
    {
      "source": 280,
      "target": 274
    },
    {
      "source": 281,
      "target": 274
    },
    {
      "source": 282,
      "target": 274
    },
    {
      "source": 283,
      "target": 200
    },
    {
      "source": 284,
      "target": 283
    },
    {
      "source": 285,
      "target": 283
    },
    {
      "source": 286,
      "target": 283
    },
    {
      "source": 287,
      "target": 283
    },
    {
      "source": 288,
      "target": 200
    },
    {
      "source": 289,
      "target": 288
    },
    {
      "source": 290,
      "target": 288
    },
    {
      "source": 291,
      "target": 288
    },
    {
      "source": 292,
      "target": 288
    },
    {
      "source": 293,
      "target": 288
    },
    {
      "source": 294,
      "target": 288
    },
    {
      "source": 295,
      "target": 288
    },
    {
      "source": 296,
      "target": 288
    },
    {
      "source": 297,
      "target": 183
    },
    {
      "source": 298,
      "target": 297
    },
    {
      "source": 299,
      "target": 297
    },
    {
      "source": 300,
      "target": 297
    },
    {
      "source": 301,
      "target": 297
    },
    {
      "source": 302,
      "target": 297
    },
    {
      "source": 303,
      "target": 297
    },
    {
      "source": 304,
      "target": 183
    },
    {
      "source": 304,
      "target": 349
    },
    {
      "source": 305,
      "target": 304
    },
    {
      "source": 305,
      "target": 187
    },
    {
      "source": 305,
      "target": 92
    },
    {
      "source": 305,
      "target": 427
    },
    {
      "source": 306,
      "target": 304
    },
    {
      "source": 307,
      "target": 304
    },
    {
      "source": 308,
      "target": 304
    },
    {
      "source": 308,
      "target": 349
    },
    {
      "source": 309,
      "target": 304
    },
    {
      "source": 309,
      "target": 349
    },
    {
      "source": 310,
      "target": 304
    },
    {
      "source": 311,
      "target": 304
    },
    {
      "source": 311,
      "target": 427
    },
    {
      "source": 312,
      "target": 183
    },
    {
      "source": 313,
      "target": 312
    },
    {
      "source": 314,
      "target": 312
    },
    {
      "source": 315,
      "target": 312
    },
    {
      "source": 316,
      "target": 312
    },
    {
      "source": 317,
      "target": 312
    },
    {
      "source": 318,
      "target": 312
    },
    {
      "source": 319,
      "target": 312
    },
    {
      "source": 320,
      "target": 312
    },
    {
      "source": 321,
      "target": 183
    },
    {
      "source": 322,
      "target": 321
    },
    {
      "source": 323,
      "target": 321
    },
    {
      "source": 324,
      "target": 321
    },
    {
      "source": 325,
      "target": 321
    },
    {
      "source": 326,
      "target": 321
    },
    {
      "source": 327,
      "target": 321
    },
    {
      "source": 328,
      "target": 321
    },
    {
      "source": 329,
      "target": 321
    },
    {
      "source": 330,
      "target": 321
    },
    {
      "source": 331,
      "target": 213
    },
    {
      "source": 332,
      "target": 213
    },
    {
      "source": 333,
      "target": 213
    },
    {
      "source": 334,
      "target": 213
    },
    {
      "source": 335,
      "target": 213
    },
    {
      "source": 336,
      "target": 213
    },
    {
      "source": 337,
      "target": 213
    },
    {
      "source": 338,
      "target": 213
    },
    {
      "source": 339,
      "target": 70
    },
    {
      "source": 340,
      "target": 70
    },
    {
      "source": 341,
      "target": 70
    },
    {
      "source": 342,
      "target": 70
    },
    {
      "source": 343,
      "target": 70
    },
    {
      "source": 344,
      "target": 70
    },
    {
      "source": 345,
      "target": 70
    },
    {
      "source": 346,
      "target": 70
    },
    {
      "source": 347,
      "target": 70
    },
    {
      "source": 348,
      "target": 70
    },
    {
      "source": 349,
      "target": 183
    },
    {
      "source": 350,
      "target": 349
    },
    {
      "source": 351,
      "target": 349
    },
    {
      "source": 352,
      "target": 349
    },
    {
      "source": 353,
      "target": 349
    },
    {
      "source": 353,
      "target": 427
    },
    {
      "source": 354,
      "target": 349
    },
    {
      "source": 354,
      "target": 187
    },
    {
      "source": 355,
      "target": 1
    },
    {
      "source": 356,
      "target": 126
    },
    {
      "source": 357,
      "target": 126
    },
    {
      "source": 358,
      "target": 126
    },
    {
      "source": 359,
      "target": 126
    },
    {
      "source": 360,
      "target": 126
    },
    {
      "source": 360,
      "target": 187
    },
    {
      "source": 361,
      "target": 355
    },
    {
      "source": 362,
      "target": 361
    },
    {
      "source": 362,
      "target": 364
    },
    {
      "source": 363,
      "target": 361
    },
    {
      "source": 363,
      "target": 408
    },
    {
      "source": 364,
      "target": 355
    },
    {
      "source": 364,
      "target": 134
    },
    {
      "source": 365,
      "target": 364
    },
    {
      "source": 366,
      "target": 364
    },
    {
      "source": 367,
      "target": 364
    },
    {
      "source": 368,
      "target": 134
    },
    {
      "source": 368,
      "target": 52
    },
    {
      "source": 369,
      "target": 52
    },
    {
      "source": 370,
      "target": 52
    },
    {
      "source": 371,
      "target": 52
    },
    {
      "source": 372,
      "target": 187
    },
    {
      "source": 373,
      "target": 372
    },
    {
      "source": 374,
      "target": 372
    },
    {
      "source": 375,
      "target": 372
    },
    {
      "source": 376,
      "target": 372
    },
    {
      "source": 377,
      "target": 372
    },
    {
      "source": 378,
      "target": 372
    },
    {
      "source": 379,
      "target": 372
    },
    {
      "source": 380,
      "target": 372
    },
    {
      "source": 381,
      "target": 372
    },
    {
      "source": 382,
      "target": 372
    },
    {
      "source": 383,
      "target": 187
    },
    {
      "source": 384,
      "target": 383
    },
    {
      "source": 385,
      "target": 383
    },
    {
      "source": 385,
      "target": 305
    },
    {
      "source": 386,
      "target": 383
    },
    {
      "source": 387,
      "target": 383
    },
    {
      "source": 387,
      "target": 91
    },
    {
      "source": 388,
      "target": 383
    },
    {
      "source": 389,
      "target": 383
    },
    {
      "source": 390,
      "target": 383
    },
    {
      "source": 391,
      "target": 383
    },
    {
      "source": 392,
      "target": 383
    },
    {
      "source": 393,
      "target": 383
    },
    {
      "source": 394,
      "target": 91
    },
    {
      "source": 394,
      "target": 305
    },
    {
      "source": 395,
      "target": 91
    },
    {
      "source": 396,
      "target": 91
    },
    {
      "source": 397,
      "target": 91
    },
    {
      "source": 398,
      "target": 91
    },
    {
      "source": 399,
      "target": 91
    },
    {
      "source": 399,
      "target": 305
    },
    {
      "source": 400,
      "target": 91
    },
    {
      "source": 400,
      "target": 305
    },
    {
      "source": 401,
      "target": 91
    },
    {
      "source": 402,
      "target": 305
    },
    {
      "source": 403,
      "target": 305
    },
    {
      "source": 403,
      "target": 427
    },
    {
      "source": 404,
      "target": 305
    },
    {
      "source": 405,
      "target": 305
    },
    {
      "source": 406,
      "target": 305
    },
    {
      "source": 407,
      "target": 305
    },
    {
      "source": 408,
      "target": 187
    },
    {
      "source": 409,
      "target": 408
    },
    {
      "source": 410,
      "target": 408
    },
    {
      "source": 411,
      "target": 408
    },
    {
      "source": 412,
      "target": 408
    },
    {
      "source": 413,
      "target": 408
    },
    {
      "source": 414,
      "target": 408
    },
    {
      "source": 415,
      "target": 408
    },
    {
      "source": 416,
      "target": 408
    },
    {
      "source": 417,
      "target": 360
    },
    {
      "source": 418,
      "target": 360
    },
    {
      "source": 419,
      "target": 360
    },
    {
      "source": 420,
      "target": 360
    },
    {
      "source": 421,
      "target": 360
    },
    {
      "source": 422,
      "target": 360
    },
    {
      "source": 423,
      "target": 360
    },
    {
      "source": 424,
      "target": 360
    },
    {
      "source": 425,
      "target": 360
    },
    {
      "source": 426,
      "target": 360
    },
    {
      "source": 427,
      "target": 92
    },
    {
      "source": 427,
      "target": 187
    },
    {
      "source": 428,
      "target": 92
    },
    {
      "source": 429,
      "target": 92
    },
    {
      "source": 430,
      "target": 92
    },
    {
      "source": 431,
      "target": 92
    },
    {
      "source": 432,
      "target": 92
    },
    {
      "source": 433,
      "target": 92
    },
    {
      "source": 434,
      "target": 427
    },
    {
      "source": 435,
      "target": 427
    },
    {
      "source": 436,
      "target": 427
    },
    {
      "source": 437,
      "target": 354
    },
    {
      "source": 438,
      "target": 354
    },
    {
      "source": 439,
      "target": 354
    },
    {
      "source": 440,
      "target": 354
    },
    {
      "source": 441,
      "target": 354
    },
    {
      "source": 442,
      "target": 354
    },
    {
      "source": 443,
      "target": 354
    },
    {
      "source": 444,
      "target": 354
    },
    {
      "source": 445,
      "target": 354
    },
    {
      "source": 446,
      "target": 354
    },
    {
      "source": 447,
      "target": 1
    }
  ]
};
      var w = window.innerWidth;
      var h = window.innerHeight;

      var focusNode = null;
      var highlightNode = null;

      var textCenter = false;
      var outline = false;

      var minScore = Math.min(...graph.nodes.map(n => n.modularity));
      var maxScore = Math.max(...graph.nodes.map(n => n.modularity));

      var color = d3.scale
        .linear()
        .domain([
          minScore,
          (minScore + maxScore) / 4,
          (minScore + maxScore) / 2,
          ((minScore + maxScore) * 3) / 4,
          maxScore,
        ])
        .range(["lime", "yellow", "red", "deepskyblue"]);

      var highlightColor = "blue";
      var highlightTrans = 0.1;

      const citedBy = graph.nodes
        .map(n => n.cited_by)
        .filter(n => n != null)

      const maxCitedBy = Math.max(...citedBy)
      const minCitedBy = Math.min(...citedBy)

      var size = d3.scale
        .pow()
        .exponent(1)
        .domain([minCitedBy, maxCitedBy])
        .range([8, 24]);

      var force = d3.layout
        .force()
        .linkDistance(h / (graph.nodes.length / 10))
        .charge(-300)
        .size([w, h]);

      var defaultNodeColor = "#ccc";
      var defaultLinkColor = "#888";
      var nominalBaseNodeSize = 8;
      var nominalTextSize = 10;
      var maxTextSize = 24;
      var nominalStroke = 1.5;
      var maxStroke = 4.5;
      var maxBaseNodeSize = 36;
      var minZoom = 0.1;
      var maxZoom = 7;
      var svg = d3.select("body").append("svg");
      var zoom = d3.behavior.zoom().scaleExtent([minZoom, maxZoom]);
      var g = svg.append("g");
      svg.style("cursor", "move");

      var linkedByIndex = {};
      graph.links.forEach(function (d) {
        linkedByIndex[d.source + "," + d.target] = true;
      });

      function isConnected(a, b) {
        return (
          linkedByIndex[a.index + "," + b.index] ||
          linkedByIndex[b.index + "," + a.index] ||
          a.index == b.index
        );
      }

      force.size([w, h]);

      force
        .nodes(graph.nodes)
        .links(graph.links)
        .start();

      function getLine(data) {

        const x1 = data.source.x;
        const y1= data.source.y;
        const x2 = data.target.x;
        const y2 = data.target.y;

        const r = size(data.target.cited_by) + 1;

        const m = (y2 - y1) / (x2 - x1);
        const b = y1 - m * x1;

        const c = Math.sqrt(Math.pow((y2 - y1), 2) + Math.pow((x2 - x1), 2))
        const a = y2 - y1
        const cos = a / c

        const a2 = cos * r
        const b2 = Math.sqrt(Math.pow(r, 2) - Math.pow(a2, 2))

        const x = x2 > x1 ? x2 - b2 : x2 + b2;
        const y = y2 - a2;

        const path = 'M ' + data.source.x + ',' + data.source.y + ' L ' + x + ',' + y;
        return path;
      }

      var link = g
        .selectAll(".link")
        .data(graph.links)
        .enter()
        .append("svg:path")
        .attr("d", getLine) 
        .attr("stroke", defaultLinkColor)
        .attr("fill", "red")
        .style("stroke-width", nominalStroke)
        .style("marker-end", "url(#end)")

      var node = g
        .selectAll(".node")
        .data(graph.nodes)
        .enter()
        .append("g")
        .attr("class", "node")
        .call(force.drag);

      var timeout = null;

      node.on("dblclick", function (d) {
        clearTimeout(timeout);

        timeout = setTimeout(function () {
          window.open(d.url, "_blank");
          d3.event.stopPropagation();
        }, 300);
      });

      var tocolor = "fill";
      var towhite = "stroke";
      if (outline) {
        tocolor = "stroke";
        towhite = "fill";
      }

      var circle = node
        .append("path")
        .attr(
          "d",
          d3.svg
            .symbol()
            .size(function (d) {
              return (
                Math.PI * Math.pow(size(d.cited_by) || nominalBaseNodeSize, 2)
              );
            })
            .type(function (d) {
              return d.type;
            })
        )
        .style(tocolor, function (d) {
          if (isNumber(d.modularity) && d.modularity >= 0) return color(d.modularity);
          else return defaultNodeColor;
        })
        .style("stroke-width", nominalStroke)
        .style(towhite, "white");

      svg.append("svg:defs").selectAll("marker")
	  .data(["end"])
	.enter().append("svg:marker")
	  .attr("id", String)
	  .attr("viewBox", "0 -5 10 10")
	  .attr("refX", 10)
	  .attr("refY", 0)
	  .attr("markerWidth", 6)
	  .attr("markerHeight", 6)
	  .attr("orient", "auto")
          .style("fill", defaultLinkColor)
	.append("svg:path")
	  .attr("d", "M 0,-5 L 10,0 L 0,5")
          .style("stroke", defaultLinkColor);

      var text = g
        .selectAll(".text")
        .data(graph.nodes)
        .enter()
        .append("text")
        .attr("dy", ".35em")
        .style("font-size", nominalTextSize + "px");

      node
        .on("mouseover", function (d) {
          setHighlight(d);
        })
        .on("mousedown", function (d) {
          d3.event.stopPropagation();
          focusNode = d;
          setFocus(d);
          if (highlightNode === null) setHighlight(d);
        })
        .on("mouseout", function (d) {
          exitHighlight();
        });

      d3.select(window).on("mouseup", function () {
        if (focusNode !== null) {
          focusNode = null;
          if (highlightTrans < 1) {
            circle.style("opacity", 1);
            text.style("opacity", 1);
            link.style("opacity", 1);
          }
        }

        if (highlightNode === null) exitHighlight();
      });

      function exitHighlight() {
        highlightNode = null;
        if (focusNode === null) {
          svg.style("cursor", "move");
          if (highlightColor != "white") {
            circle.style(towhite, "white");
            text.text('')
            link.style("stroke", function (o) {
              return isNumber(o.score) && o.score >= 0
                ? color(o.score)
                : defaultLinkColor;
            });
          }
        }
      }

      function setFocus(d) {
        if (highlightTrans < 1) {
          circle.style("opacity", function (o) {
            return isConnected(d, o) ? 1 : highlightTrans;
          });

          text.style("opacity", function (o) {
            return isConnected(d, o) ? 1 : highlightTrans;
          });

          link.style("opacity", function (o) {
            return o.source.index == d.index || o.target.index == d.index
              ? 1
              : highlightTrans;
          });
        }
      }

      function setHighlight(d) {
        svg.style("cursor", "pointer");
        if (focusNode !== null) d = focusNode;
        highlightNode = d;

        if (highlightColor != "white") {

          circle.style(towhite, function (o) {
            return isConnected(d, o) ? highlightColor : "white";
          });
          
          text.attr("dx", function (d) {
            return size(d.cited_by)
          });

          text.text(function (o) {
            if (isConnected(d, o)) {
              let title = o.title;
              if (o.year) title = title + " (" + o.year + ")";
              if (o.authors) title = title + " - " + o.authors;
              return title
            } else {
              return ""
            }
          });

        }
      }

      zoom.on("zoom", function () {
        var stroke = nominalStroke;
        if (nominalStroke * zoom.scale() > maxStroke)
          stroke = maxStroke / zoom.scale();
        link.style("stroke-width", stroke);
        circle.style("stroke-width", stroke);

        var baseRadius = nominalBaseNodeSize;
        if (nominalBaseNodeSize * zoom.scale() > maxBaseNodeSize)
          baseRadius = maxBaseNodeSize / zoom.scale();
        circle.attr(
          "d",
          d3.svg
            .symbol()
            .size(function (d) {
              return (
                Math.PI *
                Math.pow(
                  (size(d.cited_by) * baseRadius) / nominalBaseNodeSize ||
                    baseRadius,
                  2
                )
              );
            })
        );

        if (!textCenter)
          text.attr("dx", function (d) {
            return (
              (size(d.cited_by) * baseRadius) / nominalBaseNodeSize ||
              baseRadius
            );
          });

        var textSize = nominalTextSize;
        if (nominalTextSize * zoom.scale() > maxTextSize)
          textSize = maxTextSize / zoom.scale();
        text.style("font-size", textSize + "px");

        g.attr(
          "transform",
          "translate(" + d3.event.translate + ")scale(" + d3.event.scale + ")"
        );
      });

      svg.call(zoom);

      resize();
      d3.select(window).on("resize", resize);

      force.on("tick", function () {
        node.attr("transform", function (d) {
          return "translate(" + d.x + "," + d.y + ")";
        });
        text.attr("transform", function (d) {
          return "translate(" + d.x + "," + d.y + ")";
        });

        link.attr("d", getLine)

        node
          .attr("cx", function (d) {
            return d.x;
          })
          .attr("cy", function (d) {
            return d.y;
          });
      });

      function resize() {
        var width = window.innerWidth,
          height = window.innerHeight;
        svg.attr("width", width).attr("height", height);

        force
          .size([
            force.size()[0] + (width - w) / zoom.scale(),
            force.size()[1] + (height - h) / zoom.scale(),
          ])
          .resume();
        w = width;
        h = height;
      }

      function isNumber(n) {
        return !isNaN(parseFloat(n)) && isFinite(n);
      }

    </script>
  </body>
</html>
